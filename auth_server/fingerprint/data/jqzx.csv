num,url,title,content
1,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740647&idx=1&sn=ed974c6f5283a4312b545c63ba67af19&chksm=871ad3d9b06d5acf0969ec08882736b7104d06e8101a91d7f77084e88d9bae86eb96623f80ec&scene=27,想轻松复现深度强化学习论文？看这篇经验之谈,"近期深度强化学习领域日新月异，其中最酷的一件事情莫过于 OpenAI 和 DeepMind 训练智能体接收人类的反馈而不是传统的奖励信号。本文作者认为复现论文是提升机器学习技能的最好方式之一，所以选择了 OpenAI 论文《Deep Reinforcement Learning from Human Preferences》作为 target，虽获得最后成功，却未实现初衷。如果你也打算复现强化学习论文，那么本文经验也许是你想要的。此外，本文虽对强化学习模型的训练提供了宝贵经验，同时也映射出另外一幅残酷画面：强化学习依然难免 hype 之嫌；强化学习的成功不在于其真正有效，而是人们故意为之。 首先，整体而言，强化学习问题要远比预期的更为棘手。 主要原因是强化学习本身非常敏感，需要纠正大量的细节，如果不这么做，后面诊断问题所在会非常难。 实例 1：基本实现完成之后，训练效果并未跟上。我对问题所在充满各种想法，但经过数月的苦思冥想，发现问题出现在关键阶段中的奖励归一化和像素数据。尽管想通了这点，却仍未搞明白整个问题：像素数据进入的奖励探测器网络的准确度刚刚好，我花了很长时间终于明白仔细检查已预测的奖励足以发现奖励归一化漏洞。一句话，搞明白发生了什么问题几乎是偶然性的， 。 实例 2：最后的代码清理完成之后，我多少有些错误地实现了 dropout。奖励探测器网络需要一对视频片段作为输入，由权重共享的两个网络同等处理。如果你添加 dropout，并在每个网络中不小心给了它相同的随机种子，每个网络将获得不同的 dropout，因此视频片段将不会被同等处理。正如结果表明完全修正它会破坏训练，尽管网络的预测准确度看起来一模一样。 在我印象中这种情况非常普遍（比如《Deep Reinforcement Learning Doesn't Work Yet》）。我的解读是你要像对待数学问题一样对待强化学习项目。它不同于编程问题，你可以在数天内完成它；它更像是你在解决一个谜题，没有规律可循，唯一的方法是不断尝试，直到灵感出现彻底搞明白。 这需要你不断尝试，并对实现过程中的困惑保有最敏锐的嗅觉。 该项目中有很多这样的点，其中唯一的线索就是那些看起来无关紧要的小事情。比如，某些时候采用不同帧之间的差异作为特征会更加奏效。通过一些新特征继续向前会非常诱人，但我很困惑当时在我工作的简单环境中会造成如此大的差异。这只有通过思考这些困惑并意识到采用不同帧之间的差异，才能给正则化问题提供线索。 我不完全确定如何使人在这方面做更多，但我目前最好的猜测是： 学习识别困惑的感觉。「事情不太对的感觉」有很多种，有时是代码很丑，有时是担心浪费时间在错误的事情上。但有时是「你看到了一些意料之外的事情」。能够精确知道令自己不舒服的事情很重要，因此你可以…… 培养思考困惑来源的习惯。一些不舒服的原因最好选择忽略（比如原型设计时的代码风格），但困惑并不是。一旦遇到困惑。立即调查其来源对你来说很重要。 无论如何：做好每次卡住数周的准备。（并相信坚持下来就会攻克难关，并留意那些小的细节。） 说到和过去的编程经验的区别，第二个主要学习经验是观念模式的区别，即需要长时间的工作迭代。 调试过程大致涉及 4 个基本步骤： 收集关于问题性质的证据； 基于已有证据对问题作出假设； 选择最可能成立的假设，实现一个解决办法，看看结果如何； 重复以上过程直到问题解决。 在我做过的大部分编程工作都习惯于快速反馈。如果有程序不工作了，你可以在数秒或数分钟内做出改变并查看有没有奏效。收集证据是很简单的工作。实际上，在快速反馈的情况下，收集证据可能比作出假设要简单得多。当你能凭直觉想到解决方案（并收集更多证据）时，为什么还要花费那么多时间考虑所有的可能性呢？换句话说，在快速反馈的情况下，你可以通过尝试而不是仔细考虑并迅速地缩小假设空间。 但当单次运行时间达到 10 小时的时候，尝试和反馈的策略很容易使你浪费很多的时间。 并行运行多个解决方案会有帮助，如果（a）你有计算机集群的云计算资源；（b）由于上述的强化学习中的各种困难，如果你迭代得太快，可能永远无法意识到你真正需要的证据。 从「多实验、少思考」到「少实验、多思考」的转变是提高效率的关键。当调试过程需要耗费很长的迭代时间时，你需要倾注大量的时间到建立假设上，即使需要花费很长的时间，比如 30 分钟甚至 1 小时。在单次实验中尽可能详实地检验你的假设，找到能最好地区分不同可能性的证据。 转向「少实验、多思考」的关键是保持细节丰富的工作日志。当每次实验的运行时间较少的时候，可以不用日志，但在实验时间超过一天的时候，很多东西都容易被忘记。我认为在日志中应该记录的有： 日志 1：你现在所需要的具体输出； 日志 2：把你的假设大胆地写出来； 日志 3：简单记录当前的进程，关于当前运行实验想要回答的问题； 日志 4：实验运行的结果（TensorBoard 图，任何其它重要观测），按运行实验的类型分类（例如按智能体训练的环境）。 我起初记录相对较稀疏的日志，但到了项目的结束阶段，我的态度转变成了「记录我头脑中出现过的所有东西」。这很费时，但也很值得。部分是因为某些调试过程需要交叉参照结果，这些结果可能是数天前或数周前做出的。部分是因为（至少我认为）思考质量的通常提升方式是从大量的更新到有效的心理 RAM。 为了从所做的实验中得到最大的效果，我在实验整个过程中做了两件事： 首先，持记录所有可以记录的指标的态度，以最大化每次运行时收集的证据量。有一些明显的指标如训练／验证准确率，但是在项目开始时花费一点时间头脑风暴，研究哪些指标对于诊断潜在问题比较重要是很有益的。 我这么推荐的部分原因是由于事后偏见：我发现哪些指标应该更早记录。很难提前预测哪些指标有用。可能有用的启发式方法如下： 对于系统中的每个重要组件，考虑什么可以被度量。如果是数据库，那么度量它的增长速度。如果是队列，则度量各个项的处理速度。 对于每个复杂步骤，度量其不同部分所花费时间。如果是训练循环，则度量运行每个批次要花费多长时间。如果是复杂的推断步骤，则度量每个子推断任务所花费的时间。这些时间对之后的性能 debug 大有裨益，有时候甚至可以检查出难以发现的 bug。（例如，如果你看到某个部分花费时间很长，那么它可能出现内存泄露。） 类似地，考虑搜集不同组件的内存使用情况。小的内存泄露可能揭示所有问题。 另一个策略是查看别人使用什么度量指标。在深度强化学习中，John Schulman 在其演讲《Nuts and Bolts of Deep RL Experimentation》中给出了一些好主意（视频地址：https://www.youtube.com/watch?v=8EcdaCk9KaQ；slides 地址：http://joschu.net/docs/nuts-and-bolts.pdf；摘要：https://github.com/williamFalcon/DeepRLHacks）。对于策略梯度方法，我发现策略熵是判断训练是否开始的优秀指标，比 per-episode 奖励更加敏锐。 不健康和健康的策略熵图示例。失败模式 1（左）：收敛至常数熵（随机选择动作子集）。失败模式 2（中）：收敛至零熵（每次选择相同的动作）。右：成功的 Pong 训练运行中的策略熵。 如果你在记录的指标中看到了一些可疑的现象，记得注意混淆，宁可假设它很重要也不要轻视，比如一些数据结构的低效实现。（我因为忽视了每秒帧数的微小而神秘的衰减，导致好几个月没找到一个多线程 bug。） 如果你能一次性看到所有指标，那么 debug 就容易多了。我喜欢在 TensorBoard 上有尽可能多的指标。用 TensorFlow 记录任意指标有点棘手，因此考虑使用 easy-tf-log（https://github.com/mrahtz/easy-tf-log），它提供简单的 tflog(key, value) 接口，无需任何额外设置。 另一件有助于从运行中获得更多信息的事情是，花时间尝试和提前预测失败。 多亏了事后偏见，在回顾实验过程时往往很容易发现失败原因。但是真正令人挫败的是在你观察之前，失败模式就已经很明显了。开始运行后，第二天回来一看失败了，在你开始调查失败原因之前，你就已经发现：「噢，一定是因为我忘记 frobulator 了」。 简单的事情是有时你可以提前触发「半事后观察」。它需要有意识的努力——在开始运行之前先停下来思考五分钟哪里可能出错。我认为最有用的是： 问问自己：「如果这次运行失败了，我会有多惊讶？」 如果答案是「不会很惊讶」，那么想象自己处于未来情境中：运行失败了，问自己：「哪些地方可能出问题：」 修复想到的问题。 重复以上过程直到问题 1 的答案是「非常惊讶」（或至少是「要多惊讶就多惊讶」）。 总是会有很多你无法预测的失败，有时你仍然遗漏了一些明显的事情，但是这个过程至少能够减少一些因为没有提前想到而出现的愚蠢失误。 最后，该项目最令人惊讶的是花费时间，以及所需的计算资源。 前者需要从日历时间的角度来看。我最初的估计是它作为业余项目，应该花费 3 个月时间，但它实际上用了 8 个月。（而我一开始预估的时间就已经很消极了！）部分原因是低估了每个阶段可能花费的时间，但是最大的低估是没有预测到该项目之外出现的其他事情。很难说这个规律有多广泛，但是对于业余项目来说，把预估时间乘以 2 可能是不错的方法。 更有趣的是每个阶段实际花费的时间。我原本的项目计划中主要阶段时间表基本如下： 写代码不费时，费时的是调试。事实上，在一个所谓的简单环境上花费的时间 4 倍于最初的实现。（这是我第一个花了数小时的业余项目，但所得经验与过去的机器学习项目相似。） 注：从一开始就仔细设计你认为什么应该是强化学习的「简单」环境。尤其是，仔细思考：（a）你的奖励是否真正传达解决任务的正确信息，是的，这很容易弄砸；（b）奖励是仅依赖之前的观测结果还是也依赖当前的动作。在你进行任意的奖励预测时，后者都可能是相关的。 第二个令人惊讶的事情是项目所需的计算时间。我很幸运可以使用学校的机房，虽然只有 CPU 机，但已经很好了。对于需要 GPU 的工作（如在一些小部分上进行快速迭代）或机房太繁忙的时候，我用两个云服务进行实验：谷歌云计算引擎的虚拟机、FloydHub。 谷歌云计算引擎挺好的，如果你只想用 shell 访问 GPU 机器，不过我更多地是在 FloydHub 上进行实验的。FloydHub 是针对机器学习的云计算服务。运行 floyd run python awesomecode.py，FloydHub 会设置一个容器，加载和运行你的代码。使 FloydHub 如此强大的两个关键因素是： GPU 驱动预安装的容器和常用库。 每次运行都可以自动存档。每次运行时使用的代码、开始运行的命令、任意命令行输出和任意数据输出都可以自动保存，并通过网页接口设置索引。 第二个功能非常重要。对于任何项目，对尝试过程的详细记录和复现之前实验的能力都是绝对必要的。版本控制软件有所帮助，但是 a）管理大量输出比较困难；b）需要极大的勤勉。（比如，如果你开始一些运行，然后做了一点更改，启动了另一次运行，当你提交第一批运行的结果时，是否能够清楚看到使用了哪些代码？）你可以仔细记录或展开自己的系统，但是使用 FloydHub 压根不需要花费这么多精力。 我喜欢 FloydHub 的其他原因是： 运行结束时容器自动关闭。无需检查容器是否关闭、虚拟机是否关闭。 账单比云虚拟机更加直接。 我认为 FloydHub 的一个痛点在于不能自定义容器。如果你的代码中有大量的依赖包，你需要在所有运行启动前安装它们。这限制了短期运行上的迭代次数。当然，你可以创建一个「dataset」，其中包含了对文件系统的安装依赖包的改变，然后在每次运行起始阶段复制该 dataset 的文件（例如，create_floyd_base.sh）。这很尴尬，但仍比不上处理 GPU 驱动的时候。 FloydHub 相比谷歌云虚拟机更贵一些：1.2 美元/小时用一台 K80 GPU 的机器，对比 0.85 美元/小时用一台配置相似的虚拟机。除非你的预算很有限，我认为 FloydHub 带来的额外便利是值得的。只有在并行运行大量计算的时候，谷歌云虚拟机才是更加划算的，因为你可以在单个大型虚拟机上堆栈。 总的来说，该项目花了： 计算引擎上 150 个小时的 GPU 运行时间和 7700 个小时的（wall time × cores）的 CPU 运行时间。 FloydHub 上 292 个小时的 GPU 运行时间。 大学计算机集群上的 1500 个小时（wall time, 4 to 16 cores）的 CPU 运行时间。 我惊讶地发现在实现项目的 8 个月期间，总共花费了 850 美元（FloydHub 花了 200 美元，谷歌云虚拟机花了 650 美元）。 但是即使花了这么多的精力，我在项目的最后阶段仍然遇到了很大的惊（ jing） 喜（xia）：强化学习可能不太稳定以至于我们需要使用不同的随机种子重复运行多次以确定性能。 例如当我感觉完成了基本工作，我就会直接在环境上执行端到端的测试。但是即使我一直使用最简单的环境，我仍然遇到了非常大的问题。因此我重新回到 FloydHub 进行调整并运行了三个副本，事实证明我认为优秀的超参数只在三次测试中成功了一次。 为了让你确切感受到需要做的计算的量级： 使用 A3C 和 16 个工作站，Pong 需要 10 个小时来训练； 这是 160 个 CPU 小时； 训练 3 个随机种子，则是 480 个 CPU 小时。 至于计算开销： 对于 8 核机器，FloydHub 大约每小时花费 0.5 美元； 因此 10 小时需要花费 5 美元； 同时运行 3 个随机种子，则每次运行需要花费 15 美元。 从《Deep Reinforcement Learning Doesn't Work Yet》这篇文章中，我们知道，那些不稳定性是正常的、可接受的。实际上，即使「五个随机种子（常用的报告指标）也可能不足以得到显著的结果，因为通过仔细的选择，你可以得到非重叠的置信区间。」 因此在 OpenAI Scholars programme 中提供 25000 美元的 AWS 信贷实际上并不疯狂，这可能正是确保你的计算可靠的大致成本。 我要表达的意思是，如果你想要完成一个深度强化学习项目，确保你知道你正趟进的是什么浑水，确保你已经准备好付出多少时间成本和多少经济成本。 总之，复现一篇强化学习论文很有趣。但在这之后，回头看看你有哪些技能真正得到了提升。同时，我也很好奇复现一篇论文是不是对过去数月时间的最佳利用。 一方面，我确实感觉到了机器学习工程能力的提升。我在识别常见的强化学习实现错误上更有自信了；我的工作流程在整体上变得更好了；从这篇特定的论文中，我学到了关于分布式 TensorFlow 和非共时设计的很多东西。 另一方面，我并不认为我的机器学习研究能力有很大提高（这才是我当初的真正目的）。 和实现不同，研究的更加困难的部分似乎总在有趣但易驾驭、具体的思想，以及你确实花费时间实现并认为得到最高回报的思想之后出现。挖掘有趣的思想似乎取决于（a）丰富的概念词汇；（b）对思想的良好品味。我认为阅读有影响力的论文、写总结，并对它们做严谨分析是兼顾两者的好办法。 因此，无论你想提高工程技能还是研究技能，深度考虑都是值得的。如果你在某方面比较欠缺，最好启动一个项目来针对性提高。 如果你想要提高两者，最好是先阅读论文，直到你找到真正感兴趣的东西，能用简洁的代码进行实现，并尝试对其进行扩展。 如果你希望处理一个强化学习项目，下面是一些更具体的注意内容。 选择需要复现的论文 分为几部分查找论文，并避免需要多个部分协同处理的论文。 强化学习 如果我们的强化学习是作为更大系统中的一个组件，请不要尝试自己实现强化学习算法。这是一个很大的挑战，并且我们也能学习到非常多的东西，但是强化学习目前仍然不够稳定，我们不能确定到底是大型系统存在问题还是作为系统一部分的强化学习存在问题。 在做任何事前，先要查看用基线模型在我们的环境上训练智能体有多么困难。 不要忘了归一化观察值，因为模型很多地方都要使用这些观察值。 一旦我们认为模型已经基本好了就直接完成一个端到端的测试，那么成功的训练要比我们预期的更加脆弱。 如果我们正在使用 OpenAI Gym 环境，注意在-vo 的环境中，当前动作有 25% 的时间会被忽略，并复制前面的动作以替代，这样会减少环境的确定性。如果我们不希望增加这种额外的随机性，那么就要使用-v4 环境。另外，默认环境只会从模拟器中每隔 4 帧抽取一次，以匹配早期的 DeepMind 论文。如果不希望这种采样，可以使用 NoFrameSkip 环境控制。结合上面的确定性与不跳过采样，我们可以使用 PongNoFrameskip-v4。 一般机器学习 由于端到端的测试需要很长时间才能完成，因此如果我们需要做一些重构会浪费大量时间。我们需要在第一次实现就检查错误并试运行，而不是在训练完后重新编写代码与结构。 初始化模型大概需要花 20s，且因为语法检测会浪费大量的时间。如果你不喜欢使用 IDE 或只能在服务器用 shell 访问与编辑，那么可以花点时间为编辑器配置 linter。或者每当我们尝试运行时遇到语法错误，可以花点时间令 linter 可以在在未来捕捉它。 不要仅仅使用 Dropout，我们还需要注意网络实现中的权重共享，批归一化同样也需要注意这一点。 在训练过程中看到内存占用有规律地上升？这可能是验证集过大。 如果使用 Adam 作为优化器发现一些奇怪的现象，那可能是因为 Adam 动量有问题。可以尝试使用 RMSprop 等不带动量的优化器，或设置 Adam 的超参数β1 为零。 TensorFlow 如果你想调试计算图中某个内部节点，可以使用 tf.Print，这个函数会打印该节点在每一次运行计算图时的输入。 如果你仅为推断过程保存检查点，则通过不保存优化器的参数而节省很多空间。 Session.run() 会出现很大的计算开销，如果可以的话将一个批量中的多个调用分组并运行计算图。 如果在相同机器上运行多个 TensorFlow 实例，那么就会得到 GPU 内存不足的报错。这可能是因为其中一个实例尝试保存所有的 GPU 内存，而不是因为模型过大的原因。这是 TF 的默认选项，而如果需要修改为只保存模型需要的内存，可以查看 allow_growth 选项。 如果你希望从一次运行的多个模块中访问计算图，那么应该可以从多个线程中访问相同的计算图，但目前锁定为只允许单线程一次读取。这看起来与 Python 全局解释器锁不同，TensorFlow 会假定在执行繁重任务前释放。 在使用 Python 过程中，我们不需要担心溢出问题，在 TensorFlow 中，我们还需要担心以下问题： 如果 GPU 不可用，注意使用 allow_soft_placement 返回到 CPU。如果你编码的东西无法在 GPU 运行，那么可以移动到 CPU 中： 我们并不知道有多少运算不能在 GPU 上并行化运行，但为了安全起见，我们可以手动回退 CPU： 心理状态 讲真，不要对 TensorBoard 上瘾。不可预测的奖励是对 TensorBoard 上瘾的完美示例：大部分时间你检测运行的如何，这没什么，但在训练过程，有时检测中忽然就中了大奖。所以有时非常刺激。如果你开始感觉每分钟都想要检查 TensorBoard，那你就需要设定合理的检查时间了。 以下是强化学习的一些入门资源： Andrej Karpathy 的《Deep Reinforcement Learning: Pong from Pixels》很好的介绍了强化学习的理论动机与直觉：http://karpathy.github.io/2016/05/31/rl/ 更多有关强化学习的理论，查看 David Silver 的文献：http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html。该资源对深度强化学习介绍不多，但却教授了理解论文时需要的词汇。 John Schulman 的《Nuts and Bolts of Deep RL Experimentation》课程中包含了很多实践时的注意点，链接：https://www.youtube.com/watch?v=8EcdaCk9KaQ。 想要了解深度强化学习的现状，可以查看以下文章： Alex Irpan 写的《Deep Reinforcement Learning Doesn't Work Yet》，机器之心对此文做了中文编译《变革尚未成功：深度强化学习研究的短期悲观与长期乐观》。 Vlad Mnih 的视频《Deep RL Bootcamp Frontiers Lecture I: Recent Advances, Frontiers and Future of Deep RL》，链接：https://www.youtube.com/watch?v=bsuvM1jO-4w Sergey Levine 的《Deep Robotic Learning》演讲，注重改建机器人泛化与采样的效率，链接：https://www.youtube.com/watch?v=eKaYnXQUb2g。 Pietel Abbeel 在 NIPS 2017 上的 Keynote，讲解了深度强化学习近期的技巧。链接：https://www.youtube.com/watch?v=TyOooJC_bLY。 IJCAI 2018 阿里妈妈国际广告算法大赛于 2018 年 2 月正式启动，获奖队伍将有机会前往斯德哥尔摩参加 IJCAI 2018。 "
2,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740583&idx=4&sn=71cc6673ef2d55befa0a200ad9333a6f&chksm=871ad319b06d5a0f0601bacc864c17483cfffad8e6ec52f28559ccd736cc0cbd49dcaf896ed9&scene=27,ICASSP 2018 | 阿里巴巴论文提出Advanced LSTM：关于更优时间依赖性刻画在情感识别方面的应用,"论文：《高级长短期记忆网络：关于更优时间依赖性刻画在情感识别方面的应用》（Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition）   论文地址：https://arxiv.org/pdf/1710.10197.pdf 摘要： 长短期记忆网络（LSTM）隐含了这样一个假设，本层的现时状态依赖于前一时刻的状态。这种「一步」的时间依赖性，可能会限制 LSTM 对于序列信号动态特性的建模。在这篇论文里，针对这样的一个问题，我们提出了高级长短期记忆网络（advanced LSTM (A-LSTM)），利用线性组合，将若干时间点的本层状态都结合起来，以打破传统 LSTM 的这种局限性。在这篇文章中，我们将 A-LSTM 应用于情感识别中。实验结果显示，与应用传统 LSTM 的系统相比，应用了 A-LSTM 的系统能相对提高 5.5% 的识别率。 研究背景 LSTM 现在被广泛的应用在 RNN 中。它促进了 RNN 在对序列信号建模的应用当中。LSTM 有两个输入，一个来源于前一层，还有一个来源于本层的前一个时刻。因此，LSTM 隐含了这样一个假设，本层的现时状态依赖于前一时刻的状态。这种「一步」的时间依赖性，可能会限制 LSTM 对于序列信号动态特性的建模（尤其对一些时间依赖性在时间轴上跨度比较大的任务）。在这篇论文里，针对这样的一个问题，我们提出了 advanced LSTM (A-LSTM)，以期打破传统 LSTM 的这种局限性。A-LSTM 利用线性组合，将若干时间点的本层状态都结合起来，因此不仅可以看到」一步「以前的状态，还可以看到更远以前的历史状态。 在这篇文章中，我们把 A-LSTM 应用到整句话层级（utterance level) 上的情感识别任务中。传统的情感识别依赖于在整句话上提取底端特征（low level descriptors) 的统计数据，比如平均值，方差等等。由于实际应用中，整句话中可能会有一些长静音，或者是一些非语音的声音，这种统计数据就可能不准确。在这篇论文中，我们使用基于注意力模型（attention model) 的加权池化 (weighted pooling) 递归神经网络 (recurrent neural network) 来更有效的提取整句话层级上的特征。 高级长短期记忆网络 A-LSTM 利用线性组合，将若干时间点的本层状态都结合起来。这其中的线性组合是利用与注意力模型 (attention model) 类似的机制进行计算的。具体公式如下：   Fig 1 中 C'(t) 即为前面若干时间状态的线性组合。这个线性组合以后的时间状态将被输入下一时间点进行更新。可以想象，每次的更新都不只是针对前一时刻，而是对若干时刻的组合进行更新。由于这种组合的权重是有注意力模型控制，A-LSTM 可以通过学习来自动调节各时间点之间的权重占比。如果依赖性在时间跨度上比较大，则更远以前的历史状态可能会占相对大的比重；反之，比较近的历史状态会占相对大的比重。   Fig 1 The unrolled A-LSTM 加权池化递归神经网络   在这篇论文中，我们使用基于注意力模型的加权池化递归神经网络来进行情感识别（见 Fig 2)。这一神经网络的输入是序列声学信号。利用注意力模型，我们的神经网络可以自动调整各个时间点上的权重，然后将各个时间点上的输出进行加权平均（加权池化）。加权平均的结果是一个能够表征这一整串序列的表达。由于注意力模型的存在，这一表达的提取可以包含有效信息，规避无用信息（比如输入序列中中的一些长时间的静音部分）。这就比简单的计算一整个序列的统计数值要更好（比如有 opensmile 提取的一些底端特征）。为了更好的训练模型，我们在情感识别任务之外还添加了两个辅助任务，说话人识别和性别识别。我们在这个模型当中使用了 A-LSTM 来提升系统性能。 实验 在实验阶段，我们使用 IEMOCAP 数据集中的四类数据（高兴，愤怒，悲伤和普通）。这其中一共有 4490 句语音文件。我们随机选取 1 位男性和 1 位女性说话人的数据作为测试数据。其余的数据用来训练（其中的 10% 的数据用来做验证数据）。我们采用三个衡量指标，分别为无权重平均 F-score（MAF），无权重平均精密度（MAP），以及准确率（accuracy)。 我们提取了 MECC, 信号过零率（zero crossing rate), 能量，能量熵，频谱矩心 (spectral centroid)，频谱流量 (spectral flux)，频谱滚边（spectral rolloff),12 维彩度向量（chroma vector), 色度偏差 (chroma deviation), 谐波比（harmonic ratior) 以及语音基频，一共 36 维特征。对这些序列特征进行整句话层级上的归一化后，将其送入系统进行训练或测试。 在这个实验中，我们的系统有两层神经元层，第一层位全连接层（fully connected layer)，共有 256 个精馏线性神经元组成（rectified linear unit)。第二层位双向长短期记忆网络（bidirectional LSTM (BLST))。两个方向一共有 256 个神经元。之后即为基于注意力模型的加权池化层。最上方为三个柔性最大值传输函数层，分别对应三个任务。我们给三个任务分配了不同的权重，其中情感识别权重为 1，说话人识别权重为 0.3，性别识别为 0.6。如果是应用 A-LSTM，我们就将第二层的 BLSTM 替换成双向的 A-LSTM，其他的所有参数都不变。这里的 A-LSTM 选取三个时间点的状态作线性组合，分别为 5 个时间点前（t-5)，3 个时间点前 (t-3)，以及 1 个时间点前 (t-1)。实验结果如下：   其中的 mean LSTM 与 A-LSTM 比较类似，唯一区别是，当我们为选取的几个时间点的状态作线性组合的时候，不是采用注意力模型，而是简单的做算术平均。 结论 与应用传统 LSTM 的系统相比，应用了 A-LSTM 的系统显示出了更好的识别率。由于加权池化过程是将所有时间点上的输出进行加权平均，因此系统性能的提升只可能是来源于 A-LSTM 更加灵活的时间依赖性模型，而非其他因素，例如高层看到更多时间点等等。并且，这一提升的代价只会增加了数百个参数。 "
3,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740647&idx=3&sn=e738543e59b2564a9eb09d227214c9a7&chksm=871ad3d9b06d5acfee887a18b0392c76d1578952d85e280bffbfa95f2af61913e4d6350dd05a&scene=27,深度 | 斯坦福大学李飞飞等人谈AI医疗：病床边的计算机视觉,新英格兰医学杂志 Serena Yeung、 N. Lance Downing、 Li Fei-Fei、 Arnold Milstein Panda 近日，世界上最权威的医学杂志之一《新英格兰医学杂志》上出现了一篇来自斯坦福大学的文章。其中，来自这所大学多个实验室的研究员分享了自己对 AI 医疗的观点，其中包括斯坦福大学人工智能实验室主管李飞飞。经授权，机器之心对此文章做了中文编译。 病床边的计算机视觉——从辅助驾驶到保护病人安全的人工智能   1999 年，美国医学研究所（Institute of Medicine）发布了一份关于医院中可预防的病人伤害的报告，该报告促使公众开始要求对医疗机构中出现的可预防疾病和医疗错误进行公开记录与经济处罚。随后，陆续出现了各种待办事项清单、协议、根本原因分析、用于建立关于安全的文化的项目和早期形式的技术辅助方法。然而迄今为止，医院仍未达成零病人伤害的目标，尤其是涉及到偏离预期的病床边实践做法时——其涵盖范围从可靠的手部清洁到中心线插入。 通过改进临床过程、文化和狭隘的技术辅助，我们有可能接近零病人伤害的极限。如果我们认为疲惫的临床医生会可靠地执行复杂的医院治疗过程中的每个行为步骤，那我们就忽视了来自认知科学的基本原理，即人类的行为通常是基于易出错的「快速思考」模式。[1] 即使通过视频馈送查看重症监护病床的远程位置的医院工作人员也不能立即发现和纠正病床边行为差错，比如没有重置床栏、约束带或充气式康复靴。 计算机视觉也许能成为临床辅助方法的一大来源，这是人工智能（AI）领域中一个快速发展的领域。AI 领域广义上是指智能机器的发展，其既关注功能（比如理解口语），也重视发展性的方法（比如机器学习）。计算机视觉让机器可以看到并理解视觉世界。机器学习需要根据数据中的模式构建知识，而不是由人类程序员指定知识。当用于识别人、物体及其运动等计算机视觉任务时，需要相机和成像传感器为学习提供数据。比如，当有数千张根据品种标注的数字化的狗照片时，计算机可以通过机器学习方法在「训练」阶段消化这些数据并设计出一个能准确区分不同犬种的算法。 计算机视觉不再只是科学幻想，其快速发展可部分归功于「深度学习」，这是一种使用多层神经网络的机器学习方法，其分层计算的设计灵感部分源自生物神经元的结构。计算机视觉发展速度的一个参照是谷歌用于支持自动驾驶汽车的计算机视觉系统。在近期一个 12 个月的时间段里，其表现就从每 700 英里需要一次人类干预提高到了一次能完成超过 5000 英里的全自动驾驶（见下图）。如果计算机视觉可以检测到驾驶员的危险变道并安全控制车辆转向，那么它能否类似地通过分析运动来检测同样重要的临床医生行为，或病人活动中的意外偏差？   用于自动驾驶汽车的计算机视觉的进展，2014-2015 年。 这里的数据是谷歌自动驾驶汽车在需要人类干预之前自动行驶的平均英里数。数据来自加利福尼亚州车辆管理局的《自动化车辆参与报告（Autonomous Vehicle Engagement Reports）》。 计算机视觉在临床应用上的价值将会得到证明，我们有理由对此保持乐观。计算机视觉注定将在临床分析的医疗图像筛查方面站稳脚跟。最近有一项研究发现：计算机视觉在分类良性和恶性皮肤病变上的表现可以媲美 21 位经过认证的皮肤科医生。[2] 还有一些小型研究也在放射影像和病理学影像的解读上得到了类似的早期进展。除了静态的医学图像之外，研究也正在向解读临床医生和病人行为的视频数据方面延展。斯特拉斯堡大学的研究者为一间手术室配置了传感器，并在计算机识别手术流程上得到了准确的结果。约翰·霍普金斯大学的研究者在重症监护病房中应用了计算机视觉来量化病人活动能力的进展。 现在，来自斯坦福大学工程与医学院、露西尔•帕卡德儿童医院（LPCH）和 Intermountain LDS Hospital 的研究者正合作在一家医院的全院范围内部署计算机视觉应用，来实时识别临床医生的行为。由于工作人员和病人隐私方面的顾虑，这里没有使用视频相机，而是使用了深度传感器和热传感器来收集用于机器解读的数据（NEJM.org 上提供了视频和本研究的全部文本）。深度传感器可以收集反射的红外信号，基于传感器与人和物体的表面特征的距离来创建轮廓图像。但是，这种图像缺乏彩色视频所具有的那种表面细节。热传感器可以检测人和物体表面的温度的微小差异，能够创建揭示运动中的人体外形的热图，还能检测到呼吸微弱和尿失禁发作等生理事件，而且在光照和黑暗环境中都有效。这些研究者正在研究能否在保护隐私的同时，在医院病房中通过多种图像感知方法的组合来准确识别临床上重要的病床边行为。 斯坦福和 Intermountain 的研究者选择了手部清洁合规性作为他们的第一个计算机视觉识别的目标，因为这对临床治疗而言非常重要，但在管理上能做的事情很有限。[3] 尽管在识别手部清洁行为上还有使用其它数据类型（比如射频识别系统产生的数据）的方法，但这些研究者假设由于深度传感器能获取更丰富的连续图像数据，所以他们能无需中断临床流程就能提供更准确和更精细的识别。为了评估基于计算机视觉的方法的有效性，这些研究者使用深度学习训练了一个检测手部清洁事件的神经网络。因为训练过程需要向神经网络提供它可以学习的有标注图像，所以研究人员标注了病人房间门口的手部清洁事件和非手部清洁事件的深度图像。所得到的机器算法仅使用深度数据就能在 LPCH 以 95.5% 的准确度持续不断地检测是否是手部清洁事件。当应用在来自 LDS Hospital 的图像上时，在 LPCH 开发的算法无需使用本地收集的图像进行任何额外训练就能达到 84.6% 的准确度，尽管这两家医院在深度传感器在墙上安装的位置、洗手液分配器的类型和门口的特征上都存在差异。 这种使用来自环境传感器的数据的计算机视觉在评估病床边行为上有相对于当前系统的结构性优势，比如每月派「神秘顾客」观察手部清洁合规性或让护士观察医生是否按照规范进行了中心线插入。环境计算机视觉可以不停止地工作且不会疲劳，而且运行的可变成本非常低，也不受不完美的安全文化的影响。因为基于计算机视觉的识别系统可以被训练用来识别不同的病床边活动，所以如果与电子病历整合到一起，可能还能免去临床医生的让人沮丧的文档和数据录入任务，从而让他们能专注以病人为中心的活动。 由于 AI 在临床方面的某些早期应用效果不佳，所以招致了一些怀疑。[4] 威胁技术成功应用的因素包括数据质量差、机器生成临床决定的复杂计算步骤导致的低可解释性、无法配合常规的临床流程等等因素。通过收集不受人类文档错误影响的数据，计算机视觉有可能缓解其中一个威胁因素。鉴于医疗与信息技术的混合的经历，AI 应用需要克服这些难题才能从「炒作高峰」快速迈向医疗价值的稳步实现。如果开发和部署成功，环境计算机视觉将有潜力以超越人类的水平实时识别各种病床边的临床医生和病人行为 [5] 并发送用户设计的提示。这样的系统可以在医生或护士进入病人房间前未执行手部清洁时提醒他们，在复杂手术的过程中提醒外科医生不要漏掉重要步骤，或通知护士一位烦躁不安的病人马上就要拔出气管导管的危险情况。使用计算机视觉来持续监控病床边行为可以让医生和护士免去更适合机器的低价值工作，增强临床医生的工作——而不会取代他们。 在广泛采用这样的技术之前，还有很多情况有待了解。我们可以将其恰当地比作自动驾驶汽车：它们不会立即就主宰道路，然而可以非常确信它们在中期是可行的。尽管安全的医院护理存在独特的难题，但如果其它行业所见证的生产力提升能说明问题，那么计算机视觉可能会给临床治疗的质量和效率带来显著的贡献，同时能让临床医生将精力集中在精细的决策、与病人互动和提供共情式护理（empathic care）上。鉴于计算机视觉在其它行业内准确度和成本问题的快速改善，这种技术可能很快就将让我们离解决这个看似无法解决的问题更近一步，即：预期临床医生行为的日益增长的复杂性和人类的易错性之间不匹配的问题。 原文链接：http://www.nejm.org/doi/full/10.1056/NEJMp1716891?rss=searchAndBrowse& 
4,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740583&idx=2&sn=9687798dbfe1375a4342125cdee28224&chksm=871ad319b06d5a0fa58fa24859ab0ab8ba5b8d300c615119c4bab6a8d4ddc6a73d379da7a1c3&scene=27,资源 | 从图像处理到语音识别，25款数据科学家必知的深度学习开放数据集,"本文介绍了 25 个深度学习开放数据集，包括图像处理、自然语言处理、语音识别和实际问题数据集。 介绍 深度学习（或生活中大部分领域）的关键在于实践。你需要练习解决各种问题，包括图像处理、语音识别等。每个问题都有其独特的细微差别和解决方法。 但是，从哪里获得数据呢？现在许多论文都使用专有数据集，这些数据集通常并不对公众开放。如果你想学习并应用技能，那么无法获取合适数据集是个问题。 如果你面临着这个问题，本文可以为你提供解决方案。本文介绍了一系列公开可用的高质量数据集，每个深度学习爱好者都应该试试这些数据集从而提升自己的能力。在这些数据集上进行工作将让你成为一名更好的数据科学家，你在其中学到的知识将成为你职业生涯中的无价之宝。我们同样介绍了具备当前最优结果的论文，供读者阅读，改善自己的模型。 如何使用这些数据集？ 首先，你得明白这些数据集的规模非常大！因此，请确保你的网络连接顺畅，在下载时数据量没有或几乎没有限制。 使用这些数据集的方法多种多样，你可以应用各种深度学习技术。你可以用它们磨炼技能、了解如何识别和构建各个问题、思考独特的使用案例，也可以将你的发现公开给大家！ 数据集分为三类——图像处理、自然语言处理和音频/语音处理。 让我们一起看看吧！ MNIST 链接：https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/ MNIST 是最流行的深度学习数据集之一。这是一个手写数字数据集，包含一个有着 60000 样本的训练集和一个有着 10000 样本的测试集。对于在现实世界数据上尝试学习技术和深度识别模式而言，这是一个非常好的数据库，且无需花费过多时间和精力进行数据预处理。 大小：约 50 MB 数量：70000 张图像，共分为 10 个类别。 SOTA：《Dynamic Routing Between Capsules》 参考阅读： 终于，Geoffrey Hinton 那篇备受关注的 Capsule 论文公开了 浅析 Geoffrey Hinton 最近提出的 Capsule 计划 先读懂 CapsNet 架构然后用 TensorFlow 实现，这应该是最详细的教程了 Capsule 官方代码开源之后，机器之心做了份核心代码解读 MS-COCO 链接：http://cocodataset.org/#home COCO 是一个大型数据集，用于目标检测、分割和标题生成。它有以下几个特征： 目标分割 在语境中识别 超像素物品分割 33 万张图像（其中超过 20 万张是标注图像） 150 万个目标实例 80 个目标类别 91 个物品分类 每张图像有 5 个标题 25 万张带有关键点的人像 大小：约 25 GB（压缩后） 数量：33 万张图像、80 个目标类别、每张图像 5 个标题、25 万张带有关键点的人像 SOTA：《Mask R-CNN》 参考阅读： 学界 | Facebook 新论文提出通用目标分割框架 Mask R-CNN：更简单更灵活表现更好 深度 | 用于图像分割的卷积神经网络：从 R-CNN 到 Mask R-CNN 资源 | Mask R-CNN 神应用：像英剧《黑镜》一样屏蔽人像 ImageNet 链接：http://www.image-net.org/ ImageNet 是根据 WordNet 层次来组织的图像数据集。WordNet 包含大约 10 万个短语，而 ImageNet 为每个短语提供平均约 1000 张描述图像。 大小：约 150 GB 数量：图像的总数约为 1,500,000；每一张图像都具备多个边界框和各自的类别标签。 SOTA：《Aggregated Residual Transformations for Deep Neural Networks》（https://arxiv.org/pdf/1611.05431.pdf） Open Images 数据集 链接：https://github.com/openimages/dataset Open Images 是一个包含近 900 万个图像 URL 的数据集。这些图像使用包含数千个类别的图像级标签边界框进行了标注。该数据集的训练集包含 9,011,219 张图像，验证集包含 41,260 张图像，测试集包含 125,436 张图像。 大小：500GB（压缩后） 数量：9,011,219 张图像，带有超过 5000 个标签 SOTA：Resnet 101 image classification model (trained on V2 data)： 模型检查点：https://storage.googleapis.com/openimages/2017_07/oidv2-resnet_v1_101.ckpt.tar.gz Checkpoint readme：https://storage.googleapis.com/openimages/2017_07/oidv2-resnet_v1_101.readme.txt 推断代码：https://github.com/openimages/dataset/blob/master/tools/classify_oidv2.py VisualQA 链接：http://www.visualqa.org/ VQA 是一个包含图像开放式问题的数据集。这些问题的解答需要视觉和语言的理解。该数据集拥有下列有趣的特征： 265,016 张图像（COCO 和抽象场景） 每张图像至少包含 3 个问题（平均有 5.4 个问题） 每个问题有 10 个正确答案 每个问题有 3 个看似合理（却不太正确）的答案 自动评估指标 大小：25GB（压缩后） 数量：265,016 张图像，每张图像至少 3 个问题，每个问题 10 个正确答案 SOTA：《Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge》（https://arxiv.org/abs/1708.02711） 街景门牌号数据集（SVHN） 链接：http://ufldl.stanford.edu/housenumbers/ 这是一个现实世界数据集，用于开发目标检测算法。它需要最少的数据预处理过程。它与 MNIST 数据集有些类似，但是有着更多的标注数据（超过 600,000 张图像）。这些数据是从谷歌街景中的房屋门牌号中收集而来的。 大小：2.5GB 数量：6,30,420 张图像，共 10 类 SOTA：《Distributional Smoothing With Virtual Adversarial Training》（https://arxiv.org/pdf/1507.00677.pdf） 这篇论文中，日本京都大学提出了局部分布式平滑度（LDS），一个关于统计模型平滑度的新理念。它可被用作正则化从而提升模型分布的平滑度。该方法不仅在 MNIST 数据集上解决有监督和半监督学习任务时表现优异，而且在 SVHN 和 NORB 数据上，Test Error 分别取得了 24.63 和 9.88 的分值。以上证明了该方法在半监督学习任务上的表现明显优于当前最佳结果。 CIFAR-10 链接：http://www.cs.toronto.edu/~kriz/cifar.html 该数据集也用于图像分类。它由 10 个类别共计 60,000 张图像组成（每个类在上图中表示为一行）。该数据集共有 50,000 张训练集图像和 10,000 个测试集图像。数据集分为 6 个部分——5 个训练批和 1 个测试批。每批含有 10,000 张图像。 大小：170MB 数量：60,000 张图像，共 10 类 SOTA：《ShakeDrop regularization》（https://openreview.net/pdf?id=S1NHaMW0b） Fashion-MNIST 链接：https://github.com/zalandoresearch/fashion-mnist Fashion-MNIST 包含 60,000 个训练集图像和 10,000 个测试集图像。它是一个类似 MNIST 的时尚产品数据库。开发人员认为 MNIST 的使用次数太多了，因此他们把这个数据集用作 MNIST 的直接替代品。每张图像都以灰度显示，并具备一个标签（10 个类别之一）。 大小：30MB 数量：70,000 张图像，共 10 类 SOTA：《Random Erasing Data Augmentation》（https://arxiv.org/abs/1708.04896） IMDB 电影评论数据集 链接：http://ai.stanford.edu/~amaas/data/sentiment/ 该数据集对于电影爱好者而言非常赞。它用于二元情感分类，目前所含数据超过该领域其他数据集。除了训练集评论样本和测试集评论样本之外，还有一些未标注数据可供使用。此外，该数据集还包括原始文本和预处理词袋格式。 大小：80 MB 数量：训练集和测试集各包含 25,000 个高度两极化的电影评论 SOTA：《Learning Structured Text Representations》（https://arxiv.org/abs/1705.09207） Twenty Newsgroups 数据集 链接：https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups 顾名思义，该数据集涵盖新闻组相关信息，包含从 20 个不同新闻组获取的 20000 篇新闻组文档汇编（每个新闻组选取 1000 篇）。这些文章有着典型的特征，例如标题、导语。 大小：20MB 数量：来自 20 个新闻组的 20,000 篇报道 SOTA：《Very Deep Convolutional Networks for Text Classification》（https://arxiv.org/abs/1606.01781） Sentiment140 链接：http://help.sentiment140.com/for-students/ Sentiment140 是一个用于情感分析的数据集。这个流行的数据集能让你完美地开启自然语言处理之旅。数据中的情绪已经被预先清空。最终的数据集具备以下六个特征： 推文的情绪极性 推文的 ID 推文的日期 查询 推特的用户名 推文的文本 大小：80MB（压缩后） 数量： 1,60,000 篇推文 SOTA：《Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets》（http://www.aclweb.org/anthology/W17-5202） WordNet 链接：https://wordnet.princeton.edu/ 上文介绍 ImageNet 数据集时提到，WordNet 是一个大型英语 synset 数据库。Synset 也就是同义词组，每组描述的概念不同。WordNet 的结构让它成为 NLP 中非常有用的工具。 大小：10 MB 数量：117,000 个同义词集，它们通过少量的「概念关系」与其他同义词集相互关联 SOTA：《Wordnets: State of the Art and Perspectives》（https://aclanthology.info/pdf/R/R11/R11-1097.pdf） Yelp 数据集 链接：https://www.yelp.com/dataset 这是 Yelp 出于学习目的而发布的开放数据集。它包含数百万个用户评论、商业属性（businesses attribute）和来自多个大都市地区的超过 20 万张照片。该数据集是全球范围内非常常用的 NLP 挑战赛数据集。 大小：2.66 GB JSON、2.9 GB SQL 和 7.5 GB 的照片（全部压缩后） 数量：5,200,000 个评论、174,000 份商业属性、200,000 张照片和 11 个大都市地区 SOTA：《Attentive Convolution》（https://arxiv.org/pdf/1710.00519.pdf） Wikipedia Corpus 链接：http://nlp.cs.nyu.edu/wikipedia-data/ 该数据集是维基百科全文的集合，包含来自超过 400 万篇文章的将近 19 亿单词。你能逐单词、逐短语、逐段地对其进行检索，这使它成为强大的 NLP 数据集。 大小：20 MB 数量：4,400,000 篇文章，包含 19 亿单词 SOTA：《Breaking The Softmax Bottelneck: A High-Rank RNN language Model》（https://arxiv.org/pdf/1711.03953.pdf） Blog Authorship Corpus 链接：http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm 该数据集包含从数千名博主那里收集到的博客文章，这些数据从 blogger.com 中收集而来。每篇博客都以一个单独的文件形式提供。每篇博客至少出现 200 个常用的英语单词。 大小：300 MB 数量：681,288 篇博文，共计超过 1.4 亿单词。 SOTA：《Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution》（https://arxiv.org/pdf/1609.06686.pdf） 欧洲语言机器翻译数据集 链接：http://statmt.org/wmt18/index.html 该数据集包含四种欧洲语言的训练数据，旨在改进当前的翻译方法。你可以使用以下任意语言对： 法语 - 英语 西班牙语 - 英语 德语 - 英语 捷克语 - 英语 大小： 约 15 GB 数量：约 30,000,000 个句子及对应的译文 SOTA：《Attention Is All You Need》 参考阅读： 学界 | 机器翻译新突破：谷歌实现完全基于 attention 的翻译架构 资源 | 谷歌全 attention 机器翻译模型 Transformer 的 TensorFlow 实现 Free Spoken Digit 数据集 链接：https://github.com/Jakobovski/free-spoken-digit-dataset 这是本文又一个受 MNIST 数据集启发而创建的数据集！该数据集旨在解决识别音频样本中口述数字的任务。这是一个公开数据集，所以希望随着人们继续提供数据，它会不断发展。目前，它具备以下特点： 3 种人声 1500 段录音（每个人口述 0- 9 各 50 次） 英语发音 大小： 10 MB 数量： 1500 个音频样本 SOTA：《Raw Waveform-based Audio Classification Using Sample-level CNN Architectures》（https://arxiv.org/pdf/1712.00866） Free Music Archive (FMA) 链接：https://github.com/mdeff/fma FMA 是音乐分析数据集，由整首 HQ 音频、预计算的特征，以及音轨和用户级元数据组成。它是一个公开数据集，用于评估 MIR 中的多项任务。以下是该数据集包含的 csv 文件及其内容： tracks.csv：记录每首歌每个音轨的元数据，例如 ID、歌名、演唱者、流派、标签和播放次数，共计 106,574 首歌。 genres.csv：记录所有 163 种流派的 ID 与名称及上层风格名（用于推断流派层次和上层流派）。 features.csv：记录用 librosa 提取的常见特征。 echonest.csv：由 Echonest（现在的 Spotify）为 13,129 首音轨的子集提供的音频功能。 大小：约 1000 GB 数量：约 100,000 个音轨 SOTA：《Learning to Recognize Musical Genre from Audio》（https://arxiv.org/pdf/1803.05337.pdf） Ballroom 链接：http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html 该数据集包含舞厅的舞曲音频文件。它以真实音频格式提供了许多舞蹈风格的一些特征片段。以下是该数据集的一些特点： 实例总数：698 单段时长：约 30 秒 总时长：约 20940 秒 大小：14 GB（压缩后） 数量：约 700 个音频样本 SOTA：《A Multi-Model Approach To Beat Tracking Considering Heterogeneous Music Styles》（https://pdfs.semanticscholar.org/0cc2/952bf70c84e0199fcf8e58a8680a7903521e.pdf） Million Song 数据集 链接：https://labrosa.ee.columbia.edu/millionsong/ Million Song 数据集包含一百万首当代流行音乐的音频特征和元数据，可免费获取。其目的是： 鼓励研究商业规模的算法 为评估研究提供参考数据集 作为使用 API 创建大型数据集的捷径（例如 The Echo Nest API） 帮助入门级研究人员在 MIR 领域展开工作 数据集的核心是一百万首歌曲的特征分析和元数据。该数据集不包含任何音频，只包含导出要素。示例音频可通过哥伦比亚大学提供的代码（https://github.com/tb2332/MSongsDB/tree/master/Tasks_Demos/Preview7digital）从 7digital 等服务中获取。 大小：280 GB 数量：一百万首歌曲！ SOTA：《Preliminary Study on a Recommender System for the Million Songs Dataset Challenge》（http://www.ke.tu-darmstadt.de/events/PL-12/papers/08-aiolli.pdf） LibriSpeech 链接：http://www.openslr.org/12/ 该数据集是一个包含约 1000 小时英语语音的大型语料库。数据来源为 LibriVox 项目的音频书籍。该数据集已经得到了合理地分割和对齐。如果你还在寻找起始点，那么点击 http://www.kaldi-asr.org/downloads/build/6/trunk/egs/查看在该数据集上训练好的声学模型，点击 http://www.openslr.org/11/查看适合评估的语言模型。 大小：约 60 GB 数量：1000 小时的语音 SOTA：《Letter-Based Speech Recognition with Gated ConvNets》（https://arxiv.org/abs/1712.09444） VoxCeleb  链接：http://www.robots.ox.ac.uk/~vgg/data/voxceleb/ VoxCeleb 是一个大型人声识别数据集。它包含来自 YouTube 视频的 1251 位名人的约 10 万段语音。数据基本上是性别平衡的（男性占 55％）。这些名人有不同的口音、职业和年龄。开发集和测试集之间没有重叠。对大明星所说的话进行分类并识别——这是一项有趣的工作。 大小：150 MB 数量：1251 位名人的 100,000 条语音 SOTA：《VoxCeleb: a large-scale speaker identification dataset》（https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf） 为了帮助你练习，我们还提供了一些真实生活问题和数据集，供读者上手操作。这一部分，我们列举了 DataHack 平台上关于深度学习的问题。 推特情感分析数据集 链接：https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/ 涉及种族主义和性别歧视的偏激言论已成为 Twitter 的难题，因此将这类推文与其它推文分开已十分重要。在这个实际问题中，我们提供的 Twitter 数据包含普通言论和偏激言论。作为数据科学家，你的任务是确定哪些推文是偏激型推文，哪些不是。 大小： 3 MB 数量： 31,962 篇推文 印度演员年龄检测数据集 链接：https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/ 对于深度学习爱好者来说，这是一个令人着迷的挑战。该数据集包含数千名印度演员的图像，你的任务是确定他们的年龄。所有图像都由人工从视频帧中挑选和剪切而来，这导致规模、姿势、表情、亮度、年龄、分辨率、遮挡和妆容具有高度可变性。 大小：48 MB（压缩后） 数量：训练集中有 19,906 幅图像，测试集中有 6636 幅图像 城市声音分类数据集 链接：https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/ 该数据集包含超过 8000 个来自 10 个类别的城市声音片段。这个实际问题旨在向你介绍常见分类场景中的音频处理。 大小：训练集 - 3 GB（压缩后）、测试集 - 2 GB（压缩后） 数量：来自 10 个类别的 8732 个标注城市声音片段（单个片段音频时长 <= 4s） 原文链接：https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/ "
5,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740647&idx=4&sn=14ee22780ec9014b70f775ade1f8ebb7&chksm=871ad3d9b06d5acfd8d11e38271b3147a633c4985ee967b01ca37965e7d4bc66c88fad041b48&scene=27,ICASSP 2018 | 阿里巴巴论文提出针对影视作品的语音情感识别信息融合框架,语音领域的顶会 ICASSP 2018 将于 4 月 15-20 日在加拿大阿尔伯塔卡尔加里市举行。据机器之心了解，国内科技巨头阿里巴巴语音交互智能团队有 5 篇论文被此大会接收。本文对论文《 An Ensemble Framework of Voice-Based Emotion Recognition System for Films and TV Programs 》进行了介绍。 欢迎大家向机器之心推荐优秀的 ICASSP 2018 相关论文。 论文：《一种针对影视作品的语音情感识别信息融合框架》（An Ensemble Framework of Voice-Based Emotion Recognition System for Films and TV Programs） 论文链接：https://arxiv.org/abs/1803.01122.pdf 摘要： 情感识别（即识别开心、忧伤等）现在愈来愈受到人们的关注，因为它可以提升人机交互界面的用户体验，进而提升产品的用户粘性，并在心理医疗健康方面等具有独特价值。基于语音的情感识别尤其具有现实意义，因为基于语音的人机交互界面具有相对较低的硬件要求。但是，在现实中，周围环境中存在着许多噪声，这些噪声将会降低系统的识别性能。在本文中我们提出了一套包含多个子系统的复合情感识别框架。这一框架会深入挖掘输入语音中与情感相关的各个方面的信息，从而提高系统的顽健性。 研究背景 在现实生活中，基于语音的人工智能系统处在复杂的场景当中，因而会面临各种各样的挑战。对于情感识别来说，主要的挑战来自于两个方面：1. 周围存在背景噪声，因而传统的特征提取，比如在整句话层面上提取统计参数的方法将受到严重干扰； 2. 用户说话的方式比较随意，不能如实验室中那样很好地控制输入语音，有时候用户会有一些发出一些非语音的声音，比如哭声，笑声，咳嗽声等，这些声音有些与情感有关，有些则完全无关。面对这两个挑战，我们提出了一套复合情感识别框架。这套框架会对底层和高层特征进行识别，因此可以对一些背景噪声有一定的顽健性；同时这套框架也会利用注意力模型（attention model）学习特征序列中重要时间点的特征，以及利用语音中的文本信息对情感信息进行分类——这些机制可以有效避免用户的非语音声音或者长静音对识别的干扰。 复合情感识别框架 在本文中，我们提出了一套复合的情感识别框架。这一框架由若干子系统组合而成，其中包括基于整句话（utterance level) 底层特征 (low level descriptor) 的识别系统，基于整句话高层表述的识别系统，基于序列特征的识别系统，以及基于语义信息的识别系统（见 Fig 1)。   其中，基于整句话底层特征的识别系统为一个深度神经网络，采用多任务训练 (multitask learning) 方式进行训练 (见 Fig 2)，采用的特征为从 opensmile 提取的 Interspeech 2010 LLD 特征集。在这个神经网络中，我们在 trunk 部分有两层隐层（hidden layer)(每层 4096 个神经元)，在 branch 部分，每个任务有一层隐层（1024 神经元），之后有一层 柔性最大激活函数（softmax)。其中我们的神经元均使用精馏线性单元（rectified linear unit）。 基于整句话高层表述的识别系统也是采用一个深度神经网络，同样也是采用多任务训练方式进行训练。采用的特征为 200 维 iVector(从一个由 4000 小时语音训练的语音识别 (ASR) 系统中提取）。这里我们采用的网络结构与底层特征识别系统的神经网络相同，唯一的区别为，这个一个系统在 trunk 部分每一层只有 1024 个神经元。 基于序列特征的子系统采用递归神经网络，对输入序列进行建模，在递归神经网络上采用基于 attention model 的加权池化层 (weighted pooling)(见 Fig 3)，将输入的一个序列提取成一个高层表述。基于这个高层表述进行分类。这一子系统也采用多任务训练方式进行训练。这一递归网络与上述神经网络的大致结构相似，区别为在 trunk 部分，我们使用了 RNN，并且在 RNN 上利用 attention based weighted pooling layer 来提取高端表述（high level representation）。   上述三个子系统中的多任务训练，我们采用三个任务，情感识别为主任务（权重为 1），说话人识别（权重为 0.3）和性别识别（权重为 0.6）为辅助任务。在多任务训练中，由于系统可以看到更多的任务信息，可以更好地检视输入的特征，因此可以更好地训练神经网络。 除了上述三个子系统外，还有一个子系统是基于文本的子系统。该子系统采用支持向量机（support vector machine)，使用了从语音识别系统中获取的文本。这一系列子系统的识别结果会通过线性相加组合起来，从而得到最后的结果。 实验 我们在多模情感识别竞赛 2017 数据集（MEC 2017) 上测试这一套框架。MEC 2017 数据集是采集自影视作品，其中包含了许多背景噪声（汽车噪声，工厂噪声等等），以及说话人的非语音声音（哭声、笑声等等）。其中各类情感的分布如下。   根据 MEC 2017 的建议，我们采用无权重平均 F-score（MAF）和准确率作为我们的衡量标准。考虑到数据库中的数据不平衡性，我们主要关注 MAF 指标。 实验中，我们采用两套系统作为参照系统，一套是 MEC2017 建议的 random forest 系统，还有一套是利用 Interspeech 2017 特征集搭建 DNN 的情感识别系统。具体实验结果如下：   由实验结果可以看到，我们提出的这一套框架，可以远远超过参照系统（分别增加了 11.9% 和 7.8% 准确率)。即使四个子系统的识别率参差不齐，最后组合之后的结果依然超过了所有的子系统，可以推测这个过程中全面检视输入信息，可以很有效的提高识别准确率和系统顽健性。 结论 我们将这一套系统应用于中文的影视作品数据库上。之所以应用到这一数据库上，是因为影视作品中的场景比较接近现实生活。结果显示，我们的系统可以全面超越现有的基于深度学习的前沿系统。这一成功，可以说明我们的这一套框架可以有助于在现实中实现情感识别。 
6,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740583&idx=3&sn=f82637dc3fd70eb332454cbb8b03658a&chksm=871ad319b06d5a0f64d7da4d7291eaa4aa64b1cef331a84411cc2ecd8d3bdd8cf75779e3e0a3&scene=27,入门 | 一文介绍机器学习中基本的数学符号,"本文介绍了机器学习中的基本数学符号。具体来说有算数符号，包括各种乘法、指数、平方根以及对数；数列和集合符号，包括索引、累加以及集合关系。此外，本文还给出了 5 个当你在理解数学符号遇到困难时可以应急的小技巧。 在机器学习中，你永远都绕不过数学符号。 通常，只要有一个代数项或一个方程符号看不懂，你就完全看不懂整个过程是怎么回事了。这种境况非常令人沮丧，尤其是对于那些正在成长中的机器学习初学者来说更是如此。 如果你能了解一些基本的数学符号以及相关的小技巧，那你就在看懂机器学习方法的论文或书籍描述上前进了一大步。 在本教程中，你将学到机器学习技术描述中遇到的基本数学符号。 在学完整个教程后，你会知道： 算术符号，包括若干种乘法、指数、平方根以及对数 数列和集合符号，包括索引、求和以及集合关系 5 种当你看不明白数学符号的时候可以采用的应急方法 让我们开始学习吧！ 机器学习中的基本数学符号 本教程分为 7 个部分，分别是： 1. 看不懂数学符号的沮丧 2. 算术符号 3. 希腊字母 4. 数列符号 5. 集合符号 6. 其他符号 7. 更多帮助资源 你在阅读机器学习算法的相关内容时会遇到一些数学符号。举例来说，这些符号可能会被用来： 描述一个算法 描述数据的预处理 描述结果 描述测试工具 描述含义 你可能在论文、教科书、博文以及其他地方看到这些描述。相关代数项常常会给出完整定义，但你还是会看到不少陌生的数学符号。我曾多次深受其苦，简直太令人感到挫败了！ 在本教程中，你会复习到一些帮助你看懂机器学习方法描述的基本数学符号。 在本节中，我们将重温一些基础算数中你不太熟悉的符号，以及毕业之后一些可能遗忘的概念。 简单算术 算术的基本符号你已很熟悉。例如： 加法：1 + 1 = 2 减法：2 – 1 = 1 乘法：2 x 2 = 4 除法：2 / 2 = 1 大多数的数学运算都有一个对应的逆运算，进行相反的运算过程；比如，减法是加法的逆运算，而除法是乘法的逆运算。 代数 我们常希望用更抽象的方式来描述运算过程，以将其与具体的数据或运算区分开来。因此代数的运用随处可见：也就是用大写和/或小写字母来代表一个项，或者一个数学符号体系中的概念。用希腊字母来代替英文字母也是很常见的用法。数学中的每一个领域都可能有一些保留字母，这些字母都会代表一个特定的东西。尽管如此，代数中的项总应在描述中被定义一下，如果作者没有去定义，那是他的问题，不是你的错。 乘法符号 乘法是一个常见的符号，有几种记法。一般是用一个小小的「ⅹ」或者星号「*」来代表乘法： c = a x b c = a * b 你有时也会看到用一个点来代表乘法，比如： c = a . b 这个式子其实和下式是一样的意思： c = a x b 或者你可能会看到运算符被省略，先前被定义的代数项之间没有符号也没有空格，比如： c = ab 这还是一样的意思。 指数和平方根 指数就是一个数字的幂次。这个符号写作正常大小的原数（底数）以及一个上标数（指数），例如： 2^3 这个表达式的计算结果就是 3 个 2 连乘，或者说是 2 的立方： 2 x 2 x 2 = 8 求一个数的幂，就默认是求它的平方。 2^2 = 2 x 2 = 4 平方运算的效果可以用开方来逆转。开方在数学中是在被开方的数字上面加一个开方符号，这里简单起见，直接用「sqrt()」函数来表示了。 sqrt(4) = 2 式中，我们知道了指数的结果 4，以及指数的次数 2，我们想算出指数的底数。事实上，开方运算可以是任意次指数的逆运算，只是开方符号默认次数为 2，相当于在开方符号的前面有一个下标的 2。我们当然可以试着写出立方的逆运算，也就是开立方符号： 2^3 = 8 3 sqrt(8) = 2 对数和 e 当我们求 10 的整数次幂的时候，我们常称之为数量级。 10^2 = 10 x 10 or 100 对这个运算求逆的另一方法是求这个运算结果（100）以 10 为底数的对数；用符号来表达的话就写作 log10()。 log10(100) = 2 这里，我们已知指数的结果和底数，而要求指数的次数。这让我们在数量级上轻松地缩放。除此之外，由于计算机中使用二进制数学，求以 2 为底数的对数也是常用的运算。例如： 2^6 = 64 log2(64) = 6 还有一个非常常见的对数是以自然底数 e 为底数的。符号 e 是一个专有符号，代表一个特殊的数字或者说一个称为欧拉数的常数。欧拉数是一个无限不循环小数，可以追溯到无穷的精度。 e = 2.71828... 求 e 的幂被称为自然指数函数： e^2 = 7.38905... 求自然对数的运算就是这个运算的逆运算，记作 ln(): ln(7.38905...) = 2 忽略更多数学细节，自然指数和自然对数在数学中非常有用，因为它们能用来抽象地描述某一系统的持续增长，比如说复利这样的指数级增长体系。 希腊字母在数学中用来代表变量、常数、函数以及其他的概念。比如说，在统计学中我们用小写的希腊字母 mu 来代表平均值，而小写的希腊字母 sigma 表示标准差。在线性回归中，我们用小写字母 beta 来代表系数，诸如此类。学会所有希腊字母的大小写以及怎么念会带来极大的帮助。 维基百科词条「数学、科学及工程中的希腊字母」是个非常有用的使用指南（https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering），因为上面列出来了在数学和科学不同领域内每一个希腊字母的常见用法。 机器学习中的符号常用来描述数列运算。一个数列可以是一列数据，或者代数项。 索引 读懂数列符号的关键是要弄明白数列中的索引符号。一般来说符号中会明确数列的起点和终点，比如从 1 到 n，这里的 n 是数列的长度。在数列中的项都会用一个诸如 i、j、k 的下标来作为索引，就像数组的符号一样。比如说，a_i 就是数列 a 中的第 i 个元素。如果数列是二维的，那就需要用到 2 个索引；比如：b_{i,j} 就是数列 b 的第 i 行, 第 j 列的元素。 数列运算 我们也可以对一个数列进行数学运算。有两类运算时常被用到，所以有专门的简写运算符来表示它们：累加和累乘。 数列累加 对一个数列的累加用大写的希腊符号 sigma 来表示，而累加的内容则用变量名来表示，同时在 sigma 符号的下面明确开始的索引（如 i=1），在 sigma 符号的上面明确结束的索引（如 n）。 Sigma i = 1, n a_i 这就是数列 a 的第一个元素到第 n 个元素的累加。 数列累乘 数列的累乘是用大写的希腊字母 pi 来表示的。而对累乘范围的描述方式与数列累加类似，开始的索引写在符号下面，结束的索引在符号上面。 Pi i = 1, n a_i 这就是数列 a 的第一个元素到第 n 个元素的累乘。 一个集合就是一组互不相同的元素的整体。在定义机器学习中的一些代数项的时候我们可能会遇到集合符号。 数字集合 你最常见的集合是数字集合，比如说有的代数项会定义在整数集或实数集内。这些常见的数字集合包括： 所以自然数的集合: N 所有整数的集合: Z 所有实数的集合: R 当然还有很多其他的数字集合，你可以参考维基百科中的「特殊集合」词条。我们在定义代数项的时候常指的是实值或者实数，而不是浮点数。浮点数在计算机运算中实际上是离散的数字。 集合关系 在定义代数项的时候常会看到集合关系符号，集合关系符号看起来就像是一个大写的「E」一样。 1 a E R 这表示定义 a 属于 R 集，或者说 a 属于实数集。同样，也有许多集合运算符；常见的两个集合运算符包括： 并集, 就是把两个集合的元素都包含进来：A U B 交集，就是只包括同时出现在两个集合中的元素：A ^ B 更多相关内容可以参考维基百科中的「集合」词条：https://en.wikipedia.org/wiki/Set_(mathematics)。 本节中我会列出一些较常见的其他符号。一种常见的情况是我们会先抽象地定义一个方法，然后用单独的符号来重新定义一个具体的实现。比如，如果我们在估计一个变量 x，可以在 x 上加一些符号来代表这些估计，比如： x-bar（x 上方有一横） x-prime（x 右上角有一小撇） x-hat（x 上方有一折线） 同一个符号在诸如数学的子领域或不同对象的语境下可能含义不同。比如说，|x|就是个很容易令人不解的符号，在不同的情况下可以指： |x|: x 的绝对值. |x|: 向量 x 的长度. |x|: 集合 x 的势. 本教程只提及了基础的数学符号。有很多数学的子学科与机器学习更相关，需要更详细地复习一下。包括： 线性代数 统计学 概率论 微积分 可能还有一些多变量分析和信息论的内容。 本部分将列示一些当你被机器学习中的数学符号折磨时可以用到的小建议。 考虑一下作者 你在阅读的论文或者书籍总有一个作者。这个作者可能犯错，可能有疏忽，也可能是因为他们自己也不明白自己在写什么，才让你如此迷惑。从符号的限制中逃离片刻，然后想想作者的目的。他们到底想把什么讲清楚？也许你甚至可以用电子邮件、Twitter、Facebook、领英等方式来联系作者让他帮你解释清楚。你放心，大多数学者都希望其他人能够理解并好好利用他们的研究成果。 上维基百科查一查 维基百科上有符号列表，可以帮助你缩小符号含义的可能范围。我建议你从这两个词条开始： 「数学符号表」（https://en.wikipedia.org/wiki/List_of_mathematical_symbols） 「数学、科学和工程中的希腊字母」（https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering） 用代码简述出来 数学运算不过就是对数据进行函数处理。把你读到的任何东西都用变量、for-循环等写成伪代码展示出来。这个过程中你可能打算使用某个脚本语言来处理自己随意写出来的数组，或者甚至一张 Excel 表格的数据。 当你阅读并理解了文章中的技术改进，那你随之写出来的核心代码才会取得更好的结果，最终经过不断的改进，你就会写出一个小小的原型机，可以自己玩耍了！我一度不相信这个方法行得通，直到看到一个学者仅用几行 MATLAB 代码和随意编写的数据就写出了一篇非常复杂的论文的核心代码。这令我大吃一惊，因为我以前一直坚信机器学习的系统必须完整地编写出来并且使用真实数据才能运行，所以要学习任何一篇文章只有找到原始的代码和数据这一条路可走。但是我真的错了。不过话说回来，那个学者真的是个天才。 现在我一直都在用这种方法学习机器学习，不过我是用 Python 写出新学到的技巧的核心代码。 换条路试试 有一个我在搞懂新技术时常用的小技巧，即找到所有引用了包含该技术的论文的其他论文，看看其他人如何演绎、解释这个新技术时常能够解除我在读原始描述产生的误解。不过这个办法不总是有效，反而会更加迷惑，引入了更多令人误解的方法和新符号。但是总体来说，这个办法还是有效的。 在网上向大神请教 说实话，有很多线上论坛里的人们很愿意向别人解释数学。你可以在屏幕上截张困扰你的符号图，写清楚出处和链接，然后连同你的困惑一起发布在问答网站上。推荐以下两个入门网站： https://math.stackexchange.com/ https://stats.stackexchange.com/ 你都有哪些弄明白数学符号的小技巧呢？不妨在评论区留言。 推荐阅读 如果你想进一步深入了解，这一部分会告诉你更多相关资源。 Section 0.1. Reading Mathematics, Vector Calculus, Linear Algebra, and Differential Forms, 2009：http://www.math.cornell.edu/~hubbard/readingmath.pdf The Language and Grammar of Mathematics, Timothy Gowers：http://assets.press.princeton.edu/chapters/gowers/gowers_I_2.pdf Understanding Mathematics, a guide, Peter Alfeld：https://math.stackexchange.com/ 在本教程中，你了解了在阅读机器学习相关技术时可能会遇到的基础数学符号。具体来说，你学到了： 算数符号，包括各种乘法、指数、平方根以及对数。 数列和集合符号，包括索引、累加以及集合关系。 5 个当你在理解数学符号遇到困难时可以帮到你的小技巧。  "
7,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740647&idx=2&sn=126cfbae604daf219e2fa3f279e966d0&chksm=871ad3d9b06d5acfb67d7e7f13fec5a90f4a8c124129a120cb27dacfc56cf7d6e08979f206d5&scene=27,专栏｜阿里妈妈资深技术专家刘凯鹏解读基于深度学习的智能搜索营销,"作者：刘凯鹏 本文结合阿里电商业务场景的特点，介绍了阿里在匹配端和排序端的基于深度学习的一些工作。 搜索营销（sponsored search）是目前广告主在互联网上进行数字化营销的主要手段之一，也是机器学习技术在工业界最成功的应用场景之一。在搜索营销的场景下，广告主可以通过出价的方式参与流量分配，平台的主要任务是优化流量分配和计价，实现效率的最大化和生态的健康发展。其中核心的技术问题主要围绕如何高效的分配流量来展开，包括：如何理解用户的意图？如何对用户进行表达？如何对广告进行表达？如何挖掘用户和广告的关系？如何对用户的行为进行预估？如何对流量进行定价等等。 近年来机器学习技术在搜索，推荐，营销等领域得到了广泛的应用。阿里妈妈搜索营销团队也一直致力于探索前沿技术和具体业务场景的深度融合，推动业务的发展。围绕以上的技术问题给出结合自身业务特点的解法。具体说来，流量分配的过程通常分为流量匹配阶段和排序阶段： 流量匹配阶段的主要任务是理解用户意图，在超大规模的全量集合中找到合适的候选集进行粗排，降低后续排序阶段的计算量； 排序阶段解决的主要问题则是对用户的深度理解，对用户行为进行精准的预估。 下面我们将从这两个阶段的实际问题出发，介绍我们的一些工作。    1.1 基于用户行为异构图的个性化检索框架 在搜索营销中，匹配端负责理解用户的搜索意图，需要快速准确地从海量广告中检索出一个小规模的高质量广告候选集，设计过程中需要兼顾系统的效果与效率。匹配系统又可以被看作为一个检索系统。在个性化时代，随着越来越多的个性化信息（用户在平台上的查询、浏览、点击、收藏、加购等行为）被引入系统，传统的基于搜索词和广告相关性的检索方法无法识别用户个性化的搜索意图。此外，基于相关性的优化目标和平台的目标（RPM、CTR、GMV 等）并不完全一致。 为了解决这些问题，我们突破了以「关键词」和「相关性」为核心的传统检索框架，提出了一种面向最终目标，基于用户行为异构图的个性化检索框架。 图 1. 基于用户行为异构图的个性化意图匹配。异构图中包含了三种节点：用户个性化的搜索意图信号、丰富的广告检索键和广告。 在这个新的智能检索系统中，我们首先使用用户在平台上的历史行为构建出一张用户行为异构图。异构图中节点分别表示「用户搜索信号」、「广告检索键」和「广告」，边分别表示「用户搜索意图信号改写」关系和「广告召回」关系。接着，检索系统面向平台 RPM、CTR 等指标，学习异构图中边的权重，挖掘出重要的改写关系和广告召回关系。这样，通过对异构图的深入挖掘，检索系统直接面向平台目标，同时进行了「用户搜索意图信号改写」和「广告召回」两个检索子任务的统一联合学习。最后，检索系统根据模型的边挖掘结果，智能地自动构建相应的「改写索引」和「广告召回索引」。 通过两个模型智能构建的索引，检索系统将用户行为异构图和模型挖掘结果存储下来，实现了对线上搜索请求的高效检索。由于新的智能检索模型不再强制要求广告商购买关键词，我们在新的检索系统使用 OCPC 策略，在保证广告商 ROI 的基础上，决定广告的点击收费。 关于此工作的论文《Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in E-Commerce Sponsored Search》已被 WWW2018 接收。 1.2 基于大规模异构网络 Embedding 的向量化检索模型 图 2. a) 使用全淘数据构建大规模异构网络；b) 通过深度学习技术对图中节点进行 Embedding；c) 根据用户搜索意图节点和广告节点的 Embedding 向量距离，使用 ANN 搜索进行匹配。   然而，基于上述方法的广告检索框架，依然是一种「硬」检索方式，索引关系受限制于用户行为的分布，面临着长尾流量覆盖不足等问题，另外浅层线性模型也限制了检索的精度。因此，我们进一步提出基于大规模网络 Embedding 的向量化检索模型。 通过 Graph Embedding、深度学习等技术，向量化检索模型可以得到用户搜索意图和广告在同一空间的深度向量表示，进而通过向量之间的距离进行全局检索，即提升了计算精度，又可解决长尾流量覆盖的问题。我们使用全淘数据构建超大规模异构网络（数亿节点和数百亿边），网络中包含多种类型的节点（如搜索词、商品、广告等）和多种类型的边（如用户行为、广告主行为、内容相似度等）。 这使得我们面临两方面的巨大挑战：一是巨大的网络规模产生海量的训练样本，二是如何构造模型来学习异构节点的复杂关系。为此，我们提出了一种创新的大规模异构网络 Embedding 算法和基于图数据库的分布式训练方法。 相关工作已经形成论文，正在投稿中，后续会公开出来。 1.3 联合优化的向量化检索模型和深度粗排模型 图 3 联合优化的向量化检索模型和深度粗排模型 在搜索营销的场景中，存在着多种检索通道，比如传统的关键词检索、上文提到的个性化检索和向量化检索；而整个匹配阶段是个多阶段检索过程，在这些检索通道之后还需要经过一个粗排序阶段。 在工业届中，在精排序阶段应用深度学习模型，已经有比较成熟的方案。然而在粗排阶段应用深度学习模型，并没有很好的解决方案，因为在粗排阶段需要处理的候选集规模比精排阶段高 1 到 2 个数量级，深度学习模型在这里会面临很大的挑战，需要很好的平衡 Efficiency 和 Effectiveness。 针对这个问题，我们创新的提出了一种联合优化向量化检索模型和粗排模型的方法。该方法通过 Multi-task Learning 的方式端到端的学出两个模型。在向量化检索模型中，特征上除了上文提到的 Graph Learning 出来的向量，还加入了图像向量等，行为序列上采用 RNN 建模；粗排模型和向量检索模型公共特征和大部分网络结构，只在最后一层有一些区别，这样在在线 Inference 阶段可以节省大量计算。 相关工作已经形成论文，正在投稿中，后续会公开出来。  2 智能排序 排序阶段要解决的主要问题是对用户的行为进行精准预估。近年来，深度学习方法在图像、NLP、语音信号处理等领域取得了显著的发展。在推荐、搜索领域也有很多的工作出现。例如 wide&deep[1] 结构对 id 特征和连续特征进行融合，同时兼顾模型的记忆能力和泛化能力；fnn[2]，deepfm[3] 等对稀疏特征的组合关系进行自动学习等等。 在阿里的搜索营销系统中，我们也对深度学习进行了探索，结合自身的业务特点，提供了一套解决方案。在阿里这样一个电商的场景中，一个主要的特点是以图像为主要的信息载体，商品／广告的内容包括图像，文本描述等等。用户在平台的行为是一个逛街的过程，用户进行搜索、浏览、对平台展现的商品进行比对，决定是否点击，然后对喜爱的商品进行收藏、购买。用户的行为之间存在比较强的内在关联。用户在当前时刻的点击意愿，既与当前时刻的用户看到的信息有关，又与这个用户的前置行为密切相关。因此，对于用户的行为预估，一方面需要对用户当前看到的内容有很好的表达，另一方面也需要去发掘用户的一系列行为之间的内在联系。 在对用户的行为进行建模的过程中，我们借鉴了大脑对信息的感知过程去模拟。整个模型分为感知网络，记忆网络和判断网络三个部分（图 4）。感知网络对当前的输入信息进行表达学习，对当前的 query、ad、user，上下文场景信息进行特征表达学习；记忆网络用来存储用户的历史行为，使用外部存储来记录与当前样本相关的历史信息；判断网络用来学习用户的历史行为和当前感知内容之间的关系，基于用户的感知内容和历史行为对用户当下的行为进行预估。 图 4 基于深度学习的用户行为预估优化结构图 2.1 感知网络 感知网络的作用是对<User, Query, Context, Ad>这样的四元组进行特征表达学习。样本的原始信息中包含了大量的非结构化数据，都在感知网络中转化为合适的表达形式。感知网络提供了多种感知通路，对当前样本的原始信息（连续变量、离散值变量等）进行预处理和 Embedding 操作，形成激励信号。通过 Embedding 模块，把离散值变量映射成固定长度的连续值向量，便于和其他的连续特征进行融合。Embedding 的过程类似于 word2vec 的映射过程。Embedding 向量可以区分原始信息的差异，向量的夹角和距离也能刻画原始信息的相似程度。 2.2 记忆网络 记忆网络的作用是存储用户的历史行为。记忆网络包括三部分内容：记忆如何存储、记忆如何查询和记忆如何更新。 在电商环境中，用户的行为不仅受到短期行为的影响，一些长期行为（比如用户在上一季度的点击购买行为）也会对当前的行为预估起到指导作用。因此，我们在记忆网络中设计了长期记忆和短期记忆两种记忆存储类型，分别采用不同的信息压缩方式。 对短期记忆保留更多的用户原始行为，对长期记忆进行压缩，保留用户的累积状态。在记忆查询模块，通过多种类型的内存读（Memory Read）操作，使用当前激励信息在历史记忆内容中查询相关的信息计算输出记忆结果。在记忆更新模块，在当前样本进行展示之后，调用不同的内存写模块（Memory Writer）把激励信号、点击标签和记忆查询结果更新到外部存储中，用于后续的行为预测。 2.3 判断网络 判断网络的作用是学习用户的历史行为和当前感知内容之间的关系，并且基于用户的感知内容和历史行为对用户当下的行为进行预估。判断网络由两部分组成。第一部分是信号融合模块，把记忆网络的输出信息和感知网络输出的当前样本激励信号进行融合，融合的方式可以采用拼接，内积或是各种 pooling 的方式，在我们的网络中，我们采用了拼接的方式来进行融合。第二部分是深度神经网络，目前使用多层的全连接网络对拼接信号进行处理。 相关工作已经形成论文，正在投稿中，后续会公开出来。 在本文中，我们结合阿里电商业务场景的特点，介绍了我们在匹配端和排序端的基于深度学习的一些工作。包括面向平台最终目标的个性化检索框架，联合优化向量化检索模型和粗排模型，以及基于深度学习的用户行为预估模型。这些工作给我们的系统中带来了很大的效果提升。目前这些工作有了一个很好的开端，后续会基于目前的框架对各个模块做进一步的优化。 "
8,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740647&idx=5&sn=fc7b947d672f6e3607cc8cd817c60471&chksm=871ad3d9b06d5acf8cbfa2433170f71bf49ad28c85547c56805206054e248ad1390155815d36&scene=27,机器之心「AI00」三月榜单：小米科技,"We believe AI should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible. －OpenAI 这不仅是一份榜单，更是一个开源项目，主要基于以下几点： 人工智能是一个复杂庞大的体系，涉及众多学科，也关乎技术、产品、行业和资本等众多要素，报告的写作团队只代表他们的专业观点，有自己的局限性，需要更多行业专家参与进来加以修正和完善。 人工智能技术和行业的发展瞬息万变，而报告的制作周期较长，其中的内容和数据势必会落后于行业的最新进展，无法同时满足时效性和高质量的要求。而领域内参与者的及时更新可以解决这个问题。 我们深刻地理解在没有专业用户反馈的情况下所做出报告的质量局限性，所以希望用工程界「Agile Development」的理念来对待我们的报告，不断收集专业反馈来持续提升报告质量。 人工智能是一个永恒命题，我们不仅会把「100 家公司」这个主题持续做下去，还会陆续开展其他主题。这个过程需要人工智能领域不同的参与者加入进来。 向 OpenAI 、「斯坦福人工智能百年研究」和「Open Source」致敬。 为此，我们邀请人工智能领域的科学家、技术专家、产业专家、专业投资人和读者加入进来，共同完成这项人工智能的长期研究。 如果你对「AI00」感兴趣，可在公众号对话框回复「AI00」（注：字母 AI 加数字 00）查看本开源项目的具体参与方式。 「AI00」第一期的榜单得到了来自投资界、人工智能产业界和学界的众多反馈，我们也依此对榜单的信息作出了一些订正和调整。且自榜单发布以来，其中的多家创业公司被科技巨头高价收购或发展为独角兽。 在最新一期的 AI00 榜单中，我们加入了即将上市的国内科技公司小米。最近，这家公司不仅推出了新款智能音箱「小米 AI 音箱 Mini」，也将人工智能图像处理等技术加入了自家最新旗舰手机 MIX2S 中。 小米科技是一家专注于智能硬件、智能家居和软件开发的企业，于 2010 年 4 月 6 日成立。在产品上，这家公司以安卓 UI「MIUI」起步，2011 年推出第一款小米手机，并迅速获得市场认可。随后的几年中，小米不断扩张其移动通信产品线，并将业务渗透到了其他智能设备领域。2016 年起，小米开始积极涉足家电事业，致力于开发的家庭物联网用于构建完整生态圈，实体产品则交由旗下投资的生态链公司所承包生产。 据介绍，小米已布局人工智能技术两年之久，并于 2017 年 3 月先期推出了具有人工智能语音对话功能的小米电视 4A。小米投入的第一个主要成果就是 2017 年 7 月份发布的小米 AI 音箱，它内置了小米自研的人工智能助理「小爱同学」。 2018 年 3 月 27 日，小米发布了 MIX 2S 手机，其 AI 技术加持的拍摄功能成为产品亮点。在人工智能算法的帮助下，其「AI 双摄」可以对拍照背景进行虚化，并对 206 种场景进行智能识别、实时优化拍摄效果。此外，MIX 2S 也加入了超分辨率文字识别、拍照翻译等内含机器学习技术的功能。 此前有消息称，小米计划于 2018 年第三季度末于港交所上市，估值有望达到 900-1100 亿美元，PE（市盈率）为 60 倍。 以下为 AI00 三月榜单： 搜狗 中国 人工智能综合研究 搜索引擎、知识图谱、输入法、语音交互产品等 估值约50亿美元 美国 自然语言处理 个性化智能助理 三轮融资共获得 3.4 亿美元 中国 语音技术和自然语言处理 智能家居、车载、电信等行业解决方案 市值约 393 亿人民币 中国 智能语音交互和自然对话 车载、智能家居和智能机器人等智能硬件的语音交互服务 B 轮：2 亿人民币 中 / 美 声源分离、声音增强、声纹识别、麦克风阵列 会议转录，通讯，机器人，智能家居，虚拟现实，增强现实，混合现实 未透露 中国 情感对话机器人、语音情感技术、多模态情感识别 竹间个人助理机器人小影、金融机器人、客服机器人 2500万美元融资 美国 语音识别技术、自然语言处理技术（NLP） 电话语言反馈、预测销售结果、自动信息检索 1400万美元的A轮融资 美国 智能客服 理解和分类用户请求 1200万美元的A轮融资 美国 计算机视觉 图像及视频识别 API B 轮 3000 万美元 美国 计算机视觉和深度学习 实时面部表情分析和情绪识别解决方案 四轮融资共获得 3372 万美元 新加坡 计算机视觉、视觉搜索、图像识别 电子商务、移动商务、 在线广告等图像识别解决方案 两轮融资共获得 1400 万美元 美国 计算机视觉、深度学习和数据科学 分析海量卫星图像，用于经济趋势分析和公益研究 C轮融资共 5千万美元 美国 计算机视觉和数据科学 将卫星图像识别用于农业、城市规划和灾害响应等 2015 年 5 月融资 1830 万美元 美国 计算机视觉和深度学习 通过 DLFP 平台为农业提供数据分析和预测的解决方案 B轮 3千万美元 中国 计算机视觉和深度学习 人脸识别、危险品识别、行为检测、车辆检测等的安防监控系统 6亿美元C轮融资 中国 计算机视觉和深度学习 Face++ 人脸识别云服务平台、Image++ 图像识别平台、VisionHacker 移动游戏工作室 C轮4.6亿美元融资 中国 计算机视觉和深度学习 基于图像理解的信息获取和人机交互服务 3.8亿元C轮融资 中国 计算机视觉和深度学习 图像识别、视频鉴黄、智能审核、图片增值等云服务 新一轮千万美元融资 波士顿 深度学习、计算机视觉 帮助机器人和智能设备学习和适应环境的软件 A 轮融资约1400 万美元 中国 人脸识别、计算机视觉 金融机构人脸识别应用、公安系统实时布控、追逃等 B 轮 5亿人民币 中国 计算机视觉、人脸识别 智能安防、智慧金融、智慧楼宇等领域 A 轮亿元级 公司 美国 深度学习芯片架构 DPU (Dataflow Processing Unit) 3轮融资共5900万美元 美国 人工智能芯片技术 用于机器学习的第二代神经网络软件框架 CDNN2 纳斯达克上市，市值 9.12 亿美元 美国 基于 FPGA，针对于服务器端的高性能深度学习平台 被移动设备直接嵌入的深度学习模块 未透露 中国 深度学习 中国首款神经网络处理器 1亿美元 A 轮融资 中国 深度学习 DPU 平台 深度学习 DPU 平台 A+轮约4000万美元融资 英国 深度学习硬件和软件开发 开源软件框架 Poplar 和「智能处理器」IPU A 轮融资 3000 万美元 Groq 美国 深度学习硬件、芯片 暂无信息 1030 万美元融资 瑞士 机器人及自动化技术 工业机器人、智能设备 468.95 亿美元市值 日本 机器人及自动化技术 工业机器人 4.4兆日元市值 德国 机器人及自动化技术 工业机器人 美的 272 亿美元拿下库卡 94.55% 的股份 美国 机器人、人机交互 智能机器人 1800万美元新一轮融资 丹麦 机器人及自动化技术 工业机器人 未透露 美国 计算机视觉、机器人技术 无人机，软件服务 五轮融资超 1 亿 2 千 600 万美元 日本 可穿戴设备 医疗助理机器人 2083.56 亿日元市值 美国 计算机视觉、机器人技术 电子产品，家用机器人 27亿美元市值 德国 计算机视觉、机器人技术 代步机器人，残障专用智能设备 2000万美元市值 英国 智能机器，自动视觉定位及室内地图构建 清洁机器人 未透露 中国 机器人及自动化技术 工业机器人和行业解决方案 约 289 亿市值 中国 机器人 工业机器人、智能装备和行业解决方案 未透露 中国 计算机视觉、无人机控制、环境及障碍感知、视觉跟随、自动寻路 无人机航拍和图像传输 估值约100亿美元 美国 可以接入机器人的智能模块，工厂、仓库等自动化技术 智能设备 种子轮 700 万美元融资 Arterys 美国 深度学习系统生成医疗图像 深度学习分析系统 Arterys System 1200 万美元 A 轮融资 美国 深度学习、大数据、图像检测 癌症检测系统 三轮融资共 1500 万美元 VoxelCloud（体素科技） 美国 深度学习 医疗影像分析云服务 千万美金 A 轮融资 美国 深度学习和大数据 癌症诊疗 1000万美元最新融资 美国 大数据和机器学习技术 通过数据分析为放射肿瘤学家提供临床决策支持，用于个性化医疗。 两轮融资共 132 万美元 美国 深度学习 药物发现 4500万美元A轮 美国 深度学习 通过药物研发平台 DUMA™来评估大型公共和私有数据集，以迅速识别药物，并对药物和疾病的匹配度按照概率进行排序。 种子轮 340 万美元 美国 深度学习 Interrogative Biology® 平台结合病人生物学和人工智能分析来进行药物发现、开发和诊断等。 未透露 美国 机器学习、自然语言处理 拥有 MedxExchange 、MedxInsights 和 MedxCare 三款服务产品的医疗人工智能平台，提供数据、医疗洞见和健康管理服务。 融资 660 万美元 加拿大 深度学习、基因生物学 精准医疗 种子轮 370 万美元 中国 大数据、人工智能 通过数据挖掘和机器分析提供个人性健康指数分析和预测。 A 轮融资近 10 亿人民币 美国 数据挖掘、预测分析 本地分析软件和云服务 五轮融资 4430 万美元 美国 数据挖掘、机器学习 信用服务 四轮共 1.12 亿美元 英国 信用评级 金融产品的信用评级 四轮共 700 万美元 美国 数据挖掘、机器学习 AppBank、金融业务自动化 市值 867.10 亿美元 中国 人工智能、数据挖掘 智能助理、信用评级和风险管理等应用 估值 600 亿美元 Citadel 美国 人工智能、数据挖掘 对冲基金 目前掌管至少 260 亿美元资产 美国 自动驾驶 自动驾驶汽车 谷歌无人驾驶项目开始以独立公司的身份运营 美国 自动驾驶 电动汽车 498.35 亿美元市值 美国 深度学习 自动驾驶汽车 5000 万美元B轮融资 以色列 智能 3D 传感、传感器融合和精准地图和定位等核心自动驾驶技术 物美价廉的高清晰度固态激光雷达 6500万美元B轮融资 美国 雷达和专用短程通信安全系统 自动驾驶卡车 最新一轮6000万美元融资（2017 年 4 月） 美国 计算机视觉、数据挖掘 交通安全和表现的智能解决方案 六轮融资共 1.8 亿美元 美国 全自动驾驶 全新的自动驾驶汽车 3 轮融资共 2.9 亿美元 中国 计算机视觉和深度学习 自动驾驶 5500 万美金的 C 轮融资 中国 计算机视觉、智能驾驶 辅助驾驶系统 A 轮数千万人民币 Argo AI 美国 人工智能和自动驾驶技术发 自动驾驶汽车 福特将持续注资 10 亿美元 美国 人工智能和自动驾驶技术 自动驾驶汽车Level 4 无人配送车 A 轮融资 9200 万美元 英国 利用机器学习和独家算法来检测和响应以前未识别的网络威胁 Darktrace 的核心产品为「企业免疫系统」(EIS) 三轮共融资 1.045 亿美元 美国 自动驾驶、机器学习、数据挖掘 自动驾驶汽车、智能交通和智能出行应用 12 轮融资 87.1 亿美元，估值 660 亿美元 美国 机器学习 开源 AeroSolve 机器学习框架、智能助手、智能推荐、定价 9 轮共融资 20 亿 9 千万美元 美国 云计算、深度学习、数据处理 CRM 解决方案 市值约 638.37 亿美元 美国 机器学习 企业通讯应用，bots 平台 总融资 5.4 亿美元，估值约 38 亿美元 美国 人工智能、大规模分布式计算 解决复杂商业问题的综合智能系统 1.03 亿美元 C 轮融资，三轮共 1.3578 亿美元 美国 数据挖掘 基于社交网络的数据分析服务 1.3 亿美元 D 轮融资。五轮共 1.83 亿美元 美国 认知计算、深度学习、自然语言处理 法务研究智能辅助工具 未透露 中国 自动驾驶、机器学习、数据挖掘 自动驾驶汽车、智能交通和智能出行应用 超55亿美元新一轮融资 （2017 年 4 月） 中国 深度学习、自然语言处理、图像识别 媒体产品的应用 估值约 120 亿美元 中国 基于云端的深度神经网络算法、图像、语音、自然语言理解和运动控制、技术集成 智能机器解决方案 A+ 轮近亿美元 美国 机器学习、数学科学 使用人工智能来预防网络攻击 已融资 1.77 亿美元 美国 机器学习 利用人工智能/机器学习来开发网络安全应用的公司 4 轮共 5360 万美元融资 SparkCognition 美国 机器学习、人工智能、数据分析 使用机器学习和人工智能技术来分析预测网络安全漏洞与系统故障 新一轮3250万美元融资 英国 人工智能基础研究 AlphaGo、医疗健康、谷歌内部产品应用。 以 4 亿英镑（约 5.32 亿美元）被谷歌收购 美国 人工智能基础研究 新的计算机视觉系统，机器人视觉 五轮获得 7200 万美元融资 美国 深度学习 Inkling 脚本语言和集成开发环境 Mastermind 760 万美元A轮融资 日本 深度学习 深度学习操作系统 Chainer，机器学习在物联网的应用 9500 万美元最新一轮融资 美国 深度学习 深度学习企业应用包 SKIL、开源框架 Deeplearning4j 种子轮融资 300 万美元 美国 机器学习 开源机器学习平台和商业化支持 四轮融资 3360 万美元 美国 数据挖掘、机器学习 为铁路、建筑等大行业提供数据预测分析 SaaS 服务 B轮融资5000万美元（2017 年 4 月） 美国 机器学习 为数据科学家提供图像、文本的识别和分析的工具 4轮融资共 438 万美元 中国 机器学习 金融应用和「先知」平台 三大国有银行联合投资金额未公开 美国 机器学习 Bayesian Program Synthesis 可以自行编写代码，用最优的方法解释收集到的数据 来自 DARPA 的 770 万美元投资、来自 Felicis Ventures 的 450 万美元种子轮融资 美国 机器学习 机器学习平台公司，DataRobot 平台上有数百个开源机器学习算法 5400 万美元 C 轮融资 美国 机器学习与人工智能平台 PetuumOS、Poseidon 框架、Petuum Healthcare Solutions 9300万美元B轮融资 美国 算法 类似于苹果 App Store 的「算法应用」商店 1050 万美元 A 轮融资 美国 人工智能综合研究 TensorFlow 等开源框架，Google Photos、Now、Inbox 和搜索等多项产品和服务、硬件 市值 6701 亿美元 美国 人工智能综合研究 多个开源框架和硬件平台，Messenger、社交网络和定向广告等多项产品和服务 市值 4296 亿美元 美国 人工智能综合研究 云服务、Echo 等智能家居、机器人、电商产品应用 市值 4696 亿美元 美国 人工智能综合研究 CNTK 等开源框架，Cortana、小冰等多项产业和服务，硬件 市值 5362 亿美元 美国 人工智能综合研究 Watson、行业认知计算解决方案、量子计算机等 市值 1434 亿美元 美国 人工智能综合研究 基于智能手机等硬件的多项产品和硬件、智能助手、智能家居、医疗等 市值 8067 亿美元 中国 人工智能综合研究 开源框架 PaddlePaddle、百度大脑、自动驾驶、互联网应用 市值 849.78 亿美元 中国 人工智能综合研究 云服务、人工智能平台 DT PAI、电商产品应用 市值3079 亿美元 中国 人工智能综合研究 互联网应用 25683.98亿人名币 美国 人工智能硬件 GPU、深度学习超级计算机 DGX-1、自动驾驶超级计算机 Xavier 市值 832.07 亿美元 美国 人工智能硬件 CPU、Xeon Phi、Nervana 市值 1706.30 亿美元 美国 人工智能硬件 移动智能设备芯片 市值约 899.36 亿美元 美国 全可编程技术和器件 All Programmable FPGA、SoC 和 3D IC 提供商 160.70 亿美元 华为 中国 人工智能综合研究、硬件 人机交互设备应用、芯片等 2017 年以 785.108 亿美元营业收入首次打入《财富》前百强 京东 中国 人工智能综合研究 电商产品应用、金融 市值约 559 亿美元 小米科技 中国 人工智能综合研究、智能硬件 智能音箱、智能手机、IOT 据传即将上市 「 AI00 开源项目」参与方式： "
9,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740555&idx=4&sn=7309ad030eb9352517b468492f2285a0&chksm=871ad335b06d5a23d93395877603c3a571d3d48008dcc93a98789c277f73cc46cc103c79fb11&scene=27,CVPR2018 | 让AI识别语义空间关系：斯坦福大学李飞飞组提出「参考关系模型」,"图像不仅仅是一组目标集合，同时每个图像还代表一个相互关联的关系网。在本文中，李飞飞等人提出了利用「参考关系」明确区分同类实体的任务。实验结果表明，该模型不仅在 CLEVR、VRD 和 Visual Genome 三个数据集上均优于现有方法，并且是可解释的，甚至能发现完全没见过的类别。 日常用语中的参考式表达可以帮助我们识别和定位周围的实体。例如，我们可以用「踢球的人」和「守门的人」将两个人区分开（图 1）。在这两个例子中，我们通过两人与其他实体的关系来明确他们的具体身份 [24]。其中一个人在踢球，而另一个人在守门。我们的最终目标是建立计算模型，以明确其他词汇与哪些实体相关 [ 36 ]。 为了实现这种交互，我们引入了参考关系（referring relationships），即在给定关系的情况下，模型应该清楚场景中的哪些实体在该关系中用作参考。从形式上讲，该任务需要输入带有<subject - predicate - object>关系的图像，并输出主体和客体的定位。例如，我们可以将上面的示例表示为<person -kicking - ball>和<person - guarding - goal>（图 1）。以前的研究工作已经尝试在参考式表达理解的背景下明确区分同一类别的各个实体 [ 29，25，43，44，12 ]。它们的任务需要输入自然语言，例如「a person guarding the goal」，从而产生需要自然语言和计算机视觉组件的评估。精确地指出这些模型所产生的错误是来自语言还是来自视觉模块可能有点困难。我们的任务是参考表达的一种特殊运用，通过连接结构化关系输入减少对语言建模的需要。 参考关系在前期任务的核心保留并改善算法难题。在客体定位文献中，一些实体 (如斑马和人) 差别非常明显，很容易被区分开来，而另一些客体（如玻璃和球）则较难区分 [ 30 ]。造成这些困难的原因包括某些成分尺寸小、不易区分。这种难度上的差异转化为参考关系任务。为了应对这一挑战，我们提出这样一种思路：如果我们知道另一个实体在哪里，那检测一个实体就会变得更容易。换句话说，我们可以借助踢球的人为条件来发现球，反之亦然。我们通过展开模型及通过谓词定义的运算符在主客体之间迭代传递消息来训练这种循环依赖关系。 然而，对这个谓词运算符建模并不简单，这就引出了我们的第二个挑战。传统上，以前的视觉关系论文已经能为每个谓词训练了一个基于外观的模型 [21, 24, 27]。不幸的是，谓词语义的急剧变化（取决于所涉及的实体）增加了学习谓词模型的难度。例如，谓词 carrying 的语义在以下两种关系之间可能有很大差异：<person - carrying - phone> 和 <truck - carrying -hay>。受心理学移动焦点理论 [ 19，37 ] 的启发，我们通过使用谓词作为从一个实体到另一个实体的视觉焦点转移操作来绕过这一挑战。当一个移位操作学习将焦点从主体转移到客体时，逆谓词移位以相似的方式将焦点从客体转移回主体。经过多次迭代，我们将主体和客体之间的这些不对称焦点转移实施为每个谓词 [ 39，10 ] 的不同类型的消息操作。 总而言之，我们介绍了参考关系这一任务，它的结构化关系输入使得我们可以评估识别图片中同一类别实体的能力。我们在包含视觉关系的三个视觉数据集（CLEVR [13], VRD [24] 和 Visual Genome [18]）上评估我们的模型。这些数据集中的 33 %、60.3 % 和 61 % 的关系涉及不明确的实体，即相同类别中的多个实例的实体。我们扩展我们的模型以使用属于场景图的关系来执行焦点扫视 [ 38 ]。最后，我们证明了在没有主体或客体的情况下，这一新模型仍然可以明确各个实体，同时还可以辨别来自以前从未见过的新类别实体。 我们的模型使用带有 TensorFlow 后端的 Keras 进行编写。 模型地址：https://github.com/StanfordVL/ReferringRelationships。 论文：Referring Relationships 论文链接：https://arxiv.org/abs/1803.10362 摘要： 图像不仅仅是一组目标集合，同时每个图像还代表一个相互关联的关系网。实体之间的这些关系承载着语义功能，帮助观察者区分一个实体中的实例。例如，一张足球比赛的图片中可能不止一人，但每个人都处在不同的关系中：其中一人在踢球，另一人则在防守。在本文中，我们提出了利用这些「参考关系」明确区分同类实体的任务。我们引入了一个迭代模型，利用该模型区分参考关系中的两个实体，二者互为条件。我们通过谓词建模来描述以上关系中实体之间的循环条件，这些谓词将实体连接为从一个实体到另一个实体的焦点移位。实验结果表明，该模型不仅在 CLEVR、VRD 和 Visual Genome 三个数据集上均优于现有方法，而且能作为可解释神经网络的一个实例。此外，它还能产生可视的有意义的谓词移位。最后，我们提出，通过将谓词建模为注意转移，我们甚至可以区分模型没见过的类别中的实体，从而使我们的模型发现完全没见过的类别。 "
10,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740517&idx=3&sn=d7d6d7deb2d31b53bf1ac8a0b097e7c6&chksm=871ad35bb06d5a4db786fc38dda2879b967e52a00a61dbee89ad9dd64f67dfa1ef894ddd2977&scene=27,业界 | OpenAI举办迁移学习竞赛：评估强化学习算法表现,  近日，OpenAI 发布了一个迁移学习竞赛，来评判强化学习算法从先前经验进行泛化的能力。在典型的强化学习研究中，算法的测试与训练环境是一样的，这对于记忆能力更好以及有很多超参数的算法来说更有优势。而 OpenAI 发布的这个竞赛是在先前未见过的视频游戏上测试算法。此外，该竞赛使用的是 OpenAI 将经典游戏融入到 Gym 中做出的新平台 Gym Retro。 竞赛地址：https://contest.openai.com/ Gym Retro 的 GitHub 地址：https://github.com/openai/retro OpenAI Retro 竞赛给出了在《刺猬索尼克》系列游戏上的多级别训练集，然后在 OpenAI 定义级别的测试集上评估算法。这里有两个机密测试集：一个用于在竞赛进行的时候竞争排行榜，另一个仅在最终排名的时候使用一次。此外，OpenAI 提供了训练集/测试集的分划级别建议，被用于技术报告的所有结果，以及下面的学习曲线。 比赛日期从 4 月 5 日持续到 6 月 5 日。 OpenAI 还发布了 retro-baseline，演示了如何在此竞赛任务上运行数种强化学习算法。 retro-baseline 地址：https://github.com/openai/retro-baselines 在训练阶段，你可以使用自己想要的任何环境或者数据集，但测试阶段你只有 18 个小时（100 万时间步），并在之前从未见过的游戏级别上的环境或者数据集上进行测试。18 小时玩一个游戏也许听起来像是很长的时间，但在此训练预算上，已有的强化学习算法依然要比人类差很多。 索尼克基准 为了更详细地描述这个基准，以及提供一些基线结果，OpenAI 放出了一个技术报告。 技术报告 PDF：https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/retro-contest/gotta_learn_fast_report.pdf 该报告包含关于此基准的详细细节以及从 Rainbow DQN、PPO 到简单随机猜测算法 JERK 的所有结果。JERK 通过随机采样行为序列对索尼克进行优化，且在训练过程中，它更频繁地重复得分最高的行为序列。 通过利用训练级别的经验，可以极大地提升 PPO 在测试级别的表现。在训练级别上预训练，然后在测试级别上精调，网络表现有近一倍的提升，使得它要比最强的基线结果都要好。虽然这不是第一例强化学习中成功的迁移学习案例，但依然令人兴奋，因为它展示了迁移学习有极可靠的高效率。 但强化学习算法要想媲美人类表现，还有很长的路要走。如前面所说，在训练级别上玩 2 个小时，在每个测试级别上玩 1 个小时，人类能够获得比强化学习算法高很多的得分，即使用了迁移学习的强化学习算法也比不过人类。 Gym Retro 测试版 OpenAI 于今天发布 Gym Retro，这是一个将经典视频游戏封装作为强化学习环境的系统。该初步发行版本包含了来自「SEGA Mega Drive and Genesis Classics Steam Bundle」的 30 个 SEGA Genesis 游戏，以及来自「Arcade Learning Environment」（街机学习环境）的 2600 个 Atari 游戏。 Arcade Learning Environment 集合了 2600 个 Atari 游戏，并结合了强化学习的接口，它在过去五年中已成为了强化学习研究的主要驱动。这些 Atari 游戏相比之前的强化学习基准更具多样性和复杂性，并被设计来学习运动技能和人类玩家的问题解决能力。 Gym Retro 测试版利用了比 Atari、SEGA Genesis 更加现代化的控制台，为强化学习研究扩展了数量规模和复杂性。Genesis 上的游戏在某些维度上（物理、物体外观等）有很多层次上的相似性，在另一些维度上（布局、道具等）又有所区别，这使得其可以很好地支持迁移学习。由于利用了更好的硬件（例如，500 倍于 Atari 的 RAM、更宽泛的控制输入种类和更好的图像支持），它们相比 Atari 游戏也有更大的复杂性。 Gym Retro 由「Retro Learning Environment」所启发，但相比之下更加灵活；例如，在 Gym Retro 中，你可以通过 JSON 文件而不是 C++代码指定环境定义，因而能更好地整合新的游戏： Gym Retro 是 OpenAI 第二次尝试建立强化学习环境的大型数据集。它建立在 2016 年末的 Universe 项目的某些类似思想上，但 OpenAI 并没有从那些实现上得到好的结果，因为 Universe 环境是异步运行的，只能实时运行。由于对游戏状态的检测基于屏幕，这些结果通常是不可靠的。Gym Retro 将 Arcade Learning Environment 的模型扩展为规模大得多的潜在游戏集合。 
11,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740555&idx=3&sn=17a509ce70a7ba71f1f7890ae69f59cd&chksm=871ad335b06d5a2318301c7fb9b77fb580ae16ade3bc663455230f8b80fac77be7b23cb102fc&scene=27,教程 | 如何使用深度学习执行文本实体提取,"TowardsDataScience 本文介绍了如何使用深度学习执行文本实体提取。作者尝试了分别使用深度学习和传统方法来提取文章信息，结果深度学习的准确率达到了 85%，远远领先于传统算法的 65%。 项目地址：https://github.com/dkarunakaran/entity_recoginition_deep_learning 文本实体提取是自然语言处理（NLP）的主要任务之一。随着近期深度学习领域快速发展，我们可以将这些算法应用到 NLP 任务中，并得到准确率远超传统方法的结果。我尝试过分别使用深度学习和传统方法来提取文章信息，结果非常惊人：深度学习的准确率达到了 85%，远远领先于传统算法的 65%。 本项目的目标是把文章中的每个单词标注为以下四种类别之一：组织、个人、杂项以及其他；然后找到文中最突出的组织和名称。深度学习模型对每个单词完成上述标注，随后，我们使用基于规则的方法来过滤掉我们不想要的标注，并确定最突出的名称和组织。 在这里要感谢 Guillaume Genthial 这篇关于序列标注的文章（https://guillaumegenthial.github.io/），本项目建立在这篇文章的基础之上。 架构 上图是对每个单词进行分类标注的模型高级架构。在建模过程中，最耗时间的部分是单词分类。我将解释模型的每个组成部分，帮助读者对模型组件有一个全面的、更高层次的理解。通常，模型组件可分为三部分： 单词表征：在建模第一步，我们需要做的是加载一些预训练词嵌入（GloVe）。同时，我们需要从字符中提取出一些含义。 语境单词表征：我们需要利用 LSTM，对语境中的每一个单词得到一个有意义的表征。 解码：当我们得到表示单词的向量后，我们就可以用它进行预测。 hot encoding（用数值表示单词） 深度学习算法只接受数值型数据作为输入，而无法处理文本数据。如果想要在大量的非数值场景下使用深度神经网络，就需要将输入数据转变数值形式。这个过程就是 hot encoding。 下面是一小段实现 hot encoding 的代码示例： 同样地，我们必须获取输入数据中的所有字符，然后将其转化为向量，作为字符嵌入。 单词嵌入 & 字符嵌入 单词嵌入是处理文本问题时使用的一种通过学习得到的表征方式，其中含义相同的单词表征相近。通常，我们利用神经网络来实现单词嵌入，其中使用的单词或短语来自于词库，并需要转变为实数构成的向量形式。 但是，在数据集上生成词向量计算成本很高，我们可以使用一些预训练的单词嵌入来避免这个问题：比如使用斯坦福大学的 NLP 研究者提供的 GloVe 向量。 字符嵌入是字符的向量表征，可用于推导词向量。之所以会使用字符嵌入，是因为许多实体并没有对应的预训练词向量，所以我们需要用字符向量来计算词向量。这里有一个详细介绍字符嵌入的在线资源：http://minimaxir.com/2017/04/char-embeddings/。 LSTM 传统神经网络 VS 循环神经网络（RNN） 循环神经网络（RNN）是人工神经网络的一种，用于序列数据中的模式识别，例如文本、基因组、手写笔迹、口语词汇，或者来自传感器、股市和政府机构的数值型时间序列数据。它可以「理解」文本的语境含义。 LSTM 是一种特殊的循环神经网络，相比于简单的循环神经网络，它可以存储更多的语境信息。简单的 RNN 和 LSTM 之间的主要区别在于它们各自神经元的结构不同。 对于语境中的每一个单词，我们都需要利用 LSTM 得到它在所处语境中的有意义表征。 如果你想了解更多关于 LSTM 和 RNN 的知识，可以参阅以下文章 1. http://karpathy.github.io/2015/05/21/rnn-effectiveness (http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 2. https://deeplearning4j.org/lstm.html 条件随机场（CRF） 在预测标注最后的解码步骤中，我们可以使用 softmax 函数。当我们使用 softmax 函数时，它给出单词属于每个分类的概率。但这个方法给出的是局部选择；换句话说，即使我们从文本语境中提取出了一些信息，标注决策过程依然是局部的，我们在使用 softmax 激活函数时，并没有使用到邻近单词的标注决策。例如，在「New York」这个词中，我们将「York」标注为一个地方，事实上，这应该可以帮助我们确定『New』对应地方的开始。 在 CRF 中，我们的输入数据是序列数据；同时，我们在某个数据点上进行预测时，需要考虑先前文本的语境。在本项目中，我们使用的是线性链 CRF。在线性链 CRF 中，特征只依赖当前标注和之前的标注，而不是整个句子中的任意标注。 为了对这个行为建模，我们将使用特征函数，该函数包含多个输入值： 句子ｓ 单词在句子中的位置ｉ 当前单词的标注 l_i  前一个单词的标注 l_i−1 接下来，对每一个特征函数 f_j 赋予权重 λ_j。给定一个句子ｓ，现在我们可以根据下式计算ｓ的标注ｌ：对句子中所有单词的加权特征求和。 基于词性标注的特征函数示例 如果 l_i= ADVERB，且第 i 个单词以『-ly』结尾，则 f_1(s,i,l_i,l_i−1)=1，否则取 0。如果对应的权重 λ1 为正，且非常大，那么这个特征基本上就表示我们倾向于把以『-ly』结尾的单词标注为 ADVERB。 如果 i=1，l_i= VERB，且句子以问号结尾，则 f_2(s,i,l_i,l_i−1)=1，否则取 0。如果对应的权重 λ2 为正，且非常大，那么这个特征基本上就表示我们倾向于把疑问句的第一个单词标为 VERB。（例，「Is this a sentence beginning with a verb?」） 如果 l_i−1= ADJECTIVE，且 l_i= NOUN，则 f_3(s,i,l_i,l_i−1)=1，否则为０。对应权重为正时，表示我们倾向于认为名词跟在形容词之后。 如果 l_i−1= PREPOSITION，且 l_i= PREPOSITION，则 f_4(s,i,l_i,l_i−1)=1。此函数对应的权重 λ4 为负，表示介词不应该跟着另一个介词，因此我们应该避免这样的标注出现。 最后，我们可以通过取指数和归一化，将这些得分转换为 0~1 之间的概率 p(l|s)。 总之，要建立一个条件随机场，你只需要定义一组特征函数（可以依赖于整个句子、单词的当前位置和附近单词的标注）、赋予权重，然后加起来，最后如果有需要，转化为概率形式。简单地说，需要做两件事情： 1. 找到得分最高的标注序列； 2. 在全体标注序列上求出概率分布。 幸运的是，TensorFlow 提供了相关的库，帮助我们可以很容易地实现 CRF。 CRF r 相关阅读资料： 1. http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/ 2. http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf 对于每一个单词，我们希望建立一个向量来捕捉其意义以及和任务相关的特征。我们将该向量构建为 GloVe 单词嵌入与包含字符级特征的向量的级联。我们还可以选择使用一些特定的神经网络，自动提取出这些特征。在本文中，我们将在字符层面上使用双向 LSTM 算法。 我们将 CONLL 数据集中的所有单词都进行 hot-encode，这些单词都在 GloVe 单词嵌入中有对应的实体。如上文所述，神经网络只接受向量，不接受文本，因此我们需要将单词转换为向量。CONLL 数据集包含单词及其对应标注。在 hot encoding 后，单词和标注都被转换成了向量。 用于 hot encoding 单词及其对应标注的代码： 用于提取单词、标注和字符向量的代码： 现在，我们使用 TensorFlow 内置的函数加载单词嵌入。假定 embeddings 是一个 GloVe 嵌入的 numpy 数组，其中 embeddings[i] 表示第 i 个单词的向量形式。 现在，我们可以构建根据字符得到的单词嵌入。这里，我们不需要任何预训练字符嵌入。 一旦得到了单词表征，我们就可以直接在词向量序列上运行 bi-LSTM，得到另一个向量序列。 现在，每个单词都和一个向量对应，其中向量记录了这个单词的含义、字符和语境。我们使用向量来做最后的预测。我们可以使用全连接神经网络求出一个向量，该向量中每个条目对应每个标注的得分。 最后，我们使用 CRF 方法来计算每个单词的标注。实现 CRF 只需要一行代码！下面的代码计算出了损失，同时返回了在预测时很有用的 trans_params。 现在，我们可以定义我们的训练算子： 一旦我们定义好模型，在数据集上完成很少的几次迭代，就可以得到训练好的模型了。 TensorFlow 提供了存储模型权重的功能，这样我们就可以在之后的场景中复原训练好的模型。无论什么时候需要进行预测，我们都可以加载模型权重，这样就不需要重新训练了。 def save_session (self) def restore_session (self, dir_model) 每篇文章都被分解为单词再输入到模型中，然后经过上文所述一系列过程，得到输出结果。模型最终输出结果将每个单词分为 4 类：组织、个人、杂项以及其他。这个算法通过基于规则的方法过滤结果，然后进一步正确提取出文本中最突出的名称和组织，它并没有达到 100% 的准确率。 原文链接：https://towardsdatascience.com/entity-extraction-using-deep-learning-8014acac6bb8 "
12,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740555&idx=1&sn=a5cb93451fadd03f9c56750eaf76cc6f&chksm=871ad335b06d5a233d659d444faaa62759e2ec488d2723cfc1def2c9713b0919b29819074b61&scene=27,一门面向所有人的人工智能公开课：MIT 6.S191，深度学习入门,"对初学者来说，有没有易于上手，使用流行神经网络框架进行教学的深度学习课程？近日，麻省理工学院（MIT）正式开源了在线介绍性课程「MIT 6.S191：深度学习入门」。该课程包括一系列有关神经网络及其在序列建模、计算机视觉、生成模型和强化学习等方面的基本应用知识。 课程链接：http://introtodeeplearning.com/ 课程视频：https://www.youtube.com/watch?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&v=JN6H4rQvwgY 课程 GitHub：https://github.com/aamini/introtodeeplearning_labs MIT 6.S191： MIT 关于深度学习算法及其应用的官方入门课程 MIT 6.S191 不仅仅是一门深度学习课程系列。在设计它时，我们希望做的更多。我们想要听众具备必须的实践技能，部署自己的深度学习模型，并将其应用到课程之外倍感兴奋、深受启发的问题上。 因此，在这门课程的教学中，我们选用了 TensorFlow。我们设计了两个基于 TensorFlow 的软件 lab 作为演讲课程的补充，一个是通过循环神经网络聚焦于音乐生成，另一个则专注于医疗图像的气胸检测。 MIT 6.S191 的课程设计原则是尽可能地易于上手，不仅针对于不同背景不同水平的人，也针对于 MIT 社区内外的人。 相应地，首个 lab 介绍 TensorFlow 基础——构建和执行计算图、sessions 以及在深度学习中常用的一般操作，同样也会重点介绍 TensorFlow 最新的重大更新：Eager mode。 这些背景知识有助于学生在 TensorFlow 中构建音乐生成和气胸检测模型。 循环神经网络（RNN）多是应用于序列建模和预测任务，诸如从股票趋势到自然语言处理，再到医疗信号识别（比如心电图），范围异常广泛。你可以查看课程中的深度序列建模一节以了解 RNN 及其应用的相关背景知识。 RNN 同样适用于音乐生成，可以捕捉时序数据中的依赖关系。在第一个 lab 中，学生致力于编码音乐文件数据集，界定 TensorFlow 中的 RNN 模型，并从中采样以生成全新的音乐。 音乐生成 RNN 模型 该数据集是一个流行音乐片段的集合，被编码成向量格式以馈送给 RNN 模型。一旦数据被处理，下一步则是使用该数据集定义和训练 RNN 模型。 该模型基于单个 LSTM 模块，其中状态向量追踪两个连续节点之间的时间依赖关系。在每一时间步，先前节点的序列被馈送进模块，LSTM 最后单元的最后输出馈送给全连接层。因此在先前所有时间步的节点给定的情况下，我们可以输出下一节点在时间步 t 上的概率分布。下图是这一过程的可视化。 我们为学生提供构建 RNN 模型并定义合适的计算图的指导。再次，我们已经设计这些 lab，供有兴趣的人使用，不管其有无 TensorFlow 经验。 该 lab 首先致力于设置相关超参数，定义占位符变量，初始化 RNN 模型的权重。学生接着定义其自己的 RNN(input_vec, weights, biases)，它接受相应的输入变量并定义一个计算图。 Lab 允许学生试验不同的损失函数，优化方案，甚至是准确度指标： 生成新音乐 乐趣并不止于构建和训练 RNN！毕竟，该 lab 是关于音乐生成——剩下的是使用 RNN 实际地创建新音乐。 该 Lab 指导学生把一个种子馈送给已训练的模型（毕竟如果没有什么开始的话，它无法预测任何新的音符！），接着使用已训练的 RNN 迭代地预测每一个连续的音符。这相当于从 RNN 在每个时间步输出的下一个音符的概率分布中随机抽样，然后使用这些样本生成一首新歌。 像以前一样，我们只给学生一个指导架构，定义采样由其自己决定。 为了提供已生成歌曲的一个采样，我们继续并训练模型，接着从中采样生成新歌。试听一下由已训练模型生成的一个实例：https://soundcloud.com/alexanderamini/mit-6s191-rnn-song。 生成更多逼真的音乐 你很可能已发现，这里还有大量的提升空间。我们希望学生继续通过我们提供的框架，调试超参数，扩充数据集来生成更加悦耳的音乐。 第二个 lab 补充了课程中用于计算机视觉的深度学习一节。学生有机会在逼真的医疗图像数据集上使用 CNN 检测疾病。特别地，学生使用一组真实的胸部 X 射线构建模型，检测和分类被预测有气胸的扫描，这种情况发生在肺与胸壁之间的空气量异常的情况下。 我们把这一 lab 推进到分类之外，以尝试解决可解释性的问题——什么是反映网络为什么和如何把一个特定类别标签分配给指定图像的量化指标。为解决这一问题，学生部署了一项称之为类别激活映射的特征可视化技术，以获得对区分性图像区域的理解。 数据集 在这里，我们使用了 ChestXRay 数据集的一个子集。顾名思义，这是一个大型 X 射线胸透照片数据集，并标有相应的诊断信息。 鉴于它是一个基于真实世界信息的数据集，其中含有大量噪音。在这里，我们希望让学生们能够使用真实的数据，以便了解包装和注释数据会面临的挑战——特别是在计算机视觉的背景下。 CNN 模型 学生们将使用与训练的 CNN 模型展开气胸疾病检测任务；在这里，我们不会让神经网络保持黑箱状态，而是会提供模型和训练模型调用的代码，并希望以此能够让学习者充分参与整个实验。此外，lab 还将实现成本和预测函数，以及 CNN 分类器的评估指标（如 ROC 曲线）。 用 CAM 解释 CNN 的输出 这一 lab 的主要目的是应用类激活图（CAM）来解释 CNN 的输出。虽然目前已有很多用于图像分类的 CNN 模型资源，但我们发现很少有介绍可解释性的 lab。但对于初学者而言，认识并接受深度学习局限性是非常重要的——这些都是本 lab 的一部分，也是整个课程的一部分。将 CAM 结合到实验中也为学生们提供了阅读和上手实践最新研究成果的机会，这会是一种非常有意义的体验。 CAM 是一种可视化图片在 CNN 中最后的卷积层上被「注意」区域的方法。请注意，CAM 可视化适用于在最终全连接层之前具有全局平均池化层的架构，其中我们输出最后一个卷积层上每个单元的特征映射的空间平均值。 CAM 有效地高亮了输入图像中分配特定类别标签最重要的部分。也可以直观地说：一个类的 CAM 是基于每个特征图，将图像分配给该类的重要性。CNN 中的特征映射反映了图像中特定视觉图案（即特征）的存在。我们通过对特征映射的重要性加权的特征映射和来计算 CAM。因此，在重要信道中具有更大激活的输入图像的区域在 CAM 中被赋予了更大的权重，因此显得「更热」。 在气胸分类器的背景下，这相当于强调胸透照片中识别（或未识别到）气胸最为重要的那些像素。 为了进行具体说明，我们让 F_k 代表 k-th 在卷积神经网络最后一个卷积层上的特征图，然后让 w_k 代表 k-th 在特征凸和全连接层上的权重。于是，用于气胸检测的类激活图为： 在对最终的类激活图进行上采样以后，我们可以把胸透照片中与气胸检测最相关的区域可视化（至少是从神经网络的角度看）。 该 lab 从头到尾演示了 CAM 在 TensorFlow 中计算和可视化的整个过程。学生们需要定义函数来提取特征图，计算 CAM 的权重： 在这里，学生们需要将从最后的卷积层中提取的 Feature_maps 输入，并从全连接层 dense_weights 输入到 CAM 计算的函数中，然后定义上采样过程。 正如气胸阳性的胸透照片示例所展示的那样，CAM 最终可以可视化为一张热图。 或许这个 lab 最有意思的部分是它所引发的讨论。学生们需要仔细研究模型对输入胸透照片进行错误分类的实例，CAM 在这些实例中的样子，并思考自己可以对模型做出哪些修改来突破这些限制。构建一种可以「窥探」神经网络内部运行机制的算法可以帮助激发学生们的好奇心，并让他们体会到机器学习中可解释性的重要性。 以上这些教学 Lab 都是 MIT 6.S191 系列课程所独有的，授课者为本课程进行了专门设计。目前，所有 MIT 6.S191 课程的视频都可以在网上免费观看了。  原文链接：https://medium.com/tensorflow/mit-6-s191-introduction-to-deep-learning-24994d705aca IJCAI2018 阿里妈妈国际广告算法大赛于 2018 年 2 月正式启动，获奖队伍将有机会前往斯德哥尔摩参加 IJCAI2018。 "
13,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740517&idx=2&sn=1cf855299c42bcc930265f2f93696a12&chksm=871ad35bb06d5a4dd7cf445172332a4625806d28a4fefe4eefcf9e1b9a81e9106e9dd00a3fa6&scene=27,教程 | 可视化CapsNet，详解Hinton等人提出的胶囊概念与原理,"CapsNet 将神经元的标量输出转换为向量输出提高了表征能力，我们不仅能用它表示图像是否有某个特征，同时还能表示这个特征的旋转和位置等物理特征。本文从可视化的角度出发详解释了 CapsNet 的原理的计算过程，非常有利于直观理解它的结构。 尽管卷积神经网络已经做出了令人惊艳的成绩，但还是存在着一些根本性问题。是时候开始思考新的解决方案和改进了。现在就让我们一起来了解一下胶囊网络（capsules networks）。 在之前的文章中我曾简要地讨论过胶囊网络（https://hackernoon.com/capsule-networks-are-shaking-up-ai-heres-how-to-use-them-c233a0971952）是如何解决一些传统问题的。在过去的几个月里，我一直沉浸在各种各样的胶囊网络里。我觉得现在是时候一起更加深入地探索胶囊网络的实际运作方式了。 为了让后面的讨论更加容易，我开发了一款与胶囊网络实现配套的可视化工具，它能够让您看到网络的每一层是如何工作的。这些内容都可以在 GitHub 上找到（https://github.com/bourdakos1/CapsNet-Visualization）。 如下所示是 CapsNet 的结构。如果您现在还不理解每个部分的具体含义，不必担心。我会尽我所能一层一层地进行详细讨论。 part 0: 网络输入 胶囊网络（CapsNet）的输入是提供给神经网络的真实图片。在这个例子中输入的图片长宽都为 28 个像素。一张图片实际上有 3 个维度用来存储不同颜色通道的信息。 因为是灰度图，而用作示例的图片仅仅有一个颜色通道。大家所熟悉的大多数图片都有 3 或 4 个通道用来存储红-绿-蓝和可能用于存储灰度或透明度的额外通道。 每个像素都被表示为 0 到 255 之间的一个数字并且被存储在一个 28x28x1 的矩阵 [28, 28, 1] 里。每一个像素的颜色越明亮，其数值越大。 Part 1a: 卷积 胶囊网络的第一个部分是传统的卷积网络。那么什么是卷积网络，它是怎么工作，而又有什么作用呢？ 我们的目标是要从输入图像中提取一些非常基础的特征，比如边缘或者曲线。 那么我们是怎么做到的呢？ 让我们来思考一个边缘情况： 如果我们看到这个图片上的一些点，我们就能够从中发现出一种模式。而现在我们关注于这些点左右两侧的颜色： 你也许会注意到当这个点在边缘时，其两侧颜色之间的差别会很大。 如果我们纵览图片中的每一个像素并且用它和左右点之间的差异值来替换掉原始的值会发生什么？理论上这个图片除边缘外的部分会全部变成黑色。 我们可以通过循环来遍历图片中每一个像素并进行上述处理： for in 但是这是比较低效的。而实际上，我们可以使用卷积操作。更技术地来讲，这其实是「互相关」，但大家更喜欢称之为卷积。 本质上卷积操作和上述循环的效果几乎是一样的，但它可以充分利用矩阵运算的优势来提高效率。 卷积操作一开始会在图像的一角设置一个小的窗口，然后移动它遍历整幅图像。在每一个位置我们都仅仅关注窗口所覆盖的像素并且将其中全部的像素值与对应的权重相乘并求和。 而这个窗口实质上是一个被称为「卷积核」的权重矩阵。 我们仅仅关注 2 个像素，但是当我们把它周围的窗口打包起来的时候，就可以让他们之间的像素变成胶囊。 你能够找到一组权重与这些像素相乘并求和得到我们想要的值吗？ Spoilers below! 我们可以进行这样的操作： 有了这些权重，我们可以得到卷积核： kernel = [1 0 -1] 当然，卷积核通常来说是方阵，所以我们可以用 0 来填充其他位置： 这里有一个很棒的动态图来描述卷积的运算过程： 在步幅为 1 的情况下，输出的维度为输入的维度减去卷积核的维度再加 1，比如 (7—3) + 1 = 5（更多相关内容请看下一小节）。 经过卷积变换后的图片如下所示： 你也许会注意到在变换后的图片中一些边缘丢失了。具体来说，是那些水平方向的边缘。为了突出这些信息，我们需要另外一个卷积核来处理像素上方和下方的信息。比如： 此外，这两个卷积核在处理其他的角度或者模糊的边界时都无法获得很好的效果。因此我们要使用多个卷积核（在我们的胶囊网络实现中，我们使用了 256 个卷积核）并且卷积核一般有更大的尺寸以增加处理的空间（我们使用 9x9 的的卷积核）。 其中一个经过训练后的卷积核如下所示。虽然不是很明显，但我们还是可以看出它是一个更加鲁棒的放大版边缘探测器。它仅仅用来找到那些从亮变暗的边缘。 注意：我对所有的值都进行了取整，因为他们太长了，比如：0.01783941。 幸运的是，我们不需要手动地设计这些卷积核。训练过程可以自动完成这一工作。所有的卷积核一开始都是空的（或者随机初始化），而在训练过程中他们被不断调整使得最终的输出和我们的目标更接近。 如下所示是经过训练后最终得到的 256 个卷积核（为了方便理解我给他们上了色）。我们用蓝色来表示负数, 用 0 来表示绿色，用黄色来表示正数. 并且颜色越强，其绝对值越大。 用所有的卷积核处理完图片后，我们可以得到一个含有 256 张输出图片的栈。 Part 1b: 线性整流函数 ReLU（线性整流函数）也许听起来很复杂，但实际上非常简单。作为一个激活函数，它对输入的值进行处理然后输出。如果输入的值是负数，那么输出为 0，如果输入的值是正数，那么输出和输入完全一致。 代码如下： 如图所示： 我们用这个函数对所有卷积输出进行处理。 为什么我们要这么做？因为如果我们不使用激活函数对神经元层的输出进行处理，那么整个网络就可以被描述为一个线性的函数，这样一来我们所有的努力就都失去意义了。 添加一个非线性的部分使得我们可以描述所有种类的函数。我们可以使用很多不同种类的函数来作为激活函数，只是 ReLU 是最流行的一种，因为它使用起来最方便。 第一个使用了 ReLU 的卷积层输出如下所示： Part 2a: 初级胶囊层（PrimaryCaps） 一开始我们使用一个普通的卷积层作为初级胶囊层。但这次我们要对前面输出的 256 个特征图进行操作，因此我们使用一个 9x9x256 的卷积核而不是 9x9x1 的卷积核。 那么我们究竟想要得到什么？ 在第一个卷积层我们在寻找简单的边角和曲线。现在我们希望从上一步得到的边缘信息中找到更加复杂一点的形状。 这次我们的步长是 2，即每次移动 2 个像素而不是 1 个。使用更大的步长可以让我们更快地降低输出尺寸。 注意：一般来说，输出的维度是 12，但由于我们的步长为 2，所以输出向量的维度减半。比如：((20—9) + 1) / 2 = 6。 我们会对输出再进行 256 次卷积操作，这样我们最终可以得到一个含有 256 个输出（6x6）的栈。 但这一次我们想要的不仅仅是一些糟糕而普通的旧数据。 我们要把这个栈切分成 32 层，其中每层有 8 个块， 我们称之为「胶囊层」， 每个胶囊层有 36 个「胶囊」。 更进一步地说，这意味着每个胶囊含有一个拥有 8 个值的数组，而一般我们称之为向量。 我所要强调的是： 这些「胶囊」是我们新的像素。 对于单一的像素值来说，我们仅仅能够存储在特定位置是否有一个边角的置信度。数值越大，置信度越高。 而对于一个胶囊，我们可以在每个位置存储 8 个值！这让我们有机会存储更多的信息而不仅仅是是否在某个位置找到了某种形状。但我们想要存储的还有哪些信息呢？ 当看到下面的形状的时候，你能够发现什么？如果你需要在对方不能观看的情况下告诉他如何复原这个形状，你会怎么说？ 这个图片非常基础，所以我们仅仅需要描述很少的一些细节。： 形状的类型 位置 旋转 颜色 尺寸 我们可以称之为「实例化参数」。对于更复杂的图片我们最终需要更多的细节。他们可以包括姿态（位置、尺寸、方向、）、畸变、速度、反照率、色调、质地等等。 你也许还记得当我们为边缘检测设计卷积核的时候，它只会在某一个具体的角度起作用，因此对于每一个角度我们都需要一个卷积核。而事实上我们完全可以摆脱上述的过程，因为当我们在处理边缘时，描述边缘的方式是非常有限的。 当我们在处理形状这一层次的特征时，我们不想给每一个角度的长方形、三角形、椭圆等等都去设计对应的卷积核。这太不明智了，尤其是当处理像光线一样有 3 个维度的旋转和特征的更复杂的形状时更是如此。 而这恰恰正是传统的神经网络无法很好地处理微小旋转的原因。 当我们从边缘到形状，再从形状到物体传递特征的时候，如果能够有更多的空间去存储额外有用的信息将会是很有帮助的。 如下是两个胶囊层（一个用来处理长方形一个用来处理三角形）和 2 个传统像素输出的一个简化版对比： 与传统的 2 维或者 3 维的向量类似，这个向量也有角度和长度。长度描述概率而角度描述实例化的参数。在上面的例子中，这个角度实际上和形状的角度是一致的，但通常来说不一定是这样。 在实际中想要可视化展示上述的向量是不可行或者至少是非常困难的，因为他们有 8 个维度。 那么既然我们在胶囊中存储了这些额外的信息，我们应该可以在此基础上重构出最初的图片。 这听起来很不错，但我们要怎样才能使得网络学习到这些内容呢？ 在训练传统卷积神经网络的时候，我们仅仅关心模型是否能够预测正确的类别。而在训练胶囊网络的时候，我们有另外的一种叫做「重构」的方法。每次重构都尝试仅仅使用我们给出的向量复原原始图片。然后我们使用重构图片和目标图片的接近程度对模型进行评估。 这里有一个简单的例子。而在接下来的部分中我会介绍更多的细节。 Part 2b: 非线性变换 Squashing 在得到胶囊之后，我们会再对其进行一次非线性变换（比如 ReLU），但这次的公式比之前略微难懂。这个函数成比例扩大了向量的值，使得它在角度不变的情况下长度有所增加。这样我们就可以让向量分布在 0 和 1 之间从而实际上获得一个概率值。 经过压缩操作后胶囊向量的长度如下所示。在这时想要猜出每个胶囊的目标几乎是不可能的。 要注意的是每个像素实际上都是一个长度为 8 的向量。 Part 3: 一致性路由 接下来的一步是决定将要被传递给下一个层级的信息。在传统的网络中，我们也许会使用类似于「最大池化」的一些方式。最大池化通过仅仅传递某一区域中激活值最大的像素到下一层的方式来降维。 然而，在胶囊网络中，我们将使用一种被称作「一致性路由」的方式。其中每一个胶囊都试图基于它自己猜测下一层神经元的激活情况。 看到这些预测并且在不知道输入的情况下，你将选择把哪一个物体传递给下一层呢？也许是船，长方形的胶囊和三角形的胶囊都在船应该是什么样子上达成了一致。但他们并没有在预测房子的样子上达成一致。所以很有可能这个物体不是一个房子。 通过一致性路由，我们仅仅将有用的信息传递给下一层并且丢弃掉那些可能使结果中存在噪音的数据。这让我们能够做出更加智能的选择而不仅仅是像最大池化一样选择最大的数字。 在传统网络中，错位的信息并不会造成什么困惑。 而在胶囊网络中，这些特征相互之间将不会达成一致： 正如我们所期望的那样，这种方式在直觉上是可行的。而在数学上，它究竟是怎么运作的呢？ 我们的目标是预测 10 个不同的数字（每个数字为一类） 注意：在船和房子的例子中，我们的目标是预测两个物体，而现在我们要预测 10 个。 与船和房子的例子不同，在这里我们不是要预测实际的图片，而是要试图预测描述图片的向量。 通过计算向量和每一个类别的权重矩阵的乘积，我们可以获得胶囊对于每一个类的预测结果。 注意我们有 32 个胶囊层，并且每个胶囊层有 36 个胶囊。这意味着我们总共有 1152 个胶囊。 经过计算，最终你将得到一个含有 11520 个预测值的列表。 每个权重实际上是一个 16x8 的矩阵，所以每个预测都是胶囊向量同这个权重矩阵的乘积。 正如你看到的那样，我们的预测结果是一个 16 维的向量。 维度 16 是从何而来呢？这是一个任意的选择，就像我们最初使用 8 个胶囊一样。 但需要注意的是，我们如果我们想要选择更深层的胶囊网络，我们就要扩大胶囊的维度。从直觉上来说，这是可行的。因为我们使用的网络越深，特征表达就越复杂，需要我们再现的参数就更多。举例来说，描述一整张脸比描述一只眼睛需要更多的信息。 下一步是要找到在这 11520 个预测中和其他预测一致性最高的内容。 想要可视化高维的向量是很有难度的一件事情，为了更符合人的直觉，我们假设这些向量仅仅是 2 维空间中的点。 首先我们计算所有点的平均值。每个点在最初都被赋予了同样的重要性。 接下来我们可以测量每个点和平均值点之间的距离。距离越远的点，其重要程度就越低。 然后我们在考虑每个点的不同的重要性，重新计算平均值： 正如你能够看到的，随着我们重复进行这个循环，那些和其他点不一致的的点开始消失。而那些相互之间高度一致的点则最终将被传递给激活值最高的的下一层。 Part 4: DigitCaps 达成一致后，我们最终可以得到 10 个 16 维的向量，每个向量都和一个数字相对应。这个矩阵就是我们最终的预测结果。这个向量的长度是数字被找出的置信度——越长越好。这个向量也可以被用于生成输入图片的重构。 这就是在输入为 4 的情况下向量的长度分布情况。 第五个方块区域是最明亮的，意味着较高的置信度。注意数字 0 是第一类，所以我们在这里给出的预测是数字 4. Part 5: 重构 这个代码实现中重构的部分相对比较简单，它仅仅是一个多层的全连接网络，但重构本身的过程是非常有趣的。 如果我们重构 4 个输入向量，我们将会得到： 如果我们调整这些滑动器，我们可以看到每一个维度是如何影响这 4 个输入的： 我推荐大家下载使用这个可视化工具来观察在不同输入下滑动向量数值是如何影响重构过程的。 clone cd 运行可视化工具 接下来用浏览器访问 http://localhost:5000 (http://localhost:5000/) 总结 我认为对于胶囊网络的重构是令人惊叹的。考虑到我们仅仅使用了一个简单的数据集来训练当前的模型，这让我不由期待经由大量数据训练的成熟胶囊网络结构，及其效果。 我非常期待看到控制更为复杂图像的重构向量将对模型产生怎样的影响。因此在接下来的工作中，我将在 CIFAR 和 smallNORB 数据上对胶囊网络进行测试。 "
14,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740555&idx=2&sn=0235fd83469d71a9ac3bc09aa56f4f84&chksm=871ad335b06d5a234ed0425062a0597b086d79754b976178be6b37b5b4b07f87fd8bc3102fb3&scene=27,观点 | Facebook数据泄露事件之后，还有哪些AI危机在等着我们？,自今年三月以来，由 Facebook 数据泄漏、「大数据杀熟」等事件引起，人们对于数据隐私和人工智能技术未来的担忧等话题的讨论突然又迎来了一轮热潮。这些事件正告诉我们，隐私数据及对其加以利用而形成的人工智能技术不仅可以影响国内国外，而且已经深入了生活的很多部分。 最近，谷歌研究员，深度学习库 Keras 作者 François Chollet 对 Facebook 事件发声，阐述了自己对于 AI 发展的担忧和建议。 社交网络服务正越来越多地控制我们消费的信息，我们从新闻流中看到的东西已经变成了算法的「策划」。社交媒体算法会越来越多地决定我们阅读哪些文章、看到哪些电影预告片、我们保持联系的对象，以及我们收集到的，表达意见的反馈。——François Chollet 声明： 这些是我个人的观点，不是替我的雇主发言。如果你引用这篇文章，请如实地陈述这些观点：它们是个人的、推测性的，是非对错请自行评判。 如果你出生在 20 世纪 80 年代和 90 年代左右，可能还记得现在已经绝迹的「电脑恐惧症」，在 21 世纪初的一段时间，我亲眼目睹过这一现象——随着个人电脑进入我们的生活，出现在工作场所和家里，很多人都表现地很焦虑、恐惧甚至出现攻击行为。虽然我们中的一些人被计算机所吸引，并对它们潜在的潜力充满敬畏，但大多数人并不了解它们。他们觉得它们很奇怪和深奥，在许多方面会受到威胁。人们担心会被技术取代。 我们中的大多数人对技术转变很反感，甚至很恐慌。或许任何变化都会导致这种现象。但很明显，我们担心的大部分事情最终都不会发生。 一晃几年过去，电脑反对者已经学会了生活在电脑的时代并享受它们带来的便捷。计算机并没有取代我们，也没有引发大规模的失业——现在我们不能想象没有笔记本电脑、平板电脑和智能手机的生活。可能会「带来威胁」的变化已经转变成为带来舒适的现状。但与此同时，由于我们不再担心，电脑和互联网的出现对我们产生了威胁，这些威胁几乎没有人在 20 世纪 80 年代和 90 年代警告过我们。无处不在的大规模监视，黑客对我们设备或个人数据追踪，社交媒体的心理异化，我们耐心和专注能力的丧失，易受影响的政治或宗教激进化的思想，敌对的外国实力通过社交网络对西方民主国家的破坏。 如果我们大部分的恐惧都被认为是荒谬的，相反地，过去由于技术变化而发生的真正令人担心的发展都是发生前大多数人的不担心造成的。一百年前，我们无法真正预测到我们正在发展的运输和制造技术会导致一场新形式的工业战争，在两次世界大战中摧毁数千万人。我们并没有早早意识到收音机的发明会促成一种新的大众宣传形式，促进了意大利和德国的法西斯主义的兴起。20 世纪 20 年代和 30 年代理论物理学的进展并没有伴随着关于这些发展如何能够形成核武器，使世界永远处于即将毁灭的威胁之下的报刊文章。而现在，即使数十年来关于我们这个时代最严重的问题——气候——已经迫在眉睫，美国大部分的公众（44%）仍选择忽视它。作为一个文明，我们似乎很难正确识别未来的威胁并对它们有合理的担心，正如我们似乎非常容易因荒谬的恐惧而出现恐慌一样。 现在，与过去很多次一样，我们正面临着一场根本性变革的新浪潮：认知自动化，可由关键字「AI」大致概括。就像过去很多时候一样，我们担心这种新技术会伤害我们——人工智能会导致大规模失业，或人工智能会有自己的力量，变成超人，并选择摧毁我们。 但如果我们担心错误的事情，就像之前我们每次担心的一样呢？如果人工智能的真正危险远远不是许多人今天惊慌的「超级智能」和「奇点」观点呢？在这篇文章中，我想提高人们对人工智能真正担心的问题的认识：人工智能所实现人类行为的高效、高度可扩展的操纵，以及企业和政府的恶意使用。当然，这不是认知技术发展产生的唯一的有形风险——还有很多其他的，特别是与机器学习模型在害偏见有关的问题。其他人比我对这些问题更加警惕。我选择写有关关于大规模人口操纵的文章是因为我认为这种风险很迫切而且不甚明显。 现在这种风险已成为现实，并且在接下来的几十年里，一些长期的技术趋势将会进一步放大这种风险。随着我们的生活越来越数字化，社交媒体公司越来越了解我们的生活和思想。与此同时，他们越来越多地通过行为控制向量，特别是通过算法新闻，控制我们的信息消费。这将人类行为变成了一个优化问题，一个人工智能问题：社交媒体公司可以迭代调整其控制向量以实现特定行为，就像游戏人工智能会迭代改进其游戏策略来得分升级。这个过程中唯一的瓶颈是循环中的算法智能，正如发生的这样，最大的社交网络公司目前正在投入数十亿美元进行基础人工智能研究。 我来详细解释一下。 社交媒体作为心理学监狱 在过去的 20 年，我们的私人和公共生活已经向网上转移。我们每天花更多时间盯着屏幕，世界正在转向数字信息消费、修改或创造的阶段。 这种长期趋势的副作用是公司和政府正在收集有关我们的大量数据，特别是通过社交网络服务。我们与谁交流，我们说了什么，我们一直在消费什么（图像、电影、音乐和新闻），我们在特定时间的心情。最终，几乎我们感知的所有东西和我们所做的一切都会记录在某个远程服务器上。 理论上这些数据使收集它的对象为个人和团队建立非常准确的心理模型。你的观点和行为可以与成千上万的类似的人交叉关联，使你对你选择的内容产生不可思议的理解，可能比通过单纯的反思可实现的预测性更强（例如，Facebook 通过算法评估出的你的个性比你的朋友更准）。这些数据可以提前几天预测你何时开始新的关系（以及和谁），以及何时结束当前的关系。或谁有自杀的危险。或你在一个选举中即使在犹豫不决的情况下最终会投哪一方。而且这不仅仅能对个人级别进行分析，而且对大型群体更有预测性，因为平均行为会排除随机性和个人异常值。 数字信息消费作为一种心理控制向量 这并不会停留在被动的数据收集消费。社交网络服务正越来越多地控制我们消费的信息，我们从新闻流中看到的东西已经变成了算法的「策划」。不透明的社交媒体算法会越来越多地决定我们阅读哪些文章、看到哪些电影预告片、我们保持联系的对象，以及我们收集到的，表达意见的反馈。 通过多年的曝光，我们消费信息的算法处理使算法对我们的生活掌握了相当大的权力，包括我们是谁，我们成为了谁。如果 Facebook 长时间决定你看的新闻（真实的或假冒的），你看的政治地位的变动，以及看你新闻的人，那 Facebook 就会实际控制你的世界观和政治信仰。 Facebook 的业务根基在于影响人心，也就是向客户推销服务——广告，包括政治广告。因此，Facebook 已经建立了一个微调算法引擎来实现。这款引擎不仅能够影响你下一次购买哪个品牌的智能音箱，还可以影响你的情绪，通过调整给你呈现的内容来随意地让你生气或快乐。它甚至可以改变选举。 人类行为作为优化问题 总之，社交网络公司可以同时了解我们的一切，并控制我们的消费信息。这个趋势正在加速，当你可以接触感知和行动时，那就是 AI 问题。你可以开始建立一个针对人类行为的优化循环，在这个循环中，你可以观察目标的状态并不断调整提供给你的信息，直到开始观察你想要查看的选择和行为。人工智能领域的一大部分，特别是「强化学习」，是尽可能有效解决这些优化问题的算法研究方向，趋向于闭环流程，对手头的目标进行完全控制，也就是对我们进行完全控制。通过把我们的生活转移到数字领域，我们更容易受到人工智能算法的影响。 由于人类的思想非常容易受到社交操作的简单模式的影响，所以这一切变得更加容易。例如，考虑以下的攻击向量： 身份强化：这是历史上最早的广告就开始利用的老技巧，仍和之前的效果一样好，包括将给定的看法与您确定的标记相关联（或希望你做了），从而让你自动掌握目标的想法。在人工智能优化社交媒体消费的背景下，控制算法可以确保你只能看到它希望你看到的和你自己的身份标记共同出现的观点，相反地，它不希望你看到算法不想让你看到的观点。 消极的社交强化：如果你发表了一篇文章，表达了控制算法不希望你持有的观点，系统可以选择只将你的帖子呈现给那些持有相反观点或极度批判这些观点的人（可能是熟人，陌生人或机器人）。反复多次，这种社会反弹可能使你偏离你的初始观点。 积极的社交强化：如果你发表了一篇文章，表达了控制算法希望传播的观点，那么它可以选择将它只呈现给「喜欢」它的人（甚至可能是机器人）。这会使你的信念加强，让你觉得你是支持大多数人的一部分。 取样偏差：算法也可能向你呈现你朋友（更可能是媒体）发布的支持算法希望你持有的观点的帖子。置身于这样一个信息泡沫中，你会觉得这些观点获得的支持比实际更多。 参数个性化：该算法可能会观察将特定的内容呈现给某些心理属性与你相近的人，导致它想要的观点上的转换。然后，它可能给你呈现一些对于具有你的特定观点和生活经验的人而言会最有效的内容。长期如此，该算法甚至可以从头开始生成这些最有效的内容，尤其对于你来说。 从信息安全的角度来看，你会称这些漏洞为：可用于接管系统的已知漏洞。就人类头脑来说，这些漏洞永远不会被修补，它们只是我们工作的方式，存在于我们的 DNA 中。人类的思维是一个静态的、易受攻击的系统，会越来越多地受到更加聪明的人工智能算法的攻击，这些算法将同时查看我们所做的和相信的所有事情，并完全控制我们消费的信息。 当前的情况 值得注意的是，将人工智能算法放在我们的信息饮食中引起的大规模人口操纵，特别是政治控制，不一定需要非常先进的人工智能。你不需要自我意识，超级人工智能是一个可怕的威胁——即使目前的技术可能就已经足够了。社交网络公司已经研究了几年，并取得了显著的成果。虽然他们可能只是试图最大化「参与」并影响你的购买决定，而不是操纵你的世界观，但他们开发的工具已经被敌对国家为了政治目的而劫持——如 2016 年英国脱欧公投或 2016 年美国总统选举。这已经是现实，但如果现在大规模人口操纵已经成为可能，那理论上来说，为什么世界还没有被颠覆呢？ 简而言之，我认为这是因为我们对人工智能的了解还不够，但可能马上就不是这样了。 直到 2015 年，业界的所有广告定位算法都只是在逻辑回归上运行。事实上，很大程度上现在仍然如此，只有最大的玩家转向了最先进的模式。Logistic 回归出现在计算时代之前，是用于个性化的最基本的技术之一。这就是为什么你对网上看到的这么多广告都不感兴趣的原因。同样，敌对国家用来影响公众舆论的社交媒体机器人几乎没有用 AI。目前它们都非常原始。 近年来机器学习和人工智能取得了飞速的进展，才刚刚开始部署针对算法和社交媒体机器人。2016 年机器学习才开始进入新闻传播和广告网络，谁也不知道接下来会发生什么。Facebook 在 AI 研究和开发方面投入了大量资金，它明确的目标是成为该领域的领导者，这一点非常惊人。当你的产品是社交新闻推送时，自然语言处理和强化学习有什么用呢？ 我们正在看着一家这样的公司，该公司可为近 20 亿人类构建细致的心理特征模型，作为其中许多人的主要新闻来源，运行大规模的行为操纵实验，旨在开发目前为止的最佳人工智能技术。对我来说，它令我害怕，而且我认为 Facebook 甚至可能不是最令人担心的威胁。许多人喜欢假装大公司是现代世界的全能统治者，但他们拥有的权力与政府相差甚远。如果对我们的思想进行算法控制，政府可能会成为最差的行为者，而不是公司。 现在，我们能做些什么呢？我们如何保护自己？作为技术人员，我们做什么才可以通过我们的社交新闻推送来避免大规模操纵的风险？ 硬币的另一面：AI可以为我们做什么 重要的是，这种威胁的存在并不意味着所有的算法策略都不好，或者说所有的目标广告都不好。相反，这两者都可以起到有价值的作用。 随着互联网和人工智能的兴起，将算法应用于我们的信息获取路径不仅仅是必然的趋势，而且是一个理想的趋势。随着我们的生活越来越数字化和互联，信息越来越密集，我们需要人工智能作为我们与世界的接口。长远来看，教育和自我发展将是人工智能最有影响力的应用之一，这将是动态的，这些动态变化几乎完全反映了试图操纵你的恶意 AI 支持的新闻推送。算法信息管理帮助我们的潜力很大，使人们更好地发挥他们的个人潜力，并帮助更好地管理社会。 这个问题不是 AI 本身，问题在于控制。 我们不应该让新闻推送算法操纵用户实现不透明的目标，比如动摇他们的政治观点，或者极大地浪费他们的时间，而应该让用户控制算法优化的目标。毕竟，我们谈论的是，关于你的新闻、你的世界观、你的朋友、你的生活，技术对你的影响应该由你自己来控制。信息管理算法不应该是为了服务与我们自身利益相反的目标而设计的神秘力量；相反，他们应该成为我们手中的工具，一种可以实现我们目的的工具，比如教育和个人而不是娱乐。 这里有一个想法——任何大规模采用的算法新闻采访应该： 透明地传达推送算法目前优化的目标，以及这些目标如何影响你的信息获取。 给你设定这些目标的直观工具，例如，应该可以配置新闻源在特定的方向最大限度地提高学习和个人成长。 具备始终可见的特性，测量你在 Feed 上花费的时间。 具备工具可以控制在优化 Feed 上花费的时间，例如每日的时间目标，算法将通过此目标设计让你脱离优化 Feed。 我们应该建立人工智能来为人类服务，而不是为了利润或政治利益来操纵它们。如果新闻算法不像赌场运营商或宣传人员那样运作，事情会变成什么样？如果它们更接近一位导师或一个好的图书管理员，通过对你的心理以及其他数百万其他人员的心理的敏锐理解，向你推出一本最能引起你共鸣并使你成长的一本书。一种你生活的导航工具——人工智能可以引导你通过体验空间的最佳路径到你想去的地方。你能想象经历过数百万生命的系统的镜头来看待自己的生活吗？或者与一个读过每本书的系统一起写书？又或者与一个能看到当前人类知识全部范围的系统合作研究？ 在完全控制与你交互的 AI 产品中，更复杂的算法而不是威胁，将会是一个积极的方面，它可以让你更有效地实现自己的目标。 构建反脸书 总的来说，，未来人工智能将成为我们与数字信息组成的世界的接口。这同样赋予个人更大的生命控制权，甚至可能完全没有机构。不幸的是，今天的社交媒体正在走一条错路，我们很久之后才能扭转局面。 而业界需要开发激励使影响用户的算法受用户控制，而不是通过人工智能利用用户的思想来获取利润或政治利益的产品分类和市场。我们需要努力实现反脸书（anti-Facebook）的产品。 在遥远的未来，这些产品可能会是 AI 助手的形式。数字导师编程帮助你，使你控制与它们交互时想要达到的目标。在目前，搜索引擎可以被看作是 AI 驱动信息界面的早期的、更原始的例子，它为用户提供服务而不是试图绑架他们的心理世界。搜索是查找特定目标时特意用的工具，而不是被动地让你接收选举人向你推荐的东西。你应该辨别它可以为你做什么。搜索引擎应该尝试最小化从发生到解决，从问题到答案的时间，而不是最大化地浪费你的时间。 你可能会想，因为搜索引擎仍然是我们和我们消费的信息之间的 AI 层，它是否可以使它的结果更倾向于试图操纵我们？是的，每种信息管理算法都存在这种风险。但与社交网络形成鲜明对比的是，这种情况下的市场激励实际上与用户需求一致，使搜索引擎尽可能相关和客观。如果它们不能最大限度地发挥作用，那么用户转向竞争产品基本没有阻碍。重要的是，搜索引擎比社交新闻的心理攻击范围小的多。我们在这篇文章中描述的威胁需要产品中出现 以下大部分特点： 感知和行动：产品不仅应该控制显示给你的信息（新闻和社交更新），还应该能够通过「喜欢」、聊天信息和更新的状态来「感知」你当前的心理状态。没有感知和行动，就不能建立强化学习循环（reinforcement learning loop）。作为古典传播的潜在渠道，只读优化推送有害无益。 我们生活的中心性：产品应该是至少一部分用户的主要信息来源，典型的用户应该每天花费几个小时在上面。辅助和专业性的推送（feed）（如亚马逊的产品推荐）不会是一个严重的威胁。 社会的组成使心理控制向量（特别是社会强化）更广泛和有效，客观的新闻推送只占我们思想的一小部分。 业务激励意在操纵用户并使用户在产品上花费更多时间。 大多数 AI 驱动的信息管理产品都不符合这些要求。另一方面，社交网络是风险因素可怕的组合。作为技术专家，我们应该倾向于那些不具备这些特征的产品，抵制将它们结合在一起的产品，只要因为他们有可能发生危险的滥用行为。建立搜索引擎和数字助理而不是社会新闻源，使你的推荐引擎透明、可配置、富有建设性，而不是像投币口一样的机器——最大限度地提高「参与度」，浪费人们的时间。公司应该将精力投资在用户界面、用户体验和人工智能专业知识上，为你的算法构建好的配置面板，使你的用户能够按需使用你的产品。 重要的是，我们向用户普及这些问题，防止他们拒绝操纵产品，导致足够的市场压力使技术行业的激励机制与消费者的激励机制被迫调整。 结论：前方路上的岔路口 社交媒体不仅足够了解我们，可以为个人的团队建立强大的心理模型，还越来越多地控制我们的信息饮食。它有一系列有效的心理疗效，可以操纵我们相信的东西，我们的感受以及我们的所作所为。 一个足够先进的人工智能算法，可以连续循环访问我们的心理状态的感知和行动，可以用来有效地劫持我们的信仰和行为。 让 AI 作为我们的信息接口本身不是问题。这样人工智能交互界面，如果设计的好，有可能为我们所有人带来巨大的利益。关键是：用户应该完全控制算法的目标，将其用作实现自己目标的工具（与使用搜索引擎的方式相同）。 作为技术专家，我们有责任推销抵制不可控产品，并致力构建使用户控制的信息界面。 不要将 AI 用作操纵用户的工具；相反，使 AI 作为用户的工具，使其在很多情况下获得更多的代理权。 一条路通向让我十分害怕的地方，另一条会通向更人性化的未来。我们仍有时间选择更好的一条。如果你使用这些技术，请牢记一点，你可能没有邪恶的意图，你可能根本不在乎，或你可以仅仅评估自己的受限股权（RSU），而不关注我们共同的未来。但是无论你是否在意，你掌握了数字世界的基础设备，所以你的选择会影响我们所有人。你最终必须向所有这些负责。 
15,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740555&idx=5&sn=bda483b740a231c0b48d36922bf92465&chksm=871ad335b06d5a23b0a7c4801fbfb7026e8ac0fa6bba801f18605bcf47ecf2db51765c30f269&scene=27,报名 | INTERFACE#5袁进辉（老师木）：深度学习引擎的最优架构,自深度学习流行以来，深度学习框架也成为了各大公司重点发展研究的方向。而当地时间 3 月 30 日，最新一届谷歌 TensorFlow 开发者峰会将会召开，势必吸引众多关注。谷歌 TensorFlow、PyTorch、MXNet、百度 PaddlePaddle 等框架，哪一种才是最合适自己的？在框架之争火热的今天，我们邀请到了 OneFlow 的袁进辉老师为我们分析已有框架的优缺点，分享深度学习框架应该怎么做的观点。 演讲者：袁进辉（老师木） 讲者简介：袁进辉（老师木），2008 年 7 月自清华大学计算机系获得工学博士学位，获得清华大学优秀博士学位论文奖，在计算机视觉及多媒体领域顶级会议上发表多篇论文，连续多年获得美国国家技术标准局组织的视频检索评测比赛的第一名。2010 年负责研发斯诺克比赛「鹰眼」系统，该产品打败来自英国的竞品开始服务于各项国际大赛，并被中国国家队作为日常训练辅助系统。2012 年作为早期成员加入 360 搜索创业团队，一年后产品上线成为国内市场份额第二的搜索引擎。2013 年加入微软亚洲研究院从事大规模机器学习平台的研发工作。2014 年，发明了当时世界上最快的主题模型训练算法和系统 LightLDA，只用数十台服务器即可完成以前数千台服务器才能实现的大规模主题模型，该技术成功应用于微软在线广告系统，被当时主管研究的全球副总裁周以真称为「年度最好成果」。2015 年至 2016 年底，专注于搭建基于异构集群的深度学习平台，项目荣获微软亚洲研究院院长特别奖 (top 1%)。2017 年创立北京一流科技有限公司，致力于打造分布式深度学习平台的事实工业标准。 时间：2018.04.14 地点：北京市朝阳区酒仙桥东路电子城研发中心 A2 楼一层机器之心演讲厅 14:00-14:30 签到 14:30-15:30 嘉宾分享  15:30-16:00 现场提问、交流 
16,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740482&idx=3&sn=f678397cc87bd4405ba26944cbe1076a&chksm=871ad37cb06d5a6a603993f5055611612e4a4501fea007a4e3a41dfd9466d92b5c219483cd4f&scene=27,资源 | 你需要的Scikit-learn中文文档：步入机器学习的完美实践教程,"Scikit-learn 中文文档已经由 ApacheCN 完成校对，这对于国内机器学习用户有非常大的帮助。该文档自 2017 年 11 月初步完成校对，目前很多细节都已经得到完善。该中文文档包含了分类、回归、聚类和降维等经典机器学习任务，并提供了完整的使用教程与 API 注释。入门读者也可以借此文档与教程从实践出发进入数据科学与机器学习的领域。 中文文档地址：http://sklearn.apachecn.org Scikit-learn 是基于 Python 的开源机器学习库，它基于 NumPy 和 SciPy 等科学计算库，并支持支持向量机、随即森林、梯度提升树、K 均值聚类等学习算法。Scikit-learn 目前主要由社区成员自发进行维护，且专注于构建机器学习领域内经广泛验证的成熟算法。 Scikit-learn 项目最早为数据科学家 David Cournapeau 于 2007 年发起的 scikits.learn 项目，且 Scikit 的名字可视为 SciPy Toolkit，即 SciPy 的第三方扩展。Scikit-learn 大部分都是由 Python 构建，但还是有很多核心算法是由 Cython 完成而实现更好的效果，例如支持向量机就是由 Cython 构建。 在监督学习部分，Scikit-learn 提供了广义线性模型、支持向量机、最近邻算法、高斯过程、朴素贝叶斯、决策树和集成方法等算法教程，同时还介绍了特征选择、随即梯度下降算法、线性与二次判别分析等在监督学习中非常重要的概念。 除了监督学习，半监督学习中的标签传播算法和无监督学习中的聚类与降维算法都有非常多的教程。此外，在模型选择中，文档教程描述了交叉验证的使用、估计器超参数的调整、模型评估方法和模型持久化概念等。 数据预处理是机器学习非常重要的部分，我们可以使用归一化等方法大大降低前向传播与学习算法的计算复杂度，也可以利用缺失值插补和特征提取等方法增加数据的有效性。以下选取了 SVM 的部分使用教程，读者可借此了解 Scikit-learn 中文文档的组织形式与基本内容，更完整的内容前查看原文档。 支持向量机 (SVMs) 可用于以下监督学习算法分类、回归和异常检测。支持向量机的优势在于： 在高维空间中非常高效。 即使在数据维度比样本数量大的情况下仍然有效。 在决策函数（称为支持向量）中使用训练集的子集，因此它也是高效利用内存的。 通用性：不同的核函数 核函数 与特定的决策函数一一对应。 支持向量机的缺点包括： 如果特征数量比样本数量大得多，在选择核函数时要避免过拟合，而且正则化项是非常重要的。 支持向量机不直接提供概率估计，这些都是使用昂贵的五次交叉验算计算的。 在 scikit-learn 中，支持向量机提供 dense（numpy.ndarray , 可以通过 numpy.asarray 进行转换) 和 sparse（任何 scipy.sparse）样例向量作为输出。然而，要使用支持向量机来对 sparse 数据作预测，它必须已经拟合这样的数据。使用 C 代码的 numpy.ndarray (dense) 或者带有 dtype=float64 的 scipy.sparse.csr_matrix (sparse) 来优化性能。 分类 SVC、NuSVC 和 LinearSVC 能在数据集中实现多元分类： SVC 和 NuSVC 是相似的方法，但是接受稍许不同的参数设置并且有不同的数学方程。另一方面，LinearSVC 是另一个实现线性核函数的支持向量分类。记住 LinearSVC 不接受关键词 kernel，因为它被假设为线性的。它也缺少一些 SVC 和 NuSVC 的成员（members）比如 support_。 和其他分类器一样，SVC、NuSVC 和 LinearSVC 将两个数组作为输入：[n_samples, n_features] 大小的数组 X 作为训练样本，[n_samples] 大小的数组 y 作为类别标签 (字符串或者整数)： >>>  from import >>>  0 0 1 1 >>>  0 1 >>>  >>>  1.0 200 None 0.0 'ovr' 3 'auto' 'rbf' -1 False None True 0.001 False 在拟合后, 这个模型可以用来预测新的值： >>>  2. 2. 1 SVMs 决策函数取决于训练集的一些子集, 称作支持向量. 这些支持向量的部分特性可以在 support_vectors_、support_和 n_support 找到： >>>  # 获得支持向量 >>>  0. 0. 1. 1. >>>  # 获得支持向量的索引get indices of support vectors >>>  0 1 >>>  # 为每一个类别获得支持向量的数量 >>>  1 1 以上是 SVM 简单的介绍，这些内容都由 ApacheCN 翻自 Scikit-learn 。最后，感谢参与翻译文档的志愿者，正因为他们，开源社区才能有如此高质量的学习资料。 "
17,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740517&idx=4&sn=4ff827791587eed69a0dec2e6e051220&chksm=871ad35bb06d5a4d5857170af5a8e736cb22c18709cdb5f51b8f2e6d223f7b7a0433ec9e023f&scene=27,CVPR 2018 | 腾讯AI Lab、MIT等机构提出TVNet：可端到端学习视频的运动表征,尽管端到端的特征学习已经取得了重要的进展，但是人工设计的光流特征仍然被广泛用于各类视频分析任务中。为了弥补这个不足，由来自腾讯 AI Lab、MIT、清华、斯坦福大学的研究者完成并入选 CVPR 2018 Spotlight 论文的一项研究提出了一种能从数据中学习出类光流特征并且能进行端到端训练的神经网络：TVNet。机器之心对本论文进行了摘要介绍，详情请参阅原论文。另外，该研究成果的 TensorFlow 实现已经发布在 GitHub 上。   论文地址：https://arxiv.org/abs/1804.00413 代码地址：https://github.com/LijieFan/tvnet   图像分类和目标检测等基于图像的任务已经在深度学习（尤其是卷积神经网络（CNN））的推动下实现了革命性的发展。但是，视频分析方面的进展却并不尽如人意，这说明学习时空数据的表征是非常困难的。我们认为其中主要的难点是：寻找视频中明显的运动信息（motion cue）需要某种新型网络设计，而这些设计尚未被找到和测试。   尽管已经有些研究在尝试通过在空间和时间维度上同时执行卷积运算来学习特征，但是人工设计的光流（optical flow）特征在视频分析上仍有广泛和有效的应用。光流，顾名思义，是指两个连续帧之间的像素位移。因此，将光流应用到视频理解任务上可以明确而方便地实现运动线索的建模。然而，这种方法很低效，估计光流的计算和存储成本往往很高。目前成功将光流应用于视频理解的重要方法之一是 two-stream model [33]，其在光流数据上训练了一个用于学习动作模式的卷积网络。研究者们已经提出了一些不同的 two-stream model 的扩展，并在动作识别和动作检测等多种任务上实现了当前最佳水平。   尽管表现出色，但当前的基于光流的方法存在一些显著缺陷：   训练是一种双阶段过程。第一个阶段是通过基于优化的方法（比如 TVL1 [42]）提取每两个连续帧的光流。第二个阶段是基于提取出的光流数据上训练一个 CNN。这两个阶段是分开的，而且来自第二个阶段的信息（比如梯度）无法被用于调节第一个阶段的过程。 光流提取的空间和时间成本很高。提取出的光流必须写入到磁盘上，以便训练和测试。对于包含大约 1 万段视频的 UCF101 数据集而言，通过 TVL1 方法提取所有数据的光流需要消耗一个 GPU 一整天时间，而将原来的场作为浮点数来存储这些光流需要至少 1TB 的存储空间（为了节省存储成本，通常需要线性归一化成 JPEG）。   为了解决上述问题，我们提出了一种全新的神经网络设计，可以端到端的方式学习类光流的特征。这个名叫 TVNet 的网络是通过模仿和展开 TV-L1 的迭代优化过程而获得的。尤其值得一提的是，我们将 TV-L1 方法中的迭代形式化为了神经网络的自定义层。因此，我们的 TVNet 的设定基础良好，无需任何额外训练就能直接被使用。   此外，我们的 TVNet 是可以端到端地训练的，因此可以自然地连接到特定任务的网络上（比如动作分类网络），进而实现「更深度」的可端到端训练的架构。因此，无需再预计算或存储光流特征。   最后，通过执行端到端学习，被初始化为标准的光流特征提取器的 TVNet 的权重可以得到进一步的调节。这让我们可以发现更丰富且更针对任务的特征（相比于原始的光流），从而实现更优的表现。   为了验证我们提出的架构的有效性，我们在两个动作识别基准（HMDB51 和 UCF101）上执行了实验，比较了我们提出的 TVNet 和几种互相竞争的方法。   总体而言，本论文有以下贡献： 我们通过将 TV-L1 方法的迭代展开成自定义的神经层，开发了一种学习视频中的运动的全新神经网络。这个网络名叫 TVNet，具有良好的初始化。 尽管我们提出的 TVNet 是以特定的 TV-L1 架构初始化的，但相比于标准的光流，它可以在进一步微调之后学习到更丰富和更面向任务的特征。 相比于其它动作表征学习的方法（比如 TV-L1、FlowNet2.0 和 3D Convnets），我们的 TVNet 在两个动作识别基准上实现了更优的准确度，即在 HMDB51 上实现了 72.6% 的准确度、在 UCF101 上实现了 95.4% 的准确度。 算法 1：用于光流提取的 TV-L1 方法 图 1：由 TV-L1、TVNet（无训练）、TVNet（有训练）得到的类光流运动特征的可视化结果 图 2：（a）将 TV-L1 展开成 TVNet 的过程示意图。对于 TV-L1，我们只描述了算法 1 中的单次迭代。我们将 TV-L1 中的双三次翘曲（bicubic warping）、梯度和散度计算重新形式化为了 TVNet 中的双线性翘曲（bilinear warping）和卷积运算。（b）用于动作识别的端到端模型。 图 3：TV-L1 和 TVNet-50 在 MiddleBurry 上估计得到的光流示例。经过训练后，TVNet-50 可以提取出比 TV-L1 更精细的特征。 表 3：在 HMDB51 和 UCF101 上的各种运动描述器的分类准确度。上面一部分是之前最佳的动作表征方法的结果；中间一部分给出了 4 种基准方法的准确度；下面的结果表明我们的 TVNet-50 模型在这两个数据集上都得到了最佳表现。   表 4：在 HMDB51 和 UCF101 上的平均分类准确度   图 4：TV-L1 和 TVNet-50 在 UCF101 数据集上得到的运动特征。从第一列到最后一列，我们分别展示了输入图像对（仅第一张图像）、TV-L1 得到的运动特征、无训练和有训练的 TVNet-50 得到的运动特征。有意思的是，使用训练后，TVNet-50 可以得到比 TV-L1 及  TVNet-50 的非训练版本更抽象的运动特征。这些特征不仅自动移除了背景的运动（参见「punch」示例），而且还捕捉到了运动物体的轮廓。    论文：用于视频理解的运动表征的端到端学习（End-to-End Learning of Motion Representation for Video Understanding） 尽管端到端学习的表征近来取得了成功，但视频分析任务广泛使用的仍然还是人工设计的光流特征。为了弥补这一不足，我们提出了一种全新的可端到端训练的神经网络 TVNet，可从数据中学习类光流特征。TVNet 是一种特定的光流求解器 TV-L1 方法，并且通过将 TV-L1 中的优化迭代展开成神经层而进行了初始化。因此，TVNet 无需任何额外训练就能直接使用。此外，它还可以自然地嫁接到其它特定任务的网络上，从而得到端到端的架构；因此，这种方法无需预计算和在磁盘上预存储特征，比之前的多阶段方法更加高效。最后，TVNet 的参数可以通过端到端训练进一步优化。这让 TVNet 不仅可以学习光流，而且还可以学习到更丰富的且更针对任务的动作模式。我们在两个动作识别基准上进行了广泛的实验，结果表明了我们提出的方法的有效性。我们的 TVNet 实现了比所有同类方法更优的准确度，同时在特征提取时间方面也能与最快的同类方法媲美。 
18,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740482&idx=4&sn=285aaf67f5e350fc8f719df11c7f8919&chksm=871ad37cb06d5a6a69489c82c8069520c54576faf659768858ace14dc251fd8df05770a8278f&scene=27,学界 | FAIR等机构联合提出IntPhys：你的智能系统的物理知识，比得上婴儿吗？,"婴儿和许多动物对物体的相互作用有直观理解，并能逐步掌握物体恒常性、因果关系、重力、形状不变性等直观、非语言概念。受此启发，Facebook AI Research 等机构联合提出了一个评估框架和基准数据集 IntPhys，通过测试系统区分可能与不可能事件的能力，来诊断其对直观物理的理解程度。该项目对于无监督学习和现实交互系统等应用有重要意义。 尽管机器视觉在许多任务（面部识别 [ 68]、目标识别 [ 33，26] 、目标分割 [ 52] 等）上取得了引人瞩目的进展，但是人工系统对复杂场景的理解还远远达不到人类水平。场景理解不仅涉及目标分割和跨时间目标追踪，还涉及目标之间的空间和时间关系表征，并能够预测它们在物理世界中的交互方式。 实验证据表明，非常小的婴儿和许多动物确实对物体的相互作用有直观的理解，他们利用这种「直观的物理学」来预测未来的物体状态并规划他们的行动 [ 4 ]。在 2 - 4 个月的时候，婴儿就能够从物体恒常性、实体性和时空连续性的角度分析视觉输入 [ 29，65] 。在 6 个月的时候，他们理解了稳定、支持和因果关系的概念 [63，6，5] 。在 8 到 10 个月之间，他们掌握了重力、惯性和碰撞动量守恒的概念；10 至 12 个月之间，他们掌握了形状不变性 [ 70 ]。这种隐性知识是直观的和非语言的（相对于物理课中教授的形式化知识），并且遵循与早期语言习得平行的发展路径。这两种情况都发展快速、自发，而且没有护理人员的明确培训 [ 53 ]。 在生物体中，直观物理是一种潜在的构造：它只能通过对特定任务（如计划、解决问题）的影响来间接地观察和衡量，或者在人类中，通过口头描述和解释来观察和衡量。还可以通过测量对「魔术」的惊讶反应来揭示，即对物理上不可能发生的事件（例如物体不知从哪里消失或出现、相互穿越或无视重力等）表示无法理解或好奇。直观物理的潜在性质对视觉系统提出了两个难题：评估挑战和工程挑战。 评估挑战可以表述为：给定一个人工视觉系统，定义一个量化该系统对（直观）物理了解程度的度量。可能的解决方案是通过真实世界的应用来测量直观物理现象，比如视觉问答 ( VQA )、目标追踪或行动规划（见图 1）。然而，这可能会遇到两种风险： ( a ) 数据集偏差；( b ) 测量噪声。第一个风险（又称之为 Clever Hans 问题 [ 28 ]）是现实生活中的应用程序数据集经常存在的固有统计偏差，这使得有时只需最小程度的干预就可以获得良好的性能，有时却相反。第二种风险是系统的整体性能是其各部分性能的复杂函数；因此，如果一个 VQA 系统比另一个具有更好的性能，这可能不是因为它更好地理解物理，而是因为它有更好的语言模型。 我们提出了一个框架，称之为「物理合理性测试」，它以无任务和无模型的方式直接评估系统的直观物理能力。这一框架的灵感来源于对婴儿和动物直观物理的研究。它将物理推理过程重塑为一个简单的是非分类问题：呈现简单场景的视频，并询问所描绘的事件在物理上是否可能。技巧在于准备匹配的视频集，其中物理背离在可能与不可能的电影帧之间引入最小差异。通过改变物理背离的性质，可以探索不同类型的推理 （关于物体及其属性的规律、关于物体移动和交互的规律等）。 考虑到我们的方法涉及到自然界中不能自发出现的事件，它应该作为一种诊断测试，而不是一种训练物理推理系统的实用方法。然而，它的优点在于，它可以应用于已经在其他任务上训练的各种系统。只要这些系统对计算给定场景的全局标量数的要求最小，我们就可以将其解释为「合理性」得分。任何基于概率或重构误差的系统都可以容易地得到这样的分数。 工程挑战可表述如下：构建一个系统，尽可能多地融入直观的物理（至少像婴儿一样，作为初始阶段）。我们已经放弃使用不可能事件的视频来训练这样一个系统，理由是我们需要注重实用性。另一种使用带有高级标注（物理实体、定律或关系等）的监督学习方法，也是不切实际的。首先，系统可以在不执行完整 3D 重建的情况下对场景具有良好的物理理解。第二，婴儿无需输入任何高级标签就可以学习直观物理。事实上，他们只经历「积极」的物理事件（物理上可能的事件）。此外，婴儿可以从他们的环境中获得有用的反馈，因为他们在运动控制方面变得更有能力，尽管这种反馈只存在于可能发生的事件中。所以，解决该挑战的一种方法是构建无监督或弱监督的系统，该系统使用婴儿可获得的相同类型的数据 （即，大量的感官观察数据、有限但信息丰富的环境反馈、仅包含积极事件的实例），来学习物理定律。 这里，我们提出了一个直观物理的基准 IntPhys，其目标是同时尝试解决评估挑战和工程挑战。它由一个 python 接口游戏引擎（UnrealEngine 4）构建的合成视频组成，实现了逼真的物理效果和精确的控制。训练集仅包括积极事件，即由固定不动的智能体从第一人称角度观看可能事件的视频。这可能比婴儿面临的任务更困难，因为婴儿可以探索和与环境互动。但是有趣的地方在于，通过这种简化输入可以获得多大程度的直观物理能力，这种输入在现实世界中很容易通过摄像机大量收集。此外，这使得模型的比较更加容易，因为它们都使用相同的训练数据。测试集是根据我们的评估框架构建的，也就是说，它要求系统输出合理性得分，并根据其将可能视频与不可能视频区分的能力进行评估。该测试集还可以用作以其他方式（真实视频、虚拟环境中的交互式训练等）训练的系统的独立诊断评估 [58,27,10]。 4 IntPhys 基准（V1.0） 我们在本文介绍 IntPhys 的第一个版本。IntPhys 是一个基准，目标是解决视觉系统中直观物理能力的工程和评估挑战。第一版的重点是无监督学习，只测试问题层次结构的第一个模块 ( O1，物体恒常性)。未来的版本将包括表 1 中更多的模块。 表 1：直观物理框架的概念模块列表。 该基准由三个部分组成：( 1 ) 仅包含在虚拟环境中移动和交互的简单无生命物体的物理上可能的事件的训练集；( 2 ) 包含物理上可能的和物理上不可能的视频的开发集和测试集，如上所述以元组匹配； ( 3 ) 评估软件。通过这三个组成部分，以及对测试集的人类合理性判断结果，为人类感知建模算法提供了参考。 图 3：训练集中的帧示例。 表 2：使用 MTurk 对 IntPhys（模块 O1）测试集中的人类合理性判断的平均错误率。*代表根据我们的入选标准，此数据点已被「强制」为零。 图 6：语义掩码预测器的输出示例。从左到右:输入图像、真实语义掩码、预测语义掩码。 论文：IntPhys: A Framework and Benchmark for Visual Intuitive Physics Reasoning（一个用于视觉直观物理推理的评估框架和基准数据集） 论文地址：https://arxiv.org/pdf/1803.07616.pdf 摘要： 为了达到人类在复杂视觉任务中的表现性能，人工系统需要在宏观对象、运动、力等方面融入对世界的理解。在婴儿直观物理研究的启发下，我们提出了一个评估框架，通过测试给定系统区分视频中可能与不可能事件的能力，来诊断给定系统对直观物理的理解程度。测试要求系统计算整个视频的物理合理性得分。它没有偏见，可以测试一系列具体的物理推理技能。然后，我们介绍了新基准数据集的第一个版本，该版本旨在通过使用游戏引擎构建的视频，以无监督的方式学习直观物理。我们介绍了两个用未来帧预测目标训练的深层神经网络基线系统，并在可能 vs 不可能的辨别任务上进行了测试。通过将结果与人类数据进行比较分析，我们对未来帧预测体系结构的潜力和局限性给出了新见解。 "
19,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740517&idx=1&sn=0b69d9a42f5ca18ed513cee309cc27a1&chksm=871ad35bb06d5a4d9f4d30e848c023247358db64af5b6d3fe6453412d992c1c5c49d4fe3aca6&scene=27,94页论文综述卷积神经网络：从基础技术到研究前景,"卷积神经网络（CNN）在计算机视觉领域已经取得了前所未有的巨大成功，但我们目前对其效果显著的原因还没有全面的理解。近日，约克大学电气工程与计算机科学系的 Isma Hadji 和 Richard P. Wildes 发表了论文《What Do We Understand About Convolutional Networks?》，对卷积网络的技术基础、组成模块、当前现状和研究前景进行了梳理，介绍了我们当前对 CNN 的理解。机器之心对本论文进行了摘要式的编译，更详细的信息请参阅原论文及其中索引的相关文献。 论文地址：https://arxiv.org/abs/1803.08834  1.1	动机 过去几年来，计算机视觉研究主要集中在卷积神经网络（常简称为 ConvNet 或 CNN）上。这些工作已经在广泛的分类和回归任务上实现了新的当前最佳表现。相对而言，尽管这些方法的历史可以追溯到多年前，但对这些系统得到出色结果的方式的理论理解还很滞后。事实上，当前计算机视觉领域的很多成果都是将 CNN 当作黑箱使用，这种做法是有效的，但其有效的原因却非常模糊不清，这严重满足不了科学研究的要求。尤其是这两个可以互补的问题：（1）在被学习的方面（比如卷积核），究竟被学习的是什么？（2）在架构设计方面（比如层的数量、核的数量、池化策略、非线性的选择），为什么某些选择优于另一些选择？这些问题的答案不仅有利于提升我们对 CNN 的科学理解，而且还能提升它们的实用性。 此外，目前实现 CNN 的方法需要大量训练数据，而且设计决策对结果表现有很大的影响。更深度的理论理解应该能减轻对数据驱动的设计的依赖。尽管已有实证研究调查了所实现的网络的运行方式，但到目前为止，这些结果很大程度上还局限在内部处理过程的可视化上，目的是为了理解 CNN 中不同层中发生的情况。 1.2	目标 针对上述情况，本报告将概述研究者提出的最突出的使用多层卷积架构的方法。要重点指出的是，本报告将通过概述不同的方法来讨论典型卷积网络的各种组件，并将介绍它们的设计决策所基于的生物学发现和/或合理的理论基础。此外，本报告还将概述通过可视化和实证研究来理解 CNN 的不同尝试。本报告的最终目标是阐释 CNN 架构中涉及的每一个处理层的作用，汇集我们当前对 CNN 的理解以及说明仍待解决的问题。 1.3 报告提纲 本报告的结构如下：本章给出了回顾我们对卷积网络的理解的动机。第 2 章将描述各种多层网络并给出计算机视觉应用中使用的最成功的架构。第 3 章将更具体地关注典型卷积网络的每种构造模块，并将从生物学和理论两个角度讨论不同组件的设计。最后，第 4 章将会讨论 CNN 设计的当前趋势以及理解 CNN 的工作，并且还将重点说明仍然存在的一些关键短板。 总的来说，本章将简要概述计算机视觉领域中所用的最突出的多层架构。需要指出，尽管本章涵盖了文献中最重要的贡献，但却不会对这些架构进行全面概述，因为其它地方已经存在这样的概述了（比如 [17, 56, 90]）。相反，本章的目的是为本报告的剩余部分设定讨论基础，以便我们详细展示和讨论当前对用于视觉信息处理的卷积网络的理解。 2.1 多层架构 在近来基于深度学习的网络取得成功之前，最先进的用于识别的计算机视觉系统依赖于两个分离但又互补步骤。第一步是通过一组人工设计的操作（比如与基本集的卷积、局部或全局编码方法）将输入数据变换成合适的形式。对输入的变换通常需要找到输入数据的一种紧凑和/或抽象的表征，同时还要根据当前任务注入一些不变量。这种变换的目标是以一种更容易被分类器分离的方式改变数据。其次，被变换的数据通常用于训练某些类型的分类器（比如支持向量机）来识别输入信号的内容。通常而言，任何分类器的表现都会受到所使用的变换方法的严重影响。 多层学习架构为这一问题带来了不同的前景，这种架构提出不仅要学习分类器，而且要从数据中直接学习所需的变换操作。这种形式的学习通常被称为「表征学习」，当应用在深度多层架构中时即被称为「深度学习」。 多层架构可以定义为允许从输入数据的多层抽象中提取有用信息的计算模型。一般而言，多层架构的设计目标是在更高层凸显输入中的重要方面，同时能在遇到更不重要的变化时变得越来越稳健。大多数多层架构都是将带有交替的线性和非线性函数的简单构建模块堆叠在一起。多年以来，研究者已经提出了很多不同类型的多层架构，本章将会覆盖计算机视觉应用中所采用的最为突出的此类架构。人工神经网络是其中的关注重点，因为这种架构的表现非常突出。为了简单起见，后面会直接将这类网络称为「神经网络」。 2.1.1 神经网络 典型的神经网络由一个输入层、一个输出层和多个隐藏层构成，其中每一层都包含多个单元。 图 2.1：典型神经网络架构示意图，图来自 [17] 自动编码器可以定义为由两个主要部分构成的多层神经网络。第一个部分是编码器，可以将输入数据变换成特征向量；第二个部分是解码器，可将生成的特征向量映射回输入空间。 图 2.2：典型自动编码器网络的结构，图来自 [17] 2.1.2 循环神经网络 当谈到依赖于序列输入的任务时，循环神经网络（RNN）是最成功的多层架构之一。RNN 可被视为一种特殊类型的神经网络，其中每个隐藏单元的输入时其当前时间步骤观察到的数据和其前一个时间步骤的状态。 图 2.3：标准循环神经网络的运算的示意图。每个 RNN 单元的输入都是当前时间步骤的新输入和前一个时间步骤的状态；然后根据 计算得到新输出，这个输出又可被馈送到多层 RNN 的下一层进行处理。 图 2.4：典型 LSTM 单元示意图。该单元的输入是当前时间的输入和前一时间的输入，然后它会返回一个输出并将其馈送给下一时间。LSTM 单元的最终输出由输入门、输出门和记忆单元状态控制。图来自 [33] 2.1.3 卷积网络 卷积网络（CNN）是一类尤其适合计算机视觉应用的神经网络，因为它们能使用局部操作对表征进行分层抽象。有两大关键的设计思想推动了卷积架构在计算机视觉领域的成功。第一，CNN 利用了图像的 2D 结构，并且相邻区域内的像素通常是高度相关的。因此，CNN 就无需使用所有像素单元之间的一对一连接（大多数神经网络都会这么做），而可以使用分组的局部连接。第二，CNN 架构依赖于特征共享，因此每个通道（即输出特征图）是在所有位置使用同一个过滤器进行卷积而生成的。 图 2.5：标准卷积网络的结构的示意图，图来自 [93] 图 2.6：Neocognitron 的结构示意图，图来自 [49] 2.1.4 生成对抗网络 典型的生成对抗网络（GAN）由两个互相竞争的模块或子网络构成，即：生成器网络和鉴别器网络。 图 2.7：生成对抗网络的一般结构的示意图 2.1.5 多层网络的训练 如前面讨论的一样，多种多层架构的成功都很大程度上取决于它们的学习过程的成功。其训练过程通常都基于使用梯度下降的误差的反向传播。由于使用简单，梯度下降在训练多层架构上有广泛的应用。 2.1.6 简单说说迁移学习 使用多层架构提取的特征在多种不同数据集和任务上的适用性可以归功于它们的分层性质，表征会在这样的结构中从简单和局部向抽象和全局发展。因此，在其层次结构中的低层级提取的特征往往是多种不同任务共有的特征，因此使得多层结构更容易实现迁移学习。 2.2 空间卷积网络 理论上而言，卷积网络可以应用于任意维度的数据。它们的二维实例非常适用于单张图像的结构，因此在计算机视觉领域得到了相当大的关注。有了大规模数据集和强大的计算机来进行训练之后，CNN 近来在多种不同任务上的应用都出现了迅猛增长。本节将介绍为原来的 LeNet 引入了相对新颖的组件的比较突出的 2D CNN 架构。 2.2.1 CNN 近期发展中的关键架构 图 2.8：AlexNet 架构。需要指出，虽然从图上看这是一种有两个流的架构，但实际上这是一种单流的架构，这张图只是说明 AlexNet 在 2 个不同 GPU 上并行训练的情况。图来自 [88] 图 2.9：GoogLeNet 架构。（a）典型的 inception 模块，展示了顺序和并行执行的操作。（b）由层叠的许多 inception 模块构成的典型 inception 架构的示意图。图来自 [138] 图 2.10：ResNet 架构。（a）残差模块。（b）由层叠的许多残差模块构成的典型 ResNet 架构示意图。图来自 [64] 图 2.11：DenseNet 架构。（a）dense 模块。（b）（b）由层叠的许多 dense 模块构成的典型 DenseNet 架构的示意图。图来自 [72] 2.2.2 实现 CNN 的不变性 使用 CNN 的一大难题是需要非常大的数据集来学习所有的基本参数。甚至拥有超过 100 万张图像的 ImageNet 等大规模数据集在训练特定的深度架构时仍然被认为太小。满足这种大数据集要求的一种方法是人工增强数据集，具体做法包括对图像进行随机翻转、旋转和抖动（jittering）等。这些增强方法的一大优势是能让所得到的网络在面对各种变换时能更好地保持不变。 2.2.3 实现 CNN 的定位 除了识别物体等简单的分类任务，CNN 近来也在需要精准定位的任务上表现出色，比如形义分割和目标检测。 2.3 时空卷积网络 使用 CNN 为各种基于图像的应用带来了显著的性能提升，也催生了研究者将 2D 空间 CNN 扩展到视频分析的 3D 时空 CNN 上的兴趣。一般而言，文献中提出的各种时空架构都只是试图将空间域 (x,y) 的 2D 架构扩展到时间域 (x, y, t) 中。在基于训练的时空 CNN 领域存在 3 种比较突出的不同架构设计决策：基于 LSTM 的 CNN、3D CNN 和 Two-Stream CNN。 2.3.1 基于 LSTM 的时空 CNN 基于 LSTM 的时空 CNN 是将 2D 网络扩展成能处理时空数据的一些早期尝试。它们的操作可以总结成图 2.16 所示的三个步骤。第一步，使用一个 2D 网络处理每一帧，并从这些 2D 网络的最后一层提取出特征向量。第二步，将这些来自不同时间步骤的特征用作 LSTM 的输入，得到时间上的结果。第三步，再对这些结果求平均或线性组合，然后再传递给一个 softmax 分类器以得到最终预测。 2.3.2 3D CNN 这种突出的时空网络是将 2D CNN 最直接地泛化到图像时空域中。它直接处理 RGB 图像的时间流，并通过应用所学习到的 3D 卷积过滤器来处理这些图像。 2.3.3 Two-Stream CNN 这种类型的时空架构依赖于一种双流式（two-stream）的设计。标准的双流式架构是采用两个并行通路——一个用于处理外观，另一个用于处理运动；这种方法类似于生物视觉系统研究中的双流式假设。 2.4 整体讨论 需要重点指出的是，尽管这些网络在很多计算机视觉应用上都实现了很有竞争力的结果，但它们的主要缺点仍然存在：对所学习到的表征的确切本质的理解很有限、依赖于大规模数据训练集、缺乏支持准确的表现边界的能力、网络超参数选择不清晰。 鉴于 CNN 领域存在大量悬而未决的问题，本章将介绍典型卷积网络中每种处理层的作用和意义。为此本章将概述在解决这些问题上最突出的工作。尤其值得一提的是，我们将从理论和生物学两个角度来展示 CNN 组件的建模方式。每种组件的介绍后面都总结了我们当前的理解水平。 3.1 卷积层 卷积层可以说是 CNN 架构中最重要的步骤之一。基本而言，卷积是一种线性的、平移不变性的运算，其由在输入信号上执行局部加权的组合构成。根据所选择的权重集合（即所选择的点扩散函数（point spread function））的不同，也将揭示出输入信号的不同性质。在频率域中，与点扩散函数关联的是调制函数——说明了输入的频率组分通过缩放和相移进行调制的方式。因此，选择合适的核（kernel）对获取输入信号中所包含的最显著和最重要的信息而言至关重要，这能让模型对该信号的内容做出更好的推断。本节将讨论一些实现这个核选择步骤的不同方法。 3.2 整流 多层网络通常是高度非线性的，而整流（rectification）则通常是将非线性引入模型的第一个处理阶段。整流是指将点方面的非线性（也被称为激活函数）应用到卷积层的输出上。这一术语借用自信号处理领域，其中整流是指将交流变成直流。这也是一个能从生物学和理论两方面都找到起因的处理步骤。计算神经科学家引入整流步骤的目的是寻找能最好地解释当前神经科学数据的合适模型。另一方面，机器学习研究者使用整流的目的是为了让模型能更快和更好地学习。有趣的是，这两个方面的研究者往往都认同这一点：他们不仅需要整流，而且还会殊途同归到同一种整流上。 图 3.7：多层网络的文献中所使用的非线性整流函数 3.3 归一化 正如前面提到的，由于这些网络中存在级联的非线性运算，所以多层架构是高度非线性的。除了前一节讨论的整流非线性，归一化（normalization）是 CNN 架构中有重要作用的又一种非线性处理模块。CNN 中最广泛使用的归一化形式是所谓的 Divisive Normalization（DN，也被称为局部响应归一化）。本节将介绍归一化的作用并描述其纠正前两个处理模块（卷积和整流）的缺点的方式。同样，我们会从生物学和理论两个方面讨论归一化。 3.4 池化 不管是生物学启发的，还是纯粹基于学习的或完全人工设计的，几乎所有 CNN 模型都包含池化步骤。池化运算的目标是为位置和尺寸的改变带来一定程度的不变性以及在特征图内部和跨特征图聚合响应。与之前几节讨论的三种 CNN 模块类似，池化在生物学和理论研究上都具有支持。在 CNN 网络的这个处理层上，主要的争论点是池化函数的选择。使用最广泛的两种池化函数分别是平均池化和最大池化。本节将探索相关文献中描述的各种池化函数的优点和缺点。 图 3.10：平均池化和最大池化在 Gabor 滤波后的图像上的比较。（a）展示了不同尺度的平均池化的效果，其中（a）中上面一行是应用于原始灰度值图像的结果，（a）中下面一行是应用于 Gabor 滤波后的图像上的结果。平均池化能得到灰度值图像的更平滑的版本，而稀疏的 Gabor 滤波后的图像则会褪色消散。相对而言，（b）给出了不同尺度的最大池化的效果，其中（b）中上面一行是应用于原始灰度值图像的结果，（b）中下面一行是应用于 Gabor 滤波后的图像上的结果。这里可以看到，最大池化会导致灰度值图像质量下降，而 Gabor 滤波后的图像中的稀疏边则会得到增强。图来自 [131] 对 CNN 架构中各种组件的作用的论述凸显了卷积模块的重要性，这个模块很大程度上负责了在网络中获取最抽象的信息。相对而言，我们对这个处理模块的理解却最少，因为这需要最繁重的计算。本章将介绍在尝试理解不同的 CNN 层所学习的内容上的当前趋势。同时，我们还将重点说明这些趋势方面仍有待解决的问题。 4.1 当前趋势 尽管各种 CNN 模型仍继续在多种计算机视觉应用中进一步推进当前最佳的表现，但在理解这些系统的工作方式和如此有效的原因上的进展仍还有限。这个问题已经引起了很多研究者的兴趣，为此也涌现出了很多用于理解 CNN 的方法。一般而言，这些方法可以分成三个方向：对所学习到的过滤器和提取出的特征图进行可视化、受理解视觉皮层的生物学方法启发的 ablation study、通过向网络设计中引入分析原理来最小化学习过程。本节将简要概述其中每种方法。 4.2 仍待解决的问题 基于上述讨论，基于可视化的方法存在以下关键研究方向： 首要的一点：开发使可视化评估更为客观的方法是非常重要的，可以通过引入评估所生成的可视化图像的质量和/或含义的指标来实现。 另外，尽管看起来以网络为中心的可视化方法更有前景（因为它们在生成可视化结果上不依赖网络自身），但似乎也有必要标准化它们的评估流程。一种可能的解决方案是使用一个基准来为同样条件下训练的网络生成可视化结果。这样的标准化方法反过来也能实现基于指标的评估，而不是当前的解释性的分析。 另一个发展方向是同时可视化多个单元以更好地理解处于研究中的表征的分布式方面，甚至同时还能遵循一种受控式方法。 以下是基于 ablation study 的方法的潜在研究方向： 使用共同的系统性组织的数据集，其中带有计算机视觉领域常见的不同难题（比如视角和光照变化），并且还必需有复杂度更大的类别（比如纹理、部件和目标上的复杂度）。事实上，近期已经出现了这样的数据集 [6]。在这样的数据集上使用 ablation study，加上对所得到的混淆矩阵的分析，可以确定 CNN 架构出错的模式，进而实现更好的理解。 此外，对多个协同的 ablation 对模型表现的影响方式的系统性研究是很受关注的。这样的研究应该能延伸我们对独立单元的工作方式的理解。 最后，这些受控方法是很有前景的未来研究方向；因为相比于完全基于学习的方法，这些方法能让我们对这些系统的运算和表征有更深入的理解。这些有趣的研究方向包括： 逐步固定网络参数和分析对网络行为的影响。比如，一次固定一层的卷积核参数（基于当前已有的对该任务的先验知识），以分析所采用的核在每一层的适用性。这个渐进式的方法有望揭示学习的作用，而且也可用作最小化训练时间的初始化方法。 类似地，可以通过分析输入信号的性质（比如信号中的常见内容）来研究网络架构本身的设计（比如层的数量或每层中过滤器的数量）。这种方法有助于让架构达到适宜应用的复杂度。 最后，将受控方法用在网络实现上的同时可以对 CNN 的其它方面的作用进行系统性的研究，由于人们重点关注的所学习的参数，所以这方面得到的关注较少。比如，可以在大多数所学习的参数固定时，研究各种池化策略和残差连接的作用。 "
20,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740482&idx=2&sn=546fbf2c113660937383f5083b04cc81&chksm=871ad37cb06d5a6a70f7821b5b2e927457d37bb08d52822c58e800e1f9e558fa22ca70da8add&scene=27,深度 | 吴恩达对话Yann LeCun：从相识Hinton到深度学习崛起,"深度学习专项课程 Deeplearning.ai 中，也包含吴恩达和多位深度学习大牛的对话视频，之前 YouTube 上已经公开了他与 Geoffrey Hinton、Yoshua Bengio、Ian Goodfellow、Andrej Karpathy 等人的对话视频。昨日，Deeplearning.ai 放出吴恩达对话 Yann LeCun 的视频，机器之心对此视频内容进行整理介绍。 视频链接：https://www.youtube.com/watch?v=JS12eb1cTLE&feature=youtu.be 吴恩达：Hi Yann，你研究神经网络很长一段时间了，我想听你讲下自己的故事：你是怎么开始做人工智能的？又是怎么构建神经网络的？ Yann LeCun： 我从小就对「智能」很感兴趣，例如人类智能的出现、人类的进化等等。 而且我也对科技、太空等主题感兴趣。我最喜欢的电影是《2001 太空漫游》，里面有智能机器、太空旅游、人类进化等让我痴迷的东西。其中关于智能机器的概念真的很吸引我。 后来我学习的专业是电子工程，大概在工程学院的第二年的时候，我偶然发现了一本哲学书，里面有 MIT 语言学家乔姆斯基（Noam Chomsky）和瑞士研究儿童发展的认知心理学家让·皮亚杰（Jean Piaget）的辩论。这个辩论主要围绕语言的先天机制与后天培养。我们知道，乔姆斯基主张人类拥有语言是因为大脑内有大量的单元结构，是一种先天习得的机制，而皮亚杰认为儿童的语言发展是通过后天学习到的。这两方都聚拢了一批支持者为其辩护。 MIT 关注于感知机的西蒙·派珀特（Seymour Papert）支持皮亚杰的观点，他表示感知机（Perceptron）是首个可以学习的机器，而我之前从未听过。我读了那篇文章，觉得「可以学习的机器」听起来真妙。 所以，我开始在几所大学的图书馆里面搜索任何我能找到的、讨论感知机的书。随后我意识到，50 年代这个领域的论文很多，然而到了 60 年代，这种讨论随着西蒙与他人合著的关于感知机一本书而终止。 吴恩达：那大概是什么年份？ Yann LeCun： 大概是 20 世纪 80 年代。 所以，后来我和大学的几位数学教授做了一些神经网络方面的项目。但 80 年代没有人能够与我讨论，因为基本上这个领域像消失了一样，很少人研究感知机。我只能自己做实验，写了很多用于模拟的程序，读了很多神经科学方面的书。 在结束工程课程之后，我学习了芯片设计。进行了一些项目之后，我觉得自己需要做这方面的研究，以解决当时比较重要的一类问题：如何训练多层的神经网络。60 年代的文献中明确表示这是一类没有解决的重要问题。我当时也读了很多文章，你知道 Kunihiko Fukushima 的 Neo-cognitron 论文提出的层级架构，非常类似于卷积网络，但没有类似反向传播的学习算法。 论文链接：http://www.rctn.org/bruno/public/papers/Fukushima1980.pdf 后来，我在法国又碰到一小批人，他们也对此感兴趣，但他们称之为自动机网络（Automata Networks）。他们让我看了一些研究 Hopfield 网络的论文。你知道，这个网络并不是特别流行，但却是第一个带有联合记忆的神经网络。在 80 年代早期，这些研究重新引起了一些研究团体对神经网络的兴趣，其中大部分是物理学家，比如凝聚态物理学家，也有少数心理学家。这时候，工程师和计算机科学家并不参与讨论神经网络。 他们也让我看了另外一篇论文，即刚刚发布的预印版论文《Optimal Perceptual Inference》，这是第一篇有关玻尔兹曼机的论文，作者是 Geoffrey E. Hinton 和 Terrence J. Sejnowski。这篇论文讨论了隐藏单元，提出了多层神经网络比单纯的分类器更强大。我当即就说，我觉得我需要见见这群人，因为我对这些问题也非常感兴趣。后来，我读博士时参与了一场同事们组织的 workshop，当时 Terrence 做了演讲，我也碰到了 Hinton。 那是 1985 年，这是非常吸引人的一个 workshop，有很多早期的理论神经科学等领域的研究者。当时，我也碰见了后来招我进贝尔实验室的人。 在 workshop 上，我告诉 Terrence 我正在研究的类似反向传播的算法，这时 David E. Rumelhart、Geoffrey E. Hinton、Ronald J. Williams 三人的论文还没发表（指论文《Learning representations by back-propagating errors》），而 Terrence 与 Hinton 是朋友，常常互通有无。所以当时 Terrence 已经在做反向传播相关的工作，但他当时没告诉我。 所以，他回到美国告诉 Hinton，法国有个小孩也在做和我们同样的工作。然后几个月后（6 月），法国有另外一个会议，Hinton 是 keynote 演讲者，他介绍了用反向传播的玻尔兹曼机。演讲结束后大概有 50 人围绕在他身边，而他对主办方说的第一句话是，「你知道有个小孩叫 Yann LeCun 吗？」——当时，他在 Proceedings 上读了我的法语版论文，他懂一点法语，再加上数学公式，他能看懂那大概是反向传播。所以我们一起吃了午饭，然后成为了朋友。 吴恩达：所以你们分别独立重造了反向传播？ Yann LeCun： 是的，或者说，我们意识到链式法则的重要性，也就是那些研究最优控制的学者所说的伴随态方法（adjoint state method），是非常重要的。而那才是反向传播真正被「发明」的环境： 20 世纪 60 年代的最优控制研究领域。「在不同的层级利用梯度」就是反向传播的实质，而这个概念在不同时期的不同语境下反复出现。但是 Rumelhart，Hinton 和 Williams 的论文让这个概念变得流行了起来。 吴恩达：让我们快进几年，你在 AT&T 贝尔实验室工作过一段时间，期间，你的重要工作之一是发明了 LeNet。我在我的课程上提到了它，并且当我还是贝尔实验室的一名暑期实习生的时候，我就听说过你的工作。能否给我们讲述更多关于 AT&T 以及 LeNet 的故事呢？ Yann LeCun： 我实际上是在博士后时期开始研究卷积神经网络的，当时我在多伦多大学 Jeff Hinton 组。我写了早期相关的代码，并做了第一批实验。 当时还没有 MNIST，我用鼠标画了一些字符，用数据扩增技术增加了数据量，然后用这个数据集测试模型的效果。 我比较了全连接神经网络、局部连接但是不共享参数的网络、以及局部连接且共享参数的网络，后者就是第一代卷积神经网络。 模型在小规模数据集上效果很好，有更好的表现，而且在卷积结构下没有过拟合。而当我在 1988 年 10 月进入贝尔实验室之后，我做的第一件事就是放大模型规模，因为那里有更快的电脑。就在我进入实验室几个月之前，我的老板 Larry Jekyll（后来他成为了贝尔实验室主任）问我：「我们应该在你来之前先订购电脑，你想要什么？」当时我们在多大，我们有一部 Sun-4，当时最好的电脑，所以我对他说，「如果（在实验室）也有一台的话会很赞。」于是他们订购了一台，所以我就自己拥有了一台电脑！ 要知道，在多大，我们是整个系共享一台电脑，但是现在我自己就有一部电脑！当时 Larry 对我说，「在贝尔实验室，你不会因为省钱而闻名天下的」，这简直太棒了。而且，他们已经在字母识别任务上工作了一段时间，积累了一个「庞大」的数据集，叫做 USPS，有 5000 个训练样本。所以我立刻训练了一个卷积神经网络，你可以叫它 LeNet one，在 USPS 数据集上取得了相当不错的效果——比实验室或者外部人员试过的所有方法都要好。 那时我们就知道，我们做出了一些成果。此时距离我加入贝尔实验室仅仅三个月。 这是第一个版本的卷积神经网络，我们用了带有步幅（strides）移动的卷积运算，但没有单独的子采样（subsampling）也没有池化层（pooling），换言之每个卷积都在直接进行下采样。这样设计的原因是因为我们无法承担每个点都有一个卷积的计算量。 第二个版本有单独的卷积、池化和下采样层，这就是 LeNet one。我们在 Neural Computation 和 NIPS 上发表了一系列论文。 有趣的是，我在 NIPS 上做了一个关于这篇论文的演讲，Jeff Hinton 当时就在观众席。当我回到我的座位（我当时正好坐在他旁边），他说，「你的演讲传递一个信息：如果你做出所有合理的选择，模型会变得能用。（If you do all the sensible things, it actually works.）」 吴恩达：可以肯定的是这项工作在这之后继续创造着历史，这个思路开始被用于读取支票。 Yann LeCun： 是，当时这项技术开始在 AT&T 内部应用，但是几乎没有在外部使用。 我很难解释其原因，但是我觉得有这样一些影响因素： 其一，当时是 80 年代末，没有互联网，我们有基于 FTP 的电子邮件，但那还并不是真正的互联网。 其二，没有任何两个实验室采用相同的软件或硬件平台。有些人用工作站，有些人用 PC，没有类似 Python 或者 MATLAB 这样的框架，人们都要自己写代码。 我和 Leon Powe 两个人花了大概一年半时间，基本上就写了个神经网络模拟器。而且因为没有 MATLAB 或者 Python，你得自己写解释器（interpreter）来控制你的模型，所以我们写了自己的 Lisp 解释器，LeNet 也就完全是用 Lisp 以及一个数值计算后端写的。结构和现在的框架很像，有不同的模块，你可以把它们相互连接起来，然后进行微分计算等所有那些现在为人熟悉的框架（Torch、PyTorch 或者 TensorFlow）都具有的功能。 然后我们和一群工程师一起开发了一系列应用。那是一群非常聪明的人，比如有的人原来是理论物理学家，后来来到贝尔实验室做了工程师。Chris Burgess 就是其中之一，他后来在微软研究院有着杰出的职业生涯。Craig Knoll 也是其中之一。我们和很多优秀的人合作，把这个特别的概念变成现实。 我们一起开发了很多应用，比如字母识别系统，那是一个结合了卷积神经网络和类似条件随机场（CRF）的模型，用来识别字母序列，而不是单独的字母。 吴恩达：是的，我读了那篇 LeNet 论文，先让数据通过一个神经网络，然后再让其通过一个自动机把识别出来的字母合并在一起。 Yann LeCun： 是的，论文的前半在讲卷积神经网络，那是全文最激动人心的部分。而后半部分鲜少有人读过。后半部分讲的是序列层面的判别式学习（discriminantive learning）以及不带归一化（normalization）过程的结构预测，非常像 CRF。 这是个很成功的做法。当时 AT&T 的一个子公司 NCR 的一支产品团队是我们的「客户」，负责把我们的系统嵌入到能读取支票的 ATM 机等设备中。这个系统被部署到一家大型银行的那天，我们团队在一家高档餐厅吃饭庆祝，然而消息传来说，AT&T 决定拆分改组。当时是 1995 年，拆分后，AT&T 变为三家公司，分别是 AT&T，Lucent Technologies 和 NCR。工程团队被分到了 Lucent Technologies，产品团队被分到了 NCR。遗憾的是，AT&T 的律师团动用他们无尽的智慧之后决定把卷积神经网络的专利（是的，卷积神经网络是有专利的，但是谢天谢地它在 2007 年已经过期了）分给了 NCR，然而 NCR 完全没有人知道卷积神经网络到底是什么。所以 NCR 的人们手握卷积神经网络的专利却根本不知道自己掌握的是什么。而我们在另一家公司，无法再进行与卷积神经网络相关的研究。 吴恩达：除了那些神经网络很火的时期之外，你在「神经网络寒冬」里也坚持对神经网络的信念，那是什么感觉？ Yann LeCun： 某种意义上来说我坚持了，另一个角度来看我也并没有。我始终相信这类技术会走回前沿，人们会找到在实际生活中应用它们的办法，我心底始终对此坚信不疑。但是在 1996 年，随着 AT&T 的拆分，我们所有的字母识别的工作基本上都停滞了，而我也升职成为了部门负责人，我必须为团队找到可以做的选题。当时还是互联网的萌芽期，我持有一个观点：随着互联网的兴起，我们必须找到把所有纸面上的知识转移到电子化世界的方法，所以我开始了一个叫 DjVu 的项目，旨在压缩扫描文件好让它们得以分布在互联网上。这个项目在一段时间内很有趣，取得了一定的成功，虽然 AT&T 也不知道该用它来做什么。 吴恩达：是的，我对那个项目有印象，旨在帮助线上传播研究工作。 Yann LeCun： 是的，我们扫描了整本 NIPS 论文集，然后把它发布在了网上，来展示这项技术如何工作。我们可以把高清的扫描页压缩到几 kb。 吴恩达：你在非常早期的工作中展示出来的（对卷积神经网络的）信念现在已经席卷了计算机视觉领域，并且在持续影响其它领域，谈谈你是怎么看待这整个过程的。 Yann LeCun： 我刚刚提到，我在很早就预见了这一切的发生。其一我一直相信这方法能行，虽然它需要很快的电脑和大量数据，但是我一直认为这是正确的做法。我在贝尔实验室的时候就见证了机器持续不断朝着越来越强大的方向进步，在贝尔实验室时期，我们就在设计芯片来运行卷积神经网络，我们用两块不同的芯片来高效地运行卷积神经网络，所以我们看到了芯片性能的改善，相信这会是一个持续不断的过程。 然而在 90 年代中期，对神经网络的兴趣逐渐走向衰微，所以这个过程没有立刻发生。在 1995 年到 2002 年这 6、7 年时间里，基本上没人进行相关研究。 微软在 21 世纪初，他们就用卷积神经网络做了中文字符识别，在法国以及其他地方也有一些小的有关人脸检测的工作，但规模都非常小。 我最近发现，有不少组提出的想法本质上与卷积神经网络非常相似，但是并没有发表，用于医疗图像分析。这些想法大部分是在卷积系统语境下，因此它并未应用到职业领域中。我的意思是在对卷积神经网络进行研究之后，研究者并未真正意识到它的力量，不过卷积神经网络依然得到了发展。 你知道很多人提出的想法是类似的，或者隔了几年提出的想法是类似的，但是在 2012 年 ImageNet 挑战赛出现后，研究者的兴趣改变得非常快。2012 年末在佛罗伦萨 ECCV 举办的 ILSVRC 2012 是一个非常有趣的赛事，ECCV 上有一个关于 ImageNet 的 workshop，每个人都知道 Geoffrey Hinton 团队的 Alex Krizhevsky 以极大的优势赢得了比赛，每个人都在等待 Alex Krizhevsky 的演讲，大会委员会的大部分人不知道 AlexNet 是什么。我的意思是他们听我讲过这个网络，在 CVPR 2000 会议上，但是大部分人并没有太注意它。一些资深的研究者知道 AlexNet 是什么，而社区中的大部分年轻人并不了解它。 然后 Alex Krizhevsky 进行了演讲，他并没有解释 AlexNet 是什么，因为他来自机器学习社区，认为所有人都已经知道了 AlexNet 的架构。很多人感到震惊，你可以看到在 Alex Krizhevsky 进行演讲的时候，人们的想法改变了，包括领域内非常资深的人。是的，计算机视觉领域的改变正是从那时开始。 吴恩达：那么今天你仍然保留 NYU 的教职，并且仍然在 FAIR 任职。我知道你对公司如何开展研究有着独到的见解，可以分享一下吗？ Yann LeCun： 我认为过去四年在 Facebook 的经历最美好的体验就是我被赋予充分的自由来按照自己认为最合适的方式建设 FAIR，因为这是 Facebook 内部第一个研究组织。 Facebook 是一家以工程为中心的公司，目前又重新聚焦生存或一些短期问题。在创立快 10 年的时候，这家公司成功上市，那时候差不多就开始思考下一个 10 年的问题。他们告诉我扎克伯格关于未来 10 年的想法，哪些问题将变得重要。那时候，Facebook 的生存已经不是问题。对于大公司来说，或者对于当时只有 5000 名员工的 Facebook 来说，那是一个转折点，它可以开始思考下一个 10 年的问题，思考技术的发展重点。 马克及他的团队认为，AI 将会成为一项关键的社交网络技术，同时这也是 Facebook 的使命所在。因此，他们探索了几种 AI 的利用方式。他们组建了一个小型的内部团队，用卷积网络在人脸识别和其他几个方面取得了很好的效果，这激发了他们的兴趣。于是他们开始尝试聘用一批年轻的研究者，收购一家 AI 公司，还有其他一些类似的举措。最终，他们决定聘用该领域的资深专家并创建了一个研究组织。最初，这种做法遭遇了一点文化冲击，因为公司使用的研究方法与工程大相径庭。人们会问，为什么你的时间比别人长，范围比别人大？研究者们对于他们想要选择的研究领域非常保守。在很早的时候我就清楚，研究应该是开放的，我们不仅需要鼓励，还应该要求研究者去发表研究成果，同时以一种我们熟悉的衡量标准去衡量这些成果，从而让我们有机会了解这些研究。马克和 CTO 迈克表示 Facebook 是一个开放的公司，我们在开源方面也有很多贡献。我们的 CTO 就曾致力于开源，此外公司还有不少人也是如此。可以说开放是刻在 Facebook 骨子里的。因此，或许这让我们有信心建立开源研究组织，Facebook 不像其他有些公司一样对知识产权有着执着的追求，这种文化使得我们更容易与高校合作，在产业界和学界都能有所涉猎。 如果你看看我过去四年发表的论文，就会发现大部分论文是我 NYU 的学生一起写的，因为在 Facebook 我做了很多实验室组织工作，指导科研方向等，但是我没有涉及个人研究项目，让我的名字出现在论文上。我并不在意论文。你会想要呆在幕后，不想和实验室里的人竞争。   吴恩达：对想要进入 AI 领域的人，你有什么建议？ Yann LeCun： 如今与我刚刚进入 AI 领域的时候已经大不相同了。我认为现在比较棒的是人们很容易就能达到一定水平，比如有方便使用的现成工具、TensorFlow、PyTorch 等，计算机会相对便宜，在家里就能训练卷积网络、循环网络。而且，你也可以在线学习。所以，你看现在高中生都在做 AI，我认为这非常棒。现在，从学生开始越来越多的人对机器学习、人工智能感兴趣。 我的建议是，如果你想做 AI，就要高度参与其中，例如为开源项目做贡献，或者实现一些标准算法。就像找到自己认为重要的论文，重现里面的算法，开源出来。如果你写的东西有用，你就会受到关注。这样，你可能就会收到中意公司的工作 offer，或者参与到喜欢的 PhD 项目中。 "
21,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740482&idx=1&sn=bc7c425677e0bd38abc1eea50fb84e05&chksm=871ad37cb06d5a6a6b70ed954addca6106f1cb935798f5445d8e5353ccc3e61baf4712367e73&scene=27,让AI掌握星际争霸微操：中科院提出强化学习+课程迁移学习方法,在围棋之后，即时战略游戏星际争霸是人工智能研究者们的下一个重要目标。近日，中科院自动化所提出了一种强化学习+课程迁移学习方法，让 AI 智能体在组队作战的条件下掌握了微操作的能力，该研究或许可以让多智能体 AI 方向的发展向前推进一步。该论文已被学术期刊 IEEE Transactions on Emerging Topics in Computational Intelligence 收录。 该研究的代码和结果已公开：https://github.com/nanxintin/StarCraft-AI 人工智能（AI）在过去的十年中已经有了巨大的进展。作为 AI 研究的绝佳测试平台，游戏自从 AI 诞生之时就在其身边推动技术的发展，与人工智能产生联系的游戏包括古老的棋盘游戏、经典的 Atari 街机游戏，以及不完美信息博弈。这些游戏具有定长且有限的系列动作，研究人员只需要在游戏环境中控制单个智能体。此外，还有多种更加复杂的游戏，其中包含多个智能体，以及复杂的规则，这对于 AI 研究非常具有挑战性。 在本论文中，我们专注于即时战略游戏（RTS）来探索多智能体的控制。RTS 游戏通常需要即时反应，这与棋盘游戏的回合制不同。作为最为流行的 RTS 游戏，《星际争霸》拥有庞大的玩家基础和数量众多的职业联赛——而且这个游戏尤其考验玩家的策略、战术以及临场反应能力。对于游戏 AI 的研究，星际争霸提供了一个理想的多智能体控制环境。近年来，星际争霸 AI 研究取得了令人瞩目的进展，这得益于一些星际争霸 AI 竞赛，以及游戏 AI 接口（BWAPI）的出现。最近，研究人员开发出了一些更加有效的平台来推动这一方向的发展，其中包括 TorchCraft、ELF 和 PySC2。 星际争霸 AI 旨在解决一系列难题，如时空推理、多智能体协作、对手建模和对抗性规划 [ 8 ]。目前，设计一款基于机器学习的全星际游戏 AI 是不现实的。许多研究者将微操作为星际争霸人工智能研究的第一步 [11]。在战斗场景中，单位必须在高度动态化的环境中航行，攻击火力范围内的敌人。星际争霸有很多微操方法，包括用于空间导航和障碍规避的潜在领域 [12] [13]、处理游戏中的不完整性和不确定性的贝叶斯建模 [14]、处理建造顺序规划和单位控制的启发式博弈树搜索 [15]，以及用于控制单个单位的神经进化方法 [16]。 作为一种智能学习方法，强化学习 ( RL ) 非常适合执行序列决策任务。在星际争霸微操任务中，RL 方法有一些有趣的应用。Shantia 等人使用在线 Sarsa 和带有短期记忆奖励函数的神经装配 Sarsa 来控制单位的攻击和撤退 [ 17 ]。它们利用视觉网格获取地形信息。这种方法需要手工设计，而且输入节点的数量必须随着单元的数量而改变。此外，他们还采用增量学习方法将任务扩展到具有 6 个单元的更大场景中。但是，增量学习的成功率仍然低于 50 %。温德尔等人在微操作中使用不同的 RL 算法，包括 Q 学习和 Sarsa [ 18 ]。他们控制一个强大的单位对抗多个彼此之间不存在协作的弱单位。 在最近的几年里，深度学习在处理复杂问题上已经实现了令人瞩目的成果，也大大提高了传统强化学习算法的泛化能力和可扩展性 [5]。深度强化学习（DRL）可以让智能体学习如何通过端到端的方式在高维状态空间中做出决策。Usunier 等人提出了一种通过深度神经网络进行微操作的强化学习方法。他们使用 greedy MDP 在每个时间步上有顺序地为单位选择动作，通过零阶优化（zero-order optimization）更新模型。这种方法能够控制玩家拥有的所有单位，并检视游戏的全局状态。Peng 等人则使用 actor-critic 方式和循环神经网络（RNN）来打星际争霸的对战（参见： ）。单位的控制由隐藏层中的双向 RNN 建模，其梯度更新通过整个网络高效传播。另一方面，与 Usunier 和 Peng 设计集中控制器的工作不同，Foerster 等人提出了一个多智能体 actor-critic 方法来解决去中心的微操作任务，这种方法显著提高了集中强化学习控制器的性能 [22]。 对于星际争霸的微操，传统方法在处理复杂状态、行动空间和学习合作策略方面存在困难。现代方法则依赖于深度学习引入的强大计算能力。另一方面，使用无模型强化学习方法学习微操通常需要大量的训练时间，在大规模场景中，这种情况更为明显。在中科院自动化所的新研究中，研究人员试图探索更高效的状态表示以打破巨大状态空间引发的复杂度，同时提出了一种强化学习算法用以解决星际争霸微操中的多智能体决策问题。此外，研究人员还引入了课程迁移学习（curriculum transfer learning），将强化学习模型扩展到各种不同场景，并提升了采样效率。 本论文的贡献主要分为三部分。首先，我们提出了一种高效的状态表征方法以处理星际争霸微操中的大型状态空间。这种方法考虑了单位的属性与距离，并允许双方使用任意数量的单位。与其他相关研究相比，我们的状态表征方法将更加高效、简洁。其次，我们提出了一种参数共享的多智能体梯度下降 Sara(λ) 算法（PSMAGDS）来训练我们的单位。使用神经网络作为函数近似器，智能体会共享集中化策略的参数，并同时使用自己的经验更新策略。这种方法能有效地训练同质智能体，并且鼓励合作行为。为了解决稀疏问题和延迟奖励，我们在 RL 模型中引进了包含小型中间奖励的奖励函数。该奖励函数能提升训练过程，并成为帮助单位相互协作的内在动力。最后，我们提出了一种迁移学习方法来扩展模型适应各种情景。与从头开始学习相比，这种方法在训练速度上有非常大的提升，并且在学习性能上也有很大的扩展。在大规模场景中，我们应用课程迁移学习（curriculum transfer learning）方法成功地训练了一组单位。就胜率而言，我们提出的方法在目标场景中优于很多基线方法。 本论文由六部分组成。第二节描述了星际争霸微操问题，以及强化学习和课程迁移学习的背景。在第三节中，本论文提出了用于微操的强化学习模型，包括状态表征方法、网络架构和行动定义。在第四节中，本论文介绍了参数共享的多智能体梯度下降 Sara(λ) 算法（PSMAGDS）和奖励函数。在第五节中，研究者介绍了本论文使用的星际争霸微操场景和训练细节。在最后第六节中，研究者对实验结果进行了分析，并进一步讨论模型学习到的策略。 图 2：课程迁移学习图示。存储通过解决源任务而获得的知识，逐渐应用到 M Curricular 任务上以更新知识。最终，知识被应用于目标任务。 图 3：一个单位在星际争霸微操场景中的学习模型表示。状态表示含三个部分，神经网络被用作函数逼近器。网络输出移动的 8 个方向和攻击动作的概率。 在这一研究中，星际争霸微操被定义为多智能体强化学习模型。我们提出了参数共享多智能体梯度下降 Sarsa(λ)（PSMAGDS）方法来训练模型，并设计了一个奖励机制作为促进学习过程的内在动机。整个 PS-MAGDS 强化学习范式如图 4 所示： 图 4：StarCraft 微操场景中的 PS-MAGDS 强化学习图示。 微操场景中不同单元的属性对比 图 5：实验中 StarCraft 微操场景中的表征。左：人族巨人 vs. 狂热者；中：人族巨人 vs. 狗；右：机枪兵 vs. 狗。 机枪兵 VS. 狗的微操的课程设计。M：机枪兵，Z：狗。 两个大场景中使用基线方法的模型的性能对比。M：机枪兵，Z：狗。 不同课程场景和未知场景中的胜率。M：机枪兵，Z：狗。 图 12：3 个人族巨人 vs. 6 个狂热者的微操场景中的样本游戏回放。 图 13：3 个人族巨人 vs. 20 只狗的微操场景中的样本游戏回放。 论文: StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning 论文链接： https://arxiv.org/abs/1804.00810 摘要： 近年来，即时战略游戏已成为游戏 AI 的一个重要领域。本论文展示了一种强化学习和课程迁移学习方法，可在星际争霸微操中控制多个单元。我们定义了一种高效的状态表征，破解了游戏环境中由大型状态空间引起的复杂性，接着提出一个参数共享多智能体梯度下降 Sarsa( λ )(PS-MAGDS) 算法训练单元。学习策略在我们的单元中共享以鼓励协作行为。我们使用一个神经网络作为函数近似器，以评估动作价值函数，并提出一个奖励函数帮助单元平衡其移动和攻击。此外，我们还用迁移学习方法把模型扩展到更加困难的场景，加速训练进程并提升学习性能。在小场景中，我们的单元成功学习战斗并击败了胜率为 100% 的内置 AI。在大场景中，课程迁移学习用于渐进地训练一组单位，并展示在目标场景中一些基线方法上的出众性能。通过强化学习和课程迁移学习，我们的单元能够在星际争霸微操场景中学习合适的策略。 
22,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740583&idx=1&sn=bf6cc9870f734f2391da40c94a387fac&chksm=871ad319b06d5a0fc22ee3becbc8471a18424f697eb12dabdcfed0cb4e6e3808b732cd0d6b27&scene=27,商汤科技C轮战略融资6亿美元，晋级世界第一AI独角兽,今日，中国人工智能创业公司商汤科技 SenseTime 宣布完成 6 亿美元 C 轮融资。继去年 7 月宣布 B 轮融资 4.1 亿美元后，商汤科技再次创下全球人工智能领域单轮融资记录；据彭博社消息，商汤估值已达 30 亿美元。据商汤科技信息，该轮融资由阿里巴巴集团领投，新加坡主权基金淡马锡、苏宁等投资机构和战略伙伴跟投。 商汤科技成立于 2014 年，2017 年 7 月获投 4.1 亿美元 B 轮融资，引起业内极大的关注。而在团队方面，2017 年左右商汤团队就已有 700 人，其中有 120 位拥有博士学位。 2017 年 11 月份，路透社发布新闻表示商汤科技公司创始人汤晓鸥在接受采访时向路透社透露说商汤正在计划 IPO，并尽早于明年在美国设立一个研发中心。而后商汤官方表示，「公司有未来上市计划，但是并无具体时间表。」 时隔不到一年，今天商汤宣布此高额 C 轮融资，由阿里巴巴集团领投，新加坡主权基金淡马锡、苏宁等投资机构和战略伙伴跟投。 在官方发布中，商汤科技联合创始人、CEO 徐立表示：「作为全球领先的人工智能平台公司，商汤科技 C 轮融资将进一步夯实公司在人工智能领域的领军地位：首先，以商汤原创技术为核心，赋能更多行业；其次，与全球头部伙伴进行深度合作，进一步拓展商业版图；第三，强化上下游产业链，深化商汤在人工智能产业链布局。」 阿里巴巴作为领投方，去年 11 月份就已有媒体爆出其对商汤科技的高额投资。在融资公告中，阿里巴巴集团执行副主席蔡崇信表示：「商汤科技积极拓展人工智能领域，尤其在深度学习和视觉计算方面，其科研能力让人印象深刻。阿里巴巴在人工智能领域的投入已为旗下业务带来显著效益，今后我们将继续在人工智能领域作出投资。我们期待与商汤科技的战略合作能够激发更多创新，为社会创造价值。」 在融资公告中，阿里巴巴集团执行副主席蔡崇信表示：「商汤科技积极拓展人工智能领域，尤其在深度学习和视觉计算方面，其科研能力让人印象深刻。阿里巴巴在人工智能领域的投入已为旗下业务带来显著效益，今后我们将继续在人工智能领域作出投资。我们期待与商汤科技的战略合作能够激发更多创新，为社会创造价值。」 商汤科技表示，此次阿里巴巴集团领投，淡马锡、苏宁等多家顶级投资机构和战略伙伴跟投，将为商汤提供更丰富的应用场景，更强大的海外布局能力，加速 AI 技术落地。商汤科技完成 C 轮融资后，将进一步强化在安防、手机、自动驾驶及互动娱乐等行业的落地优势，加快在智能零售、金融、教育等领域的技术落地，拓展商业版图，加速人工智能平台化发展。 
23,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740446&idx=2&sn=96334284ae9337a31d52ae0579ed0f1f&chksm=871ad2a0b06d5bb6895945c338f510919c0c31ffae24d41239c8a164870e81821ae5037e7576&scene=27,教程 | 用数据做酷的事！手把手教你搭建问答系统,"TowardsDataScience 本文介绍了如何基于 SQuAD 数据集搭建问答系统及其重要组件。 我最近很愉快地完成了斯坦福深度学习自然语言处理课程（CS224N），学到了很多新的东西。在结课项目中我基于斯坦福问答数据集（SQuAD）实现了一个问答系统。在这篇博客中，我将为大家介绍搭建问答系统所需要的主要模块。 完整代码 GitHub 地址：https://github.com/priya-dwivedi/cs224n-Squad-Project 斯坦福问答数据集（SQuAD）是一个全新的阅读理解数据集，由众包人员基于一系列维基百科文章的提问和对应的答案构成，其中每个问题的答案是相关文章中的文本片段或区间。SQuAD 包含关于 500 多篇文章的超过 100000 个问答对，规模远远超过其他阅读理解数据集。 最近一段时间，各种类型的模型在 SQuAD 数据集上的效果获得了快速的发展，其中最新的一些模型在问答任务中甚至取得了和人类相当的准确率。 SQuAD 数据集中的语境、问题和答案的示例 语境：阿波罗计划于 1962 至 1972 年间进行，期间得到了同期的双子座计划（1962 年 - 1966 年）的支持。双子座计划为阿波罗计划成功必需的一些太空旅行技术做了铺垫。阿波罗计划使用土星系列火箭作为运载工具来发射飞船。这些火箭还被用于阿波罗应用计划，包括 1973 年到 1974 年间支持了三个载人飞行任务的空间站 Skylab，以及 1975 年和前苏联合作的联合地球轨道任务阿波罗联盟测试计划。 问题：哪一个空间站于 1973 到 1974 年间承载了三项载人飞行任务？ 答案：Skylab 空间站 SQuAD 的主要特点： i) SQuAD 是一个封闭的数据集，这意味着问题的答案通常位于文章的某一个区间中。 ii) 因此，寻找答案的过程可以简化为在文中找到与答案相对应部分的起始索引和结束索引。 iii) 75% 的答案长度小于四个单词。 i) 嵌入层 该模型的训练集包括语境以及相关的问题。二者都可以分解成单独的单词，这些单词会被转换成使用预训练向量（如 GloVe）的词嵌入。想了解更多关于词嵌入的信息，参考《教程 | 用数据玩点花样！如何构建 skim-gram 模型来训练和可视化词向量》。同 one hot 向量相比，用词嵌入方式对单词进行表示可以更好地捕捉语境信息。考虑到没有足够的数据，我使用了 100 维的 GloVe 词嵌入并且在训练过程中没有对它们进行修改。 ii) 编码器层 我们将基于 RNN 的编码器加入到了模型的下一层当中。我们希望语境中的每一个单词能和它前后的单词产生联系。双向 GRU/LSTM 可以帮助我们达到这一目标。RNN 的输出是一系列向前、向后的隐藏向量，然后我们会将它们级联起来。类似地，我们可以使用相同的 RNN 编码器创建问题隐藏向量。 iii）注意力层 现在我们有了一个语境隐藏向量和问题隐藏向量。我们需要将这两个向量结合起来，以找到问题的答案。这时就需要用到注意力层。注意力层是问答系统的关键组成部分，因为它能帮助确定对于给定的问题我们应该「注意」文中的哪些单词。让我们从最简单的注意力模型开始： 点积注意力 点积注意力等于每个语境向量 c_i 乘每个问题向量 q_j 的结果向量 e^i（上图中的注意力分数）。之后，我们对 e^i 调用 softmax 函数来得到 α^i（上图中的注意力分布）。softmax 保证了所有 e^i 的和是 1。最终，我们计算出 a_i：注意力分布 α^i 与对应问题向量（上图中的注意力输出）的积。点积注意力也可以用下面的式子来描述： 上面提到的注意力已作为基线注意力机制在 GitHub 代码中实现。 更复杂的注意力——BiDAF 注意力 你可以用上述基本注意力层来运行 SQuAD 模型，但恐怕结果不尽人意。更复杂的注意力才能产出更好的性能。 我们来了解一下 BiDAF 论文（https://arxiv.org/abs/1611.01603）。该论文的主要观点是注意力应该是双向的——从语境到问题和从问题到语境。 我们首先计算相似度矩阵 S ∈ R^N×M，它包含每对语境和问题隐藏状态 (c_i , q_j) 的相似度分数。这里 c_i ◦ q_j 代表数组元素对应相乘，w_sim ∈ R 6h 是权重向量。S_ij 用下面的式子来表述： 之后，我们将展示 C2Q 注意力（与上面提到的点积注意力类似）。我们对 S 逐行调用 softmax 函数来获得注意力分布 α^i，用它得到问题隐藏状态 q_j 的加权和，最后得出 C2Q 注意力的输出 a_i。 现在，我们来执行 Q2C 注意力。对于每一个语境位置 i ∈ {1, . . . , N}，我们取相似度矩阵对应行的最大值： 之后我们对结果向量 m ∈ R^N 调用 softmax 函数，而这将给出关于语境位置的注意力分布 β ∈ R^N。之后，我们使用 β 得到语境隐藏状态的加权和 c_i，这也是 Q2C 注意力的输出结果 c'。以下是相关公式： 最终对于每一个语境位置 c_i，我们结合 C2Q 注意力和 Q2C 注意力的输出，下面是相关公式： 如果你觉得这一段令人费解，不用担心，注意力确实是一个复杂的话题。你可以试着一边喝茶，一边阅读这篇 BiDAF 论文。 iv) 输出层 我们就快成功了。模型的最后一层是一个 softmax 输出层，它帮助我们找出答案区间的开始和结束索引。我们通过结合语境隐藏状态和之前层的注意力向量来得到混合的结果。这些混合的结果最终会成为全连接层的输入，该层使用 softmax 来得到 p_start 向量（具备开始索引的概率）以及 p_end 结束（具备结束索引的概率）。我们知道大部分答案从开始索引到结束索引最多 15 个单词，由此我们可以寻找使 p_start 与 p_end 乘积最大的开始和结束索引。 损失函数是开始和结束位置的交叉熵损失之和。它使用 Adam Optimizer 来获得最小值。 我构建的最终模型比上面描述的要复杂一点，在利用测试集测试时获得了 75 分的 F1 分数。还行！ 关于未来探索的一些想法： 由于 CNN 运行起来比 RNN 快得多，并且更容易在 GPU 上并行计算，因此我最近一直都在用基于 CNN 的编码器而非上述 RNN 编码器进行实验。 其他的注意力机制，如 Dynamic Co-attention（https://arxiv.org/abs/1611.01604） "
24,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740387&idx=5&sn=593eec93c51a610327266f893b09fc25&chksm=871ad2ddb06d5bcb015b88d23d648bdec7b5a06ac34fef8ef92241168e77445af9178c5ff1c2&scene=27,活动 | 2018深圳国际机器人与智能系统院士论坛邀请函,今年，总理在政府工作报告中四次提及“智能”，并特别指出要“加强新一代人工智能研发应用”与“发展智能产业”。机器人与智能化，已成为中国未来的发展方向，机器人与智能产业将迎来爆发式发展。 为此，由工业和信息化部、深圳市人民政府联合指导，深圳中电国际信息科技有限公司（中电港）、深圳市机器人协会主办2018第四届深圳国际机器人与智能系统博览会将于 。展览展示机器人行业的发展成就及研发成果，促进行业的进一步交流与合作，为深化机器人产业合作与发展、拓展机器人产业市场提供了重要平台。 作为本次国际机器人展重要活动的“2018深圳国际机器人与智能系统院士论坛”也将于2018年4月9日上午在深圳会展中心簕杜鹃厅举办。论坛邀请到了曲道奎、李远清等多名IEEE Fellow、国内外知名专家、行业领袖出席论坛并发表主题演讲，共同探讨机器人与智能制造业的发展，分析市场现状、未来发展方向，以及机器人与智能系统的应用技术和行业发展策略，寻找创新契机和发展道路，共同推进机器人产业的发展。 本次论坛还将正式发布由深圳市经济贸易和信息化委员会的委托，中科院深圳先进技术研究院和深圳市机器人协会共同调研撰写的《2017年度深圳机器人产业发展白皮书》。 诚邀您出席“2018深圳国际机器人与智能系统院士论坛”。 “深圳国际机器人与智能系统院士论坛”已成功举办了四届，历届论坛吸引了来自国内外30多位海内外知名科学家、两院院士、经济学家、企业家等业界精英齐聚论坛，2000多名各行各业观众聆听了专家们的精彩演讲和参与讨论。 1、结合主论坛、专题论坛、现场讨论、解决方案展览、企业经验交流等多种形式，邀请IEEE Fellow、行业知名专家、行业领袖和企业首席科学家发表演讲，帮助企业全面了解国内外机器人与智能系统相关技术的应用与发展状况。 2、本次论坛除主论坛外还有两个分论坛，集中对“人工智能技术发展与应用”以及“3C产业智能制造应用”进行专题演讲和研讨，分论坛也将于4月9日下午同期举行。 3、本次论坛还将正式发布由深圳市经济贸易和信息化委员会的委托，中科院深圳先进技术研究院和深圳市机器人协会共同调研撰写的《2017年度深圳机器人产业发展白皮书》。 时间： 2018 年 4 月 9 日(周一)9:30—12:00  地址： 深圳会展中心五楼 簕杜鹃厅 议程： 时间： 2018 年 4 月 9 日(周一)14:00—17:00  地址： 深圳会展中心五楼玫瑰 2 厅 议程： 时间： 2018 年 4 月 9 日(周一)14:00—17:00  地址： 深圳会展中心五楼玫瑰 3 厅 议程： 会议联系人： 杨小彬 13602648557 （同微信），邮箱xb.yang@siat.ac.cn 点击 ，立即报名。 
25,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740387&idx=4&sn=ca7329d12ab6dea30ead8b436e3b328d&chksm=871ad2ddb06d5bcbf878dbc47c52c278573fc6f69312ff4dd2fcfbc73ba16d62f3049165b5f8&scene=27,学界 | UIUC & Zillow提出LayoutNet：从单个RGB图像中重建3D房间布局,近日，来自 UIUC 和 Zillow 的研究者在 arXiv 上发布论文，提出 LayoutNet——一种仅通过单张透视图或全景图就能估算室内场景 3D 布局的深度卷积神经网络（CNN）。该方法在全景图上的运行速度和预测精度比较好，在透视图上的性能是最好的方案之一。该方法也能够推广到非长方体的曼哈顿布局中。目前，该论文已经被 CVPR 2018 接收。 对于机器人和虚拟现实、增强现实这样的应用来说，从图像中估算出房间的三维布局是一个重要的任务。房间的布局指墙壁相对于相机中心的位置、方向以及高度。布局可以表示为一组经过投影处理的角落位置或边界，或者表示为一个 3D 网格。现有的研究被应用于一些特定的问题，例如通过透视图或全景图预测长方体形状的室内布局。 本论文提出了 LayoutNet，它是一个仅通过单张透视图或全景图（如图 1 所示）就能估算室内场景 3D 布局的深度卷积神经网络（CNN）。该方法在全景图上的运行速度和预测精度比较好，在透视图上的性能是最好的方案之一。该方法也能够推广到非长方体的曼哈顿布局中，例如「L」形的房间。 代码地址：https://github.com/zouchuhang/ LayoutNet 图 1. LayoutNet 根据单张等距柱状投影的全景图预测一个非长方体房间的布局。 LayoutNet 方法的工作流程包含三个步骤（如图 2 所示）。首先，系统分析消失点，并且将图像与地面对齐在一条水平线上（见 Sec. 3.1）。这种对齐方式确保了墙与墙的边界是垂直的线，根据实验结果，该操作大大降低了误差。第二步，使用一个带有编码器-解码器结构和跳跃连接的卷积神经网络直接预测图像上的角（布局中的连接处）和边界的概率图。每个角落和边界都提供了房间布局的完整表示。研究者发现，在单个网络中一起预测它们将得到更好的估计结果。最终，研究者对三维布局参数进行了优化，用于拟合预测出的角落和边界（见 Sec. 3.4）。最后三维布局优化过程的损失很难在网络中进行反向传播，但是训练过程中对 3D 参数执行的直接回归（direct regression）起到了有效的替代作用，这最大化提升了最终预测的准确度。 本文的突出贡献有： 提出了一种更加通用的根据 RGB 图像推断出布局的算法，它适用于曼哈顿布局的透视图和全景图。该系统在全景图像上有较好的运行速度和预测准确度，在透视图图像上取得了第二优的综合预测性能和最优的运算速度。 展示了利用预先计算出的消失点线索、几何约束以及后处理优化的好处，说明深度神经网络方法仍然能够从几何线索和约束中受益。研究者还展示了添加目标函数以直接回归 3D 布局参数，从而更好地预测用于最终解决布局预测问题的边界和角落。 扩展了斯坦福「2D-3D」数据集的注释 [1]，提供了可用于后续工作的房间布局注释。 图 2. 概述。LayoutNet 遵循编码器-解码器策略。网络的输入是单张 RGB 全景图和曼哈顿线图的级联。该网络将一同预测布局的边界和角落的位置。3D 布局参数损失使得预测准确率最大化提升。最终的预测结果是一个曼哈顿约束下的布局重建。 LayoutNet 网络架构如图 2 所示。该网络遵循编码器-解码器策略。深度全景编码器：输入为一个 6 通道的特征映射，即使用 Sec. 3.1 中提到的对齐方法将分辨率为 512*1024 的单个 RGB 全景图（或者分辨率为 512*512 的透视图）和三个正交消失方向上的曼哈顿线图的特征映射级联起来。编码器包含 7 个卷积层，卷积核的大小为 3*3。每个卷积之后会跟随一个 ReLU 操作和最大池化层，其下采样因子为 2。第一个卷积层有 32 个特征，研究者在每次卷积操作之后将特征规模扩大到之前的两倍。这个深度神经网络结构确保从高分辨率图像中学习到更好的特征，有助于简化解码步骤。研究者尝试在每一个卷积层之后进行批量归一化操作，但是发现这样做预测准确率降低。研究者还探索了另一种网络结构，单独将一个编码器应用于输入图像和曼哈顿线图上，但它与研究者目前使用的简单设计相比，性能没有得到提升。 表 1. 使用 PanoContext 数据集 [33] 从全景图中得到的长方体布局量化预测结果。研究者比较了 PanoContext 方法，并且在本文提出方法的各种配置参数上引入了模型简化分析。粗体数字表示训练 PanoContext 数据时得到的最佳性能。 表 3. 在研究者标注的斯坦福 2D-3D 注释数据集上的模型评估结果。研究者通过对各种变量的模型简化分析评估了 LayoutNet 方法。粗体数字表示仅仅在斯坦福 2D-3D 训练数据集上的最佳训练结果。 图 3. 在 PanoContext 数据集 [33] 上对长方体布局预测的定性分析结果（随机抽样）。研究者展示了其方法（偶数列）和当前最优方法 [33]（奇数列）的性能。每个图像由给定计算方法预测出的布局（橙色的线）和标定的真实布局（绿色的线）组成。本文方法在像素层面上是十分准确的，但是正如定量分析结果中交并比（IoU）这一测度所显示的那样，三维布局预测对即使是很小的二维预测误差都很敏感。 图 4. 在斯坦福 2D-3D 注释数据集上对长方体布局预测的定性分析结果（随机抽样）。与 PanoContext 数据集相比，这个数据集更加棘手，因为它垂直方向的视场更小，而且更加闭塞。研究者展示了其方法预测出的布局（橙色的线），并将其与真实的布局（绿色的线）进行了对比。 图 5. 对透视图的定性分析结果。研究者展示了输入的 RGB 图像，预测了边界/角落图以及最终估算出来的分布（橙色的线），并将其与真实的布局（绿色的线）进行了对比。 论文：LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image 论文链接： https://arxiv.org/abs/1803.08999 摘要：我们提出了一种根据单张图像预测房间布局的算法，它能够被推广到全景图、透视图、长方体布局和更一般化的布局中（如 L 形房间）。我们的方法可直接在全景图像上运行，而不是像近来的一些研究那样将全景图分解成多个透视图。我们的网络架构类似于 RoomNet，但是我们展示了一系列改进：根据消失点将图像对齐、预测多个布局元素（角落、边界、大小和图像转化），并且将一个带约束的曼哈顿布局和最终的预测结果进行了拟合。在全景图上，我们的方法在运算速度和预测准确度上有较好的性能；在透视图上，我们方法的预测准确度是最优方法之一，并且能够处理长方体形状布局和更一般的曼哈顿布局。 
26,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740446&idx=4&sn=65b7c8fcb3231e5fcea09b566d88dcbb&chksm=871ad2a0b06d5bb6495648b88f91a0cd50fc28626cea8099016735267bb1b97875a8f6d4a4d8&scene=27,CVPR2018 | 直接建模视觉智能体？让「小狗」动起来～,"近日，来自华盛顿大学和艾伦人工智能研究所的研究者在 arXiv 上发布论文，介绍了其处理计算机视觉任务的新方法：利用视觉数据直接建模视觉智能体。研究者对狗的相关动作进行建模，在多种度量方式下，对于给定视觉输入，其模型能成功地在各种环境下建模智能体。此外，该模型学得的表征能编码不同的信息，还可以泛化至其他的领域。目前，该论文已被 CVPR 2018 接收。 1. 引言 计算机视觉研究通常集中在一些特定的任务上，包括图像分类、目标识别、目标检测、图像分割等等。这些任务出现，并随着时间的推移逐渐成为视觉智能问题实际应用的典型代表。视觉智能涵盖了许多领域，很难正式地定义或评估。因此，这些代表性任务成为社区重点关注的对象。 本论文作者承认这些计算机视觉研究领域的代表性任务所带来的影响，也赞成对这些基本问题进行持续性的研究。然而，这些代表性任务的理想输出与视觉智能系统的期望功能之前仍然存在差距。这篇论文对视觉智能问题给出了直接的答案。受影响于近期关于行为与互动在视觉理解中作用的研究 [56, 3, 31]，本论文研究者将视觉智能问题定义为「理解视觉数据，使得智能体能够在视觉世界中执行动作并解决问题」。在这样的定义下，研究者提出学习像这样的智能体一样在视觉世界里处理问题。 通常情况下，模仿视觉智能体是一个充满挑战并且难以定义的问题。一个动作通常对应一系列包含复杂语义的运动。本论文通过将动作视为其最基本、无语义的形式——简单运动，在模仿视觉智能体方面做出了微小的贡献。 研究者将对狗建模，作为视觉智能体。狗相对人来说，有着更简单的动作空间，使研究变得相对简单。同时，它们能很好地展示视觉智能的特性，例如它们可以分辨食物、障碍、别的动物以及人类，并作出相应的反应输出。然而，它们的目的和动机通常是事先不知道的。因此研究者可以说是在建模一个黑箱。关于这个黑箱系统，我们只知道它的输入和输出。 本论文研究如何基于视觉输入学习模仿狗的行为和动作规划方式。研究者编写了一个以狗为第一人称视角的动作数据集 ( DECADE )，包括以狗为第一人称视角的视频及其对应的运动。为了记录相关的运动，研究者在狗的身体和关节处安装了惯性测量单元 (IMU)。研究者记录了这些装置的绝对位置，然后计算狗的四肢与身体之间的相对角度。 使用 DECADE 数据集，研究者探索了上面提到的三个主要问题 ( 见图 1 )：(1) 模仿狗的行为；(2) 模仿狗的动作规划方式；(3) 将狗的行为动作作为表征学习的监控信号。 在学习模仿狗的行为时，研究者通过观察狗到目前为止的观察结果来预测狗在未来可能的动作（关节屈伸）。在模仿狗的动作规划方式时，研究者解决了预测狗的系列运动动作的问题，这些动作将狗的状态从一个特定状态转变为目标状态。在利用狗作监督时，研究者发现将狗的动作用于表征学习的潜力。 结果是令人欣喜的。研究者的模型可以预测狗在各种场景下的运动（模仿狗的行为），也可以预测狗如何决定从一个状态转化为另一状态（模仿狗的动作规划方式)。除此之外，研究者还展示了根据狗的行为构建的模型也可以泛化至其他的一些任务。更重要的是，在使用狗行为模型为可行走表面预测以及场景识别等任务作预训练之后，这些任务的结果准确率都得到了提高。 论文：Who Let The Dogs Out? Modeling Dog Behavior From Visual Data 论文链接：https://arxiv.org/abs/1803.10827 摘要：我们研究了如何直接建模一个视觉智能体。计算机视觉通常专注于解决各种与视觉智能相关的子任务。我们偏离了处理计算机视觉任务的标准方法，直接对视觉智能体进行建模。我们的模型将视觉信息作为输入并直接预测视觉智能体的动作。为了达成这一目标，我们引入了 DECADE，一个包含以狗为第一人称视角的视频以及相应动作的数据集。利用这样的数据集，我们可以建模狗的行为方式和动作规划方式。在多种度量方式下，对于给定视觉输入，我们能成功地在各种环境下建模智能体。此外，相比用图像分类训练出的表征学习，我们的模型学得的表征能编码不同的信息，还可以泛化至其他的领域。特别是，通过将这种对狗的建模用于表征学习，我们在可行走表面预测和场景分类任务中得到了非常好的结果。 "
27,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740387&idx=2&sn=81c7077d3bf8d365a84aeda46ce9b804&chksm=871ad2ddb06d5bcbaa607ceab17dcc363d5eb639ae43d942d5513bbd12ac9815efe54a11fe0b&scene=27,业界 | 谷歌发布MobileNetV2：可做语义分割的下一代移动端计算机视觉架构,深度学习在手机等移动端设备上的应用是机器学习未来的重要发展方向。2017 年 4 月，谷歌发布了 MobileNet——一个面向有限计算资源环境的轻量级神经网络。近日，谷歌将这一技术的第二代产品开源，开发者称，新一代 MobileNet 的模型更小，速度更快，同时还可以实现更高的准确度。 项目链接：https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet 谷歌 2017 年推出了 MobileNetV1，它是一种为移动设备设计的通用计算机视觉神经网络，因此它也能支持图像分类和检测等。一般在个人移动设备上运行深度网络能提升用户体验、提高访问的灵活性，以及在安全、隐私和能耗上获得额外的优势。此外，随着新应用的出现，用户可以与真实世界进行实时交互，因此我们对更高效的神经网络有着很大的需求。 今天，谷歌很高兴地宣布下一代移动视觉应用 MobileNetV2 已经发布。MobileNetV2 在 MobileNetV1 的基础上获得了显著的提升，并推动了移动视觉识别技术的有效发展，包括分类、目标检测和语义分割。MobileNetV2 作为 TensorFlow-Slim 图像分类库的一部分而推出，读者也可以在 Colaboratory 中立即探索 MobileNetV2。此外，我们也可以下载代码到本地，并在 Jupyter Notebook 中探索。MobileNetV2 在 TF-Hub 中会作为模块使用，且预训练保存点可在以下地址中找到。 Colaboratory 试验地址：https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb MobileNetV2 本地实验地址：https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb 预训练模型下载：https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet MobileNetV2 基于 MobileNetV1[1] 的基本概念构建，并使用在深度上可分离的卷积作为高效的构建块。此外，MobileNetV2 引入了两种新的架构特性：1）层之间的线性瓶颈层；2）瓶颈层之间的连接捷径。MobileNetV2 的基本架构展示如下： 我们可以直观理解为，瓶颈层对模型的中间输入与输出进行编码，而内层封装了模型从像素等低级概念到图像类别等高级概念的转换能力。最后，与传统的残差连接一样，捷径能快速训练并获得更优精确度。读者可查阅文末的 MobileNetV2 论文了解更多的详情。 V2 与第一代的 MobileNet 相比有什么区别？总体而言，MobileNetV2 模型在整体延迟范围内上实现相同的准确度要更快。特别是，目前新模型减少了两倍 operations 的数量，且只需要原来 70% 的参数，在 Google Pixel 手机上的测试表明 V2 要比 MobileNetV1 快 30% 到 40%，同时还能实现更高的准确度。 MobileNetV2 不仅速度更快（降低延迟），还刷新了 ImageNet Top 1 准确度。 MobileNetV2 是一个用于目标检测和分割的非常有效的特征提取器。比如在检测方面，当 MobileNetV2 搭配上全新的 SSDLite [2]，在取得相同准确度的情况下速度比 MobileNetV1 提升了 35%。我们已通过 Tensorflow Object Detection API [4] 开源了该模型。 为实现实时语义分割，我们借助简化版 DeepLabv3 [3] 把 MobileNetV2 用作特征提取器，这将稍后公布。在语义分割基准 PASCAL VOC 2012 上，MobileNetV1 与 MobileNetV2 作为特征提取器表现相当，但是后者所需的参数量减少了 5.3 倍，在 Multiply-Adds 方面 operations 也减少了 5.2 倍。 正如我们所看到的，MobileV2 面向移动端提供了一个非常高效的模型，它能处理许多基本的视觉识别任务。最后，谷歌也希望能与广泛的学术社区和开源社区分享这个新模型，并期待它有新的提升与应用。 论文：MobileNetV2: Inverted Residuals and Linear Bottlenecks 论文链接：https://arxiv.org/abs/1801.04381 本文中我们介绍了一种新的移动端架构——MobileNetV2，其在多任务和基准以及不同模型大小的范围上进一步刷新了移动端模型的当前最佳性能。我们还介绍了如何通过全新框架 SSDLite 将这些模型高效应用于目标检测。此外，我们也展示了通过简化版 DeepLabv3（我们称之为 Mobile DeepLabv3）构建移动端的语义分割方法。 MobileNetV2 架构基于反向残差结构，其中残差块的输入和输出是较短的瓶颈层，这与在输入中使用扩展表征的传统残差模型正相反。MobileNetV2 使用轻量级深度卷积过滤中间扩展层的特征。此外，我们发现为了保持表征能力，移除短层中的非线性很重要，这提升了性能，并带来了催生该设计的直观想法。最后，我们的方法允许将输入/输出域与转换的表现性分开，从而为未来的分析提供一个简便的框架。我们在 ImageNet 分类、COCO 目标检测、VOC 图像分割上测试了 MobileNetV2 的性能，同时也评估准确度、operations 数量（通过 MAdd 测量）以及参数量之间的权衡。 
28,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740387&idx=3&sn=37e4ed85c2e7de97f5b43259adde3ae9&chksm=871ad2ddb06d5bcb96d74bc94e8d527284b3cd93c0a2a80cc61103e6762307c8ee1b660be989&scene=27,前沿 | 没有地图也能导航：DeepMind展示全新AI导航技术,当前的无人驾驶汽车高度依赖于精确的地图进行导航，尽管各家科技公司已经构建了接近完美的 3D 地图，但这种方式仍然存在一些弊端（巨大的容量、需要不断更新等）。近日，DeepMind 提出了一种端到端深度强化学习寻路方法，其训练的神经网络可以帮助汽车在没有地图的情况下正确前往目的地，这一研究或许可以帮助自动驾驶汽车技术向前迈进一大步。 论文链接：https://arxiv.org/abs/1804.00168 小时候，你是如何熟悉周围环境路线的？例如怎么去朋友家、去学校或者去杂货铺？可能没有地图，只是简单地记住街道的外观、沿路的变向。随着在附近街区的探索逐渐增多，你变得更加自信，开始学习新的、更复杂的路。有时你可能会迷路，但是在路标或者太阳（指南针）的帮助下你可以重新找到正确的路。 导航是一项重要的认知任务，帮助人类和动物在没有地图的情况下穿过复杂世界中长长的路途。此类长距离导航可同时支持自我定位（「我在这里」）和目标表征（「我要去那儿」）。 在论文《Learning to Navigate in Cities Without a Map》中，DeepMind 展示了一种交互式导航环境，该环境使用来自谷歌街景的第一人称视角图像，并游戏化该环境来训练 AI。尽管谷歌街景图像已经很标准了，但是人脸和汽车牌照比较模糊、无法辨认。DeepMind 构建了一个基于神经网络的人工智能体，可使用视觉信息（来自谷歌街景图像的像素）学会在多个城市之间导航。注意该研究是关于通常意义上的导航，并非驾驶。DeepMind 未使用交通信息，也没有尝试建模车辆控制。 当智能体到达目标地点时会得到奖励（目标地点是指定的，如经纬度坐标），就像一个没有地图、带着大量货物的快递员。随着时间的推移，该人工智能体学会用这种方式穿越整个城市。DeepMind 还展示了其智能体可在多个城市中学习执行该任务，然后稳定地泛化至新的城市。 智能体在巴黎训练时的定格动画。图像右上方是城市地图，显示目的地（红色）和智能体位置和视野（绿色）。注意该智能体无法看到地图，只能看到目的地的经纬度坐标。 不通过地图构建来学习导航 DeepMind 背离了传统的依赖于地图绘制和探索的方法（例如制图员给自己定位同时绘制地图）。相反，他们的方法是让系统像人类一样导航，不需要地图、GPS 定位或其它帮助，只需要使用视觉观测。他们构建了神经网络智能体，它以对环境的视觉观测图像为输入，并预测自己的下一个动作。他们使用了深度强化学习来端对端地训练智能体，这和最近的两项研究《LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS》以及《REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS》相似。但和这些研究所不同的是，他们没有使用小规模的虚拟环境，而使用了城市规模的真实世界数据，包括伦敦、巴黎和纽约中复杂的交汇道路、人行道、隧道以及各种拓扑结构。此外，他们的方法支持特定城市的学习和优化，以及通用的可迁移的导航行为。 模块化的神经网络架构，可迁移至新城市的导航 智能体的神经网络由三部分构成：（1）可处理图像和提取视觉特征的卷积神经网络；（2）地区特定的循环神经网络，可记忆环境，以及学习当前位置和目标位置的表征；（3）区域不变（locale-invariant）的循环网络，可以生成对智能体动作的导航策略。地区特定的模块被设计成可替换的，并且对于其导航的城市是唯一的，而视觉模块和策略模块则是区域不变的。 （a）CityNav 架构的对比；（b）MultiCityNav 架构，为每个城市提供地区特定的路径；（c）训练过程和将智能体适应到新城市的迁移过程。 正如谷歌街景中的界面一样，智能体可以在其位置旋转或走向下一个全景图。但和谷歌地图以及街景环境不同的是，智能体没有小箭头提示、局域或全局地图，也没有著名的 Pegman（学习区分公路和人行道）。智能体的目的地可能位于现实世界的数公里之外，它需要逐步利用数百个全景图才能到达目的地。 DeepMind 表示其方法提供了一种将知识迁移至新城市的机制。当智能体访问新城市时，人类当然希望它学习一系列新地标，但是没必要重新学习视觉表征或行为（如沿街道推进或在路口转弯）。因此，DeepMind 使用 MultiCity 架构，首先在多个城市中进行训练，然后冻结策略网络、视觉卷积网络和多个新城市特定路径。该方法使智能体在不遗忘之前所学知识的前提下获取新知识，与《Progressive Neural Networks》中的架构类似。 该研究中使用的曼哈顿五个区域地图 导航是人工智能研究和发展中的基础研究，尝试在人工智能体中复现人类导航也可以帮助科学家理解其生物性基础。 
29,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740446&idx=1&sn=817d07c82bbbff676972075c770e47d1&chksm=871ad2a0b06d5bb6ce2fe80a8debff87e405eb44136fde46f96e9ee03d293db0b0952d1a0bd8&scene=27,谷歌员工千人上书CEO皮查伊：抵制五角大楼AI项目Project Maven,作为拥有先进技术的科技巨头，谷歌这样的公司不免会与军事计划产生联系。在有侵犯隐私、违背伦理的风险下，是否应该接下巨额合同？最近，听闻公司正在参与美国国防部「Project Maven」AI 军事计划，很多谷歌员工发起了抵制，他们上书 CEO Sundar Pichai 的公开信现在已经获得了数千人的联名签署。 五角大楼项目「Project Maven」主要应用深度学习计算机视觉技术，旨在帮助国防部门从图像和视频中提取值得注意的对象。该项目是在 2017 年 4 月由时任副国防部长 Bob Work 在一份备忘录中首次披露的。目前，美国已经成立了一个算法战争混合任务团队（Algorithmic Warfare Cross-Functional Team），由国防情报局副部长监督。 近日，数千名谷歌员工（包括数十名高级工程师）签署了一份联名信函，抗议该公司参与五角大楼的一个项目，该项目利用人工智能解读视频图像，旨在提高无人机打击的精准度。 该信函在谷歌内部迅速流传开来，已有超过 3100 人签名，反映了硅谷与联邦政府之间的文化冲突。随着人工智能前沿技术越来越多地用于军事领域，这种冲突可能会愈演愈烈。 信是写给谷歌 CEO Sundar Pichai 的。信中写道：「我们认为谷歌不应卷入战争。」信中要求谷歌撤出五角大楼的试点项目 Project Maven，并宣布永远不会「发展战争技术」。 当然，这种理想主义的想法并不是所有谷歌员工都认同的，但对于一家以「不作恶」（信中也有引用）为座右铭的公司来说，提出这种想法是很正常的。但这显然与华盛顿庞大的国防工业不同，当然也与五角大楼不同。现任美国国防部长 Jim Mattis 经常说，五角大楼的中心目标是提高美国军队的「杀伤力」。 从早期开始，谷歌就鼓励员工就涉及公司的问题发表意见。该公司提供了内部留言板和社交网络，员工可以在这些平台上就公司的产品和政策向管理层或同事提出意见。最近，围绕谷歌努力打造一支更加多元化的员工队伍的激烈辩论日益公开化。 谷歌员工已就一系列问题发出了抗议请愿书，包括该公司落后于 Facebook 的社交网站 Google Plus 以及谷歌对保守政治行动会议（CPAC）的赞助。 在最近的一次公司会议上，员工就谷歌参与 Project Maven 提出质疑。当时，谷歌云基础设施业务主管 Diane Greene 为这一项目辩护，并试图安抚相关员工。公司发言人表示，抗议信上的大多数签名是在公司对此情况进行解释之前收集的。 该公司随后将 Project Maven 的工作描述为「非攻击性的」，尽管五角大楼的视频分析通常用于反暴动和反恐行动，国防部也曾发文表示该项目用于支持这些行动。谷歌和五角大楼都表示，谷歌的产品不会创造出无需人类操作者就能开火的自主武器系统，自主武器系统在人工智能应用中也有很大争议。 对无人机视频做进一步分析，可以挑选出人类目标并实施袭击，换个角度来说，这也有助于更好地分辨出平民，以减少无辜者被意外杀害的风险。 谷歌周二发布的一项声明并未直接提及这份致 CEO Sundar Pichai 的信函，声明中提到：「任何机器学习的军事化应用都会导致人们的合理担忧，这是自然的。我们正在公司内部积极鼓励对该重要议题进行充分讨论。」谷歌称这种交流「非常重要，也非常有益」，而熟知信件内容的多名谷歌员工仅在匿名条件下发声，并表示这是因为担心被报复。 该声明称谷歌在 Project Maven 中所参与的部分「仅限于非攻击性的目的」，虽然官方拒绝提供相关的合同内容。美国国防部称由于谷歌是主承包商 ECS Federal 在 Project Maven 的分包商，它不能提供谷歌合同或合同内容。ECS Federal 尚未回应该询问。 谷歌称五角大楼在使用基于未分类数据且「对任何谷歌云客户开放的开源目标识别软件」。谷歌表示，「这项技术用于标记图像，以帮助人工审查，其目的是把人们从高度单调的工作中解放出来。」 谷歌部分高管和五角大楼有密切联系。该公司前执行董事长 Eric Schmidt（目前仍是谷歌母公司 Alphabet 的执行董事会成员）为五角大楼顾问团 Defence Innovation Board 的成员，该顾问团还包括谷歌副总裁 Milo Medin。 在 11 月份的一个采访中，Mr. Schmidt 承认「技术社区普遍担心军工行业会使用他们的成果去错误地伤害人类」。他说他将在董事会中「至少起到交流作用」，并且建议军方「使用这些技术来帮助保护国家安全」。 谷歌这一小部分对军事合同感到不安的员工（超过 70000 名）也许并不能对公司的发展造成大的阻碍。但在人工智能研究领域中，谷歌正忙于和其它技术公司激烈「抢人」，因此如果某些候选者被谷歌的辩解所推迟，这将会造成人事聘用上的麻烦。 谷歌在内部异议中为国防合同辩护，但与此同时它的竞争对手并不羞于宣传自己的国防项目。亚马逊向国防部大力宣传其图像识别研究成果，而微软则表示他们的云技术已经与美国政府签订合同，为军事和国防机构处理机密信息。 当前的争论由 Gizmodo 首次报道，主要集中在 Project Maven。该试点项目于去年开始，旨在找到加速军方应用最新 AI 技术的方式。五角大楼发言人表示，预计该项目第一年花费不到 7000 万美元。但谷歌公开信的签名者明确希望阻止谷歌参与更大规模的五角大楼合同（人工智能国防应用不断发展）。 外界普遍认为，目前谷歌正与其他科技巨头（如亚马逊和微软）竞争一份为期多年、价值数十亿美元的国防部云服务合同。国防部首席管理官 John Gibson 在三月份的一次讲话中提到：「联合企业国防基础设施云（Joint Enterprise Defense Infrastructure Cloud）」采购计划的部分目的是「提升杀伤力和做好准备」，这凸显了将软件、云服务等相关服务与实际商业战争之间分开的难度。 谷歌员工写给 Pichai 的公开信已经在谷歌内部通信系统中传播了数周，信中认为，参与军事项目可能会疏远用户，甚至影响未来的人才引进。 「这一计划将不可挽回地损害谷歌的品牌和人才吸引力，」信中提到，「由于人们越来越担心人工智能的偏见与武器化，谷歌争取用户信任的道路已经越来越难走了。」在未来，谷歌或许会被视为加入雷神、通用动力以及 Palantir 这样的国防承包商行列。 「其他大型科技公司如微软和亚马逊也参与了这一项目，但这并不会让谷歌承担的风险变小。」信中说，「谷歌拥有独特的历史，它的座右铭是『不作恶』（Don't Be Evil），谷歌直接触及数十亿用户的生活，这让它与众不同。」 和其它新贵最终成为了强大的硅谷巨人一样，谷歌也正被迫面对早年引导公司的理念。Facebook 开始时的使命是连接世界上所有的人，但是最近因为虚假消息渠道和被俄罗斯用来影响 2016 年的选举而受到抨击，此外，它同样还被指控在美国选民中传播异议。 考虑到谷歌的历史和五角大楼目前在军事方面对 AI 的强烈需求，前五角大楼的官员和「Army of None」的作者 Paul Scharre 表示，这在谷歌内部是不可避免的。 华盛顿新国家安全中心的高级成员 Scharre 说：「技术开发者之间存在着强烈的自由主义精神，并且对政府使用技术会感到担忧。目前，人工智能突然且快速地从实验室走出来，迈进了现实生活，这尤其加剧他们的担忧。」 公开信原文： 亲爱的 Sundar： 我们认为谷歌不应卷入战争。因此，我们请求公司撤销 Project Maven，同时起草、公布并执行一项明确的决策：谷歌及其合约商永不发展战争科技。 谷歌目前正在实施 Project Maven，这是一个定制的 AI 监控引擎，它使用美国政府获取的「广域运动图像（Wide Area Motion Imagery）」来检测汽车和其他物体并追踪其运动，检测和追踪结果提交给国防部。 最近，谷歌员工在公司内部表示了对 Maven 的担忧。Diane Greene 在回应中保证该技术不会「在无人机上运行」，也「不会被用于武器开发」。虽然这排除了极小部分的直接应用，但该技术是为军事用途而开发，一旦交付，就会轻易被用于这些任务的辅助工作。 这一计划将对谷歌的品牌及人才吸引力造成不可挽回的伤害。随着对 AI 偏见化和武器化的恐惧日益加剧，谷歌谷歌争取用户信任的道路已经越来越难走了。而签署这份协议，谷歌将加入 Palantir、雷神、通用动力等公司的行列。其他大型科技公司如微软和亚马逊也参与了这一项目，但这并不会让谷歌承担的风险变小。谷歌拥有独特的历史，它的座右铭是『不作恶』，谷歌直接触及数十亿用户的生活，这让它与众不同。 我们不能把技术的道德责任外包给第三方。谷歌宣扬的价值观清晰地表明：每一个用户都在给予我们信任，永远不要辜负这一点。这个合同使谷歌的名誉受到威胁，而且与我们的核心价值观相悖。发展一项技术协助美国政府进行军事监控，而且该技术可能造成致命结果，这是不可接受的。 考虑到谷歌的道德伦理责任，以及这对谷歌名誉的潜在危害，我们请求你： 1. 立即取消这一项目。 2. 起草、公布并执行一项明确的策略，声明谷歌及其合约商永远不会染指军事技术。 
30,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740446&idx=3&sn=dea6931b3b14cfe70935a9f5d8c6e8af&chksm=871ad2a0b06d5bb65b5baa2d6fcb1296fffb6d4cafcdc51db441e9e4ed31ec00ece77f4c54a4&scene=27,深度 | 学习如何学习的算法：简述元学习研究方向现状,TowardsDataScience 要想实现足够聪明的人工智能，算法必须学会如何学习。很多研究者们曾对此提出过不同的解决方案，其中包括 UC Berkeley 的研究人员提出的与模型无关的元学习（MAML）方法。本文将以 MAML 为例对目前的元学习方向进行简要介绍。 对我而言，第一次听到元学习的预述时，是一个极其兴奋的过程：建立不仅能够进行学习，还能学会如何进行学习的机器项目。元学习试图开发出可以根据性能信号做出响应，从而对结构基础层次以及参数空间进行修改的算法，这些算法在新环境中可以利用之前积累的经验。简言之：当未来主义者们编织通用 AI 的梦想时，这些算法是实现梦想必不可少的组成部分。 本文的目的在于将这个问题的高度降低，从我们想得到的、自我修正算法做得到的事情出发，到这个领域现在的发展状况：算法取得的成就、局限性，以及我们离强大的多任务智能有多远。 为什么人类可以做到这些事？ 具体地讲：在许多强化学习任务中，和人类花费的时间相比，算法需要花费惊人的时间对任务进行学习；在玩 Atari 游戏时，机器需要 83 小时（或 1800 万帧）才能有人类几小时就能有的表现。 这种差异导致机器学习研究人员将问题设计为：人类大脑中针对这项任务使用的工具和能力是什么，以及我们如何用统计和信息理论的方法转化这些工具。针对该问题，元学习研究人员提出了两种主要理论，这两种理论大致与这些工具相关。 学习的先验：人类可以很快地学会新任务是因为我们可以利用在过去的任务中学到的信息，比如物体在空间里移动的直观的物理知识，或者是在游戏中掉血得到的奖励会比较低这样的元知识。 学习的策略：在我们的生活中（也许是从进化时间上讲的），我们收集的不仅是关于这个世界对象级的信息，还生成了一种神经结构，这种神经结构在将输入转化为输出或策略的问题上的效率更高，即使是在新环境中也不例外。 显然，这两个想法并非互相排斥，在这两个想法间也没有严格的界限：一些与现在的世界交互的硬编码策略可能是基于这个世界的深度先验的，例如（至少就本文而言）这个世界是有因果结构的。也就是说，我认为这个世界上的事情都可以用这两个标签分开，而且可以将这两个标签看作相关轴的极点。 不要丢弃我的（单）样本 在深入探讨元学习之前，了解单样本学习相关领域的一些概念是很有用的。元学习的问题在于「我该如何建立一个可以很快学习新任务的模型」，而单样本学习的问题在于「我该如何建立一个在看过一类的一个样本后，就能学会该如何将这一类分出来的模型」。 让我们从概念上思考一下：是什么让单样本学习变得困难？如果我们仅用相关类别的一个样本试着训练一个原始模型，这个模型几乎肯定会过拟合。如果一个模型只看过一幅图，比如数字 3，这个模型就无法理解一张图经过什么样的像素变化，仍然保持 3 的基本特征。例如，如果这个模型只显示了下面这列数字的前三个样本，它怎么会知道第二个 3 是同一类的一个样本呢？理论上讲，在网络学习中，我们想要的类别标签有可能与字母的粗细程度有关吗？对我们而言做出这样的推断这很傻，但是在只有一个「3」的样本的情况下，想让神经网络能做出这样的推理就很困难了。 有更多样本会有助于解决这一问题，因为我们可以学习一张图中什么样的特征可以定义其主要特征——两个凸的形状，大部分是垂直的方向，以及无关紧要的改变——线的粗细、还有角度。为了成功实现单样本学习，我们不得不激励网络，在没有给出每一个数字间差别的情况下，学习什么样的表征可以将一个数字从其他数字中区别出来。 单样本学习的常用技术是学习一个嵌入空间，在这个空间中计算出两个样本表征间的欧几里德相似性，这能很好地计算出这两个样本是否属于同一类。直观地讲，这需要学习分布中类别间差异的内部维度（在我的样本中，分布在数字中），并学习如何将输入压缩和转换成那些最相关的维度。 我发现记住这个问题是一个很有用的基础，尽管不是学习如何总结存在于类别分布中的碎片化信息和模式，而是学习存在于任务中的类的分布规律，每一类都有自己的内部结构或目标。 如果要从最抽象开始，构造一个神经网络元参数的等级，会有点像这样： 通过使用超参数梯度下降，网络从任务的全部分布中学习到有用的表征。MAML 和 Reptile 是有关于此的直接的好例子，分享层级结构的元学习是一种有趣的方法，这种方法可以通过主策略的控制学习到清晰的子策略作为表征。 网络学习要优化梯度下降的参数。这些参数就像是学习率、动量以及权重之于自适应学习率算法。我们在此沿着修改学习算法本身的轨道修改参数，但是有局限性。这就是 Learning to Learn By Gradient Descent by Gradient Descent 所做的。是的，这就是这篇文章真正的标题。 一个学习内部优化器的网络，内部优化器本身就是一个网络。也就是说，使用梯度下降更新神经优化器网络参数使得网络在整个项目中获得很好的表现，但是在网络中每个项目从输入数据到输出预测结果的映射都是由网络指导的。这就是 RL² 和 A Simple Neural Attentive Meta Learner 起作用的原因。 为了使这篇文章更简明，我将主要叙述 1 和 3，以说明这个问题的两个概念性的结局。 其他名称的任务 另一个简短的问题——我保证是最后一个——我希望澄清一个可能会造成困惑的话题。一般而言，在元学习的讨论中，你会看到「任务分布」的提法。你可能会注意到这个概念定义不明，而你的注意是对的。对于一个问题是一个任务还是多个任务中的一个分布，人们似乎还没有明确的标准。例如，我们应该将 ImageNet 视为一个任务——目标识别——还是许多任务——识别狗是一个任务而识别猫是另一个任务呢？为什么将玩 Atari 游戏视为一个任务，而不是将游戏的每一个等级作为一个独立任务的几个任务？ 我能得到的有： 「任务」的概念是用已经建立的数据集进行卷积，从而可以自然地将在一个数据集上进行学习认为是单个任务 对于任何给定分布的任务，这些任务之间的不同之处都是非常显著的（例如，每一个学习振幅不同的正弦曲线的任务和每一个在玩不同 Atari 游戏的任务之间的差别） 所以，这不仅仅是说「啊，这个方法可以推广到这个任务分配的例子上，所以这是一个很好的指标，这个指标可以在任务中一些任意且不同的分布上表现良好」。从方法角度上讲，这当然不是方法有效的不好的证据，但我们确实需要用批判性思维考虑这种网络要表现出多大的灵活性才能在所有任务中都能表现出色。 那些令人费解的动物命名的方法 在 2017 年早些时候，Chelsea Finn 及其来自 UC Berkeley 的团队就有了叫做 MAML的方法。 MAML（Model Agnostic Meta Learning，与模型无关的元学习）参见： 。 在学习策略和学习先验之间，这种方法更倾向于后者。这种网络的目标在于训练一个模型，给新任务一步梯度更新，就可以很好地归纳该任务。就像是伪代码算法。 1. 初始化网络参数 θ。 2. 在分布任务 T 中选择一些任务 t。从训练集中取出 k 个样本，在当前参数集所在位置执行一步梯度步骤，最终得到一组参数。 3. 用最后一组参数在测试集中测试评估模型性能。 4. 然后，取初始参数θ作为任务 t 测试集性能的梯度。然后根据这一梯度更新参数。回到第一步，使用刚刚更新过的θ作为这一步的初始θ值。 这是在做什么？从抽象层面上讲，这是在寻找参数空间中的一个点，就分布任务中的许多任务而言，这个点是最接近好的泛化点的。你也可以认为这迫使模型在探索参数空间时保留了一些不确定性和谨慎性。简单说，一个认为梯度能完全表示母体分布的网络，可能会进入一个损失特别低的区域，MAML 会做出更多激励行为来找到一个靠近多个峰顶端的区域，这些峰每一个的损失都很低。正是这种谨慎的激励使 MAML 不会像一般通过少量来自新任务的样本训练的模型一样过拟合。 2018 年的早些时候文献中提出了一种叫做 Reptile 的更新方法。正如你可能从它的名字中猜出来的那样——从更早的 MAML 中猜——Reptile 来自 MAML 的预述，但是找到了一种计算循环更新初始化参数的方法，这种方法的计算效率会更高。MAML 明确取出与初始化参数 θ 相关的测试集损失的梯度，Reptile 仅在每项任务中执行了 SGD 更新的几步，然后用更新结束时的权重和初始权重的差异，作为更新初始权重的梯度。 这项工作从根本上讲有一些奇怪——这看起来和将所有任务合并为一个任务对模型进行训练没有任何不同。然而，作者提出，由于对每项任务都使用了 SGD 的多个步骤，每个任务损失函数的二次导数则被赋予影响力。为了做到这一点，他们将更新分为两部分： 1. 任务会得到「联合训练损失」的结果，也就是说，你会得到用合并的任务作为数据集训练出来的结果。 2. SGD 小批次梯度都是接近的：也就是说，在通过小批次后，梯度下降的程度很低。 我选择 MAML/Reptile 组作为「学习先验」的代表，因为从理论上讲，这个网络通过对内部表征进行学习，不仅有助于对任务的全部分布进行分类，还可以使表征与参数空间接近，从而使表征得到广泛应用。 为了对这个点进行分类，我们先看一下上图。上图对 MAML 和预训练网络进行比较，这两个网络都用一组由不同相位与振幅组成的正弦曲线回归任务训练。在这个点上，两者针对新的特定任务都进行了「微调」：红色曲线所示。紫色三角代表少数梯度步骤中使用的数据点。与预训练模型相比，MAML 学到了，正弦曲线具有周期性结构：在 K=5 时，它可以在没有观察到这一区域数据的情况下更快地将左边的峰值移到正确的地方。尽管很难判断我们的解释是不是网络的真正机制，但我们可以推断 MAML 在算出两个相关正弦曲线不同之处——相位和振幅——方面做得更好，那么是如何从这些已给数据的表征进行学习的呢？ 网络一路向下 对一些人来说，他们的想法是使用已知算法，例如梯度下降，来对全局先验进行学习。但是谁说已经设计出来的算法就是最高效的呢？难道我们不能学到更好的方法吗？ 这就是 RL²（通过慢速强化学习进行快速强化学习）所采用的方法。这个模型的基础结构式循环神经网络（具体来说，是一个 LTSM 网络）。因为 RNN 可以储存状态信息，还可以给出不同输出并将这些输出作为该状态的函数，理论上讲这就有可能学到任意可计算的算法：也就是说它们都具有图灵完备的潜力。以此为基础，RL² 的作者构建了一个 RNN，每一个用于训练 RNN 的「序列」都是一组具有特定 MDP（Markov Decision Process，马尔科夫决策过程。从这个角度解释，你只需将每次 MDP 看作环境中定义一系列可能行为且通过这些行为产生奖励）的经验集合。接着会在许多序列上训练这个 RNN，像一般的 RNN 一样，这是为了对应多个不同的 MDP，可以对 RNN 的参数进行优化，可以使所有序列或试验中产生的遗憾（regret）较低。遗憾（regret）是一个可以捕获你一组事件中所有奖励的指标，所以除了激励网络在试验结束时得到更好的策略之外，它还可以激励网络更快地进行学习，因此会在低回报政策中更少地使用探索性行为。 在试验中的每一个点，网络都会通过在多个任务和隐藏状态的内容学习权重矩阵参数化函数，隐藏状态的内容是作为数据函数进行更新并充当一类动态参数集合。所以，RNN 学习的是如何更新隐藏状态的权重。然后，在一个给定的任务中，隐藏状态可以捕获关于网络确定性以及时间是用于探索还是利用的信息。作为数据函数，它可以看得到特定任务。从这个意义上讲，RNN 在学习一个可以决定如何能最好地探索空间、还可以更新其最好策略概念的算法，同时使该算法在任务的一组分布上得到很好的效果。该作者对 RL² 的架构和对任务进行渐进优化的算法进行比较，RL² 的表现与其相当。 我们可以扩展这种方法吗？ 本文只是该领域一个非常简要的介绍，我肯定遗漏了很多想法和概念。如果你需要更多（信息更加丰富）的看法，我高度推荐这篇 Chelsea Finn 的博客，此人也是 MAML 论文的第一作者。 在这几周的过程中，我试着对这篇文章从概念上进行压缩，并试着对这篇文章进行理解，在这一过程中我产生了一系列问题： 这些方法该如何应用于更多样的任务？这些文章大多是在多样性较低的任务分布中从概念上进行了验证：参数不同的正弦曲线、参数不同的躲避老虎机、不同语言的字符识别。对我而言，在这些任务上做得好不代表在复杂程度不同、模式不同的任务上也可以有很好的表现，例如图像识别、问答和逻辑问题结合的任务。然而，人类的大脑确实从这些高度不同的任务集中形成了先验性，可以在不同的任务中来回传递关于这个世界的信息。我的主要问题在于：这些方法在这些更多样的任务中是否会像宣传的一样，只要你抛出更多单元进行计算就可以吗？或在任务多样性曲线上的一些点是否存在非线性效应，这样在这些多样性较低的任务中起作用的方法在高多样性的任务中就不会起作用了。 这些方法依赖的计算量有多大？这些文章中的大部分都旨在小而简单的数据集中进行操作的部分原因是，每当你训练一次，这一次就包括一个内部循环，这个内部循环则包含（有效地）用元参数效果相关的数据点训练模型，以及测试，这都是需要耗费相当大时间和计算量的。考虑到近期摩尔定律渐渐失效，在 Google 以外的地方对这些方法进行应用研究的可能性有多大？每个针对困难问题的内部循环迭代可能在 GPU 上运行数百个小时，在哪能有这样的条件呢？ 这些方法与寻找能清晰对这个世界的先验进行编码的想法相比又如何呢？在人类世界中一个价值极高的工具就是语言。从机器学习方面而言，是将高度压缩的信息嵌入我们知道该如何转换概念的空间中，然后我们才可以将这些信息从一个人传递给另一个人。没人可以仅从自己的经验中就提取出这些信息，所以除非我们找出如何做出与学习算法相似的事，否则我怀疑我们是否真的可以通过整合这个世界上的知识建立模型，从而解决问题。 原文链接：https://towardsdatascience.com/learning-about-algorithms-that-learn-to-learn-9022f2fa3dd5 
31,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740387&idx=1&sn=ce2895d0f8ae4884f70a7640171fef62&chksm=871ad2ddb06d5bcbc967d49c7c39f7b22090269adad82d73c3ed85820287157a73d4a13814f3&scene=27,谷歌高级副总裁离职24小时闪电加盟苹果：全权负责机器学习与AI,谷歌昨天刚刚宣布了重大部门调整 ，其搜索与 AI 业务一分为二，原总监、谷歌高级副总裁 John Giannandrea 离职，而谷歌大脑的负责人 Jeff Dean 成为新 AI 部门的领导者。时间仅仅过去一天，又一个消息被《纽约时报》爆出：另一家科技巨头苹果宣布聘用 John Giannandrea，这位谷歌前高管即刻成为了苹果 AI 研究与开发的总指挥。 Giannandrea 将监管苹果的机器学习与人工智能策略，直接向 CEO 库克汇报。 据《纽约时报》报道，苹果已经聘用了前谷歌搜索与人工智能部门老大 John Giannandrea，这可以说是苹果在人工智能方面追赶竞争对手的一大举措。 周二，苹果表示 Giannandrea 将负责苹果的「机器学习与人工智能策略」，成为 16 个直接向库克汇报的高管之一。 苹果此前的领导层：库克与 15 位高管。 对苹果而言，这一人事聘用是件很成功的事。因为硅谷的许多高管、分析师都认为苹果在人工智能上已经落后。 据《纽约时报》消息：蒂姆·库克在周二上午发给职员的邮件中写道：「我们的技术必须秉持我们的价值观，John 支持我们在隐私方面的承诺，以及我们在使计算机更加智能、更加人性化过程中的周全做法。」 尽管苹果已经上升为全世界市值最大的上市公司，但是技术行业中很多人仍然认为 iPhone 的数字助理 Siri 不如谷歌和亚马逊的数字助理。 Giannandrea 现年 53 岁，苏格兰人，而 J.G.是其昵称。他带头推动了将 AI 技术整合进谷歌的产品中，包括互联网搜索、Gmail 和数字助理：Google Assistant。 2010 年，谷歌收购了 Metaweb，时任 Metaweb 首席技术官的 Giannandrea 加入了谷歌。Metaweb 的目标是成为「全世界知识的数据库」，而谷歌最终将其整合进搜索引擎中，为用户查询提供直接答案。在 Giannandrea 的任期内，AI 技术研究越来越受谷歌重视，甚至其主要的 AI 实验室 Google Brain 都搬到了首席执行官 Sundar Pichai 的办公室旁边。 目前在硅谷拥有 AI 专业技能的工程师相当吃香，有些人的年薪甚至超过了八位数。当昨天 Giannandrea 突然卸任谷歌的 AI 执行官时，他一下子成为人才市场中最抢手的技术高管。如今，人们才知道，他从来没有真正出现在市场上。 苹果之前也在 AI 领域中做出了高调的任用，包括 2016 年 10 月邀请卡内基梅隆大学教授 Russ Salakhutdinov 负责该公司的人工智能研究（参见： ）。Salakhutdinov 曾在多伦多大学 Geoffrey Hinton 门下做研究。苹果曾经表示保护移动端和在线服务端用户个人隐私的坚定立场，然而，如果这家科技巨头要扩展基于神经网络的业务，这个立场可能会为其带来诸多不便。 此前一直负责苹果 AI 技术的 Ruslan Salakhutdinov，他也是卡内基梅隆大学教授。 苹果的研究者通过池化海量数字数据来训练这些系统，部分数据来自客户服务。但是，苹果称其开发的方法允许在不损害用户隐私的前提下训练算法。 针对人类是否应该担忧快速发展的 AI 技术的争论，Giannandrea 在去年 MIT TR 的一次采访中称，这些担忧有些过度了。 「我对将出现淘汰人类的超智能系统的假设持反对态度。我理解人们为何对此感到担忧，但是我认为这只是相关宣传太多的缘故。我并未看到任何该假设即将发生的技术基础。」Giannandrea 表示。 
32,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740315&idx=5&sn=a2957046cc508fb3555740d402ec8c6c&chksm=871ad225b06d5b33f893d0672c76c9db0f3ef2465de59fbd72e5473603f4f1b2e00723e0da2e&scene=27,招聘 | Atman深耕专业领域语言智能，诚邀你的加入,"Atman 由来自微软亚洲研究院和互联网搜索中心的资深技术专家创办，开发专业领域机器翻译、机器写作、知识图谱、大数据智能采集挖掘等语言智能产品，致力于成为医学、新闻、法律等专业领域语言智能专家，为专业领域用户赋能，推动专业领域用户进入人工智能时代，助力专业领域文字智能水平实现跨越式提升。 Atman 自成立以来已经获得国际知名风险投资机构 5000 万人民币投资，获评权威机构评选「2017 年人工智能创业 50 强企业」称号。 Atman 的机器翻译产品使用自主开发的神经网络机器翻译技术，可自动翻译编辑专业文献、报告、音视频和网页，在专业领域的机器翻译质量达到或超过谷歌、百度水平。机器写作可对海量数据进行快速搜索、过滤、聚类，根据模版格式自动生成文档，适用于所有专业写作场景，可大幅减少专业报告写作过程中的繁复工作，大幅提升专业领域的写作自动化水平；知识图谱可实现海量数据的语义检索、长链推理、意图识别、因果分析，形成专业领域智能化的数据库；大数据智能采集挖掘系统为专业领域用户提供智能数据源管理、海量专业数据获取和丰富的用户交互系统，帮助用户建立强大的以专业大数据为基础的业务感知能力。 Atman 已为强生、新华社参考消息、北大法宝、君合律师事务所等世界领先药企、新闻媒体、法律服务机构开发机器翻译、机器写作、大数据智能采集挖掘等语言智能产品。 Atman 短短两年发展时间，一直在努力尝试选择不一样的战略方向，开发不一样的产品组合，实践不一样的经营理念，寻找不一样的团队伙伴。我们身处风口，切身感受到人工智能技术和产品强劲推广普及的风潮，但是我们决不做风口中吹上天的那头猪，而要做大风中用根紧紧抓住大地的那棵树。在风口中稳健发展，在潮流中不忘初心。 Atman 已经进入高速发展期，能力、品行出众的技术人才和经营管理人才是我们最重要的核心竞争力。我们制定了完善的薪酬体系和人才培养计划，保证能够吸引优秀人才加入，并能够从内部培养人才、提拔人才、任用人才，努力让每一名员工和公司一起成长发展，不负年华，不负自己。 有志于一起开创专业领域语言智能行业的小伙伴们，Atman 已经张开怀抱，迎接你的到来！ 我们在这里 在这里等你 北京公司——位于学院路甲 5 号 768 创意产业园内，紧邻北京地铁 6 号线六道口站，交通便利、园内绿树成荫、红墙青瓦、闹中取静，聚集了知乎、春雨医生、蓦然认知、即刻等著名创业公司，创业氛围浓厚。 苏州公司——位于苏州工业园区若水路 388 号苏州纳米技术国家大学科技园内，紧邻苏州地铁 2 号线松涛街站，交通便利，园内环境优雅、空气清新、花香鸟语、帅哥美女云集。办公室由上市公司金螳螂设计装修，风格现代简约，安静舒适。 公司官网 www.atman.ai 产品体验网址 translate.transgod.cn 更多的产品在这里 https://transgod.cn 公司邮箱 corp@atman360.com  招聘职位： 1. 算法架构师 职位描述： 能够独立负责 NLP 相关算法方向和团队管理； 能够对应用领域做出合理的算法架构和解决方案； 能够持续对算法和模型提出改进思路，跟进最新研究成果。 职位要求： 5 年以上机器学习相关科研或工作经验（含在校研究经历）； 能够详细描述 NLP 相关领域重要模型的算法思想； 擅长机器学习框架之一，如 Tensorflow、Theano、Torch 等； 有谷歌/微软/百度等商业研究院工作经验者优先。 2. 机器学习工程师 职位描述： 负责机器翻译相关算法的研发：模型的设计、训练、调优等； 机器翻译离线数据处理及分析。 职位要求： 1 年以上机器学习相关科研或工作经验； 能够详细描述 RNN、LSTM、GRU、Attention 模型的算法； 擅长常用机器学习框架之一，如 Tensorflow、Theano、Torch 等； 擅长科学计算编程语言之一，如 Python、Lua、R、C++等； 能在 Linux 环境下开发； 能够描述 SMT 算法优先； 具有百度／阿里／微软等公司工作经验者优先； 机器学习相关专业优秀毕业生优先。 3. 数据工程师 职位描述： 负责产品相关数据的收集、处理、挖掘，存储等相关工作； 负责后端数据平台的开发和维护； 与团队成员协同开发数据处理算法，按要求提升准确率； 针对新的业务场景，探索和实施新的技术方案，优化整体的内容处理效率； AI 相关服务（机器翻译、NLP 等）API 的开发和维护工作。 职位要求： 精通 Python 和/或 Java； 理解面向对象设计的基本原则，熟悉常用的设计模式； 具有 NLP 常用工具（如 Jieba 分词、nltk、tokenizer）使用经验者优先； 具有 Hadoop、Spark、Elasticsearch 等使用/开发经验者优先； 具有网络爬虫、结构化数据提取、数据分析等使用/开发经验者优先； 具有机器学习框架（如 TensorFlow，CNTK, Theano 等）使用经验者优先； 具有后端 API 开发经验者优先。 4. 高级前端工程师（全栈工程师） 职位描述： 前端技术类项目的日常开发与迭代升级； 负责某一方向基础前端框架的设计开发与维护； 参与前端或 nodejs 技术性攻关课题的调研、开发以及项目落地； 协助进行团队培训和技术梯队建设； 可带团队独挡一个方向。 职位要求： 6 年以上前端工作经验，nodejs 有经验者优先； 有能力设计开发完整的前端业务框架或 UI 框架，有成品者优先； 具备丰富的实战能力，有大型前端项目或 nodejs 项目者优先； 对 js 或 nodejs 有深入研究的专长领域（前端构架、UI 组件或 nodejs 库），有产出者优先； 技术视野广泛、学习能力要强，对新技术可以快速上手，及时应用； 头脑灵活，责任心强，对产品或技术有自己独特的想法和建议，有带团队经验者优先。 5. 高级 Python 工程师 职位职责： 负责机器翻译和大数据产品后台相关业务的系统开发； 负责搭建和维护产品的后台服务； 保证系统高并发访问下的稳定性以及可用性、安全性。 职位要求： 3 年以上相关工作经验，至少 2 年以上 Python 编程经验，Python 架构系统经验者优先； 熟悉分布式、微服务以及高性能 Web 的开发，有一定的系统设计能力； 善于团队合作与管理，对新技术有强烈的探索欲望； 责任心强，能够承受工作压力，有强烈的责任心； 计算机相关专业本科及以上学历。 6. 售前工程师 职位描述： 售前阶段与客户进行 AI 行业和技术层面的交流，向客户提供 AI 领域的咨询、培训、讲解及答疑等服务； 收集客户需求，提出针对性的解决方案，与客户进行深度技术交流、技术方案宣讲、应用系统演示；  完成产品、方案测试，组织技术方案编写、准备标书、参与投标； 跟踪、分析行业市场与竞争环境变化，掌握目标行业和竞争对手动态，建立、保持、强化项目售前过程中的竞争优势。 任职要求： 计算机、软件开发等相关专业统招本科及以上学历； 3 年以上软件产品售前工作经验，有 AI 领域软件产品售前工作经验优先；作为售前团队 Leader（或独立完成）完成 3 个以上软件开发项目的售前案例； 优秀的售前管理和组织能力，出色的书面方案写作能力和口头表达能力； 熟悉招投标程序，能独立完成标书撰写、现场答疑及述标； 学习能力强，能快速熟悉机器翻译及人工智能领域的应用技术，能针对客户特点提出完整行业解决方案； 团队合作意识强，团队沟通技能好，愿意接受有挑战性的工作。 7. 实施经理 职位描述： 负责机器翻译以及其他人工智能软件系统的安装、实施工作，处理软件安装环境的技术问题和数据库的数据初始化工作； 按照合同要求完成软件实施的工作进度，确保软件实施质量，解答、处理实施过程中出现的软件系统问题； 实施过程中保持和客户以及公司其他相关部门的良好沟通与协调；与客户进行技术交流，掌握和识别客户需求，完成产品的现场演示； 更新、整理系统技术支持手册、系统部署手册等实施文件。 任职要求： 计算机、软件开发等相关专业统招本科及以上学历； 3 年软件实施工作经验，有 AI 领域软件产品实施工作经验优先，作为实施团队 Leader（或独立完成）3 个以上可信解决方案的现场实施与对合作伙伴的现场培训案例； 精通 Linux 系统、x86 服务器、存储、网络、数据库、中间件以及安全产品的安装、配置和操作，同时有应用系统集成经验优先； 熟悉企业虚拟化环境和主流容器技术，熟悉大型企业 IT 基础架构和配置方法，熟悉 docker 和运维； 熟悉 GPU 服务器配置、部署、优化或者熟悉 node.js、python 者优先； 优秀的沟通能力和协调能力，团队合作意识强，愿意接受有挑战性的工作。 "
33,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740315&idx=3&sn=57f878fb2b0cfd78222727a4d713bf8d&chksm=871ad225b06d5b33a989e4dbcbf11207d8d53db1af68507be85c40fd2d8a02f0df9633b8a917&scene=27,资源 | 从Brain.js到Mind，一文收录11个移动端Javascript机器学习库,本文作者在构建 Bit 的过程中探索和尝试了把 Javascript 和机器学习结合起来使用的可能性，并由此发现了一些简洁优雅的库，可以把 Javascript、机器学习、DNN 甚至 NLP 整合起来。 「等等，什么？？这是一个可怕的想法！」 当我第一次和我们的 NLP 主要研究人员谈起这个概念时，她的原话是这样的。可能她是对的，但它也是一个非常有趣的概念，最近在 Javascript 领域得到了越来越多的关注。 在过去的一年中，我们的团队正在构建 Bit（https://bitsrc.io/），它使得用组件构建软件变得更加简单。作为工作的一部分，我们开发了 ML 和 NLP 算法，来更好地理解代码是如何编写、组织和使用的。 虽然大多数工作都是用 Python 这样的语言完成的，但 Bit 位于 Javascript 生态系统中，它的前、后端社区都是如此。 这个有趣的交集让我们探索和尝试了一起使用 Javascript 和机器学习的奇怪可能性。通过我们的研究发现，这里有一些简洁的库，可以将 Javascript、机器学习、DNN 甚至 NLP 结合在一起。 1. Brain.js Brain.js 是一个用于神经网络的 Javascript 库，用于代替 (现在已经弃用的）「brain」库，它可以和 Node.js 一起使用，或在 browser（注意计算）中使用，并为不同任务提供不同类型的网络。以下是训练网络来识别色彩对比的例子。 链接：https://github.com/BrainJS/brain.js 2. Synaptic Synaptic 是一个用于 node.js 和浏览器的 Javascript 神经网络库，它使你能够训练一阶甚至是二阶神经网络结构。该项目包括一些内置的体系结构，如多层感知机、多层长短期记忆网络、液体状态机和能够训练真实网络的训练器。 链接：https://github.com/cazala/synaptic 3. Neataptic 这个库为浏览器和 Node.js 提供快速的神经元进化和反向传播，并且有一些内置的网络，包括感知器、LSTM、GRU、Nark 等等。这里是一个简单训练的新手教程：https://wagenaartje.github.io/neataptic/docs/tutorials/training/。 链接：https://github.com/wagenaartje/neataptic 4. Conventjs 这个由斯坦福大学博士开发的流行库，在过去的 4 年里一直没有维护，但它是列表上最有趣的项目之一。它是神经网络的 Javascript 实现，支持通用模块、分类、回归、一个试验性的强化学习模块，甚至能够训练处理图像的卷积网络。 链接：https://github.com/karpathy/convnetjs 5. Webdnn 这个日本制造的库用于在浏览器上运行深度神经网络预训练模型，并且运行速度很快。由于在浏览器上运行 DNN 会消耗大量计算资源，因此该框架优化了 DNN 模型来压缩模型数据，并通过 JavaScript APIs（如 WebAssembly 和 WebGPU）加速执行。 链接：https://github.com/mil-tokyo/webdnn 6. Deeplearnjs 这个流行的库允许你在浏览器中训练神经网络，或者在推理模式下运行预训练模型，甚至声称它可以用作网页版 NumPy。通过易于读取的 API，该库可用于有用应用程序的真实性，并被积极地维护。 链接：https://github.com/tensorflow/tfjs-core 7. Tensorflow Deep Playground Deep playground 是神经网络的交互式可视化，使用 d3.js 在 TypeScript 中编写。虽然这个项目包含了一个非常基本的 tensorflow playground，但它可以被用于不同的目的，或用作令人印象深刻的不同用途的教育功能。 链接：https://github.com/tensorflow/playground Tensorflow playground 8. Compromise 这个非常流行的库提供了「在 JavaScript 中适度的自然语言处理」。它非常基本和直接，甚至可以编译成一个小文件。出于某种原因，它的适度的「足够好」的方法使其成为几乎所有需要基本 NLP 的应用程序的首选。 链接：https://github.com/spencermountain/compromise 9. Neuro.js 这个项目很棒，它为浏览器提供一个深度学习和强化学习的 Javascript 库框架。它在扩展强化学习支持下，实现一个基于全栈神经网络的机器学习框架，有些人认为这个项目是 convnet.js 的继承者。 链接：https://github.com/janhuenermann/neurojs 10. mljs 由 mljs 组织开发的一组库，为 Javascript 提供机器学习工具，它包括监督和非监督学习、人工神经网络、回归算法和用于统计、数学等的支持库。这里有一个简短的指导：https://hackernoon.com/machine-learning-with-javascript-part-1-9b97f3ed4fe5。 链接：https://github.com/mljs 11. Mind 一个用于 Node.js 和浏览器的灵活的神经网络库，主要学习做预测，使用矩阵来处理训练数据并启用可配置的网络拓扑。你还可以即插即用已学习的「Mind」，这对你的应用很有用。 链接：https://github.com/stevenmiller888/mind 其他重要的库： Natural Node.js 的一个积极维护的库，它提供标记法、词干提取（减少不必要的词根）、分类、语音学、tf-idf、WordNet、字符串相似度等。 链接：https://github.com/NaturalNode/natural Incubator-mxnet Apache MXNet 是一个深度学习框架，它允许你将符号和命令式编程与图形优化层在线结合起来以提高性能。MXnet.js 为浏览器带来了一个深度学习推理 API。 链接：https://github.com/apache/incubator-mxnet Keras JS 该库在浏览器中运行 Keras 模型，使用 WebGL 并支持 GPU。由于 Keras 使用了许多框架作为后端，所以模型也可以在 TensorFlow、CNTK 和其他框架中进行训练。 链接：https://github.com/transcranial/keras-js Deepforge 一个深度学习的开发环境，它使你能够快速设计神经网络结构和机器学习管道，并使用内置版本控制再现实验。值得一试。 链接：https://github.com/deepforge-dev/deepforge Land Lines 与其说它是一个库，不如说是一个基于谷歌 Chrome 实验的非常酷的演示/网页游戏。尽管我不确定该如何处理，但它肯定会为你带来一天中最愉快的 15 分钟。 链接：https://lines.chromeexperiments.com/ 下一步是什么？ 显然，Javascript 远没有成为机器学习选择的语言。然而，诸如性能、矩阵操作和丰富有用的库等常见问题正在慢慢消失，缩小了常见应用和有用的机器学习之间的差距。 原文链接：https://blog.bitsrc.io/11-javascript-machine-learning-libraries-to-use-in-your-app-c49772cca46c 
34,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740315&idx=1&sn=a65812b490e7064c18dcc29bbed3bbf8&chksm=871ad225b06d5b3347c234e40044c0d6d6d682dab28f45c5987a488ffec8ecc672cb9263ae63&scene=27,谷歌AI部门大改组：搜索与人工智能分家，Jeff Dean升任AI总负责人,今日，国外媒体网站 The Information 报道，2016 年谷歌合并的搜索与人工智能部门再次分裂为两个部门。而在至关重要的新 AI 部门中，谷歌大脑负责人 Jeff Dean 将担当领导者。 谷歌母公司 Alphabet 曾在 2016 年 2 月合并了谷歌的搜索与人工智能部门，并将其交由资深科学家 John Giannandrea 掌舵。本周一谷歌放出消息：这位高管已经离开，谷歌两年以后再次分裂为两大部门。这家互联网巨头一直被认为是搜索与人工智能领域的领军者，但近年来正日益受到亚马逊、苹果等公司在语音搜索等方面上的挑战。 在新的搜索部门上，谷歌选择了 Ben Gomes，它是谷歌资深员工，在过去一年搜索业务的一系列大型工程中已扮演重要的角色；而在至关重要的新 AI 部门中，谷歌大脑负责人 Jeff Dean 将担任领导者。 AI 部门的新领导者：Jeff Dean 现年 49 岁的 Jeff Dean 在获得华盛顿大学计算机科学博士学位的三年之后（1999 年）加入了谷歌公司，成为了该公司最早的员工之一。在谷歌的成长过程中，他一直是该公司的技术头面人物——设计和实现了支撑谷歌大部分产品的许多分布式计算基础设施。 谷歌 CEO Sundar Pichai 曾说过谷歌将会变成一家人工智能优先的公司；作为系统和基础设施组（Systems and Infrastructure Group）的高级成员，Dean 及其团队对实现这样的目标至关重要。 2016 年，福布斯作者 Peter High 对 Jeff Dean 进行了专访，在这个涉及范围广泛的访谈中，Dean 描述了他在谷歌的多种角色、该公司的人工智能愿景、他对谷歌如何在作为科技巨头的同时保持创业精神的想法，以及其它许多主题。读者们可通过这篇文章了解谷歌 Jeff Dean。 在成为新 AI 部门总负责人之前，Jeff Dean 担任了谷歌大脑（Google Brain）负责人的职位。谷歌大脑团队致力于拓展人工智能在研究和系统工程方面的进展，其研究涉及机器智能的各个方面，这一团队也与谷歌、Alphabet 的多个工程团队合作，致力于使用最尖端的研究成果改善人们的生活。 参考阅读： 重磅 | 福布斯深度专访 Jeff Dean——谷歌人工智能背后的大脑 据一位知情人士透露，谷歌工程部高级副总裁、在人工智能问题上有对外话语权的 John Giannandrea 将卸任谷歌的搜索与人工智能部门主管的职位。谷歌发言人在邮件中确认了这一消息。 谷歌在改组人工智能职责时，也正在全公司范围内推进搜索业务、售卖更多的移动设备和基于云的人工智能服务。谷歌 CEO Sundar Pichai 表示，将 AI 注入谷歌的产品会使它更具竞争力。 Giannandrea 的职位被一分为二，原来的搜索与人工智能部门也正式划分为搜索和人工智能两个部门。据谷歌内部人士透露，在谷歌工作 18 年的老兵 Ben Gomes 将升任搜索部门副总裁，接管所有业务。 在谷歌工作 19 年、现任谷歌大脑负责人的 Jeff Dean 将成为谷歌人工智能部门的新任领导人。Jeff Dean 联合创建并管理着谷歌大脑团队，该团队主要致力于图像识别、语言翻译和机器人等项目，并将由他继续带领。 母公司 Alphabet 已经成为了科技行业人工智能工程师和科学家的首选企业，也一直积极地收购 AI 初创公司。迄今为止，它最大的收购案是在 2014 年以超过 5 亿美元收购的 DeepMind，这项收购为 Alphabet 引入了数十名熟练掌握「深度学习」技能的工程师。 Giannandrea 先生将继续留在谷歌，并打算更加深入地探索技术。 在谷歌于 2010 年收购了 Metaweb Technologies 之后，作为 Metaweb Technologies 的联合创始人和首席技术官的 Giannandrea 先生加入了谷歌。该公司的技术之后被用于知识图谱中，知识图谱是由谷歌所建立，目的是使网页搜索更加准确。 Mr. Giannandrea 一直反对那些预测人工智能有一天会威胁人类的言论。在 10 月份的 MIT TR 的采访中，他表示 AI 的真正危险在于训练算法的数据会受到人类偏见的负面影响。 
35,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740277&idx=3&sn=312b736f2052e0074ccffeb21431bf0c&chksm=871ad24bb06d5b5d7df3a41c90d67b8310005915a0686d05d9ec93875fbf8a5df26b436f04cc&scene=27,资源 | 学到了！UC Berkeley  CS 294深度强化学习课程（附视频与PPT）,UC Berkeley CS294 深度强化学习 2017 年秋季课程的所有资源已经放出。该课程为各位读者提供了强化学习的进阶资源，且广泛涉及深度强化学习的基本理论与前沿挑战。本文介绍了该课程主要讨论的强化学习主题，读者可根据兴趣爱好与背景知识选择不同部分的课程。请注意，UC Berkeley 的 CS 294 并未被归类为在线开放课程，所有视频的使用权仅限个人学习。 课程主页：http://rll.berkeley.edu/deeprlcourse/ 所有视频的链接：https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3 知识背景 本课程要求具有 CS 189 或同等学力。本课程将假定你已了解强化学习、数值优化和机器学习的相关背景知识。本课程所需的背景资料已在下表列出。在课程中，授课人会回顾这些资料的内容，但会非常简略。 强化学习和 MDP MDP 的定义 精确算法：策略与价值迭代 搜索算法 数值优化 梯度下降、随机梯度下降 反向传播算法 机器学习 分类和回归问题：使用哪些损失函数，如何拟合线性和非线性模型 训练/测试错误、过拟合 有关强化学习与 MDP 的介绍资料： CS188 EdX 课程，从马尔可夫决策过程 I 开始：http://ai.berkeley.edu/home.html Richard S. Sutton 与 Andrew G. Barto 的《强化学习导论》，第三章和第四章：http://incompleteideas.net/book/the-book-2nd.html 有关 MDP 的介绍，请参阅吴恩达的论文《Shaping and policy search in Reinforcement learning》：http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf David Silver 的课程：http://rll.berkeley.edu/deeprlcourse/#related-materials 有关机器学习和神经网络的介绍性资料，请参阅： Andrej Karpathy 的课程：http://cs231n.github.io/ Geoff Hinton 的 Coursera 课程：https://www.coursera.org/learn/neural-networks 吴恩达的 Coursera 课程：https://www.coursera.org/learn/machine-learning/ Yaser Abu-Mostafa 的课程：https://work.caltech.edu/telecourse.html 以下是 CS 294 深度强化学习 2017 年秋季课程的主要内容概要，所有的授课文档与视频都已经发布且展示在课程主页中。 8 月 23 日：课程简介（Levine） 该课程第一节课主要是课程介绍和对强化学习基本概念的介绍。 该课程教学大纲中包含以下内容： 1. 从监督学习到决策 2. 基础强化学习：Q 学习和策略梯度 3. 高级模型学习和预测、distillation、奖励学习 4. 高级深度强化学习：置信域策略梯度、actor-critic 方法、探索 5. 开放性问题、学术讲座、特邀报告 8 月 28 日：监督学习和模仿学习（Levine） 本节课介绍监督学习，主要内容包括： 1. 序列决策问题的定义 2. 模仿学习：使用监督学习进行决策 3.（深度）模仿学习近期研究案例分析 4. 模仿学习的缺点 本节课目标： 理解监督学习定义和符号； 理解基础模仿学习算法； 理解模仿学习算法的优劣势。 模仿学习： 8 月 30：强化学习简介（Levine） 本节课介绍强化学习，主要内容包括： 1. 马尔可夫决策过程的定义 2. 强化学习问题的定义 3. 强化学习算法解析 4. 简要介绍强化学习算法类型 本节课目标： 理解强化学习定义和符号； 理解强化学习的目标； 尽可能了解所有强化学习算法。 马尔可夫链定义： 马尔可夫决策过程定义： 强化学习算法类型： 9 月 6 日：策略梯度简介（Levine） 本节课介绍了策略梯度，主要内容包括： 1. 策略梯度算法 2. 策略梯度的作用 3. 基础方差缩减：因果性（causality） 4. 基础方差缩减：基线 5. 策略梯度示例 本节课的目标： 理解策略梯度强化学习； 理解使用策略梯度时需要实际考虑的事情。 9 月 8 日：神经网络概述（Achiam） 本节课全面介绍了神经网络，主要内容包括：自动微分、TensorFlow 基础知识、构建高级计算图、log 和 debug，以及计算图库、TensorFlow 的其他 API／封装器。 9 月 11 日：actor-critic 算法简介（Levine） 本节课介绍了 actor-critic 算法，主要内容包括： 1. 改进具备 critic 的策略梯度 2. 策略评估问题 3. 折现因子 4. actor-critic 算法 本节课目标： 理解策略评估与策略梯度如何拟合； 理解 actor-critic 算法的工作原理。 actor-critic 算法： 架构设计： 9 月 13 日：价值函数介绍（Levine） 本节课介绍价值函数的应用，包括从价值函数提取策略，如何用价值函数优化策略，Q-学习算法的介绍、实际应用和扩展等。 其中强调了聚焦于价值函数而不是策略本身的重要性，这有助于简化问题；并介绍了 Q-学习的多种模式，如离线模式、在线模式等。 9 月 18 日：高级 Q-学习算法（Levine） 本节课介绍 Q-学习算法的扩展，包括如何与深度学习结合、广义的 Q-学习算法、Q-学习算法的实际应用以及连续性 Q 学习算法。重点是理解在复杂函数逼近中实现 Q-学习，以及如何将 Q-学习扩展到连续动作。 深度 Q-学习算法的典型定义。 广义的 Q-学习算法：数据收集—目标更新—Q-函数回归。 9 月 20 日：最优控制和规划（Levine） 本节课介绍了无模型和基于模型的强化学习的差别，以及在建模过程中对转换动力学的先验知识的重要性；然后介绍了多种优化方法，包括随机优化（连续型）、蒙特卡洛树搜索（离散型）和轨迹优化。重点是理解如何结合离散或连续空间的已知系统动力学知识来执行规划。 知道强化学习问题的动力学知识会通常来说使问题更加简单，围棋、汽车、机器人、视频游戏等的动力学知识都是比较容易获取的。 9 月 25 日：从数据中学习动力学系统（Levine） 上节课中介绍了当知道系统的动力学知识之后，如何对问题进行建模。 本节课将介绍当系统动力学知识未知时的解决方案，包括拟合全局动力学模型（基于模型的强化学习）以及拟合局域动力学模型。重点是理解基于模型强化学习的术语和形式，可选的模型类型，以及模型学习中的实际考虑。 9 月 27 日：利用模仿优化控制器学习策略（Levine） 上节课中介绍了当系统动力学知识未知时的解决方案，包括全局方法（基于模型的强化学习）以及局域方法（基于模型并添加约束）。但当需要系统生成策略的时候，该怎么办呢？生成策略可以更快地评估智能体的动作，并且泛化潜力更好。 本节课将介绍如何利用反向传播算法来学习策略，它和模仿优化控制的关系，然后介绍了引导策略搜索算法，最后介绍了如何权衡基于模型和无模型强化学习的选择。本节课的重点在于理解用优化控制训练策略的过程，以及多种不同方法的权衡过程。 10 月 2 日：高级强化学习和图像处理应用（客座演讲：Chelsea Finn） 本节课介绍多种高级的模型学习方法，并以图像应用为例分别展示了隐空间学习、图像空间学习、逆模型学习和预测替代数量。 其中强调了学习特征的重要性，以及在利用观测模型时，需要考虑奖励函数和目标函数的设置。 基于模型方法和无模型方法的优缺点对比。 10 月 4 日：推断和控制之间的联系（Levine） 这一课程的主要目的是理解推断和控制之间的关系，以及理解具体的强化学习算法在框架下如何实例化。最优的控制其实可以作为拟合人类行为的模型，但如果数据达不到最优，那有如何拟合人类行为？我们还是可以将强化学习作为图模型中的推断而实现控制，其中价值函数为反向信息，且最大化奖励和信息熵以训练模型。其它方法还有 Soft Q-learning 和基于信息熵正则化的策略梯度等。 如下展示了一种制定决策或控制的概率图模型： 10 月 9 日：逆向强化学习（Levine） 本节课介绍逆向强化学习，主要内容包括： 1. 手动设计奖励函数来定义一个任务 2. 当我们想从观察专家过程中学习奖励函数，然后使用强化学习时会发生什么？ 3. 使用上节课的近似最优模型，学习奖励函数。 本节课目标： 理解逆向强化学习的定义； 理解如何使用行为概率模型推导出逆向强化学习算法； 了解我们实践中使用的逆向强化学习算法。 逆向强化学习： 以下是这一章节的总结： 10 月 11 日：高级策略梯度（自然梯度、重要性采样）（Achiam） 本节课介绍高级策略梯度方法，主要内容包括： 理论： 策略梯度方法的问题 策略性能边界 单调改进理论（Monotonic Improvement Theory） 算法： 自然策略梯度 置信域策略优化 近端策略优化 自然策略梯度： 置信域策略优化： 10 月 16 日：探索（Levine） 这一章节主要介绍了什么是探索（exploration），以及为什么它在强化学习中非常重要。一般来说探索分为基于乐观探索、基于后验匹配的探索和基于信息理论的探索。探索和利用（exploitation）的均衡在强化学习中非常重要，也是非常难以解决的问题。以下展示了探索与利用之间的基本区别： 随后 Levine 详细展开介绍了为什么探索是非常困难的，包括摇臂赌博机问题等，而后重点介绍了乐观探索（Optimistic exploration）、概率匹配与后验采样，以及信息增益等探索方法。以下展示了一种探索算法。 10 月 18 日：探索（第二部分）和迁移学习（Levine） 这一章节首先复习了上堂课介绍的乐观探索、Thompson 采样风格的算法和信息增益风格的算法，然后介绍了这三类算法的近似论证。最后，讲师 Levine 还给出了一系列的延伸阅读以加强我们对探索的理解。 该课程后一部分介绍了元学习与迁移学习，以下展示了迁移学习中的一种架构： 渐进神经网络。 10 月 23 日：多任务学习与迁移（Levine） 该课程主要介绍了多任务学习与迁移学习。说到如何解决迁移学习的问题，没有一个特定的解决方案，所以此课主要对近期（当时）的各种演讲论文进行了介绍。 1.「前向」迁移：在一个任务上训练，迁移到新任务 只是试试，希望有好结果 架构迁移：渐进网络 在新任务上微调 随机化源任务域 2. 多任务迁移：在多种任务上训练，迁移到一个新任务上 基于模型的强化学习 模型精炼 情境策略 模块化策略网络 3. 多任务元学习：学习从多种任务上学习 基于 RNN 的元学习 基于梯度的元学习 10 月 25 日：元学习和并行化（Levine） 首先元学习是一种学习如何学习的方法，它在实践中与多任务学习非常相近，一般而言元学习可以分为学习一种优化器、学习一个 RNN 以捕捉经验信息和学习一种表征。如果元学习是一种快速的强化学习器，那么我们就能快速学习新任务。 一般而言元学习可以通过监督学习或强化学习构建，它可以返回优秀的表征而加速学习也可以用来构建对经验的记忆等。 该章节的后一部分介绍了强化学习中的并行化，包括强化学习到底哪一部分需要并行，如何并行以及最终怎样实现等。以下展示了我们最终需要并行的部分。 10 月 30 日：进阶模仿学习和开放性问题（Levine） 我们知道模仿学习的目标是通过监督学习在给定观察下求得行动的概率分布，而强化学习是给定环境和状态下求得行动的概率分布。模仿学习要求预先的演示且必须解决分布迁移问题，它的优点在于可以通过简单稳定的监督学习实现。而强化学习需要奖励函数且必须解决模型的探索问题，它虽然可能会无法收敛，但却能够实现任意好的性能。 这一章节介绍了结合模仿学习的监督方式和强化学习的方法： 后一部分介绍了深度强化学习的挑战，包括超参数调整、样本复杂度、泛化性能和 shenwuxu 生物学启示等。 该课程后面还有很多受邀嘉宾与它们所授的课程，包括 OpenAI 的 Igor Mordatch、谷歌的 Mohammad Norouz、伯克利和 OpenAI 的 Pieter Abbeel、伯克利的 Aviv Tamar 和 OpenAI 的 John Schulman。他们并没有提供对应的授课文档，但演讲视频在 YouTube 上都已经放出来了。 相关学习材料 机器学习夏季课程中 John 的视频： 视频 1：https://www.youtube.com/watch?v=aUrX-rP_ss4 视频 2：https://www.youtube.com/watch?v=oPGVsoBonLM 视频 3：https://www.youtube.com/watch?v=rO7Dx8pSJQw 视频 4：https://www.youtube.com/watch?v=gb5Q2XL5c8A 课程： David Silver 的强化学习课程：http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html Nando de Freita 的机器学习课程：https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ Andrej Karpathy 的神经网络课程：http://cs231n.github.io/ 相关书籍： 深度学习：http://www.deeplearningbook.org/ Sutton 和 Barto 合著的 Reinforcement Learning: An Introduction (http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)：http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html Szepesvari 的Algorithms for Reinforcement Learning：https://sites.ualberta.ca/~szepesva/RLBook.html Dynamic Programming and Optimal Control：http://www.athenasc.com/dpbook.html Markov Decision Processes: Discrete Stochastic Dynamic Programming：https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780471727828 Approximate Dynamic Programming：http://adp.princeton.edu/ 
36,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740277&idx=4&sn=f499a813d6d46a5af0782501a8a94291&chksm=871ad24bb06d5b5d9c38ce9aa0c6c9c8bf2d37e17013fa78a7288c11a8880513201873daca3b&scene=27,入门 | 从VGG到NASNet，一文概览图像分类网络,towardsdatascience 了解图像分类的不同网络架构是一项非常艰巨的任务。本文将讨论目前可在 keras 上使用的主要架构。作者将按照这些架构出现的时间顺序对其逐一讲解，并尝试以从业者的角度讨论其优缺点。 关键概念 虽然计算机视觉研究者们采取的方法各不相同，但是大体而言，他们的实验设置有着如下的趋势。本文将讨论如何进行图像预处理，数据增强用于哪类数据，优化机制以及输出层的实现方法。 预处理 通常而言，我们会计算训练集图像的平均像素值，将其从图像中减去。请注意，在 keras 环境下使用这些模型时考虑预处理方法很重要。计算机视觉模型不同，Keras 的「预处理」也不同。 数据增强 图像分类的数据集非常大。尽管如此，依然需要数据增强来提高模型泛化能力。数据增强一般包括重新缩放图像的随机裁剪、随机水平翻转、随机 RGB 颜色与亮度变换等技术。此外，也存在不同的缩放、裁剪等技术（即单尺度训练 vs 多尺度训练）。在测试阶段进行多裁剪评估也是经常使用的途径，不过该方案的计算成本更昂贵且性能改进有限。请注意，随机缩放和裁剪的目标是在不同尺寸和位置上学习对象的重要特征。Keras 并未实现所有数据增强技术的开箱即用，但可以通过 ImageDataGenerator 模块的预处理技术轻松实现。Andrew Howard 提出的数据增强技术更深入地解释了这些关键性的方法，具体参见： https://arxiv.org/ftp/arxiv/papers/1312/1312.5402.pdf 训练机制 在 keras 中可通过多 GPU 数据并行化训练模型（一般批大小为 256）。动量 SGD 或 RMSProp 是常用的优化技术。学习率的方案相对简单，要么在验证集的损失或准确率开始稳定时调低学习率，要么在固定间隔上调低学习率。通过 keras 中的「ReduceLROnPlateau」回调函数可以轻松模拟这种行为。 最后一层 图像分类网络中最后一层传统上来说是全连接层。这些层的参数量巨大，因为你需要 N×M 个参数才能从 N 个隐藏节点过渡到 M 个节点。现在，这些全连接层已经被平均池化或最大池化层替代，它们要求的参数量和计算时间比较小。在对 keras 中预先训练好的网络进行微调时，这一点非常重要，这能限制所需要添加参数的数量。 VGGNet VGGNet（https://arxiv.org/pdf/1409.1556.pdf）发布于 2014 年，作者是 Karen Simonyan 和 Andrew Zisserman，该网络表明堆叠多个层是提升计算机视觉性能的关键因素。VGGNet 包含 16 或 19 层，主要由小型的 3×3 卷积操作和 2×2 池化操作组成。 VGG 的优点在于，堆叠多个小的卷积核而不使用池化操作可以增加网络的表征深度，同时限制参数的数量。例如，通过堆叠 3 个 3×3 卷积层而不是使用单个的 7×7 层，可以克服一些限制。首先，这样做组合了三个非线性函数，而不只是一个，使得决策函数更有判别力和表征能力。第二，参数量减少了 81%，而感受野保持不变。另外，小卷积核的使用也扮演了正则化器的角色，并提高了不同卷积核的有效性。 VGG 的缺点在于，其评估的开销比浅层网络更加昂贵，内存和参数（140M）也更多。这些参数的大部分都可以归因于第一个全连接层。结果表明，这些层可以在不降低性能的情况下移除，同时显著减少了必要参数的数量。16 层和 19 层的参数预训练 VGG 在 keras 上是可以使用的。 ResNet ResNet 架构是由何凯明等人提出的，他们试图通过这个架构训练更深的网络。作者指出，增加网络深度会导致更高的训练误差，这表明梯度问题（梯度消失/爆炸）可能会导致训练收敛性等潜在问题。 尽管 20 层网络的潜在函数空间是封装在 56 层网络的空间内且运用了传统的梯度下降，但无法实现同样的效果（选自 ResNet 论文） ResNet 的主要贡献是增加了神经网络架构的跳过连接（skip connection），使用批归一化并移除了作为最后一层的全连接层。 跳过连接基于这样一种想法：只要神经网络模型能够「适当地」将信息从前一层传递到下一层，它应该能变得「无限」深。如果在更深层没有附加信息进行聚合，那么带有跳过连接的卷积层可以视为一个恒等映射函数。 通过向网络中添加跳过连接，卷积层的默认函数变成了恒等函数。卷积核学到的任何新信息都可以在基本表征中添加或减去，因此这更容易优化残差映射。跳过连接不会增加参数的数量，但可以获得更稳定的训练和显著的性能提升，这是因为可以达到更深的网络（例如深度为 34、50、101 和 152 的网络）。请注意，1×1 的卷积用于减少输出通道的个数。 除跳过连接之外，在每次卷积完成后、激活进行前都采取批归一化。最后，网络删除了全连接层，并使用平均池化层减少参数的数量。由于网络加深，卷积层的抽象能力更强，从而减少了对全连接层的需求。 GoogLeNet GoogLeNet 与 ResNet 的论文几乎同时发表，但它们引入了不同的改进方案。前面提到的两篇论文着重于提高分类网络的表征深度。 然而，GoogLeNet 仍试图扩大网络（多达 22 层），但也希望减少参数量和计算量。最初的 Inception 架构由 Google 发布，重点将 CNN 应用于大数据场景以及移动端。GoogLeNet 是包含 Inception 模块的全卷积结构。这些模块的目的是：通过构建由多个子模块（比如嵌套网络 - Inception）组成的复杂卷积核来提高卷积核的学习能力和抽象能力。 Inception 模块的实例。1x1 卷积用来减小输入/输出的维度（选自 GoogLeNet 论文）。 除了加入 Inception 模块，作者还使用了辅助分类器来提高稳定性和收敛速度。辅助分类器的想法是使用几个不同层的图像表征来执行分类任务（黄色框）。因此，模型中的不同层都可以计算梯度，然后使用这些梯度来优化训练。 GoogLeNet论文 Inception v3 Inception v3 架构中结合了几项创新点。在 Inception v3 中，主要的创新在于借鉴了 GoogLeNet 和 VGGNet 的部分原创思想，即使用 Inception 模块并通过一系列较小的卷积核更高效地表示较大的卷积核。除了小卷积之外，作者还尝试了非对称卷积（例如用 n×1 和 1×n 代替 n×n，而非多个 2×2 和 3×3 滤波器）。 一个 3x3 卷积核后跟一个 1x1 卷积核的例子，它有效地取代了一个 5x5 卷积核（图片来自 Inception v3 论文）。 作者通过执行批归一化和标签平滑化来改进正则化。标签平滑就是为每个类都分配一些权重，而不是将全权重分配给 ground truth 标签。由于网络对训练标签的过拟合程度较低，因此它应该能够更好地泛化，这与使用 L2 正则化效果相仿。 为了确保该模型在高分辨率图像和低分辨率图像上均表现良好，作者通过 Inception 模块分析了不同尺寸下的图像表征。因此，当 Inception 网络用于目标检测框架时，它们在对小分辨率和低分辨率对象进行分类时表现良好。 NASNet 我要讨论的最后一个图像分类架构是 NASNet（https://arxiv.org/pdf/1707.07012.pdf），它是使用神经结构搜索（NAS）框架构建的。NASNet 的目标是运用数据驱动和智能方法，而非直觉和实验来构建网络架构。尽管我不会详细讨论这个框架，但是可以解释一下它的总体思路。 Inception 论文表明「神经网络单元」中复杂的卷积核组合单元可以显著提升结果。NAS 框架将这种单元的构建过程定义为优化过程，然后通过叠加最佳单元来构建大型网络。 原文链接：https://towardsdatascience.com/an-overview-of-image-classification-networks-3fb4ff6fa61b 
37,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740315&idx=2&sn=9302bbdcdb3eeb73eccdb7219e87a6c1&chksm=871ad225b06d5b336945ffe58e0c787b9b73f1266b66b68207663c14ac71e031540f99d8c720&scene=27,教程 | 用Scikit-Learn构建K-近邻算法，分类MNIST数据集,"K 近邻算法，简称 K-NN。在如今深度学习盛行的时代，这个经典的机器学习算法经常被轻视。本篇教程将带你使用 Scikit-Learn 构建 K 近邻算法，并应用于 MNIST 数据集。然后，作者将带你构建自己的 K-NN 算法，开发出比 Scikit-Learn K-NN 更准更快的算法。 K 近邻算法是一种容易实现的监督机器学习算法，并且其分类性能的鲁棒性还不错。 K-NN 最大的优点之一就是它是一个惰性算法，即该模型无须训练就可以对数据进行分类，而不像其他需要训练的 ML 算法，如 SVM、回归和多层感知机。 K-NN 如何工作 为了对给定的数据点 p 进行分类，K-NN 模型首先使用某个距离度量将 p 与其数据库中其它点进行比较。 距离度量就是类似欧几里得距离之类的标准，以两个点为输入并返回这两个点之间距离的简单函数。 因此，可以假设距离较小的两个点比距离较大的两个点相似度更高。这是 K-NN 的核心思想。 该过程将返回一个无序数组，其中数组中的每一项都表示 p 与模型数据库中 n 个数据点之间的距离。所以返回数组的大小为 n。 K 近邻的 K 的含义是：k 是一个任意值（通常在 3-11 之间），表示模型在对 p 分类时应该考虑多少个最相似的点。然后模型将记录这 k 个最相似的值，并使用投票算法来决定 p 属于哪一类，如下图所示。 上图中的 K-NN 模型的 k 值为 3，箭头指向的中心点为 p，算法将对这个点进行分类。 如你所见，圆圈中的三个点是与 p 最接近或最相似的三个点。因此，使用简单的投票算法，p 将被归为「白色」，因为白色在 k 个最相似值中占大多数。 酷炫！但令人惊讶的是，这个简单的算法可以在某些情况下实现不俗的结果，并且可以应用于各种各样的问题，我们将在下面介绍。 数据： 对于这个例子，我们将使用常见的 MNIST 数据集。MNIST 数据集是机器学习中最常用的数据集之一，因为它很容易实现，而且是验证我们模型的可靠方法。 MNIST 是一组包含 70,000 个手写数字 0-9 的数据集。任意两个手写数字都不相同，有些可能很难正确分类。 算法： 我们从 Scikit-Learn 的 Python 库的 KNeighborsClassifier() 函数入手。这个函数有很多参数，但在这个例子中我们只需用少量几个参数。具体来说，我们只会传递 n_neighbors 参数的值（就是 k 值啦）。 weights 参数给出了模型使用的投票算法的类型，其中默认值是 uniform。这意味着在对 p 进行分类时，k 个点中的每一个的权重都一样。algorithm 参数也将使用默认值 auto，因为我们希望 Scikit-Learn 自动找到对 MNIST 数据进行分类的最佳算法。 以下是一个用 Scikit-Learn 构建 K-NN 分类器的 Jupyter Notebook： Notebook 地址：https://gist.github.com/samgrassi01/82d0e5f89daac3e65531a6ef497cc129#file-skl-knn-ipynb 我们通过导入所需的库直接开始。 构建数据集 我们通过制作不同的数据集来构建 K-NN 模型。我们将创建一个可以获取特定大小数据集、返回数据集大小的函数。 def mk_dataset (size) 不错。现在我们将使用这个函数来构建两个不同大小的数据集，来看看模型在不同数据量上的分类性能怎么样。 提示：制作较小的数据集时，你仍然可以进行分类，但模型毕竟少了一些数据，这可能会导致分类错误。 注意这些数据是如何为模型匹配标签的。模型需要这些标签来理解每一个点代表什么，因此可以把我们要分类的点放在一个特定的类中，而不是说「这是与待分类点最相似的类」。 现在我们将构建一个大小为 10000 的测试集。 不错！现在我们已经完成了所有的数据处理，可以开始搭建 K-NN 模型了！ 构建模型 我们首先将 Scikit-Learn K-NN 模型放在函数中，以便可以轻松调用它并对其进行调整。 def skl_knn (k, test_data, test_target, stored_data, stored_target) 测试 现在我们看看这个模型在两个不同的测试集上的运行效果。 可以的！我们的模型与人眼识别差不多！如你所见，当模型有更多的数据可以使用时（50,000 而不是 20,000 个点），它的性能会更好。更加引人注目的是，它非常简单，并且能以人类水平来获取不同图像之间的复杂关系。更多的细节分析请访问这个 GitHub repo：https://github.com/samgrassi01/Cosine-Similarity-Classifier。 厉害了！我们使用 Scikit-Learn 构建了一个非常简单的 K 近邻模型，该模型在 MNIST 数据集上表现非凡。 不足之处？分类这些点需要很长时间（两个数据集分别耗时 8 分钟和 4 分钟），讽刺的是，K-NN 仍然是最快的分类方法之一。我们必须有一个更快的方法。 构建一个更快的模型 大多数 K-NN 模型使用欧几里德距离或曼哈顿距离作为 go-to 距离度量。这些指标非常简单，而且在各种各样的情况中都表现不错。 还有一个很少用到的距离标准度量是余弦相似度。余弦相似度通常不是 go-to 距离度量标准，这是因为它违反了三角不等式，而且对负数无效。但是，余弦相似度对于 MNIST 来说很完美。它速度快、算法简单，而且比 MNIST 中应用其他距离度量的准确率稍高一些。 但是，为了得到最佳性能，我们需要自己编写 K-NN 模型。然后，我们应该能得到比 Scikit-Learn 模型更高的性能，甚至能得到更高的准确度。让我们看看以下建立的 K-NN 模型的 Notebook 吧： 构建一个更快的 KNN 分类器 Notebook 地址：https://gist.github.com/samgrassi01/15a1fe53dcde8813eed9367b103676b2#file-cos-knn-ipynb 在这个 notebook 中，我们将构建一个简单的 K-NN 模型，该模型使用余弦相似度作为距离度量对 MNIST 图像进行分类，试图找到更快或更加准确的模型。 首先，需要导入所需的库，然后构建与 Scikit-Learn K-NN notebook 相同的数据集。 使用与 Scikit-Learn K-NN notebook 相同的方法，设置完全相同的数据集。 def mk_dataset (size) 构建模型 下面，我们创建函数 cos_knn()，作为用于 MNIST 数据集的分类器。你可以利用函数的注释了解其工作原理。 def cos_knn (k, test_data, test_target, stored_data, stored_target) 测试模型 现在，就像 Scikit-Learn K-NN 模型一样，我们对 cos_knn() 模型在两个数据集上分别测试，并看看它的性能如何。 太棒了！余弦相似度模型性能超过了 Scikit-Learn K-NN！值得一提的是，该模型的分类速度和准确率都优于 Scikit-Learn K-NN（其中速度获得了很大提升），而模型却非常简单！ 为了进一步分析模型的工作原理，同时了解该模型为何在许多不同情况下比 Scikit-Learn K-NN 模型要性能更优，请参阅这个 GitHub repo：https://github.com/samgrassi01/Cosine-Similarity-Classifier。 正如 notebook 所示，该 K-NN 模型在分类速度和准确率方面都胜过了 Scikit-Learn K-NN，其中速度获得了大幅提升，而在一个数据集上的准确率提高了 1%。既然如此，我们可以在实践中继续使用这个模型了。 首先，我们知道了 K-NN 的工作机制，以及如何轻松地实现它。但最重要的是，我们发现，始终考虑需要解决的问题以及解决问题的工具非常重要。有时候，在解决问题的过程中，最好花一些时间来实践——当然，也需要建立自己的模型。正如 notebook 中所展示的那样，它可以带来极大的益处：我们第二个专有模型获得了 1.5 - 2 倍的加速，节省了很多时间。 原文链接：https://towardsdatascience.com/building-improving-a-k-nearest-neighbors-algorithm-in-python-3b6b5320d2f8 "
38,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740277&idx=2&sn=bba8cec481d997a9bfcd67777af4b84d&chksm=871ad24bb06d5b5d543046117a77f0e31136358751765ca82c9d8210f7e9c0d2b1b4335d6367&scene=27,专访 | 追一科技首席科学家杨振宇：对话机器人里不能「耳闻目览」却又「无所不在」的 AI,机器之心原创 对话机器人是「怎样炼成的」。 相比于语音和图像，自然语言是一个有「更多需求」和「更少标准答案」的领域。扎根自然语言的公司通常也不是从技术和方法出发，而是选择一个具体的需求，然后用所有可能的方法解决它。追一就是这样的一家公司，它瞄准的是「对话机器人」这个领域，把问题分类、分解、逐个建立准确高效的机器人，再有序集成起来。三月，机器之心有幸在深圳追一科技总部对首席科学家杨振宇进行了采访，我们仔细聊了聊「对话机器人是怎样炼成的」，以及在他眼里，深度学习与自然语言最好的结合方式是怎样的。 对话机器人需要解决的问题是如何分类的？ 对话机器人是一个相对比较复杂的系统，由许多个模块组成。其中最核心的模块就是语义理解，理解用户想要表达什么意思。而在利用深度学习处理商业对话机器人的语义理解问题方面，追一是国内最早的一家。 根据服务类型，对话机器人可以分为 FAQ 咨询、资料查询、任务型和闲聊四种，涉及的自然语言处理问题也各不相同。 对于  来说，模块的输出对应知识库里的一个知识点。解决问答就像解决一个大型分类问题，机器人要将用户的需求对应到知识库里的某一个答案。 知识库里的知识点数量少则几百个，多则上万个。而根据知识库大小不同，适用的模型结构也会有所不同。例如，银行类客户通常有多个复杂的业务线，知识库规模也是数以万计，直接对几万个知识点进行分类是难以取得高准确率的，因此，机器人会采用分层处理的方法，先判定问题与哪一个大领域相关，再进行详细的知识点分类。 资料查询类 对话需要从客户的输入里判定两件事：意图和实体。比如「A 公司的市盈率是多少？」这个问句里，就包含了意图「市盈率」和实体「A 公司」。成功获得这两个信息后，机器人会去一个结构化的数据库里做查询。得到答案后，按照一个预定义的格式化模板填充后返回给客户。 资料查询的一个难点是，用户在连续发问时，不会每次都重复自己的意图和实体，比如用户会在询问「A 公司的市盈率是多少？」之后，追问说，「那 B 公司的呢？」或者「那市净率呢？」。这时，系统就需要通过上下文管理，对意图和实体这些要素进行继承或切换。用户的提问到来之后，首先进行判断：用户在这一句中是否提供了某一要素？如果没有，则尝试从前文追溯继承；如果有，再判断用户是否进行了意图（实体）转移，如果是，则需要进行对应更新。 另一个难点是，用户可能不会直接说出实体全称，无法进行精准的、基于规则的匹配。因此，机器人需要结合特定用户的历史记录和用户群体的统计信息，通过学习的方法计算词与词之间转移的概率，然后进行模糊匹配。 任务型 对话是当下比较流行的一种交互形式，机器人试图以对话的形式来执行订机票、查账单、买理财等任务。任务型和资料查询类对话有相似之处：它们同样要从用户处获得两类信息：意图和「元素」。区别在于， ：它要理清进行特定任务所必要的元素有哪些，并以对话的形式确保用户提供了所有元素。以订机票举例，用户说「帮我订明天北京到上海的机票」，那么机器人在明确了任务是「订机票」之后，就要理清，用户已经提供的元素有时间、出发地、到达地，尚未提供的元素有舱位偏好、时间偏好、特定机场偏好等。只有获得了全部所需元素，机器人才能「执行任务」。 最后一类是 ，与陪护机器人的闲聊功能不同，穿插在查询、咨询问答或任务交互之间的闲聊，需要结合上下文一起识别。有时，一句话单独看是闲聊的意图，但结合上下文一起看则属于查询、咨询问答或任务交互的一部分。这种场景下的闲聊不仅要识别准，而且要保证上下文对话的流畅性，也非常有挑战。 如前所述，系统是由 FAQ 咨询、资料查询、任务型和闲聊等不同类型的机器人组成的。除此之外，还有一个中控模块，是系统的管理控制中枢。 中控 负责依据用户当前问句和历史会话记录，初步判断当前问题应该由哪个机器人来回答，然后下发给一个或多个下游机器人。下游机器人处理后，将答案以及对应的置信度返回给中控，中控根据下游返回的信息进行决策后，将最合适的响应返回给用户。 对话机器人可能发生的误判有两种不同形式。 一种是「系统知道自己可能犯错」 ：某一请求虽然分配给了特定机器人，但是机器人给出的最佳回答的置信度仍然很低，换言之，该回答能够满足用户请求的可能性很小。此时，中控会进行重新判断：其他机器人是否有置信度更高的回答？如果仍然无法找到置信度足够高的答案，系统会启动 机制，即，系统判断自己无法回答这一问题，返回给用户一个通用话术，如「十分抱歉，您的问题我暂时回答不了」，然后记录下问题，定期推送给机器人运营人员通过数据标注、知识库扩充等手段进行迭代优化。 另一种则是「系统很自信，但还是回答错了」 。这类错误要通过分析用户的反馈来发现，比如用户选择点「踩」或者继续提问，都意味着他们的需求可能没有得到满足。系统会结合分析结果把这种疑似错误的回答挑出来，形成一个叫做「质检」的任务，交给客户的质检人员判断并标注，补充进训练集进行迭代优化。与此同时，客户也拥有能够进行即时调整的人工干预工具：如果客户需要立刻对某个错误答案进行调整时，可以利用这个工具找到导致错误答案的影响因素，通过对这些影响因素的调整来达到即时生效的干预效果。 对于机器人的优化来说，极其重要的一点是在系统层面有完整的反馈学习机制，能够让机器人收集可能的错误情景，以及特定场景下的正确回答信号，然后利用这些数据和信号建立正反馈机制。 总体来说，通过初始化教育和持续运营优化，机器人一般能够达到综合准确率 95% 的效果，这样就会获得比较好的对话体验了。 对于学界而言，最好的研究是用新方法解决新问题，其次是用新方法解决老问题，最次是用老方法解决老问题。而工业界的衡量标准则截然不同，工业界的出发点是以需求为核心，无论什么方法，能落地提供极致体验的就是好方法。   举个例子，业界的系统是需要给客户提供快速干预工具的：一个无法调整的、干预不了的系统是不合格的。出于对企业形象的重视，客户需要能够监控机器的效果、需要在发现问题之后能够实时干预、需要定期检查用户反馈，让反馈数据作为新的监督信号进一步优化模型。业界搭建的优秀系统要有自适应的闭环和流动性，这是学界很少会考虑的事情。 另外，业界也需要更多地考虑系统上线时的资源限制问题。比如在自然语言领域里一个典型的测试环境，斯坦福的 SQuAD 数据集上，现在学界里效果最好的做法是把数十个不同的模型集成在一起，然而这种做法是很难在工业界大规模应用的。工业界要考虑成本、客户在云端的算力、用户从提问到得到反馈的时间等种种限制，集成少数几个模型也许是可行的，但是集成几十个高代价的模型是不切实际的。   当然，业界也在持续关注学界的新方法，希望得到方法论上的启示。 例如，深度强化学习的飞速发展也会让我们关注：能否依靠数据驱动，端到端地训练具有工业应用价值的任务型机器人？能否在有了足够多数据之后，把对话信息看做状态，用深度强化学习的方法学习一个对话策略出来呢？   另外，阅读理解领域的发展也让我们看到了不是基于「知识库」而是基于「文档库」进行回答的可能性。例如，一个特定问题的答案可能在某个文档里，如何构建一个深度学习系统，根据某一问题，定位到特定的文档，再从文档中把信息提炼出来用以回答这个问题？ 图片来自：http://blog.otoro.net/2015/01/27/neuroevolution-algorithms/ 我个人的研究背景是计算智能（Computational Intelligence），在 2015 年之前的近 10 年里，主要关注如何应用计算智能中的元启发式优化算法求解大规模复杂优化问题。这是一类基础性的研究难题，长期受到关注，比如我大约 10 年前的一个关于演化优化算法的工作，到现在已经被引用 500 多次了，而且现在还在逐渐增加，偶尔还会收到同行提出算法代码需求的邮件。随着算力的持续提升，也许元启发式算法在不久的将来会成为人工智能领域新的宠儿，我个人也非常关注它与深度学习结合的可能。 元启发式算法的最大的优点在于不要求目标函数可导， 。现在，为了求解方便，建模时往往会对实际应用的真实目标做一些简化，使得该目标函数可导，以便可以用现成的梯度下降算法求解。然而这往往不是最优方案：一方面建模的简化一般会引入求解效果上限的损失；另一方面梯度下降算法不能保证获得全局最优解，往往只会收敛到一个极值。 元启发式优化算法是一个启发式框架，一般是设计一些通用的、对问题的依赖没有那么强的启发式策略，使算法更容易收敛到全局最优解。元启发式优化算法的缺点在于算法对计算能力的要求比较高。这主要是因为梯度下降算法有明确方向，只要朝着这一个方向下降就可以了，而元启发式算法，如其中的「主流分支演化算法」，用的方法则是试错法（trial-and-error），需要在空间里进行大量的采样、评估，再决策接下来去何处继续采样更有获得全局最优解的潜力。这个过程是非常消耗算力的。 现在，学界的关注重点是能否用局部优化的思想减少采样点的数量，以及能否提高采样、评估的并行化执行能力。 此外，元启发式算法还有一个梯度下降无论如何也做不到的优势，就是有希望进行 。如今神经网络的结构是全部由人来设计的，未必或者极有可能不是最优的。利用元启发式算法能够让机器在卷积层、循环层、全连接层等不同开放组合结构构成的空间中进行搜索，组装出一个针对特定问题效果最好的结构。这个方向一般叫做 Neuroevolution，正逐渐引起越来越多的学界和业界的关注。 在中控的排序模型里，深度学习模型能够 ，让体量更小、更简单的模型就能获得同等，甚至更优的效果。 在前深度学习时代，传统的搜索推荐经常需要上千万、上亿的特征，这是因为，在没有强大的特征的情况下，只能把各种组合人为构造出来，然后用一个浅层的大模型去学习。然而深度学习模型学习出的特征本身已经包含了巨大的信息量，因此繁琐的人工特征工程变得没那么重要了，这对技术的商业化非常有利。 这种信息量是通过深度学习在面对大规模原始数据的抽象、非线性学习能力而获得的。例如，通过抓取大量金融行业的相关文本，进行统一的文本语义相似度学习建模和训练，就能得到一个强大的文本特征提取模型。这个模型可以用于证券领域、基金领域、银行服务领域等等各个行业系统中。 此外，深度学习也可以通过 的方式整合不同类型的问答数据，极大简化了启动阶段客户标注数据的工作。 知识点的构建阶段需要客户提供有标注的训练数据：每一个知识点（标准答案），都需要客户「手写」用户标准提问范式和可能的变体。构建这样的标准问答集是极其耗费时间的，说服客户进行标注也是有一定难度的，然而如果客户之前采用过人工客服，有人工客服日志，我们就可以结合人工客服日志，使用多任务学习大量削减需要人工完成的标注数据集创建工作。 追一一直偏重实用、偏重落地，我们不仅仅钻研最前沿的技术，也致力于优化落地效果，把自适应、有闭环正反馈的系统交付给用户。令人自豪的是，我们有一批能够把问题想清楚的人，正在用系统工程的思维为客户构建具有极致体验的对话机器人服务。 我们的对话机器人，不追求「一个 AI 模型端到端解决全部问题」，而是在一个完整系统的每个模块里、各个维度里，都用 AI 技术帮助提升效率、优化体验。这种优化可能帮助主系统提取了更强的特征，可能在冷启动阶段帮助客户简化了痛苦的数据标注过程，可能在运转过程里帮助分析舆情、找到客户产品设计的不合理之处。它们并不显山露水，却无处不在。 这也是追一希望向用户传达的理念：真正有「AI 意味」的系统，是「草色遥看近却无」的整体系统，而 AI 的角色，是「润物细无声」地、持续地优化其中的每一个细节。 
39,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740277&idx=5&sn=a96919def5eed87250ce35981c5c939b&chksm=871ad24bb06d5b5d55370cc6da7fdc46276df1d4fa32102b4486fa52eb57f8e9e0079225075c&scene=27,学界 | NYU联合Google Brain提出结合工作记忆的视觉推理架构和数据集,"arXiv 现有的视觉推理数据集都避开了时间和记忆的复杂性，而这两者都是现实应用中不可或缺的因素。为突破这个局限性，受认知心理学启发，纽约大学联合 Google Brain 开发了新的视觉问答数据集 ( COG ) 以及对应的网络架构。该架构能利用多模态信息和动态注意、记忆机制执行推理，初步分析表明，它能以人类可理解的方式完成任务。 1. 简介 人工智能的一个主要目标就是构建能够对感官环境进行强有力并且灵活地推理的系统 [1]。视觉提供了一个极其丰富和高度实用的领域，我们可以在其中通过建立系统对复杂的刺激执行逻辑推理 [2,3,4,5]。研究视觉推理的一个途径是对视觉问答 ( VQA ) 数据集进行建模，模型可以从中学习正确地回答关于静态图像的挑战性自然语言问题 [6,7,8,9]。尽管这些多模态数据集已经有了很大进步，但是目前的方法还存在几个局限性。首先，与推理一个问题的逻辑组成不一样，在 VQA 数据集上训练的模型刚好遵循图像中的固有统计特性的程度是不确定的 [10,11,12,13]。其次，这些数据集都避开了时间和记忆的复杂性，而这两者都是智能体设计 [1,14,15,16] 与视频分析、总结 [17,18,19] 中不可或缺的因素。 图 1. COG 数据集中的样本序列和指令。COG 数据集中的任务是测试目标识别、关系理解以及为解决问题而进行的记忆操作和适应。所有问题都可能涉及到当前图像和之前图像中的目标。注意在最后一个例子中，指令涉及到最后一个但不是最近一个「b」。前者排除了在当前图像中寻找「b」。白色箭头表示每个图像的目标响应。 为了解决 VQA 数据集中与空间关系的逻辑推理相关的缺点，Johnson 等人 [10] 最近提出了 CLEVER 来直接用于基本视觉推理模型的测试，以便与其他 VQA 数据集相结合 (例如，[6,7,8,9])。CLEVR 数据集提供了人工静态图像和关于这些图像的自然语言问题，让模型学习执行逻辑和视觉推理。最近研究中人们开发出来的网络能够达到几乎完美的准确率 [5,4,20]。 在这项工作中，研究者解决了视觉推理中的第二项限制，即关于时间和记忆的限制。推理智能体必须记住它的视觉历史中相关的片段，忽略不相关的细节，基于新的信息来更新和操作记忆，以及在后面的时间里利用这些记忆来作出决策。作者的方法就是创建一个人工的数据集，它具有时变数据中存在的很多复杂性，同时也避免了处理视频时的很多视觉复杂性和技术难题（例如，视频解码、时间平滑帧之间的冗余）。特别是，作者从认知心理学 [21,22,23,24,25] 和现代系统神经科学 [26,27,28,29,30,31] 最近几十年的研究中得到启发。这些领域有着基于空间和逻辑推理、记忆组成和语义理解将视觉推理分解为核心组件的悠久研究传统。为此，作者建立了一个称为 COG 的人工数据集，它也能用于人类的认知实验 [32,33,34]，并能够及时地训练视觉推理。 COG 数据集是基于一种能够构建三元组任务集的编程语言开发的：三元组包含图像序列、语言指令以及正确答案的序列。这些随机生成的三元组能够在大量的任务序列中训练视觉推理，解决它们需要对文本的语义理解，对图像序列中每张图像的视觉认知，以及决定时变答案的工作记忆（图 3）。研究者在编程语言中特别强调了几个参数，开发者可以通过这些参数来从易到难地设定挑战性环境，从而对问题难度进行调制。 最后，作者引入了用于有记忆视觉推理的多模态循环架构。该网络将语义、视觉模块与状态控制器相结合，状态控制器调节视觉注意力和记忆，以便正确执行视觉任务。他们证明了该模型在 CLEVER 数据集上取得当前最佳的性能。此外，该网络还提供了稳健的基线，其可以在 COG 数据集的一系列设置中实现良好的性能。通过控制变量研究和对网络的动态分析，他们发现网络采用人类可解释的注意力机制来解决这些视觉推理任务。作者希望 COG 数据集、与之对应的网络架构和相关的基线结果能够为研究时变视觉刺激下的推理提供一个有用的基准。 3.COG 数据集 图 2. 生成综合的 COG 数据集。COG 数据集基于一系列的运算符（A）, 这些运算符被组合以形成各种任务图 ( B )。( C ) 通过在任务图中指定所有运算符的属性来实例化任务。任务实例用于生成图像序列和语义任务指令。( D ) 正向传递图形和图像序列以用于正常任务执行。( E ) 生成一致的、偏差最小化的图像序列需要以反向拓扑顺序向后传递任务图，并且以反向时间顺序向后传递图像序列。 4. 网络 图 3. 本文提出的网络。图像序列被用来作为卷积神经网络 ( 绿色部分) 的输入。英语文本形式的指令被输入到顺序嵌入网络 (红色) 中。视觉短期记忆 ( vSTM ) 网络及时保存视觉空间信息并提供指向输出 ( 蓝绿色 )。vSTM 模块可以被认为是具有外部门控的卷积 LSTM 网络。状态控制器 (蓝色部分) 直接或间接提供所有注意和门控信号。网络的输出是离散的 (语言) 或 2D 连续的 (指向的)。 5. 结果 表 1. CLEVER 上的测试准确率：人类、基线、仅靠训练中的任务指令和像素输入的性能顶尖模型，以及本文提出的模型。（*）代表的是所用的预训练模型。 图 4. 本文提出网络的工作时的思想过程，通过可视化单个 CLEVER 样本的注意力和输出来展示。( A ) 来自 CLEVER 验证集的示例问题和图像。( B ) 每个思考步骤的有效特征注意图。(C) 相关的空间注意力图。(D) 语义注意力。( E ) 排名前 5 的语词输出。红色和蓝色分别表示较强和较弱。在同时特征注意到「小金属球」和空间注意到「位于红色橡胶目标之后」，被关注物体的颜色 (黄色) 反映在语词输出中。在后来的思考过程中，网络特征注意的是「大亚光球」，而正确的答案 (是) 出现在语词输出中。 图 5. 控制变量研究。CLEVER 测试集在不同的模型上的总体准确率； A 和 B 分别是 CLEVER 数据集和 COG 数据集：CLEVR 数据集的相关模型中未包含任何 vSTM 模块。（C）基于输出类型、是否涉及空间推理、操作符的数量以及任务图中的最后一个操作符来分析 COG 的准确率。 图 7. 本文提出的网络可以零样本地推广到新任务。用 44 个任务中的 43 个任务训练了 44 个网络。如图所示是 43 个已训练任务 (灰色) 的最大性能，迁移到一个没有经过训练的任务 (蓝色) 的最大性能，以及在这个任务上的机会水平（红色）。 论文：A dataset and architecture for visual reasoning with a working memory 论文链接：：https://arxiv.org/pdf/1803.06092.pdf 摘要：人工智能中存在一个令人烦恼的问题，就是对复杂的、不断变化的视觉刺激中发生的事件进行推理 (如视频分析或游戏)。受认知心理学和神经科学中丰富的视觉推理和记忆的传统研究所启发，我们开发了一个人工的、可配置的视觉问答数据集 ( COG )，这个数据集可用于人类和动物的实验。尽管 COG 比视频分析的一般问题简单得多，但它解决了许多与视觉、逻辑推理以及记忆相关的问题，这些问题对现代深度学习架构来说仍然具有挑战性。此外，我们还提出了一种深度学习架构，该架构在其他诊断 VQA 数据集 (即 CLEVER) 以及 COG 数据集的简单设置上具有竞争力。但是，COG 的某些设置可以令数据集的学习越来越困难。经过训练，该网络可以零样本地泛化到许多新任务。对在 COG 上训练的网络架构的初步分析表明，该网络以人类可理解的方式完成任务。 "
40,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740213&idx=4&sn=f0b029c6e2e47f80fb1c21fea2bbc94e&chksm=871ad18bb06d589d20bff65c91806b41aec284e3e1ce3ea508a7c16418d376250568cacb44f5&scene=27,CVPR2018 | CMU&谷歌Spotlight论文：超越卷积的视觉推理框架,人类在看到图像时可以进行合理的推理与预测，而目前的神经网络系统却还难以做到。近日，来自卡耐基梅隆大学（CMU）的陈鑫磊（现Facebook 研究科学家）、Abhinav Gupta，谷歌的李佳、李飞飞等人提出了一种新型推理框架，其 探索空间和语义关系的 推理性能大大超过了普通卷积神经网络。目前该工作已被评为 CVPR 2018 大会 Spotlight 论文。 近年来，我们在图像分类 [ 16 ]、检测 [ 37 ] 或分割 [ 3 ] 等标准识别任务方面取得了显著进展，前馈端到端学习的 ConvNet 模型的使用在其中发挥了很大作用。空间和语义上的视觉推理对人类至关重要 [ 1 ]，但我们目前的视觉系统与之不同，除了具有较大接受域的卷积之外，它们都缺乏语境推理。因此，当我们着眼于构建下一代视觉系统时，如何结合空间推理和语义推理就成为一个关键问题。 我们的目标是建立一个系统，该系统不仅可以提取、利用卷积特征的层次结构，还可以通过空间和语义关系来改进其判断。但什么是空间和语义关系？如何使用它们来提高识别能力？请看图 1。图 1 左上角是一个空间推理的实例：如果一行中四分之三的区域是「窗户」，那么第四个区域也可能是「窗户」。语义推理的一个实例（右下图）是识别「校车」，即使图中可见的校车很少或基本看不见——仅仅给出了「公交车」的实例及公交车与校车之间的联系。最后，空间-语义推理可以解释为：识别道路上的「汽车」应有助于识别「汽车」内的「人」。 利用关系进行推理的一个关键方法是迭代地进行预测。最近，已经有人尝试通过自顶向下的模块 [ 38，48 ] 或使用外显记忆 [ 51，32 ] 来整合这种推理。在使用自顶向下的模块时，具有基于类的信息的高层次特征可以与低层次特征结合使用，以提高识别性能。另一种架构是使用外显记忆。例如，Chen 和 Gupta [ 4 ] 展示了序贯对象检测，其中空间存储器用于存储先前检测到的对象，利用 ConvNets 的能力来提取有利于后续检测的密集语境模式。 然而，这些方法存在两个问题: a ) 两种方法都使用卷积堆栈来执行局部像素级推理 [ 11 ]，该方式可能不具备允许更远区域直接传递信息的全局推理能力；b ) 更重要的是，这两种方法都假定训练数据中有足够的关系实例供模型从头开始学习，但是随着类数量的增加，关系呈指数级增长，数据可能出现短缺。许多语义推理需要从很少或近乎为零的实例中学习 [ 14 ]。因此，我们需要设法利用额外的结构化信息进行视觉推理。 本研究提出了一个空间推理和语义推理的通用框架。与目前仅依赖卷积的方法不同，我们的框架还可以从知识库 [ 5，56 ] 形式的结构化信息中学习视觉识别。我们算法的核心由两个模块组成，一个是基于空间记忆 [ 4 ] 的局部模块，利用 ConvNets 进行像素级推理。我们借助并行内存更新来大幅度提高效率。另一个是基于图结构的全局模块，用于在局部区域之外进行推理。 该模块由三个部分组成： 知识图，在该图中，我们将类表示为节点并建立边来编码它们之间不同类型的语义关系； 当前图像的区域图，在该图中，节点代表图像中的区域，边代表这些区域之间的空间关系； 将区域分配给类的分配图。 利用这种结构，我们开发了一个推理模块，专门用来传递该图中的信息。局部模块和全局模块两者迭代地转出并相互交叉馈送预测，以便细化预测。注意，局部推理和全局推理不是孤立的：良好的图像理解通常需要平衡先验学习的背景知识和图像特定观察之间的关系。因此，我们的完整管道通过「关注」[3] 机制连接了两个模块的力量，从而使模型在进行最终预测时能够依赖最相关的特征。 我们的框架展示了大大超越普通 ConvNets 的性能。例如，我们可以在 ADE [ 55 ] 上实现 8.4 % 的绝对提升，这是按每级平均精度衡量的，而通过加深网络仅可以提高约 1 %。 除了用于提供预测的普通 ConvNet 之外，该框架还有两个模块来执行推理：一个是使用空间存储器 Si 的局部模块 ( Sec. 3.1 )，该模块使用另一个 ConvNet C 进行推理；另一个是全局模块 (Sec. 3.2)，该模块将区域和类视为图中的节点，通过在它们之间传递信息进行推理。这两个模块都接收组合的高级和中级特征，在交叉馈送信念的同时迭代地转出 ( Sec. 3.3 )。结合所有预测 fi 与注意 ai 产生最终预测 f(Sec. 3.4)。 在突出显示的蓝色区域可以看出基线预测和我们的模型预测比较的结果。此外还列出了其他区域以提供语境。例如，经过推理后，「右腿」与「左腿」的混淆程度降低（左上图）；尽管分辨率很低，但「桌面」上的「鼠标」被成功判断出来 (上面一行第三张图)；「洗涤剂分配器」在「洗衣机」(右上图) 的语境中被识别出来。在右下角的图中，我们展示了一个失败案例，在该图中，语境没有帮助我们识别出「遥控器」，失败的原因可能是它以前从未出现在「床头柜」上，也没有语义关系可以提供帮助。 论文：Iterative Visual Reasoning Beyond Convolutions 论文链接：https://arxiv.org/abs/1803.11189 摘要： 我们提出了一种新的迭代视觉推理框架。该框架超越了目前只具备卷积堆栈推理能力的识别系统。该框架由两个核心模块组成：一个是局部模块，使用空间记忆以并行更新的方式存储以前的信念；另一个是全局图形推理模块。我们的图模块由三个部分组成: a ) 知识图，在该图中，我们将类表示为节点并建立边来编码它们之间不同类型的语义关系；b ) 当前图像的区域图，在该图中，节点代表图像中的区域，边代表这些区域之间的空间关系；c ) 将区域分配给类的分配图。局部模块和全局模块迭代地转出并相互交叉馈送预测，以便细化预测。通过将两个模块中的最佳部分与注意机制相结合来进行最终预测。与普通 ConvNets 相比，我们的框架性能显著增强，例如，按每级平均精度衡量，我们可以在 ADE 上实现 8.4 % 的绝对改进。分析还表明，该框架对缺失区域具有较强的推理能力。  
41,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740277&idx=1&sn=0ea62fede4dc8e3d04c257bd0d59c1ff&chksm=871ad24bb06d5b5dc7f0fb9ccee7ff6bba03028bfb240ef73e2705aaf27e91e747b4b3775978&scene=27,Caffe2代码全部并入PyTorch：深度学习框架格局剧震,昨日，Caffe2 的 Github 页面突然出现了一个「巨大的改动」：Caffe2 开源代码正式并入 PyTorch，至此，Facebook 主力支持的两大深度学习框架已合二为一。这两大框架，在整个深度学习框架格局中都极受关注。 自 2017 年 1 月发布之后，由于调试、编译等多方面的优势，PyTorch 已经成为很多科研机构首选的深度学习框架；而 2017 年 4 月推出的 Caffe 2 则具有可在 iOS、Android 和树莓派等多种设备上训练和部署模型的优势。尽管获得了很多用户的支持，在面对谷歌支持的 TensorFlow 生态时，PyTorch 和 Caffe 2 各自仍有短板，此次「合并」或许会成为深度学习工程领域新形势的一个开始。 相关链接： Facebook 宣布开源 Caffe2：可在手机与树莓派上训练和部署模型 如果你紧跟 PyTorch 的开发进程，那么你可能会注意到过去几个月这个库有一些改变： PyTorch 和 Caffe2 目前会共享 CI，这是非常重要的工程工作。 PyTorch 和 PyTorch-ONNX 有非常复杂的 CI，onnxbot 触发器构建在每一个 PyTorch PR 上，并以 roundabout 的方式更新。 在「pending」状态中有后端研发工作，例如与最新和最重要的库集成（MKLDNN、cuFFT 和更多的 NNPACK 覆盖等）。 作为 PyTorch 和 Caffe2 框架的主要维护者，共享二者通用的工程性内容也就很合理了，例如算子库。 然而，在两个单独的 Github repos 上共享代码很有挑战性（不可去掉的子模块或者子树，Continuous Intergration 变得很难等）。 在协作下，我们把 Caffe2 repo 并入到了 PyTorch 的 github。也就是，如果你用命令 git clone https://github.com/pytorch/pytorch，你可以看到 caffe2 的二进制文件。 作为 PyTorch 用户，你需要知道：并没改变什么，PyTorch 的安装、搭载、使用和往常一样。 其实这并不会意味着我们的代码会失效，这只是开发和后端工程工作。如何你并不是 core-developer，这个问题甚至不会与你有任何关系。此外对于用户来说，我们同样也并不需要关注 protobuf 问题。 关于此问题，目前任 Facebook 研究科学家贾扬清在知乎上表示： 来简单答一下：因为 PyTorch 有优秀的前端，Caffe2 有优秀的后端，整合起来以后可以进一步最大化开发者的效率。目前 FAIR 大概有超过一半的项目在使用 PyTorch，而产品线全线在使用 Caffe2，所以两边都有很强的动力来整合优势。 开发效率是我在 Facebook 非常重视的一个方向：去年年中的时候启动了 ONNX 项目（初版的代码是我亲自上手写的），然后帮助搭建了 ONNX team，来增强不同框架甚至不同公司之间的协作；Caffe2 和 PyTorch 在代码层的合并也是从那个时候开始逐渐推动的一项内容。 至于进一步的计划，目前我还不方便透露，等过一个月有空再来更新吧。 
42,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740185&idx=4&sn=260d473e570d957aa240f80c0838068f&chksm=871ad1a7b06d58b1132da8361d9681aa8a9a46b0e65cb4436d3f523fcb6579fddfd199d3e6ab&scene=27,CVPR 2018 | 哈工大提出STRCF：克服遮挡和大幅形变的实时视觉追踪算法,"arXiv 视觉追踪在多样本的历史追踪结果中学习时，可能遭遇过拟合问题，并在有遮挡的情况下导致追踪失败。为此，哈尔滨工业大学在本文中提出了 STRCF。通过引入时间正则化，STRCF 可以在有遮挡情况下来成功追踪目标，同时能够很好地适应较大的外观变化。该模型在准确率、鲁棒性和速度方面都表现良好，可实时追踪目标。 最近几年我们见证了判别相关滤波器（DCF）在视觉追踪领域的飞速进展。利用训练样本的周期性假设，通过快速傅立叶变换 ( FFT ) 可以在频域中非常高效地学习 DCF。例如，最早的基于 DCF 的追踪器 (即 MOSSE[4] 的追踪速度可以达到每秒 700 帧 ( FPS )。随着特征表示 [ 14,28]、非线性核 [ 19]、尺度估计 [ 11,23,24]、最大边缘分类器 [43]、空间正则化 [ 13,18] 以及连续卷积 [5] 的引入，基于 DCF 的追踪算法得到了显著的改进，极大地提高了追踪准确率。然而，这种性能改进也带来了额外成本。大多数排名靠前的追踪器，例如 SRDCF [13] 和 C-COT [15]，已经逐渐失去早期的基于 DCF 追踪器的特征速度和实时追踪能力。例如，使用人工设计的 HOG 特征的 SRDCF [13] 的速度为大约 6 FPS，而基线 KCF [19] 的速度大约是 170 FPS。 为了更好地理解这个问题，本文剖析了 SRDCF 中准确率和速度之间的权衡。一般而言，SRDCF 的低效率可归因于三个因素: ( i ) 尺度估计；( ii ) 空间正则化；以及 ( iii ) 大规模训练集形式。图 1b 列出了 SRDCF 及其变体在两个流行基准上的追踪速度和准确率，其中包括 SRDCF (—M ) (即去除了 ( iii ) )、SRDCF (—MS ) (即去除 ( ii ) &和 ( iii ) )，以及 KCF (即去除 ( i) 、( ii ) 和 ( iii ) )。作者注意到，在去除 ( iii ) 时，可以采用线性插值 [ 4,11 ] 作为在线模型更新的替代策略。从图 1(b) 中可以看出，当添加尺度估计时，追踪器仍然保持实时能力 (约 33FPS )。但随着空间正则化和大规模训练集形式的进一步引入，追踪速度明显下降。因此，开发一种使用 ( ii ) 和 ( iii ) 的解决方案而不损失效率才是有价值的。 本文研究了在不损失效率的情况下，利用空间正则化和大型训练集形式的优点的方法。一方面，SRDCF 的高复杂度主要来源于对多幅图像的训练形式。通过去除约束条件，单图像样本上的 SRDCF 可以通过 ADMM 有效地解决。由于 SRDCF 的凸性，ADMM 也能保证收敛到全局最优。另一方面，在 SRDCF 算法中，将空间正则化集成到多幅图像的训练形式中，实现了 DCF 学习与模型更新的耦合，提高了追踪准确率。在在线被动攻击 ( PA ) 学习 [ 6] 的启发下，作者将时间正则化方法引入到单图像 SRDCF 中，得到了时空正则化相关滤波器 ( STRCF )。STRCF 是多训练图像上 SRDCF 形式的合理近似，也可用于同时进行 DCF 学习和模型更新。此外，ADMM 算法也可以直接用于求解 STRCF。因此，本文提出的 STRCF 将空间正则化和时间正则化结合到 DCF 中，可以用来加速 SRDCF。 此外，作为在线 PA 算法 [6] 的扩展，STRCF 还可以在外观大幅变化的情况下实现比 SRDCF 更鲁棒的外观建模。图 1（a）展示了对具有遮挡和变形的两个序列的追踪结果。与 SRDCF 相比，引入时间正则化后的 STRCF 对遮挡具有更强的鲁棒性，同时能够很好地适应较大的外观变化。 从图 1(b) 中可以看出，STRCF 不仅以实时追踪速度 ( 约 30FPS ) 运行，而且通过在两个数据集上的平均 OP，其性能比 SRDCF 提高了 5.7 %。综上所述，STRCF 在所有数据集上均比基线 SRDCF 有显著改进，追踪速度提高了 5 倍以上。 作者在几个基准上进行了比较实验，包括 OTB-2015 [40]、Temple-Color[25]、VOT-2016 [22]。与最先进的基于 CF（相关滤波器）和 CNN 追踪器相比，STRCF 在准确率、鲁棒性和速度方面都表现良好。 这篇论文的主要贡献如下： 通过将空间和时间正则化纳入 DCF 框架，提出了 STRCF 模型。基于在线 PA 的 STRCF 不仅可以合理地逼近多幅训练图像上的 SRDCF 形式，而且在较大的外观变化情况下比 SRDCF 具有更强的鲁棒性。 为高效求解 STRCF，开发了一种 ADMM 算法，其中每个子问题都有封闭形式的解。并且本文提出的算法可以在非常少的迭代中经验地收敛。 本文提出的 STRCF 具有人工设计的特征，可以实时运行，相比 SRDCF 在准确率上有了显著的提升。此外，STRCF 与最先进的追踪器 [9，15] 相比，性能良好。 论文：Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking（学习用于视觉追踪的空间-时间正则化相关滤波器） 论文链接：https://arxiv.org/abs/1803.08679 判别相关滤波器 ( DCF ) 在视觉追踪中是很高效的，但是会受到边界效应的影响。空间正则化 DCF ( SRDCF ) 通过对 DCF 系数施加空间惩罚来解决这一问题，在提高了追踪性能的同时不可避免地增加了复杂度。为了解决在线更新问题，SRDCF 在多幅训练图像上建立模型，进一步增加了提高效率的难度。本文将时间正则化方法引入到单样本 SRDCF 中，提出了一种时空正则化相关滤波器 ( STRCF )。在在线被动攻击 ( PA ) 算法的启发下，我们将时间正则化引入到单样本 SRDCF 中，得到了时空正则化相关滤波器 ( STRCF )。STRCF 形式不仅可以合理地逼近多训练样本的 SRDCF，而且在大的外观变化情况下比 SRDCF 具有更强的鲁棒性。此外，它可以通过乘数的交替方向法 ( ADMM ) 有效地求解。通过结合时间和空间正则化，我们的 STRCF 可以处理边界效应，同时不损失效率，并且在准确率和速度上优于 SRDCF。实验在三个基准数据集上进行: OTB-2015、Temple-Color 和 VOT-2016。与 SRDCF 相比，STRCF 采用人工设计的特征，速度提高了 5 倍，OTB-2015 和 Temple-Color 的 AUC 分数分别提高了 5.4 % 和 3.6 %。此外，与 CNN 特征相结合的 STRCF 与基于 CNN 的最先进追踪器相比，性能良好，OTB-2015 的 AUC 得分为 68.3 %。 "
43,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740315&idx=4&sn=bd4fd24138b09f0239f50e0f8e3a0b2d&chksm=871ad225b06d5b339b00188689e47f599817468b63e102a09b8d35308e9638a486c2c7d9ec2d&scene=27,学界 | 神经网络quine：自我复制 + 解决辅助任务,"近日，哥伦比亚大学的研究者发布论文，从生命的角度看待人工智能的发展。论文提出一种不寻常的自我复制神经网络，能够实现自我复制，还可以解决辅助任务，本论文使用 MNIST 图像分类作为辅助任务。 生物的生命是从第一次自我复制开始的（Marshall, 2011），而自然选择倾向于支持在复制方面更有优势的生物体，从而形成了自我完善机制。类似地，如果 AI 智能体能够在没有额外机器的情况下进行自我复制和完善，我们就可以通过自然选择构建一个人工智能的自我完善机制。 在发现 DNA 作为生物复制的物理机制之前，人工自我复制机器的概念最早是由 John von Neumann 在 20 世纪 40 年代提出的。具体来说，Von Neumann 展示了细胞自动机的初始状态和变换规则的配置，它在运行固定步数后产生初始细胞状态的副本（Von Neumann & Burks，1966）。之后 Hofstadter（1980）在《Godel，Escher，Bach: an Eternal Golden Braid》一书中以哲学家 Willard Van Orman Quine 的名字创造了术语「quine」，用来描述自我复制的表达式，如：「是句子片段」是句子片段（『is a sentence fragment』 is a sentence fragment）。 在制作物理自我复制器方面也有很多研究。值得注意的例子有 tile（Penrose，1959）、分子（Wang et al.，2011）、聚合物（Breivik，2001）和机器人（Zykov et al.，2005）。 在编程语言理论的背景下，quine 是打印自己源代码的计算机程序。一个简单的 quine 示例是空字符串，在大多数语言中，编译器会转换为空字符串。下面的代码片段是一个不寻常的两行 Python quine 示例。 虽然自我复制已经在许多自动机中被研究过，但在神经网络的研究中明显缺乏，尽管神经网络似乎是迄今为止已知的最强大的人工智能形式。 本论文识别并尝试解决构建和训练自我复制神经网络时所面临的挑战。具体来说，研究者建议将神经网络视为由一系列张量运算组成的可微计算机程序。研究目标是构建一个输出自己权重的神经网络 quine。 研究者使用三种不同方法对论文提出的方案进行测试：基于梯度的优化方法、基于非梯度的优化方法和一个叫作再生（regeneration）的新方法。研究者进一步设计了一个神经网络 quine，除了自我复制的工作外它还有一个辅助目标。本论文选择的辅助任务是 MNIST 图像分类（LeCun & Cortes，1998），任务涉及将从 0 到 9 的数字图像进行分类，它通常用作机器学习的「hello world」示例。 研究者观察到网络在自我复制能力与解决辅助任务能力之间有一个权衡。这与在自然界中观察到的繁殖与其他辅助生存任务之间的平衡类似。这两个目标通常是一致的，但是比如当一只动物处于饥饿状态时，其性激素通常会下调，以牺牲繁殖为代价优化生存。反之亦然：例如，雄性深色捕鱼蛛的交配行为导致其血压发生突然的、不可逆转的变化，这使其无法移动并容易被雌性蜘蛛吃掉（Drake，2013）。 神经网络如何引用它自己？ 直接引用（Direct Reference）问 题 ：神经网络通过一组参数 Θ 进行参数化，研究者的目标是构建一个输出为 Θ 本身的网络，这很难直接做到。假设前馈网络的最后一层有 A 输入和 B 输出。线性变换中的权重矩阵的大小是积 AB，当 A> 1 时，它大于 B。 研究者还研究了两种流行的图像生成模型的开源实现：DCGAN（Radford et al.，2016）和 DRAW（Gregor et al.，2015）。二者分别使用 1200 万和 100 万个参数来生成具备 784 个像素的 MNIST 图像。 通常，参数集 Θ 比输出的大小要大得多。为了规避这一点，我们需要一种间接的方式来引用 Θ。 间接引用（Indirect Reference） ：HyperNEAT（Stanley et al.，2009）是一种神经进化方法，通过用坐标和权重识别每个拓扑连接来描述神经网络。研究者在建立 quine 时采取了同样的策略。研究者没有让 quine 直接输出其权重，而是设置它输入一个坐标（以 one-hot 编码的方式），输出该坐标的权重。 这克服了 Θ 大于输出的问题，因为我们仅对每个坐标 c 输出标量 Θ_c。 Vanilla Quine 研究者将 vanilla quine 定义为一个前馈神经网络，其唯一的工作是输出自身的权重。 假设权重的数量是 A，第一个隐藏层的单元数量是 B，那么投影矩阵的大小将是积 AB，当 B> 1 时，它大于 A。因此，由于 one-hot 表征，我们不能使投影本身成为网络的参数。因此，研究者决定使用固定的随机投影将坐标的 one-hot 编码和隐藏层连接起来。所有其他连接，即隐藏层之间的连接以及最后一个隐藏层和输出层之间的连接，都是神经网络的可变参数。 Von Neumann 认为一个不平凡的自我复制器必然包含三个组件，这些组件自身不足以成为自我复制器：（1）复制器的描述，（2）可以克隆描述的复制机制，（3）可以将复制机制嵌入复制器本身的机制（Von Neumann & Burks，1966）。在这种情况下，为每个权重分配 one-hot 空间中的一个点的坐标系对应（1）；由神经网络计算的函数对应（2）；固定随机投影对应（3）。下文将解释研究者选择（1）、（2）、（3）的原因，同时牢记这些选择的替代方案会是未来有趣的研究方向。 辅助 Quine 研究者将辅助 quine（auxiliary quine）定义为一个除了自我复制之外还解决一些辅助任务的 vanilla quine。它负责接收辅助输入并返回辅助输出。 本论文选择图像分类作为辅助任务。MNIST 数据集（LeCun & Cortes，1998）包含从 0 到 9 手写数字的正方形图像（28×28 像素）。这些图像将作为辅助输入。将从辅助输入到网络的连接作为参数而不是随机投影是可能的，但本文只报告后者的结果。 辅助输出是十个类别的概率分布，其中具有最大概率的类别将被选择为预测分类。用 60000 张图像进行训练，10000 张图像进行测试；不需要验证集，因为本研究并不是试图对分类器的性能进行严格的优化。本研究的主要目的是证明神经网络 quine 的概念，因此 MNIST 是合适的辅助任务（它被认为是现代机器学习算法中的一个简单问题）。 论文：Neural Network Quine 论文链接：https://arxiv.org/abs/1803.05859 摘要： 自我复制是生物生命的一个重要方面，而这在人工智能系统中很大程度上被忽视了。本论文介绍如何构建和训练自我复制的神经网络。网络通过学习输出自己的权重来复制自己。该网络使用一个损失函数，可以用基于梯度或非梯度的方法进行优化。我们还描述了一种我们称之为再生（regeneration）的方法，它在没有明确优化的情况下通过向网络注入其自身参数的预测来训练网络。自我复制网络的最佳解决方案是在再生和优化步骤之间交替发现的。最后，我们介绍了一种可以解决 MNIST 图像分类等辅助任务的自我复制神经网络设计。我们观察到网络的图像分类能力与其复制能力之间存在权衡，但训练偏向于以复制为代价增加其在图像分类方面的专业度。这与自然界中观察到的繁殖和其他任务之间的平衡类似。我们认为人工智能的自我复制机制是有用的，因为它带来了通过自然选择持续改进的可能性。 "
44,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740213&idx=2&sn=43e07e08d3f6ac5cbd8ef79218df7667&chksm=871ad18bb06d589dfef295be1a3ac7a6d1423f1df281a2fb153a5dbbd73abf7ba0dc7f2294e5&scene=27,资源 | 深度学习自动前端开发：从草图到HTML只需5秒（附代码）,在人们的不断探索下，「使用人工智能自动生成网页」的方法已经变得越来越接近实用化了。本文介绍的这种名为 SketchCode 的卷积神经网络能够把网站图形用户界面的设计草图直接转译成代码行，为前端开发者们分担部分设计流程。目前，该模型在训练后的 BLEU 得分已达 0.76。 你可以在 GitHub 上找到这个项目的代码：https://github.com/ashnkumar/sketch-code 为用户创造直观、富有吸引力的网站是各家公司的重要目标，而且这是个快速进行原型、设计、用户测试循环的过程。像 Facebook 这样的大公司有着让整个团队专注于设计流程的人力，改动可能需要几周的时间，并涉及到多种利益相关者；而小型企业就没有这样的资源，因此其用户界面可能受到一些影响。 我在 Insight 当中的目标是使用现代深度学习算法大大简化设计工作流程，并使得任何规模的公司都能快速地创造并测试网页。 现有的设计工作流程 现有工作流程涉及多个利益相关者 一个典型的设计工作流程如下所示： 产品经理进行用户研究，从而制定技术参数表 设计人员将接受这些要求并尝试创建低保真原型，最终创建高保真原型 工程师将这些设计转化为代码并最终将产品交付给用户 开发周期的时间长度很快就会变成瓶颈，像 Airbnb 这样的公司已经开始使用机器学习来提高这个过程的效率了。（参见：https://airbnb.design/sketching-interfaces/） 虽然这种工具很有希望成为机器辅助设计的例子，但是尚不清楚这种模型在端到端的情况下能完全训练到什么程度，也不清楚它在多大程度上依赖于手工制作的图像特征。这肯定是无法知道的，因为它目前还是 Airbnb 专有的非开源方案。我想创造一个「从绘图到代码」技术的开源版本，可供更多开发者和设计者使用。 理想情况下，我的模型可以采用简单的网站设计手绘原型，并立即从该图像生成一个可用的 HTML 网站： 实际上，上面的例子是一个从我模型测试集图像生成的实际网站！你可以在我的 Github 页面中查看它：https://github.com/ashnkumar/sketch-code 从图像标注中获取灵感 我正在解决的问题属于程序综合（https://en.wikipedia.org/wiki/Program_synthesis）这个广义任务范畴，即工作源代码的自动生成。尽管很多程序综合能处理从自然语言要求或执行轨迹所生成的代码，但在我这个案例中，我可以从一个源图像（手绘线框图）开始，自动获得想要的代码。 机器学习领域中，有一个名为图像字幕生成的领域（https://cs.stanford.edu/people/karpathy/deepimagesent/），该领域有着充分的研究，旨在学习将图像和文本相连的模型，特别是生成关于源图片内容的描述。 我从最近一篇名为   的论文和 Emil Wallner 使用该方法的一个相关项目获得了灵感（参见： ），并决定将我的任务重构成图像字幕生成问题的一部分，即将线框图作为输入图像，将对应的 HTML 代码作为输出文本。 获取正确的数据集 考虑到图像标注的方法，我心中理想的训练数据集是成千上万对手绘线框图和它们 HTML 代码的等价物。不出所料，我无法找到这种数据集，因此我不得不为该任务创建自己的数据。 我从 pix2code 论文中提到的一个开源数据集（https://github.com/tonybeltramelli/pix2code）入手，它由 1750 张人工生成的网页截图和其对应源代码构成。 这个数据集对我而言是个很好的开始，其中有一些有趣的地方： 数据集中每个生成的网站都包含几个简单的 Bootstrap 元素例如按钮、文本框和 DIV。虽然这意味着我的模型将会因把这几个元素作为「词汇」（模型可选择用于生成网站的元素）而受限制，这种方法应该很容易推广到更大的元素词汇表中。 每个示例的源代码包含领域专用语言（DSL）的标记，这些符号是由论文作者创建的。每个标记对应于 HTML 和 CSS 的片段，且有一个编译器将 DSL 转化为工作使用的 HTML 代码。 让图片更「手绘化」 为了调整数据集以适应我的任务，我得把网站的图片弄得像是手绘的。对图片的手绘化都得益于 OpenCV 和 PIL library 的灰度转换和轮廓检测功能。 最终，我决定直接通过一系列操作来直接修改原网站的 CSS 样式表： 通过改变页面元素的边框半径实现按钮和 div 的圆润化 调整边框的粗细以模仿手绘素描，并添加阴影 将字体改为类手写字体 我的最终版本又增加了一个步骤，通过加入倾斜，偏移和旋转来进行数据增强，以模仿实际绘制的素描的不确定性。 使用图像标注模型架构 现在我已经准备好我的数据了，我可以把它输入模型进行训练了！ 我用的这个用于图像标注的模型包括三个主要部分： 一个卷积神经网路（CNN）视觉模型用于提取源图片特征 一种由编码源代码标记序列的门控循环单元（GRU）组成的语言模型 一个解码器模型（也是一个 GRU），它以前两个步的输出作为输入，预测序列中的下一个标记 为了训练这个模型，我把源代码分成标记序列。其中一个序列及其源图像是模型的单个输入，其标签是文档中的下一个标记。该模型使用交叉熵成本（cross-entropy cost）作为其损失函数，将模型预测的下一个标记与实际的标记进行比较。 在模型从头开始生成代码的推理阶段，该过程稍有不同。该图像仍然通过 CNN 网络进行处理，但文本处理仅提供一个开始序列。在每一步中，模型对序列中下一个标记的预测将返回到当前输入序列，同时作为新的输入序列输入到模型中。重复此操作直到模型预测出 <END> 标记或进程达到每个文档的标记数的预定义上限。 一旦从模型中生成了一组预测标记，编译器就会将 DSL 标记转换为 HTML，这些 HTML 可以在任何浏览器中展示出来。 用 BLEU 得分评估模型 我决定用 BLEU 评分（https://machinelearningmastery.com/calculate-bleu-score-for-text-python/）来评估模型。这是机器翻译任务中经常会用到的评估标准，它试图在给定相同输入的情况下，评估机器生成的文本与人类可能写的文本的近似程度。 实质上，BLEU 通过比较生成文本和参考文本的 n-元 序列，生成精修改后的文本。它非常适合这个项目，因为它会影响生成的 HTML 中的实际元素，以及它们之间的相互关系。 然后这是最棒的——我完全可以通过检查生成的网站来理解 BLEU 得分！ 一个完美的 1.0 的 BLEU 分数将在正确的位置生成源图像的正确元素，而较低的得分可以预测错误的元素和/或将它们放在相对于彼此错误的位置。最终我的模型能够在测试集上得到 0.76 的 BLEU 分数。 福利 - 定制样式 我觉察到的一个额外福利是，由于模型只生成页面的骨架（文档的标记），我可以在编译过程中添加一个自定义的 CSS 层，并且可以即时看到网站的不同风格。 将样式与模型生成过程分离，给使用模型带来了很多好处： 想要将 SketchCode 模型应用到自己公司产品中的前端工程师可以按原样使用该模型，只需更改一个 CSS 文件以符合其公司的样式要求 可扩展性已内置 - 使用一张源图像，模型输出可立即编译为 5、10 或 50 种不同的预定义样式，因此用户可以看到他们网站的多个版本，并在浏览器中浏览这些网站 总结与展望 通过利用图像标注的研究成果，SketchCode 能够在几秒钟内将手绘网站线框图转换为可用的 HTML 网站。 该模型有些局限性，大概包括以下几点： 由于这个模型是用一个只有 16 个元素的词汇进行训练的，它不能预测训练数据之外的标记。下一步可能是使用更多元素（如图像，下拉菜单和表单）生成其他样例网站——Bootstrap components 是个练手的好网站：https://getbootstrap.com/docs/4.0/components/buttons/ 实际生产环境中，网站有很多变化。创建一个更能反映这种变化的训练数据集的好方法是去爬取实际的网站，捕获他们的 HTML / CSS 代码以及网站内容的截图 手绘素描也有很多变化，CSS 修改技巧没有被模型完全学会。在手绘素描上生成更多变化的一种好方法是使用生成对抗网络来创建逼真的绘制网站图像 我很期待看到项目的进一步发展！  原文链接：https://blog.insightdatascience.com/automated-front-end-development-using-deep-learning-3169dd086e82 
45,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740213&idx=3&sn=af2d9dd02ec06a1a789847868ebcfa31&chksm=871ad18bb06d589ddc41c28a7468f7632af7bd3e9d59396ff60332305dd69dc8c6a5ece5877f&scene=27,观点 | 善于单挑却难以协作，构建多智能体AI系统为何如此之难？,"自 人工智能已经在 、 和 等领域达到或超越了人类专家的水平，但今天看来，智能化的机器还离我们很远。要想实现通用智能，AI 智能体必须学习如何在共享环境中与「他人」进行互动：这就是多智能体强化学习面临的挑战。 本文将通过地图寻宝问题为例，向你简要介绍多智能体系统实施时的困难程度及其原因。 「研究人工智能三十五年来的主要经验是：困难的问题是易解的，简单的问题是难解的。」Pinker (1994)，《The Language Instinct》 我之前觉得编写一个软件智能体来收集图上的宝藏是件简单的小事。但是我完全错了。编写出不愚蠢行动的智能体实际上非常困难。 「智能体是指任何通过传感器感知环境、通过效应器作用于环境的事物。」Stuart Russell&Peter Norvig，《Artificial Intelligence: A Modern Approach》 这是一个简单的多智能体问题。让 n 个智能体在完全连接的图上移动并收集宝藏。智能体的行动、感知和沟通受到限制，它们只能观察并移动到与它们直接相连的节点，只能与足够接近的智能体进行通信。 有三种智能体：探险者、收集者和无限背包（Infinite-backpack）智能体。探险者注定要探索地图，因为它们不允许收集宝藏。收集者才可以收集，但它们不能携带太多，并且必须将它们收集的宝藏分发给无限背包智能体。 智能体的感知有限，但可以记住过去的观察结果。每个智能体都有自己的世界表征形式、自己的图（真实图的子图）。智能体的子图是它们访问过的所有节点的记忆，以及它们曾经见过或走过的边。它们必须将此图传达给其他智能体，以便它们都可以共享来自所有子图的修复。 JADE（Java Agent DEvelopement Framework）将用于实现所谓的「行为」（教程地址：http://jade.tilab.com/doc/tutorials/JADEProgramming-Tutorial-for-beginners.pdf、https://gitlab.com/herpsonc/startJade）。在这个多智能体系统框架中，行为是智能体将要执行的一组指令。在每一轮中，每个智能体都按顺序执行每个行为。 你的目标：实现智能体的行为，使之在一定时间内收集尽可能多的宝藏。 看起来很简单，是吧？ （注意：该项目是《多智能体系统简介》（ANDROIDE 的一门课程）的一部分。灵感来自于生存恐怖游戏 Hunt The Wumpus，在该项目的完整版本中，智能体需要处理四处游荡的、可怕的 Wumpus）。 想象两个智能体在长廊中朝相反方向移动。图的每个节点上只能有一个智能体，所以它们必须协调行为以避免阻碍别人。考虑到这种情况，我们必须实施一个特定的协议。 协调 智能体的感知有限，并且有不同的能力。因此，合作是必不可少的。发生冲突时，必须应用解除该情况的协议。它们必须分享自己的子图，看看谁更接近高度连接的节点，并就谁来移动达成一致。 探险者智能体必须同意谁来探索未知图形的哪个部分，以优化它们的移动并防止冲突。 信息交换 在多智能体设置中交换信息以便让每个智能体都能获取全局知识的过程被称为 gossip problem。 例如，假设集合 {1,2, … ,n} 中的每个智能体都知道一部分信息，称为一个秘密。然后，一个非常简单的协议是让智能体 1 呼叫 2、3、......、n，并了解它们的秘密。当 1 知道所有的秘密时，它会呼叫给 2、......、n，告诉它们这些秘密，这样每个智能体都知道所有的秘密了。总共有 n-1+n-1=2n-2 次呼叫。实际上，最佳解决方案需要 2n-4 次呼叫，这与我们的简单算法非常接近。 然而，在我们的问题中，直到所有节点都被探索时才能知晓完整信息，这使得算法稍微复杂一些，因为总的知识是动态的（智能体探索的图越多，它们的知识总量越多）。 这时就出现了优化妥协（optimization compromise）。为了让全部智能体知道所有秘密，这 n 个智能体之间必须交换消息的最佳数量是多少？更多信息意味着更好的全局知识和更好的协调。然而，由于有数千个智能体和数百万个节点，每毫秒发送数千条消息的成本远远无法忽略，成为一个计算负担。 异步通信 智能体之间的通信是异步的。由于智能体的执行是分布式的，所以没有全局时钟来同步智能体的动作。此外，在交换信息时，每个智能体都有一个邮箱，其中包含来自其他智能体的邮件，所以通信可能会延迟。在延迟期间，一个智能体可能会移动很远，并且永远不会回复原始信息。 联盟形成 某些目标不能单独实现（比如抬起重物）。因此，智能体可能会同意组成一组智能体，称为联盟（coalition），来实现共同目标。 有了具备三种不同必要技能（探索、收集和积累）的智能体，一个至少包括三个智能体的联盟才会形成。因此，必须实施创建和更新联盟的协议。可以使用 Shapley 值（由智能体联盟创造的剩余额）来确定哪些联盟是最有价值的。 即使是在简单的问题设置中，有几个障碍出现得非常快，算法的复杂性似乎是无法克服的。当尝试构建行为类似人类的 AI 算法时，这是一个反复出现的现象。 「要让计算机如成人般地做智力测试或下棋是相对容易的，但是要让它们有如一岁小孩般的感知和行动能力却是相当困难甚至是不可能的。」Moravec（1988），《Mind Children》 如果我们用人类替代智能体，我相信他们很快就会明白如何在这个游戏中取胜，他们会传达他们在图中所看到的信息，并形成联盟来收集最多的宝藏。然而，对智能体实施严格的行为准则却是非常困难的。 莫拉维克悖论： 对人类来说容易的事对机器来说却难以置信地困难。 说到下象棋，AI 表现出了超人类的水平。但是对于基本的人类行为，例如行走或协调行动来探索地图，人工智能算法却出奇地困难。 国际象棋大师加里·卡斯帕罗夫曾在《Deep Thinking》中写道：任何足够先进的算法都不难在同时进行的比赛中击败 20 名顶级棋手。但是没有 AI（机器人）可以在拥挤的酒吧中四处走动和自行移动棋子。 机器学习在非常特定的情况下工作 但是你可能会问为什么我们不使用最新的机器学习（ML）算法来解决我们的问题呢？......ML-only 算法只能被用于特定的任务。 是的，强化学习（RL）算法非常流行，可以解决超难的问题，例如在  或围棋中展现出超人类的水平。但是这些游戏都是具备小数据输入的全可视性游戏，这与我们的寻宝问题并不相同，因为地图在开始时并不完全可见。 但是，OpenAI 不是在多智能体系统上，用机器学习算法在 Dota 2 的 5 vs 5 中战胜了人类吗？你可能会问。 是的，当在  时，OpenAI 展现了令人印象深刻的结果。但是这主要是因为它们强大的计算能力，并不是人工智能的突破。 它们的目标是利用一个包含 580 万场比赛的数据集在 5 vs 5 比赛中获胜。所以，它们似乎正在使用完全机器学习方法（从人类游戏中学习）研究多智能体问题，并且似乎缺少多智能体系统的自上而下方法。 智能体不会推断和概括。纯机器学习可用于单个智能体或完全可观察的系统，但是多智能体系统不是一个完全已知的世界，必须采用一个更普遍的方法。 我们不知道如何实现可扩展行为 在只有两个智能体在走廊上朝相反方向走的时候，我们遇到了一个问题。实施协议来处理这一特定问题是可能的。 但是如果是 100 个智能体在具备 400 个节点的地图上呢？ 少数智能体的硬编码功能与多智能体系统的可扩展和可泛化实现之间存在差距。 需要做什么 经过研究，必须开发特定的多智能体协议来解决这类问题。没有先验知识的学习不会教授智能体如何沟通，因为搜索空间太大。纯数据驱动的方法不会带来任何结果。 实现一个解决寻宝问题的算法比看起来要困难得多。构思能够解决简单问题的多智能体系统绝非易事。机器学习算法在过去十年中取得了巨大成果，但仅凭机器学习无法解决所有的人工智能问题。  "
46,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740213&idx=1&sn=0c9b2be6c64f87386789238f640cf6f8&chksm=871ad18bb06d589da067703fcf0ad9ea4b43c9367f5d8a74ef78035cbef5d3a59382acbdeb77&scene=27,登上《Cell》封面的AI医疗影像诊断系统：机器之心专访UCSD张康教授,"   人工智能（AI）有可能通过帮助人类医疗专家进行高难度分类、快速分析大量医疗图像的方式彻底改变疾病的诊断和治疗流程。近日，由加州大学圣地亚哥分校张康教授等人提出的深度学习诊断方式让我们提前看到了未来。 2018 年 2 月 22 日出版的《Cell》封面文章介绍了由加州大学圣地亚哥分校（University of California, San Diego）张康教授主导的研究成果：一种基于迁移学习，能够精确诊断致盲性视网膜疾病与肺炎的人工智能工具。该方法的表现与专业医生能力相当，并可以在 30 秒内确定患者是否应接受治疗，准确度高达 95％以上；在区分病毒性肺炎和细菌性肺炎上，新方法的准确率也超过了 90％。同时，该研究也通过显示神经网络激活区域的方法向人们提供了机器诊断的可解释性。作为中国学者主导的又一项重要研究，该文章发表以后受到了人们的广泛关注。 论文链接：http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5 该研究的主要推动者张康，是加州大学圣地亚哥分校眼科教授、眼科遗传学主任，中国第三批 「 千人计划 」 入选者，四川大学客座教授。他曾在四川大学获得生物化学学士学位，哈佛大学医学博士学位（Magna Cum Laude 荣誉），麻省理工学院（MIT）联合医学博士学位及哈佛大学遗传学博士学位。张康在约翰霍普金斯大学 Wilmer 眼科研究所完成了眼科住院医生实习期，并曾在犹他大学完成视网膜手术专科训练。 张康教授的临床和研究重点是重大疾病的基础和转化研究，寻找新的基因靶标和治疗方法。 他曾在许多著名学术期刊上发表或共同撰写了超过 200 篇同行评议论文，其中涵盖遗传学、分子生物学、干细胞、肿瘤液体活检、3D 打印及组织工程、人工智能和临床试验等多个领域。其中关于 HTRA1 基因是黄斑变性的主要易感基因的《Science》文章曾被这家期刊评为 2006 年世界科学十大进展之一。 在《Cell》上的文章发表之后，机器之心对张康教授进行了专访，我们与他对迁移学习、跨学科研究、AI 在医疗领域应用等问题进行了交流。   机器之心：发表在《Cell》上的论文《Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning》，该研究是从何时开始启动的？ 张康： 我很早就有这个想法了，真正启动是在去年（2017 年）初。 机器之心：作为眼科教授，使用计算机科学领域中也是刚刚发展起来的机器学习工具进行研究，会遇到哪些困难？ 张康： 生物医疗科学和计算机科学是两个完全不同的领域。由于计算机科学的高速发展，我们面临的首要问题就是学习并结合这些最近开发的深度学习技术，以确保我们的研究对当前和未来的计算机视觉研究及应用是相关和有用的。深度学习引入可取代之前技术的新方法而改变了计算机视觉领域。然而，这个项目的主要挑战是获得大量的 OCT 图像，并组织一个庞大而结构化的视网膜专家体系，以确保尽可能准确地标记大数据，另外我们必须组织一个优秀人工智能小组。 机器之心：新研究的图像识别模型对计算机的算力要求有多高？ 张康： 该模型在 4 个 GTX 1080 8GB GPU 中进行了训练和测试。但是，由于该模型是使用预训练权重进行训练的，从而使训练时间比训练空白神经网络要少得多。因此，还可以在合理的时间内使用更小的 GPU 甚至多个 CPU 来完成此模型的训练和推理。 机器之心：神经网络的推理是一个「黑箱」，你们提出的新方法是如何解释计算机作出「诊断」的依据的？ 张康： 我们在视网膜 OCT 图像的研究中加入了「遮挡测试」——通过卷积一个遮挡核心到输入图像上，机器会通过计算预测做出正确诊断最可能的部位，并输出含有高亮色块的「遮挡」图，这些色块就是 AI「认为」的病变部位，得出直观的为临床医生信任的诊断依据。 机器之心：神经网络在识别医疗图像时相比人类医生具有哪些优势？ 张康： 首先，通过输入大量的数据，神经网络可以获得远超过人类医生的「经验」，计算出超越人类的准确结果，在我们的系统中，我们使用超过 20 万张医学图像，通过不同的疾病分类，最终使用近 11 万张视网膜 OCT 图像训练机器。在眼病方面，能在 30 秒内正确鉴别脉络膜新生血管、糖尿病黄斑水肿、玻璃膜疣以及正常视网膜的 OCT 图像，结果的准确率、敏感度、特异度均在 95% 以上，并能得出与人类相似甚至更高的准确率。其次，计算机对比图像像素与像素之间的差异，观察到人类关注不到的细节，从而得出更精准的判断，且不像人类一样受主观性干扰。另外，我们通过「迁移学习」这种算法，还能诊断不同系统的疾病，比如我们的系统目前还能准确鉴别肺炎和正常胸部 X 线平片，区分肺炎的病原体为细菌还是病毒，准确率可达 90% 以上。 机器之心：从医学学者的角度来看，人工智能技术在医疗领域里是否会像很多媒体报道的那样「超越，甚至代替人类医生」？ 张康： 在上一个问题已经回答了，在某些方面人工智能的确有可能超越人类医生。也许在不久的将来，比较单一的、流水线作业式的领域将会被取代。但是，现阶段人工智能的作用是辅助医生而非取代医生，发展人工智能，对医疗科学的发展、医疗水准的提高，都是利大于弊的。 机器之心：深度学习先驱吴恩达（Andrew Ng）认为迁移学习（Transfer learning）是人工智能未来最有希望的发展方向，而你的研究正是应用了迁移学习。相比其他机器学习方法，它具备哪些优秀之处？ 张康： 「迁移学习」被认为是一种高效的学习技术，尤其是面临相对有限的训练数据时。相较于其他大多数学习模型的「从零开始」，「迁移学习」利用卷积神经网络（Convolutional Neural Network，CNN）学习已有的已经标记好的预训练网络系统，以医学图像学习为例，该系统会识别预系统中图像的特点，我们再继续导入含有第一层图像相似参数和结构的网络系统，最终构建出终极层级。在我们的系统中，第一层网络就是视网膜 OCT 图像，第二级网络系统使用第一级的图像寻找相应的特点，通过前向传播固定低层图像中的权重，找到已经学习的可辨别的结构，再提取更高层的权重，在其中进行反复的自我调整和反馈、传递，达到学习区分特定类型的图像的目的。我们首次使用如此庞大的标注好的高质量视网膜 OCT 数据进行迁移学习，进行常见视网膜致盲性疾病的检测及推荐治疗手段，得到与人类医生相似甚至更高的准确性。此人工智能系统还可以「举一反三」，将迁移学习用于小儿肺炎诊断。 迁移学习是深度学习的一个自然发展方向，迁移学习能让深度学习变得更加可靠，还能帮我们理解深度学习的模型。比如，我们能够知晓哪部分特征容易迁移，这些特征所对应的是某个领域比较高层、抽象的一些结构型概念。把它们的细节区分开，就能让我们对这个领域的知识表达形成一个更深的理解。这样一来，机器就可以像生物的神经系统一样终身学习，不断地对过去的知识进行总结、归纳，让一个系统越学越快，而且在学习过程中还能发现如何学习。 迁移学习在深度学习上面有极为广阔的应用前景，在图像数据资源有限的医疗领域，更高效、所需图像数量更少的迁移学习，可以说是未来 5 年内 AI 发展的热点以及深度学习成功应用的驱动力。 机器之心：这项工作中，你们与广州市妇女儿童医疗中心、四川大学华西医院、同仁眼科中心、上海市第一人民医院、大连北海医院等机构进行了合作，这些合作是如何开展的？ 张康： 感谢这些医院为我们提供了宝贵的大量原始医疗图像数据，且由不同专科的医生对图像予以分类、标注，帮助训练 AI 系统获得更精准更稳定的诊断结果。 机器之心：我们都非常期待人工智能可以帮助我们治疗疾病，发表在《Cell》上的研究距离实际应用还有多远？ 张康： 我们目前已开始在美国和拉丁美洲诊所进行小规模临床试用，进一步优化系统，在未来很快时间里应该可以大规模使用。我们会同时增加准确标注的图片数量和疾病种类，如初步筛查常见疾病、就医转诊指引等功能将可能首先应用于临床，下一步则为指导治疗方案确定、随访等，最终的目标是应用到包括初级保健、社区医疗、家庭医生、急诊室，形成大范围的自动化分诊系统等。 机器之心：计算机科学领域的研究者们（如斯坦福大学吴恩达 Andrew Ng 团队、李飞飞团队）也在致力于将计算机视觉方法应用于医疗领域。作为另一个行业的学者，你是如何看待他们的研究的？ 张康： 他们的成果为我们的研究提供了理论基础，我们学习了他们的技术，并在他们开发的「迁移学习」模型为基础作出改进，组织一个庞大而有序的视网膜专家小组，加入总共约 11 万张准确标注的视网膜 OCT 图像以及 5000 张胸部 X 线图像，构建出我们的 AI 疾病图像诊断系统。可以说，我们取得今日的突破和他们的贡献是分不开的，感谢像他们一样的计算机学者的付出，才让我们得以更好的结合计算机视觉科学与医疗科学，从而更好的服务于全人类。 机器之心：这种以医生为主导的 AI+医疗研究与其他以人工智能学者为主导的 AI+医疗研究有什么不同？ 张康： 过去的人工智能研究多以人工智能学者为主导，也许能更快的设计出更为精妙的算法，但由于其对临床医生的需求的不了解，使其真正应有于临床受到限制。现在，我们的团队由专业的医生带领人工智能学者构成，我们更能了解医生对形成诊断、确定治疗方案的需求，在我们的研究中，我们医生知道什么样的医学图像诊断价值更高，从而亲自设定规范的图片纳入标准，对图像进行标注，从而使机器从源头开始就更能获得我们想要的结果。 比如，有一些图像特征较为模糊的图像，如老年黄斑变性，某些较大的玻璃膜疣和脉络膜新生血管非常相似，我们就会偏向于采取更为严重的疾病诊断，因为我们研究的最终目的是帮助病人更可能的推荐给相应的专科医生，从而更快的获得治疗。另外，我们还可以通过我们的想法设定更为贴合实际的过滤器，并按照我们临床医生的需求不断调整；通过「遮挡实验」能够反映机器得出判断的依据。并且，我们的研究还能指导治疗方案的确定。因此我们的研究可能更能达到临床医生想要的效果，并且为临床医生所信任，也许能更快更直接的应用于临床。 机器之心：如何减少医学领域与计算机科学领域之间的隔阂，让新技术能够更好地造福人类？ 张康： 就我们团队来说，我们以临床经验丰富的医生、教授为主导，辅以有生物医学知识的科学计算经验丰富的计算机专家，还有高通和 Intel 等计算机软硬件领域的行家作为我们的技术指导，在算法的完善和使用上起到很大的帮助。同时，我们定期会一起进行沟通交流，让程序员们更好的了解我们医生的需求。只有医学领域和计算机领域的人才之间互相帮助、互相指导、通力协作，才能使新技术真正的获得应用，更好的造福人类。   机器之心：目前的机器学习方法需要大量医疗图像用于模型的训练，如何避免泄露隐私的问题？ 张康： 目前的机器学习方法的确需要大量医疗图像用于模型的训练，虽然我们使用的「迁移学习」较传统的深度学习所需的数据量少，增加相应的优质数据确能更加高效的大幅度提升训练效果。通过大量的数据输入，AI 系统可以在不断的学习过程中进行调整，不断减少误差，从而获得更稳定更准确的结果输出。 我们可以保证的是，我们用来训练机器的医疗图像都是仅有疾病表现而不包含病人基本信息的（如姓名、年龄、性别等）图像，因此不存在隐私泄漏的问题。   机器之心：如何看待中国和美国在人工智能医疗领域发展上的差距或者不同？ 张康： 个人认为，中国在人工智能医疗领域其整体发展水平与发达国家相比仍存在一定差距，在前沿基础理论的学习、人才培训交流、关键性技术的强化，以及对数据收集的优化、质量的提升、档案的系统性、增加数据调用的方便性等方面都有待完善。 目前，国内大多数医疗人工智能仍处于实验研发阶段，其整体发展水平与发达国家相比仍存在一定差距，在前沿基础理论、关键性技术、产业基础平台、人才队伍和监管体系等方面都有待完善。要相信，现阶段人工智能的作用是辅助医生而非取代医生，从而帮助患者更容易获得治疗，随着今后医疗资源的自由流动，医院可能更需要辅助诊断系统，未来医疗人工智能是否能获得更有效的应用、开发出成熟的产品，还有赖于国家对 AI 产品使用的支持、临床医生的信赖与合作。通过加强国际交流合作、人才培训、构建研发中心，发挥我国疾病库资源庞大的优势，加强数据共享、优化数据，保持人才的长期交流与协作，才能获得更好的发展。 机器之心：在这项成功的研究之后，能否透露一下你下一步的 AI+医疗研究方向？ 张康： 通过算法系统的调整，继续改进学习、导出结果等过程；我们的 AI 系统对于全身各系统可以进行图像检测的疾病都具有适用性，因此我们将进一步增加准确标注的图片数量，加入不同的图像类型，增加可诊断的眼部疾病，另外，加入包括肿瘤，儿童和妇产科，病理等其他系统的疾病图片，增加其可诊断的疾病种类。同时，在疾病预测、指导治疗等等方面增加系统的适用性。  "
47,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740099&idx=4&sn=2b3b4e6456361141f467d42ff4b942fd&chksm=871ad1fdb06d58eb6f99508f902d1199c748c0b0be81c5088e406e97a1743d4f877d16c2b9b4&scene=27,学界 | 世界权威评测冠军：百度人脸检测算法PyramidBox,"arXiv 近日，百度凭借全新的人脸检测深度学习算法 PyramidBox，在世界最权威的人脸检测公开评测集 WIDER FACE 的「Easy」、「Medium」和「Hard」三项评测子集中均荣膺榜首，刷新业内最好成绩。本文将通过论文简要介绍这一算法背后的技术。 人脸检测是各种人脸应用中的一项基本任务。Viola - Jones [1] 的开创性研究利用具有类哈尔特征的 AdaBoost 算法来训练级联的人脸与非人脸分类器。此后不断有人进行深入研究 [ 2，3，4，5，6，7 ] 来改进级联检测器。之后，[ 8，9，10 ] 通过对可变形面部关系的建模，将可变形部件模型 ( DPM ) 引入到人脸检测任务中。这些方法主要是基于设计的特征，这些特征不具有很好的可表示性，而且是通过分离的步骤训练出的。 近年来，卷积神经网络 ( CNN ) 取得了巨大突破，基于 CNN 的现代目标检测技术在人脸检测方面取得了很大进展，包括 R - CNN [ 11、12、13、14]、SSD [15]、YOLO [16]、FocalLoss [17] 及其延伸产物。得益于强大的深度学习方法和端到端的优化，基于 CNN 的人脸检测器性能显著增强，为以后的方法划定了一个新的基线。 当下基于 anchor 的检测框架旨在检测不受控制的环境中的非常规面部，例如 WIDER FACE[ 18 ]。SSH [ 19 ] 和 S3FD [ 20 ] 开发了尺度不变网络，以在单个神经网络中检测来自不同层的尺度各异的人脸。人脸 R - FCN [ 21 ] 利用位置敏感的平均池，对分数图上嵌入的响应进行重新加权，并消除人脸每个部位中非均匀分布的影响。FAN [ 22 ] 提出 anchor 级别的注意机制，通过突出面部区域的特征来检测被遮挡的面部。 虽然这些工作为设计 anchor 和相关网络来检测不同尺度的人脸提供了一种有效的方法，但如何利用语境信息进行人脸检测还没有引起足够的重视，这一问题应该在非常规人脸检测中发挥重要作用。显然，人脸从不单独出现在现实世界中，肩部或身体通常也一起出现，它们提供了可兹利用的丰富的语境关联资源，尤其是在分辨率低、模糊和外部遮挡导致面部纹理不可区分的情况下。针对这一问题，我们提出了一种新的语境辅助网络框架，以充分利用语境信号，具体步骤如下： 首先，网络不仅要能学习人脸特征，还要能学习头部和身体等语境部分的特征。为了实现这一目标，我们需要额外的标签，并设计与之匹配的 anchor。在本任务中，我们使用半监督解决方案来生成与面部相关的语境部分的近似标签，并且发明了一系列名为 PyramidAnchors 的 anchor，以便将其添加到基于 anchor 的一般架构中。 其次，高层次的语境特征应与低层次的语境特征充分结合。常规人脸和非常规人脸的外观可能存在很大差别，这意味着并非所有高级语义特征都有助于识别较小的人脸。我们研究了 Feature Pyramid Networks( FPN ) [ 23 ] 的性能，并将其修改为较低级别的 Feature Pyramid Network( LFPN )，以将对彼此有帮助的特征连接在一起。 第三，预测分支网络应充分利用联合特征。为了将目标人脸周围的语境信息与更广更深的网络相结合，我们引入了语境敏感预测模块 ( CPM )。同时，为了进一步提高分类网络的性能，我们提出了一种可以预测模块的最大输入输出层。 此外，我们还提出了一种名为「数据-anchor-抽样」的训练策略，对训练数据集的分布进行调整。为了学习更具代表性的特征，非常规人脸样本的多样性非常重要，可以通过跨样本的数据扩充来获得。 为表述清晰，本研究可以概括为以下五点： 1. 本文提出了一种基于 anchor 的语境辅助方法，即 PyramidAnchors，从而引入有监督的信息来学习较小的、模糊的和部分遮挡的人脸的语境特征。 2. 我们设计了低层次特征金字塔网络 ( LFPN ) 来更好地融合语境特征和面部特征。同时，该方法可以在单次拍摄中较好地处理不同尺度的人脸。 3. 我们提出了一种语境敏感的预测模型，该模型由混合网络结构和最大输入输出层组成，从融合特征中学习准确的定位和分类； 4. 我们提出了可以感知尺度的数据-anchor-抽样策略，改变训练样本的分布，重点关注较小的人脸。 5. 在通用人脸检测基准 FDDB 和 WIDER FACE 上，我们达到了当前最佳水平。 3.1 网络架构 基于 anchor 的拥有复杂 anchor 设计的目标检测框架已被证明在不同层级的特征图上执行预测时，可以有效地处理可变尺度的人脸。同时，FPN 结构表明将高层级特征和低层级特征融合能带来很大的优势。PyramidBox（图 1）的架构使用了于 S3FD[20] 相同的扩展 VGG16 主干架构和 anchor 尺度设计，可以生成不同层级的特征图和等比例的 anchor。低层级 FPN 被添加到这个主干架构上，并且使用一个语境敏感的预测模块作为每个 Pyramid 检测层的分支网络来获得最终的输出。该方法的关键在于我们设计了一种新型的 Pyramid anchor 方法，它可以在不同层级为每一张人脸生成一系列的 anchor。架构中每个组件的细节如下： Scale-equitable 的主干网络层。我们使用了 S3FD 中的基础层和额外卷积层作为我们的主干网络层，其中保留了 VGG16 中的 conv 1_1 层到 pool 5 层，然后将 fc 6 层和 fc 7 层转换为 conv fc 层，再添加更多的卷积层使其变得更深。 低层级特征金字塔层。为了提高人脸检测器处理不同尺度的人脸的能力，高分辨率的低层级特征扮演着关键角色。因此，很多当前最佳的研究 [25,20,22,19] 在相同的框架内构建了不同的结构来检测可变尺寸的人脸，其中高层级特征被用于检测尺寸较大的人脸，而低层级特征被用于检测尺寸较小的人脸。为了将高层级特征整合到高分辨率的低层级特征上，FPN[23] 提出了一种自顶向下的架构来利用所有尺度的高层级语义特征图。最近的研究表明，FPN 类型的框架在目标检测和人脸检测上都得到了相当不错的性能。 通过从中间层开始自上而下的结构，我们构建了低层级的特征金字塔网络 (LFPN)，其感受野接近于输入尺寸而不是顶层的一半。此外，每个 LFPN 块的结构和 FPN [23] 一样，更多信息详见图 2(a)。 图 1：PyramidBox 架构。它包含 Scale-equitable 主层、低层级特征金字塔层 (LFPN)、语境敏感的预测层和 PyramidBox 损失层。 图 2：(a) 特征金字塔网络。(b) 语境敏感的预测模块。(c)PyramidBox 损失。 表 1：从不同层开始的 LFPN 的表现。 表 2：PyramidAnchors 的参数。 表 3：语境敏感的预测模块。 表 4：PyramidBox 在 WIDER FACE 验证子集上的对比结果。 论文：PyramidBox: A Context-assisted Single Shot Face Detector  论文链接：https://arxiv.org/pdf/1803.07737.pdf 人脸检测研究从多年前就已开始，然而，在不受控制的环境中检测小的、模糊的及部分遮挡的人脸仍旧是一个有待解决的难题。针对棘手的人脸检测问题，本文提出了一种语境辅助的单次人脸检测新方法——PyramidBox。考虑到语境的重要性，我们从以下三个方面改进语境信息的利用。首先，我们设计了一种全新的语境 anchor，通过半监督的方法来监督高层级语境特征学习，我们称之为 PyramidAnchors。其次，我们提出了一种低层次级特征金字塔网络，将充分的高层级语境语义特征和低层级面部特征结合在一起，使 PyramidBox 能够一次性预测所有尺寸的人脸。再次，我们引入了语境敏感结构，扩大预测网络的容量，以提高最终的输出准确率。此外，我们还采用「数据-anchor-采样」的方法对不同尺寸的训练样本进行扩充，增加了较小人脸训练数据的多样性。PyramidBox 充分利用了语境的价值，在两个常用人脸检测基准——FDDB 和 WIDER FACE 上表现非凡，取得当前最优水平。 "
48,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740099&idx=5&sn=22bf8b2f2665a1d0794d759a34df9073&chksm=871ad1fdb06d58ebfb7dcaf088d539f7b7723eea48ebf279c508345f4a4f405f0b0457759a4a&scene=27,报名 | INTERFACE#5袁进辉（老师木）：深度学习引擎的最优架构,自深度学习流行以来，深度学习框架也成为了各大公司重点发展研究的方向。而当地时间 3 月 30 日，最新一届谷歌 TensorFlow 开发者峰会将会召开，势必吸引众多关注。谷歌 TensorFlow、PyTorch、MXNet、百度 PaddlePaddle 等框架，哪一种才是最合适自己的？在框架之争火热的今天，我们邀请到了 OneFlow 的袁进辉老师为我们分析已有框架的优缺点，分享深度学习框架应该怎么做的观点。 演讲者：袁进辉（老师木） 讲者简介：袁进辉（老师木），2008 年 7 月自清华大学计算机系获得工学博士学位，获得清华大学优秀博士学位论文奖，在计算机视觉及多媒体领域顶级会议上发表多篇论文，连续多年获得美国国家技术标准局组织的视频检索评测比赛的第一名。2010 年负责研发斯诺克比赛「鹰眼」系统，该产品打败来自英国的竞品开始服务于各项国际大赛，并被中国国家队作为日常训练辅助系统。2012 年作为早期成员加入 360 搜索创业团队，一年后产品上线成为国内市场份额第二的搜索引擎。2013 年加入微软亚洲研究院从事大规模机器学习平台的研发工作。2014 年，发明了当时世界上最快的主题模型训练算法和系统 LightLDA，只用数十台服务器即可完成以前数千台服务器才能实现的大规模主题模型，该技术成功应用于微软在线广告系统，被当时主管研究的全球副总裁周以真称为「年度最好成果」。2015 年至 2016 年底，专注于搭建基于异构集群的深度学习平台，项目荣获微软亚洲研究院院长特别奖 (top 1%)。2017 年创立北京一流科技有限公司，致力于打造分布式深度学习平台的事实工业标准。 时间：2018.04.14 地点：北京市朝阳区酒仙桥东路电子城研发中心 A2 楼一层机器之心演讲厅 14:00-14:30 签到 14:30-15:30 嘉宾分享  15:30-16:00 现场提问、交流 
49,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740185&idx=2&sn=1da15ae38b8338dcc1ca296d3fa26995&chksm=871ad1a7b06d58b1c14fe9edc6fbf28b9cfed99d893fffb47cb00a8eecb52d63b541e5d9f6d3&scene=27,入门 | 这是一份文科生都能看懂的线性代数简介,Medium 线性代数的概念对于理解机器学习背后的原理非常重要，尤其是在深度学习领域中。它可以帮助我们更好地理解算法内部到底是怎么运行的，借此，我们就能够更好的做出决策。所以，如果你真的希望了解机器学习具体算法，就不可避免需要精通这些线性代数的概念。这篇文章中，我们将向你介绍一些机器学习中涉及的关键线性代数知识。 线性代数是一种连续形式的数学，被广泛应用于理工类学科中；因为它可以帮助我们对自然现象建模，然后进行高效的计算。但是，由于线性代数是一种连续而非离散的数学，因此，很多计算机科学家都不太了解它。另外，线性代数还在几乎所有的数学学科中都拥有着核心地位：例如几何学和泛函分析。 线性代数中的概念是理解机器学习理论所必需的基础知识，尤其是对那些处理深度学习算法的人而言。在刚接触机器学习时，你可以不需要掌握线性代数。但到了一定程度后，当你希望更好地理解不同机器学习算法运作原理时，线性代数就很有用了，它可以帮助你在开发机器学习系统时更好地做决策。 在线性代数中，我们使用线性方程来表示数据，并把它们写成矩阵或向量的形式。因此，基本上你都是在与矩阵和向量打交道，而不是标量（我们会在文章的稍后部分介绍这些概念）。如果你能够想到使用一个合适的库，比如 NumPy，你就可以通过简短的几行代码，轻松实现复杂的矩阵乘法。请注意，这篇文章忽略了那些对机器学习并不重要的线性代数概念。 数学对象 标量 标量就是一个简单的数，比如 24。 向量 向量是一个有序数组，能够写成一行或者一列的形式。向量只包含一个索引，用来表示向量中的某个特定元素。比如 V_2 表示向量中的第二个元素，在上面淡黄色的图中是-8。 矩阵 矩阵是一个有序的二维数组，有两个索引。第一个索引表示行，第二个索引表示列。例如，M_23 表示的是第二行、第三列的元素，在上面淡黄色的图中是 8。矩阵可以有多个行或者列，注意一个向量也是一个矩阵，但仅有一行或者一列。 淡黄色图中有一个矩阵的例子：一个 2×3 的矩阵 (行数×列数)。下图中是另一个矩阵和对应的表示形式。 张量 三维张量是按照一定规律排列在方格中的数组，其中一个变量数字表示轴。张量有三个索引，其中第一个索引表示行，第二个索引表示列，第三个索引表示轴。例如，V_232 指向第二行、第三列、第二轴的元素，在下图右边的张量中表示 5。 张量是上面谈到的概念中最常用的一个，因为张量是一个多维数组，同时可以是一个向量或者一个矩阵，具体取决于它的索引数量。例如，一阶张量可以表示向量（1 个索引），二阶张量可以表示矩阵（2 个索引），三阶就是张量（3 个索引），更高阶的称为高阶张量（超过 3 个索引）。 运算法则 矩阵和标量的计算 如果你在一个矩阵上加、减、乘、除一个标量，你所做的就是直接对矩阵的每个元素进行这些数学运算。下图给出了矩阵数乘的一个很好的例子。 矩阵和向量的运算 对一个矩阵乘以一个向量，可以理解为对矩阵的每一行乘以向量的每一列，运算结果会是一个向量，它的行数和矩阵的行数一样。下图展示了这是如何计算的。 为了更好地理解这个概念，我们详细讲解一下第二张图中的计算步骤。为了得到结果向量中的第一个元素 16，选择拿来和矩阵相乘的向量中的元素 1 和 5，把它们与矩阵第一行中的元素 1 和 3 相乘，像这样： 1*1 + 3*5 = 16。 对矩阵第二行的元素进行相同的计算： 4*1 + 0*5 = 4。 同样，再计算矩阵第三行的元素： 2*1 + 1*5 = 7。 这里还有另一个例子： 在这里，我们给出一个备忘录： 矩阵间的加减法 矩阵间的加减法非常简单直接。这里要求，两个矩阵需要维度相同，运算结果也会是一个相同维度的矩阵。你只需要将第一个矩阵中的每一个元素和第二个矩阵中对应位置的元素相加或者相减就可以了。如下图所示：  矩阵间的乘法 如果你知道如何计算矩阵和向量间的乘法，矩阵间的乘法就也简单了。注意，只有当第一个矩阵的列数和第二个矩阵的行数相等时，才能把它们两个乘起来。运算结果会是一个矩阵，行数和第一个矩阵的行数相等，列数和第二个矩阵的列数相等。计算方法如下： 你只需要将第二个矩阵分成列向量，然后分别将第一个矩阵和每个列向量相乘。然后，将运算结果拼接成一个新的矩阵（不要把它们加起来!）。下图逐步展示了计算过程： 同样，我们也给出一个备忘录： 矩阵乘法拥有一些性质，根据这些性质，我们可以将大量计算整合成一个矩阵乘法。在下面我们会依次讨论这些性质。为了便于理解，我们会先用标量来解释这些性质，然后再使用矩阵形式。 交换律 数乘满足交换律，但矩阵乘法并不满足。这意味着，当我们在将两个标量乘在一起的时候：7×3 和 3×7 的结果是一样的，但当我们将两个矩阵相乘起来的时候：A×B 并不等于 B×A。 结合律 数乘和矩阵乘法都满足结合律。这意味着，数乘 3×（5×3）等于（3×5）×3，同时矩阵乘法 A×（B×C）等于（A×B）×C。 分配律 数乘和矩阵乘法都满足分配律。这表示，数乘 3×（5+3）等于 3×5+3×3，而矩阵乘法 A×（B+C）等于 A×B +A×C。 单位矩阵 单位矩阵是一种特殊的矩阵，不过首先，我们需要定义什么是「单位」。数字 1 是一个「单位」，因为任何数乘以 1 都等于它自身。因此，任何矩阵乘以一个单位矩阵都应该等于它自己。例如，矩阵 A 乘以单位矩阵还等于矩阵 A。 单位矩阵的主对角线元素都是 1，其余元素都是 0，你可以根据这个性质得到一个单位矩阵。同时它也是一个「方阵」，这表示它的行数和列数是相等的。 我我们之前说，矩阵乘法不满足交换律，但这里有一个例外：将一个矩阵和一个单位矩阵相乘。因此，下式是成立的：A × I = I×A = A。 矩阵的逆和转置 矩阵的逆和矩阵的转置是两种矩阵特有的性质。同样的，我们首先在实数上讨论这些性质，然后再使用在矩阵中。 1.逆运算 首先，什么是逆（倒数）? 一个数乘以它的逆（倒数）等于 1。注意，任何非零的数都有倒数。如果将矩阵和它的逆矩阵相乘，结果就应该是单位矩阵。下面的例子展示了标量的逆（倒数）： 不过，并不是每个矩阵都有逆矩阵。如果一个矩阵是方阵，而且它可逆，就可以求出它的逆矩阵。很遗憾，讨论什么矩阵可逆超出了这篇文章的范围。 我们为什么需要逆矩阵呢？这是因为我们不能计算用矩阵相除，并没有「除以矩阵」的定义，但我们可以用一个矩阵乘以一个逆矩阵，来达到相同的目的。 下图展示了一个矩阵乘以它的逆矩阵，计算结果是一个 2×2 的单位矩阵。 可以利用 NumPy 轻松计算出一个矩阵的逆矩阵（如果它可逆的话）。 2.转置 最后，我们讨论矩阵转置的性质。这基本上就是将一个矩阵沿着 45 度轴线镜像翻转。计算矩阵的转置非常简单，原始矩阵的第一列就是转置后矩阵的第一行，第二列则变成了转置后矩阵的第二行。一个 m×n 的矩阵仅仅是转成了 n×m 的矩阵。同时，矩阵 A 的元素 A_ij 等于转置后矩阵的元素 A_ji。下图展示了矩阵的转置： 总结 在这篇文章中，你接触到了一些机器学习中使用到的线性代数概念。你学会如何对这些对象进行加、减、乘、「除」。另外，你还掌握了矩阵最重要的性质，以及它们为什么可以帮我们得到更有效的计算。在这些知识的基础上，你还学习了逆矩阵和转置矩阵的概念，以及可以如何使用它们。虽然机器学习中还有很多线性代数知识，但这篇文章提供了关于最核心的概念的一些适当介绍。 
50,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740099&idx=2&sn=48990251109e0c23a70ffc99eb5da930&chksm=871ad1fdb06d58eb8bcabc69c4f86103b7009d64ffc3677873e793897fb2f2eaeb0a77822113&scene=27,资源 | Mask R-CNN神应用：像英剧《黑镜》一样屏蔽人像,"GitHub 黑镜特别篇《白色圣诞节》中有种名为「屏蔽」的黑科技，每个人安装上智能眼，可以凭意愿屏蔽动态视界中的任何人（及其相关的任何影像和声音），变成一团移动的白色马赛克。被屏蔽者无法解除这种屏蔽，除非死亡。相比之下，朋友圈的屏蔽是不是弱爆了？本文作者给出了一种自动「屏蔽」人像的脚本（不同于黑镜中的视频屏蔽），底层实现是在 MS COCO 数据集上预训练的 Mask R-CNN，但它不需要 GPU！此外，它不仅可屏蔽人像，还可以屏蔽包括长颈鹿和汽车在内的多达 80 种不同类型的物体，向黑镜中的黑科技迈出了第一步。 黑镜之《白色圣诞节》中的黑科技：屏蔽。 Person Blocker 可使用以下命令行调用并输出结果： 这个命令如下有一些参数来控制模型的推断效果： -i/--image：指定输入图像的路径与文件。 -m/—model：加载预训练 COCO 模型权重的路径，默认情况下为当前目录。如果没有指定路径，且当前目录下不存在权重文件，那么模型将自动下载预训练权重至当前目录。 -c/—color：指定掩码的颜色，它可以是引号内的 16 进制或 RGB 三元素元组的格式，默认为白色。 -o/--object：遮掩目标的可用列表或具体目标的 ID，我们可以在文件 classes.py 中看到所有可遮掩的目标，或直接使用-names flag，默认的遮掩目标为「person」。 -l/—labeled：通过检测的目标和对应的目标 ID 标注图像并保存。 -n/—names：打印目标的类别选项，并退出。 这一份脚本可输出两种图像，即静态的 png 图像和动态的 gif 图像。我们可选的遮掩目标有 80 种，如下 classes.py 文件中给出了所有的目标类别： 遮掩特定的目标需要两个步骤：首先执行推断模型并获取所有的目标 ID，然后再根据 ID 选择性地遮掩这些目标。 最后，如果读者想使用或测试该脚本，那么它需要的依赖库与 Mask R-CNN 基本相同 ： Python 3.4+ TensorFlow 1.3+ Keras 2.0.8+ Numpy, skimage, scipy, Pillow, cython, h5py matplotlib, imageio "
51,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740185&idx=1&sn=4f4cb9aa6abe3d27e684959ac1c8d4a4&chksm=871ad1a7b06d58b1bbd71760c13f66ebefc1cf9dea14a4ea1ccda70c34ec09856cee861ad196&scene=27,TensorFlow发布面向JavaScript开发者的机器学习框架TensorFlow.js,当时时间 3 月 30 日，谷歌 TenosrFlow 开发者峰会 2018 在美国加州石景山开幕，来自全球的机器学习用户围绕 TensorFlow 展开技术演讲与演示。去年的 TensorFlow 开发者大会上，该框架正式升级到了 1.0 版本，逐渐成为最流行的深度学习框架。今年，TensorFlow 发布了面向 JavaScript 开发者的全新机器学习框架 TensorFlow.js。 在大会上午的 Keynote 中，谷歌大脑负责人 Jeff Dean、TensorFlow 总监 Rajat Monga 等人围绕 TensorFlow 做了表现、流行度等方面的介绍。 据介绍，在过去的两年中，TensorFlow 不断更新，不断改善，逐渐成为社区内最为流行的深度学习框架。下图是从开源以来，TensorFlow 的重大更新，例如 TensorBoard、tfdata、tfkeras、Eager Execution 等。 而且据统计，两年内，TensorFlow 已经有一千一百万下载，超过三万的 commits，6900 以上的 pull requests，1400 多位 contributors。 今年，围绕 TensorFlow，谷歌同样做出了几项重大宣布： 1. 发布新的 TensorFlow 官方博客（http://blog.tensorflow.org/）与 TensorFlow YouTube 频道； 2. 面向 JavaScript 开发者的全新机器学习框架 TensorFlow.js; 3. 发布一系列新的库与工具：例如 TensorFlowHub、TensorFlow Probability API、Nucleus、DeepVariant 等。 在今天的几项重大宣布中，比较有趣的是面向 JavaScript 开发者的全新机器学习框架 TensorFlow.js。在下文中，机器之心对 TensorFlow.js 做了细致介绍： 在大会的 Keynote 中，TensorFlow 团队表示基于网页的 JavaScript 库 TensorFlow.js 现在已经能训练并部署机器学习模型。我们可以使用神经网络的层级 API 构建模型，并在浏览器中使用 WebGL 创建复杂的数据可视化应用。此外 Node.js 很快就会发布，它能为网站模型提供 GPU、TPU 等快速训练与推断的方法。 在 TensorFlow.js 中，我们可以使用最底层的 JavaScript 线性代数库或最高级的 API 在浏览器上开发模型，也能基于浏览器运行已训练的模型。因此，它可以充分利用浏览器和计算机的计算资源实现非常多机器学习应用。例如在网页端训练一个模型来识别图片或语音，训练一个模型以新颖的方式玩游戏或构建一个能创造钢琴音乐的神经网络等。这些新颖的模型作为案例在 TensorFlow.js 中都提供了实现代码，读者也可以跟随教程实现基于浏览器的模型。 TensorFlow.js 项目主页：https://js.tensorflow.org/ TensorFlow.js 的核心概念 TensorFlow.js 是一个开源的用于开发机器学习项目的 WebGL-accelerated JavaScript 库。TensorFlow.js 可以为你提供高性能的、易于使用的机器学习构建模块，允许你在浏览器上训练模型，或以推断模式运行预训练的模型。TensorFlow.js 不仅可以提供低级的机器学习构建模块，还可以提供高级的类似 Keras 的 API 来构建神经网络。 TensorFlow.js 的安装非常简单，我们可以直接使用 NMP 或脚本完成构建。它的使用也有非常多的文档与教程，我们只需要掌握一些基本的核心概念就能快速入手这一 JS 库。接下来，我们介绍这个库的一些核心概念。 Tensor TensorFlow.js 中的中心数据单元是张量（tensor）：一维或多维数组。一个 Tensor 实例的 shape 属性定义了其数组形状（即，数组的每个维度上有多少个值）。 Tensor 主要构造函数是 tf.tensor 函数： Variable Variable 使用一个张量值来初始化。然而，和 Tensor 不一样，它们的值是可变的。你可以用 assign 方法分配一个新的张量到一个已有的变量（variable）： Variable 主要用于在模型训练过程中保存和更新值。 Operations (Ops) Tensor 可以用于保存数据，而 Operation（Op）则可用于操作数据。TensorFlow.js 提供了多种适用于张量的线性代数和机器学习运算的 Op。由于 Tensor 是不可改变的，这些 Op 不会改变它们的值，而会返回新的 Tensor。这些运算不仅包含 add、sub 和 mul 等二元运算，同时还包括 square 等一元运算： 模型和层 从概念上说，一个模型就是一个函数，给定输入之后生成所需要的输出。 在 Tensorflow.js 有两种创建模型的方式：直接使用 Op 表示模型的运算。或者使用高级 API tf.model 来构建以层定义的模型，这在深度学习中是很常用的抽象形式。 其实除了以上的特征，Tensorflow.js 还有一些很重要的核心概念，例如内存管理、神经网络基本运算和训练过程等。但我们了解以上概念就能轻松在浏览器中构建出简单的机器学习模型，如下展示了简单线性回归的定义方法： () 目前该项目还是非常新颖的应用，我们非常容易将机器学习模型部署在网页端并在用户的浏览器与硬件实现简单的推断。虽然我们还不清楚实现的效果，但这个 JS 库真正能训练并部署机器学习模型，因此机器之心也将持续关注并尝试构建有意思的应用。 看完此届 TensorFlow 开发者峰会，你是否对此最流行的深度学习框架有更多期待？机器之心读者可在文章下留言，说出对「TensorFlow 之后的更新中最大的期待是什么？」 （公众号id：TensorFlow）联合机器之心将会选出部分专业回答赠送TF书包（仅限 10 人）。 
52,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740185&idx=3&sn=5430478b25b0b851a5c45e4d83da4807&chksm=871ad1a7b06d58b123701505746ce78ae62b34e9acc073622fa4f6ad674761d28f3790976c3e&scene=27,深度 | 可视化LSTM网络：探索「记忆」的形成,"Medium 在卷积神经网络领域中有许多可视化方面的研究，但是对于 LSTM 却没有足够的类似工具。LSTM 网络的可视化能带来很有意思的结果，由于其包含时间相关性，我们除了可以在可视化图像的空间维度上探索数据之间的关联，还可以在时间维度上探索关联的稳健性。 GitHub 地址：https://github.com/asap-report/lstm-visualisation 数据集地址https://archive.ics.uci.edu/ml/datasets/Australian+Sign+Language+signs 对于长序列建模而言，长短期记忆（LSTM）网络是当前最先进的工具。然而，理解 LSTM 所学到的知识并研究它们犯某些特定错误的原因是有些困难的。在卷积神经网络领域中有许多这方面的文章和论文，但是对于 LSTM 我们却没有足够的工具可以对它们进行可视化和调试。 在这篇文章中，我们试图部分填补这个空白。我们从澳大利亚手语（Auslan）符号分类模型中对 LSTM 网络的激活行为进行可视化，通过在 LSTM 层的激活单元上训练一个降噪自编码器来实现。通过使用密集的自编码器，我们将 LSTM 激活值的 100 维向量投影到 2 维和 3 维。多亏了这一点，我们在一定程度上能够可视化地探索激活空间。我们对这个低维空间进行分析，并试图探索这种降维操作如何有助于找到数据集中样本之间的关系。 Auslan 符号分类器 本文是 Miroslav Bartold 工程论文（Bartołd，2017）的扩展。该论文中使用的数据集来自（Kadous，2002）。该数据集由 95 个 Auslan 手语符号组成，是使用带高质量位置追踪器的手套捕获而来的。然而，因为其中一个符号的数据文件存在问题，剩下 94 个类可用。每个符号由当地手语使用者重复 27 次，且每个时间步使用 22 个数字（每只手 11 个数字）进行编码。在数据集中，最长序列的长度为 137，但由于长序列数量很少，因此我们将长度保留 90 位，并在较短序列的前端填充零序列。 Miroslav 的论文测试了几种分类器，它们全都基于 LSTM 架构，分类的准确率在 96％左右。 如果你对 LSTM 不甚熟悉，你可以看看  Christopher Olah 的博客，上面有关于 LSTM 网络非常好的解释：http://colah.github.io/posts/2015-08-Understanding-LSTMs/。 或者机器之心的文章： 。 在这项研究中，我们将重点关注一种具有 100 个 LSTM 单元单隐层的架构。该架构最后的分类层有 94 个神经元。其输入是具有 90 个时间步的 22 维序列。我们使用了 Keras 函数式 API，其网络架构如图 1 所示。 图 1 所示的 Lambda 元素从完整的激活序列中提取了最后一层激活（因为我们将 return_sequences=True 传给了 LSTM）。对于实现过程中的细节，我们希望大家查看我们的 repo。 首次尝试了解 LSTM 网络的内部结构 受到《Visualizing and Understanding Recurrent Networks》(Karpathy, 2015) 的启发，我们试图将一些对应易识别子手势的神经元局域化（并在不同的符号之间共享），例如握紧拳头或用手画圈。但是，这种想法失败了，这主要有下列五个原因： 来自定位追踪器的信号不足以完全重建手部的运动。 手势在追踪器和真实空间中的表征差异明显。 我们只有来自 http://www.auslan.org.au 的手势视频，却没有数据集中符号的实际执行的视频。 数据集以及视频中的语汇来自不同的方言，所以可能会出现同义词。 100 个神经元和 94 个符号对于人类理解而言是非常大的空间。 因此，我们只关注可视化技术，希望这能帮助我们揭开关于 LSTM 单元和数据集的一些奥秘。 去噪自编码器 为了将所有手势的 LSTM 输出激活序列可视化，我们将尝试在每一个时间步利用去噪自编码器将表征激活值的 100 维向量降为 2-3 维的向量。我们的自编码器由 5 个全连接层组成，其中第三层是具有线性激活函数的瓶颈层。 如果你对上面说的主题不甚熟悉，你可以在这里学习更多有关自编码器的知识：http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/。 为了使得图像清晰易读，线性激活函数被证明是最佳的激活函数。对于所有被测试的激活函数，所有样本路径（example path，该术语将会在下一部分中解释）都从图的（0,0）点附近开始。对于非奇对称函数 (ReLU 和 sigmoid) 而言，所有样本路径都在坐标系的第一象限。而对于奇函数而言（例如 tanh 和线性函数），所有路径在所有象限都大致是均匀分布的。但是，tanh 函数将路径压缩到 -1 和 1 附近（这使得图像太过失真），而线性函数没有这个问题。如果你对其他类型激活函数的可视化感兴趣，你可以在 repo 找到代码实现。 在图 2 中，我们展示了 2D 自编码器的架构。3D 自编码器与之几乎完全相同，不过它在第三个 Dense 层中有 3 个神经元。 在每个手势实现的所有单个时间步中，自编码器使用 LSTM 单元的输出激活向量进行训练。然后这些激活向量被打乱，其中一些冗余激活向量会被去除。冗余激活向量指的是从每个手势开始和结束中得到的矢量，其激活基本保持不变。 自编码器中的噪声服从均值为 0 标准差为 0.1 的正态分布，这些噪声被添加到输入向量当中。网络使用 Adam 优化器进行训练，来最小化均方误差。 可视化 通过向自编码器输入对应于单个手势的 LSTM 单元激活的序列，我们可以获得瓶颈层上的激活。我们将这个低维瓶颈层激活序列作为一个样本路径。 在一些样本的最后一步附近，我们给出了它所代表的手势符号名称。 在图 3 中，我们给出了训练集样本路径的可视化结果。 可视化图中的每个点都代表一个时间步、一个样本的自编码器 2D 激活值。图中点的颜色代表每个符号执行的时间步长（从 0 到 90），黑线连接单一样本路径的点。在可视化之前，每个点都由函数 lambda x: numpy.sign(x) * numpy.log1p(numpy.abs(x)) 进行转换。这种转换能够让我们更加仔细地观察每条路径的起始位置。 在图 4 中，我们展示了每个训练样本最后一步的激活。这是输入点到分类层的二维投影情况。 令人惊讶的是所有路径看起来都非常平滑并且在空间上能很好地分离，因为实际上在训练自编码器前，每个时间步和样本的所有激活操作都被打乱了。图 4 中的空间结构解释了为什么我们的最后一个分类层在如此小的训练集上（接近 2000 个样本）能达到很高的准确率。 对于那些有兴趣研究这个 2D 空间的读者而言，我们已经在以下地址提供了图 2 的大型版本： https://image.ibb.co/fK867c/lstm2d_BIG.png。  在图 5 中，我们展示了三维 LSTM 激活的可视化结果。为了清晰起见，我们只标明了一部分点。出于数据分析的目的，我们在本文第二部分只关注 2D 可视化。 分析 可视化看起来效果非常好，但是其中有没有更有意义的东西呢？如果一些路径距离很近，是否说明这些手势符号更相似？ 让我们在考虑右手和双手符号划分（我们并未看到仅用左手的符号）的情况下看看这个空间吧。这种划分是基于手持跟踪器的信号可变性统计而来的，更详细的信息参见 repo。 为了清晰起见，我们在图 6 中绘制了不含点的路径。右手手势符号用青色表示，双手手势符号用洋红色表示。我们可以清楚地看到，这两种符号都占用了空间的互补部分，并且很少彼此混淆。 现在让我们先来看看 drink-danger 对。这两者都是「青色」的手势，却占据了图 6 中间偏右侧大部分洋红色的部分。在我们的数据中，这两个手势都是单手的，但来自 Auslan signbank 的视频解释表明 danger 手势显然是双手的。 这可能是由标签错误引起的。请注意，dangerous 肯定是单手的，而且 drink 也类似（至少在手势的第一部分）。因此，我们认为标签 danger 实际上就是 dangerous。我们在图 7 中绘制了这两个手势。 在图 8 中，Who 和 soon 手势的情况很类似。手套中只有一个弯曲追踪器，且手指弯曲测量不是很精确。这也就是这两种手势在图 8 中看起来比视频中更类似的原因。 Crazy 和 think 符号的样本路径占据了图 9 中的相同空间区域。然而，think 看起来像是稍长的 crazy 手势的一个主要部分。当我们查看 Auslan signbank 中的视频时，我们发现这种关系是正确的，而且 crazy 符号看起来就像是 think 符号再加上手掌打开的过程。 在图 10 中，虽然当我们看 you 这个符号时我们发现这个符号与 crazy、think、sorry（以及其他在这里没有展示出来的手势）相互垂直，但我们在 signbank 中比较它们的视频时，我们并不能发现这些符号和 you 之间的任何相似之处。 我们应该记住，每一个 LSTM 单元的状态会记住它自己之前的状态，它在每一个时间步都由相应的输入序列馈送进来，并且在路径占据相同空间时可能会存在时间演化上的不同。因此，除了我们在分析中考虑的因素，实际上有更多变量会决定路径的形状。这可能解释了为什么在我们无法观察到符号间视觉相似性时，却能发现部分样本路径之间有交叉关系。 从可视化结果得到的部分紧密联系被证明有误。一些联系在自编码器再训练的间隔中（或 LSTM 单元再训练之后）会发生变化；有些联系则不会变化，可能代表真正的相似之处。举例而言，God 和 Science 有时在 2D 空间中共享相似的路径，有的时候又会彼此远离。 错误分类的样本 最后，让我们来看看错误分类的样本。在图 11、12 和 13 中，我们分别对在训练集、验证集和测试集中错误分类的样本进行了可视化。错误分类样本上面的蓝色标签是它们真实的类别。在其下方是模型选择的标签，用红色标记。 对于训练样本，只有三个样本被错误标记了，而且其中的两个（hurt-all 和 thank-hot）在二维空间中非常接近。Thank-hot 在视频中也很接近，但 Hurt-all 并非如此。 正如我们所料，验证集和测试集中都有更多分类错误的样本，但是这些错误在投影空间更接近的手势当中更常发生。 小结 我们将激活值的 100 维向量投影到低维空间。这种投影看上去很有意思，它似乎保留了很多（但并非全部）符号之间的关系。这些关系似乎与我们在观察现实生活中手势所感知到的关系相类似，但是在没有实际匹配手势视频来分析的情况下，我们无法确定这一点。 这些工具可以在一定程度上用于观察 LSTM 表征的结构。并且，相比与使用原始输入，它可以作为查找样本关系的更好工具。 "
53,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740072&idx=5&sn=77fa54ab433051ffbcbbc4bf94c5a8fd&chksm=871ad116b06d5800e7ab2bbd8cfa94b3ba8eb675b01ae27f173f73c22fa2c67b0a6bfe2964f0&scene=27,报名 | 用人工智能提升营销效率，阿里妈妈启动国际广告算法大赛,"IJCAI2018 阿里妈妈国际广告算法大赛于 2018 年 2 月正式启动，截止目前已吸引 34 个国家和地区，超过 4000 名选手报名参赛，比赛热度正在持续升温，获奖队伍将有机会前往斯德哥尔摩参加 IJCAI2018。 本次活动是阿里妈妈携手国际人工智能联合会议（IJCAI-2018）以及阿里云天池平台共同启动的国际性广告算法大赛。 作为阿里巴巴集团旗下的大数据营销平台，阿里妈妈不仅拥有海量的核心商业数据，还身处于一个庞大的真实市场交易场景之中。 在海量数据和真实营销场景的交互作用下，阿里妈妈目前已经在业内展现出强大的技术实力，例如构建了超大规模的学习模型，这个模型将多频的消费数据、用户多频的行为轨迹以及海量的用户习惯进行综合分析，每天训练近百 T 的数据，有着上千亿的样本。   技术探索的脚步从来没有停止。仅在 2018 年，阿里妈妈技术团队就有多篇论文入选国际学术会议——在搜索广告方面，直通车新一代智能广告检索模型的论文入选了 WWW 大会（The International World Wide Web Conference），评委一致认为该方法是对传统搜索广告检索框架的重新定义；在展示广告方面，一种有效的轻量网络训练框架被 AAAI 录用。 阿里妈妈在未来希望能吸引更多的新鲜技术力量，进一步加深人工智能对营销领域的驱动，赋能整个营销生态。 阿里妈妈在过去几年采用深度学习、在线学习、强化学习等人工智能技术来高效准确地预测用户的购买意向。然而，作为一个复杂的生态系统，电商平台中的用户行为偏好、商品长尾分布、热点事件营销等因素依然给转化率预估带来了巨大挑战。如何更好地利用海量的交易数据来高效准确地预测用户的购买意向，是人工智能和大数据在电子商务场景中需要继续解决的技术难题。 本次比赛以阿里电商广告为研究对象，提供海量真实场景（脱敏处理后）数据，参赛选手通过人工智能技术构建预测模型预估用户的购买意向，即给定广告点击相关的用户（user）、广告商品（ad）、检索词（query）、上下文内容（context）、商店（shop）等信息的条件下预测广告产生购买行为的概率（pCVR），形式化定义为：pCVR=P(conversion=1 | query, user, ad, context, shop)。 结合业务场景和不同的流量特点，我们定义了以下两类挑战： 1. 日常的转化率预估 2. 特殊日期的转化率预估 参赛对象 ：面向全社会开放，高等院校、科研单位、互联网企业等人员均可报名参赛。 组队规则 ：参赛队伍可以是单人组队或自由组合，但最多不超过三人。 初赛和复赛均提供下载数据，选手在本地进行算法调试，提交结果表至天池； 评测将在每天的 10:00 触发，若队伍一天内多次提交结果（包括多位队员提交），新结果版本将覆盖原版本，计算得分时以最后提交的版本为准； 排行按照 logloss 得分从低到高排序，排行榜将选择选手在本阶段的历史最优成绩进行排名展示； 最终线上成绩以 5 月 15 日 10:00 排行榜成绩与排名为准； 禁止私下与队伍成员之外的人员分享代码和数据，鼓励在大赛论坛面向所有选手进行公开讨论。 "
54,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740072&idx=4&sn=5de65ab3b7cff1c3b3c4068e1c44d835&chksm=871ad116b06d5800436086282e72682a21088afbdb15e5b8426bbc973203934db907b45717a1&scene=27,前沿 | DeepMind提出SPIRAL：使用强化对抗学习，实现会用画笔的智能体,DeepMind 近日，DeepMind 发布博客，提出一种结合了对抗训练和强化学习的智能体 SPIRAL。该智能体可与绘图程序互动，在数位画布上画画、改变笔触的大小、用力和颜色，并像街头艺人一样画画。也就是说，通过向 SPIRAL 提供人类用于描绘周围世界的工具，它们也可以生成类似的表征。 人类眼中的世界不只是角膜映射出的图像。比如，当我们看一幢建筑，赞美其设计精巧复杂时，我们能够欣赏到它的精巧工艺。通过创造事物的工具来解读事物是帮助我们理解世界的一项重要能力，也是人类智能的重要组成部分。 DeepMind 希望其系统能够按类似的方式构建对世界的丰富表征。例如，当系统观察一幅画的图像时，它们能够理解画家使用的笔触，而不只是看到屏幕上呈现的像素。 在《Synthesizing Programs for Images using Reinforced Adversarial Learning》研究中，DeepMind 给人工智能体配备了用于生成图像的工具，并展示了智能体可以推断出数字、字符和画像被创造出来的过程。关键是，它们学会这么做完全是出于自觉，没有使用人类标注的数据集。这与最近的研究《A Neural Representation of Sketch Drawings》恰恰相反，后者目前仍依赖于从人类演示中学习，是一个时间密集型的过程。 DeepMind 设计了一种深度强化学习智能体，该智能体可与计算机绘图程序（http://mypaint.org/）互动，在数位画布上画画、改变笔触的大小、用力和颜色。最初，这一未经训练的智能体下笔随意，其涂鸦没有明显的内容或结构。为了解决这个问题，DeepMind 不得不提出一种方式来奖励智能体，鼓励它生成有意义的涂鸦。 为此，DeepMind 训练出第二个神经网络，叫作判别器（discriminator），旨在预测特定画作是智能体生成的，还是来自现实照片数据集。绘画智能体所接受的奖励决定于它多大程度上能够「欺骗」判别器，使之认为其画作是真的。换言之，智能体的奖励信号是由自己学习而来。这和生成对抗网络使用的方法类似，但也有不同，因为 GAN 中的生成器通常是一个可以直接输出像素的神经网络。而 DeepMind 的智能体通过写图形程序与绘画环境互动，来生成图像。 在第一组实验中，智能体被训练来生成类似 MNIST 数字的图像，只对智能体显示数字，而没有数字生成的过程。通过尝试生成欺骗判别器的图像，智能体学会控制笔触，并绘制适合不同数字的风格，这种技术叫作视觉程序合成（visual program syhthesis）。 DeepMind 还训练它来重现特定图像。这里，判别器要确定重现出的图像是目标图像的复制，还是由智能体生成的。判别器判断二者的难度越大，智能体得到的奖励就越多。 关键是，该框架具备可解释性，因为它能生成一系列控制模拟画刷的动作。这意味着该模型可以将其学得的东西应用到模拟绘图程序上，以在其他类似环境中重新创建字符，如在模拟或真实的机械臂上。 也可以将该框架扩展到真实数据集上。在训练智能体绘制名人人脸时，它能够捕捉人脸、色调、发型的主要特征，就像一个寥寥几笔绘制人像的街头画家一样。 从原始感知中找到结构化表征是人类拥有且经常使用的能力。该研究显示通过向智能体提供人类用于描绘周围世界的工具，它们也可以生成类似的表征。这样，它们学会生成可简练表达因果关系的视觉程序。 尽管该研究只能代表朝灵活程序合成迈进的一小步，但 DeepMind 期望类似的技术可以赋予人工智能体类人感知、生成和交流的能力。 来看SPIRAL如何画出手写数字和名人肖像： 论文：Synthesizing Programs for Images using Reinforced Adversarial Learning 论文链接：https://deepmind.com/documents/183/SPIRAL.pdf 摘要：近年来，深度生成网络的进展带来了令人瞩目的成绩。但是，此类模型通常把精力浪费在数据集细节上，可能是因为其解码器的归纳偏置较弱。这样图形引擎就有了用武之地，因为图形引擎将低级别细节抽象化，并将图像表示为高级别程序。当前结合了深度学习和渲染器的方法受限于手动制作的相似度或距离函数、对大量监督信息的需求，或者将推断算法扩展至更丰富数据集的难度。为了缓解这些问题，我们提出了 SPIRAL，一种对抗训练的智能体，可以生成由图形引擎来执行的程序，以解释和采样图像。该智能体的目标是欺骗判别器网络（分辨真实数据和渲染数据），该智能体在分布式强化学习环境中进行训练，且训练过程无需任何监督。令人惊讶的是，使用判别器的输出作为奖励信号是使智能体获得期望输出渲染的关键。目前，这是在难度较高的现实世界数据集（MNIST、OMNIGLOT、CELEBA）和合成 3D 数据集上的第一次端到端、无监督和对抗逆图形（adversarial inverse graphics）智能体演示。 
55,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740072&idx=3&sn=b5e3bd23366ef3ba2264a35fa22ff496&chksm=871ad116b06d580008c54cec3bd0b5593a7d5a7bdf840f65b63c2d6b16f1d99e949c64a42eeb&scene=27,专栏 | 在PaddlePaddle上实现MNIST手写体数字识别,"不久之前，机器之心联合百度推出 PaddlePaddle 专栏，为想要学习这一平台的技术人员推荐相关教程与资源。 在 和 的介绍之后 ，本次专栏将教你如何在  PaddlePaddle 上实现 MNIST 手写数字识别。 目录 数据集的介绍 定义神经网络 开始训练模型 导入依赖包 初始化 Paddle 获取训练器 开始训练 使用参数预测 初始化 PaddlePaddle 获取训练好的参数 读取图片 开始预测 所有代码 项目代码 参考阅读 数据集的介绍 如题目所示, 本次训练使用到的是 MNIST 数据库的手写数字, 这个数据集包含 60,000 个示例的训练集以及 10,000 个示例的测试集. 图片是 28x28 的像素矩阵，标签则对应着 0~9 的 10 个数字。每张图片都经过了大小归一化和居中处理. 该数据集的图片是一个黑白的单通道图片, 其中图片如下:  该数据集非常小, 很适合图像识别的入门使用, 该数据集一共有 4 个文件, 分别是训练数据和其对应的标签, 测试数据和其对应的标签. 文件如表所示: 这个数据集针对 170 多 M 的 CIFAR 数据集来说, 实在是小太多了. 这使得我们训练起来非常快, 这能一下子激发开发者的兴趣。 在训练时, 开发者不需要单独去下载该数据集,PaddlePaddle 已经帮我们封装好了, 在我们调用 paddle.dataset.mnist 的时候, 会自动在下载到缓存目录/home/username/.cache/paddle/dataset/mnist 下, 当以后再使用的时候, 可以直接在缓存中获取, 就不会去下载了。 定义神经网络 我们这次使用的是卷积神经网络 LeNet-5，官方一共提供了 3 个分类器，分别是 Softmax 回归，多层感知器，卷积神经网络 LeNet-5，在图像识别问题上，一直是使用卷积神经网络较多。我们创建一个 cnn.py 的 Python 文件来定义一个 LeNet-5 神经网络，代码如下： 开始训练模型 我们创建一个 train.py 的 Python 文件来做训练模型。 导入依赖包 首先要先导入依赖包, 其中就包含了最重要的 PaddlePaddle 的 V2 包 初始化 Paddle 然后我们创建一个类, 再在类中创建一个初始化函数, 在初始化函数中来初始化我们的 PaddlePaddle，在初始化 PaddlePaddle 的时候，就要指定是否使用 GPU 来训练我们的模型，同时使用多少个线程来训练。 获取训练器 通过上面一步获取的分类器和图片的标签来生成一个损失函数, 通过损失函数就可以创建训练参数了。 之后也要创建一个优化方法，这个优化方法是定义学习率等等在训练中的处理。 最后通过训练参数，优化方法，损失函数这 3 个参数创建训练器 开始训练 最后就可以的开始训练了, 通过上一步得到的训练器开始训练, 训练的时候要用到 3 个参数.  第一个是训练数据, 这个训练数据就是我们的 MNIST 数据集。 第二个是训练的轮数, 表示我们要训练多少轮, 次数越多准确率越高, 最终会稳定在一个固定的准确率上。 第三个是训练过程中的一些事件处理, 比如会在每个 batch 打印一次日志, 在每个 pass 之后保存一下参数和测试一下测试数据集的预测准确率。 然后在 main 入口中调用我们的训练函数, 就可以训练了 在训练过程中会输出这样的日志： 使用参数预测 我们创建一个 infer.py 的 Python 文件，用来做模型预测的。 初始化 PaddlePaddle 在预测的时候也是要初始化 PaddlePaddle 的 获取训练好的参数 在训练的时候, 我们在 pass 训练结束后都会保存他的参数, 保存这些参数我们现在就可以使用它来预测了 读取图片 在使用图片进行预测时，我们要对图片进行处理,，处理成跟训练的图片一样，28*28 的灰度图，最后图像会转化成一个浮点数组。 开始预测 通过传入分类器，训练好的参数，预测数据这个 3 个参数就可以进行预测了。这个分类器就是我们之前定义的。 在 main 入口中调用预测函数 输出的预测结果是: 所有代码 infer.py 代码： train.py 代码： infer.py 代码： 项目代码 GitHub 地址:https://github.com/yeyupiaoling/LearnPaddle 参考阅读 "
56,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740099&idx=3&sn=29ab4153796ec98c6ee258513cb734d7&chksm=871ad1fdb06d58eb19262223f46055c5c38f706931039834d1f966836bcbcbad6e3243ab369c&scene=27,业界 | 带有韵律的合成语音：谷歌展示基于Tacotron的新型TTS方法,"选自 神经网络文本转语音（TTS）是自然语言处理领域的重要方向，很多谷歌的产品（如 Google Assistant、搜索、地图）都内置了这样的功能。目前的系统已经可以产生接近人声的语音，但仍然显得不够自然。在最近发表的两篇论文中，谷歌为自己的 Tacotron 系统加入了对韵律学的建模，以帮助人们利用自己的声音进行个性化语音合成。 最近，谷歌在基于神经网络的文本转语音（TTS）的研究上取得重大突破，尤其是端到端架构，比如去年推出的 Tacotron 系统，可以同时简化语音构建通道并产生自然的语音。这有助于更好地实现人机交互，比如会话式语音助手、有声读物朗诵、新闻阅读器和语音设计软件。但是为了实现真正像人一样的发音，TTS 系统必须学习建模韵律学（prosody），它包含语音的所有表达因素，比如语调、重音、节奏等。最新的端到端系统，包括 Tacotron 在内，并没有清晰地建模韵律学，这意味着它们无法精确控制语音的发声。这致使语音听起来很单调，尽管模型是在字词发音有明显变化的极具表现力的数据集上训练的。今天，谷歌共享了两篇新论文，有助于解决上述问题。 谷歌 Tacotron 的第一篇论文《Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron》介绍了「韵律学嵌入」（prosody embedding）的概念。我们加强了附有韵律学编码器的 Tacotron 架构，可以计算人类语音片段（参考音频）中的低维度嵌入。 我们为 Tacotron 增加了一个韵律学编码器。上图的下半部分是原始的 Tacotron 序列到序列模型。技术细节请详见我们的第一篇论文。 该嵌入捕捉独立于语音信息和特殊的说话者特质的音频特征，比如重音、语调、语速。在推理阶段，我们可以使用这一嵌入执行韵律学迁移，根据一个完全不同的说话者的声音生产语音，但是体现了参考音频的韵律。 嵌入也可以将时间对齐的精确韵律从一个短语迁移到稍微不同的短语，尽管当参考短语和目标短语的长度和结构相似时，该技术效果最好。 令人激动的是，甚至当 Tacotron 训练数据不包含说话者的参考音频时，我们也可以观察到韵律迁移。 这是一个很有希望的结果，它为语音交互设计者利用自己的声音自定义语音合成铺平了道路。你可以从网页上试听所有的音频。 Demo 链接：https://google.github.io/tacotron/publications/end_to_end_prosody_transfer/。 尽管有能力迁移带有高保真度的韵律，上述论文中的嵌入并没有将参考音频片段中的韵律与内容分开。（这解释了为什么迁移韵律对相似结构和长度的短语效果最佳）此外，它们在推断时需要一个参考音频片段。这引起了一个自然的问题：我们可以开发一个富有表现力的语音模型来缓解这些问题吗？ 这正是我们在第二篇论文《Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis》中所要做的。在第一篇论文的架构之上，我们提出了一种建模潜在语音「因素」的无监督新方法。这一模型的关键是其学习的是较高层的说话风格模式而不是时间对齐的精确的韵律学元素，前者可在任意不同的短语之中迁移。 通过向 Tacotron 多增加一个注意机制，使得它将任何语音片段的韵律嵌入表达为基础嵌入固定集合的线性组合。我们把这种嵌入称之为 Global Style Tokens (GST)，且发现它们能学习一个声纹风格中的文本无关变化（柔软、高音调、激烈等）——不需要详细的风格标签。 Global Style Tokens 的模型架构。韵律嵌入被分解成了「style tokens」，从而做到无监督的风格控制和迁移。更多技术细节，请查看文后论文。 在推理时间，我们可以选择或者调整 tokens 的结合权重，让我们能够迫使 Tacotron 使用特定的说话风格，不需要参考语音片段。例如，使用 GST，我们能创造出语音长度多样化的不同语句，更为「活泼」、「气愤」、「悲伤」等： GST 文本无关的特性使得它们能更理想的做风格迁移，采用特定风格的语音片段，将其风格转换为我们选择的任意目标语句。为了做到这一点，我们首先推理预测我们想要模仿风格的 GST 组合权重。然后，把这些组合权重馈送到模型，从而合成完整的不同语句，即使长度、结构不同，但风格一样。 最后，我们的论文表明，Global Style Tokens 不只能建模说话风格。当从 YouTube 未标记声纹的噪声语音上训练时，带有 GST 的 Tacotron 系统能学习表示噪声源，把不同声纹区分成独立 tokens。这意味着通过选择在推理中使用的 GST，我们能合成没有背景噪声的语音，或者合成数据集中特定未标记声纹的语音。这一激动人心的成果为我们打开了一条通向高延展且稳健的语音合成之路。详情可参见论文：Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis。 对以上介绍的两种研究的潜在应用和机遇，我们非常兴奋。同时，也有很多重要的研究问题亟待解决。我们期望把第一篇论文中的技术扩展到在目标声纹的天然音域范围中支持韵律迁移。我们也希望开发一种技术能够自动从语境中选择合适的韵律或者说话风格，例如结合 NLP 和 TTS。最后，虽然第一篇论文提出了一种做韵律迁移的客观与主观标准，但我们想要进一步的开发，从而帮助简历韵律评估的普遍可接受方法。 论文 1：Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron 论文链接：https://google.github.io/tacotron/publications/end_to_end_prosody_transfer/Towards%20End%20to%20End%20Prosody%20Transfer%20for%20Expressive%20Speech%20Synthesis%20with%20Tacotron.pdf 在此论文中，我们提出了对 Tacotron 语音合成架构的扩展，让它能够从包含想要韵律的声学表征中学习韵律的隐藏嵌入空间。我们表明，即使参照声纹与合成声纹不同，这种条件的 Tracotron 学习嵌入空间合成的语音在时间细节上极其匹配参照信号。此外，我们在文中展示了可使用参照韵律嵌入来合成不同于参照语句的文本。我们定义了多种定量以及主观性的度量标准，来评估韵律迁移，且随韵律迁移任务中的 Tacotron 模型采样自单个说话人和 44 个说话人的语音样本一起报告了结果。 论文 2：Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis 论文链接：https://google.github.io/tacotron/publications/global_style_tokens/Style%20Tokens%20Unsupervised%20Style%20Modeling%20Control%20and%20Transfer.pdf 在此研究中，我们提出了 global style tokens」(GST)，一个由 Tacotron 共同训练的嵌入库——后者是目前业内最佳的端到端语音合成系统。该嵌入的训练没有明确的标签，但仍然为相当广泛的语音表达能力进行了建模。GST 引出了一系列重要结果，其生成的软可解释「标签」可以用于以全新的方式控制合成，如独立于文本长度地合成不同速度与讲话语调的声音。它们也可以用于进行风格迁移，从单一语音剪辑中复制出说话风格，并用于整段长文本语料中。在经过充满噪音、无标签的数据训练之后，GST 可以学会区分噪音和说话人的声音，该研究为高度可扩展且具有鲁棒性的语音合成打开了道路。 同时，谷歌也将自己的语音合成技术在 Google Cloud 平台上开放，我们现在可以在多种应用中植入 Cloud Text-to-Speech，如让物联网设备对人类的指令做出应答，或制作自己的有声读物。 链接：https://cloud.google.com/text-to-speech/ 目前，该服务包含 32 种音色，支持 12 种语言。谷歌宣称其服务对 1 秒钟时长的语音反应速度仅为 50 毫秒，而价格为每处理 100 万字 16 美元。 "
57,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740072&idx=1&sn=9c7a1fb39de042c3488ed06f1dae896a&chksm=871ad116b06d58005ade4798c9fee43b8d514d9149f1c47bdd951b016c01f6910b7097681d2d&scene=27,模拟世界的模型：谷歌大脑与Jürgen Schmidhuber提出「人工智能梦境」,"人类可以在应对各种情况时在大脑中事先进行充分思考，那么人工智能也可以吗？近日，由谷歌大脑研究科学家 David Ha 与瑞士 AI 实验室 IDSIA 负责人 Jürgen Schmidhuber（他也是 LSTM 的提出者）共同提出的「世界模型」可以让人工智能在「梦境」中对外部环境的未来状态进行预测，大幅提高完成任务的效率。这篇论文一经提出便吸引了人们的热烈讨论。 论文在线交互地址：https://worldmodels.github.io/ 人类基于有限的感官感知开发关于世界的心智模型，我们的所有决策和行为都是基于这一内部模型。系统动力学之父 Jay Wright Forrester 将这一心智模型定义为： 「我们周围的世界在我们的大脑中只是一个模型。没有人的大脑可以想象整个世界、所有政府或国家。他只选择概念及其之间的关系，然后使用它们表征真实的系统。」[4] 为了处理我们日常生活中的海量信息，大脑学习对信息进行时空抽象化表征。我们能够观察一个场景，并记住其抽象描述 [5, 6]。有证据表明我们在任意时刻的感知都由大脑基于内部模型所做的未来预测而决定 [7, 8]。 一种理解大脑中预测模型的方式是：它可能不是预测未来，而是根据给出的当前运动动作预测未来的感官数据 [12, 13]。在面对危险时，我们能够本能地根据该预测模型来行动，并执行快速的反射行为 [14]，无需有意识地规划一系列动作。 以棒球为例 [15]。棒球击球手只有几毫秒时间来决定如何挥动球棒，而眼睛的视觉信号传到大脑所需时间比这更少。击球手能够快速根据大脑对未来的预测来行动，无需有意识地展开多个未来场景再进行规划 [16]。 在很多强化学习（RL）[17, 18, 19] 问题中，人工智能体还受益于过去和现在状态的良好表征，以及优秀的未来预测模型 [20, 21]，最好是在通用计算机上实现的强大预测模型，如循环神经网络（RNN）[22, 23, 24]。 大型 RNN 是具备高度表达能力的模型，可以学习数据丰富的时空表征。但是，文献中很多无模型 RL 方法通常仅使用具备少量参数的小型神经网络。RL 算法通常受限于信用分配问题（credit assignment problem），该挑战使传统的 RL 算法很难学习大型模型的数百万权重，因此在实践中常使用小型网络，因为它们在训练过程中迭代速度更快，可以形成优秀策略。 理想情况下，我们希望能够高效训练基于大型 RNN 网络的智能体。反向传播算法 [25, 26, 27] 可用于高效训练大型神经网络。本研究中，我们试图通过将智能体分为大型世界模型和小型控制器模型，来训练能够解决 RL 任务的大型神经网络。我们首先用无监督的方式训练一个大型神经网络，来学习智能体世界的模型，然后训练小型控制器模型来使用该世界模型执行任务。小型控制器使得算法聚焦于小搜索空间的信用分配问题，同时无需牺牲大型世界模型的容量和表达能力。通过世界模型来训练智能体，我们发现智能体学会一个高度紧凑的策略来执行任务。 尽管存在大量与基于模型的强化学习相关的研究，但本文并不是对该领域当前状况进行综述。本文旨在从 1990—2015 年一系列结合 RNN 世界模型和控制器的论文 [22, 23, 24, 30, 31] 中提炼出几个关键概念。我们还讨论了其他相关研究，它们也使用了类似的「学习世界模型，再使用该模型训练智能体」的思路。 本文提出了一种简化框架，我们使用该框架进行实验，证明了这些论文中的一些关键概念，同时也表明这些思路可以被高效应用到不同的 RL 环境中。在描述方法论和实验时，我们使用的术语和符号与 [31] 类似。 我们提出一种由人类认知系统启发而来的简单模型。在该模型中，我们的智能体有一个视觉感知模块，可以把所见压缩进一个小的表征性代码。它同样有一个记忆模块，可以根据历史信息对未来代码做预测。最后，智能体还有一个决策模块，只基于由其视觉和记忆组件创建的表征来制定行动。 2.1. VAE (V) 模型 环境在每一时间步上为我们的智能体提供一个高维输入观测，这一输入通常是视频序列中的一个 2D 图像帧。VAE 模型的任务是学习每个已观测输入帧的抽象压缩表征。 在我们的试验中，我们使用一个变分自编码器 (VAE) (Kingma & Welling, 2013; Jimenez Rezende et al., 2014) 作为 V 模型。 2.2. MDN-RNN (M) 模型 尽管在每一时间帧上压缩智能体的所见是 V 模型的任务，我们也想压缩随着时间发生的一切变化。为达成这一目的，我们让 M 模型预测未来，它可以充当 V 预期产生的未来 z 向量的预测模型。由于自然中的很多复杂环境是随机的，我们训练 RNN 以输出一个概率密度函数 p(z) 而不是一个确定性预测 z。 2.3. 控制器 (C) 模型 在环境的展开过程中，控制器 (C) 负责决定动作进程以最大化智能体期望的累加奖励。在我们的试验中，我们尽可能使 C 模型简单而小，并把 V 和 M 分开训练，从而智能体的绝大多数复杂度位于世界模型（V 和 M）之中。 2.4. 合并 V、M 和 C 下面的流程图展示了 V、M 和 C 如何与环境进行交互： 在这一章节中，我们描述了如何训练前面所述的智能体模型，并用来解决 Car Racing 任务。就我们所知，我们的智能体是解决该任务并获得预期分数的第一个解决方案。 总结而言，Car Racing 实验可以分为以下过程： 1. 从随机策略中收集 10000 个 rollouts。 2. 训练 VAE（V）将视频帧编码为 32 维的隐向量 z。 3. 训练 MDN-RNN（M）建模概率分布 P(z_{t+1} | a_t, z_t, h_t)。 4. 定义控制器（C）为 a_t = W_c [z_t, h_t] + b_c。 5. 使用 CMA-ES 求解 W_c 和 b_c 而最大化预期累积奖励。 因为我们的世界模型能够对未来建模，因此我们能自行假设或预想赛车场景。给定当前状态，我们可以要求模型产生 z_{t+1} 的概率分布，然后从 z_{t+1} 中采样并作为真实世界的观察值。我们可以将已训练的 C 放回由 M 生成的预想环境中。下图展示了模型所生成的预想环境，而该论文的在线版本展示了世界模型在预想环境中的运行。 如果我们的世界模型足够准确，足以处理手边的问题，那么我们应该能够用实际环境来替换世界模型。毕竟，我们的智能体不直接观察现实，而只是观察世界模型呈现给它的事物。在该实验中，我们在模仿 VizDoom 环境的世界模型所生成的幻觉中训练智能体。 经过一段时间训练后，我们的控制器学会在梦境中寻路，逃离 M 模型生成怪兽的致命火球攻击（fireballs shot）。 我们把在虚拟幻境中训练的智能体放在原始 VizDoom 场景中进行测试。 由于我们的世界模型只是该环境的近似概率模型，它偶尔会生成不遵循真实环境法则的轨迹。如前所述，世界模型甚至无法确切再现真实环境中房间另一端的怪兽数量。就像知道空中物体总会落地的孩子也会想象存在飞越苍穹的超级英雄。为此，我们的世界模型将被控制器利用，即使在真实环境中此类利用并不存在。 在我们的实验中，任务相对简单，因此使用随机策略收集的数据集可以训练出较好的世界模型。但是如果环境复杂度增加了呢？在难度较大的环境中，在智能体学习如何有策略地穿越其世界后，它也仅能获取世界的一部分知识。 更复杂的任务则需要迭代训练。我们需要智能体探索自己的世界，不断收集新的观测结果，这样其世界模型可以不断改善和细化。迭代训练过程（Schmidhuber, 2015a）如下： 1. 使用随机模型参数初始化 M、C。 2. 在真实环境中试运行 N 次。智能体可能在运行过程中学习。将运行中的所有动作 a_t 和观测结果 x_t 保存在存储设备上。 3. 训练 M 对 P(x_t+1, r_t+1, a_t+1, d_t+1|x_t, a_t, h_t) 进行建模。 4. 如果任务未完成，则返回步骤 2。 论文：World Models 论文链接：https://arxiv.org/pdf/1803.10122.pdf 摘要 ：我们探索构建流行的强化学习环境之下的生成神经网络。我们的「世界模型」可以无监督方式进行快速训练，以学习环境的有限时空表征。通过使用提取自世界模型的特征作为智能体的输入，我们可以训练一个非常紧密且简单的策略，解决目标任务。我们甚至可以完全通过由世界模型本身生成的虚幻梦境训练我们的智能体，并把从中学会的策略迁移进真实环境之中。 "
58,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740072&idx=2&sn=d437f1ebfd3c8dc0d7c5d116acf142e6&chksm=871ad116b06d5800446c43ce1be906fa723029146a7f6a9e1d6038e505fb649673263be0d6ee&scene=27,业界 | 谷歌大脑开源多种新型神经网络优化器，AutoML探索新架构,Google Research Blog   谷歌大脑团队希望用类似于 AutoML 发现新神经网络架构的方法，探索自动发现新优化器的实现。他们发现了多种性能优异的优化器，并已将其开源。 开源地址（TensorFlow）：https://www.tensorflow.org/api_docs/python/tf/contrib/opt 深度学习模型已在谷歌的众多产品中部署，如搜索、翻译和图片。优化方法的选择在深度学习模型的训练中发挥着重要作用。例如，尽管随机梯度下降在许多情况下都能良好运转，但更加高级的优化器速度会更快，尤其是在深度非常大的网络的训练中。然而，由于优化问题的非凸性，开发新的神经网络优化器颇具挑战。Google Brain 团队希望用类似于 AutoML 发现新的有竞争力的神经网络架构的方法，探索是否可能自动发现新的优化器。 在《Neural Optimizer Search with Reinforcement Learning》中，谷歌提出了一种发现优化方法的方法，其优化重点是深度学习架构。谷歌使用这种方法发现了两个新的优化器——PowerSign 和 AddSign，它们在各种不同的任务和架构中颇具竞争力，包括 ImageNet 分类器和谷歌的神经机器翻译系统。为了帮助其他人从这项工作中受益，谷歌已在 Tensorflow 将该优化器开源。 神经优化器搜索（Neural Optimizer Search）利用循环神经网络控制器，该控制器可以访问与优化相关的简单原语列表。这些原语包括梯度或梯度的运行平均值，并具有 1010 个以上可能组合的搜索空间。然后，控制器为该搜索空间中的候选优化器或更新规则生成计算图。 该论文利用候选更新规则 ( U ) 在 CIFAR10 上对一个子卷积神经网络执行几个 epoch 的训练，最后将验证准确率 ( R ) 作为奖励反馈给控制器。通过强化学习训练控制器，以最大化采样更新规则的验证准确率。该过程如下图所示： 有趣的是，我们发现的优化器是可解释的。例如，在 PowerSign 优化器中，我们将每一次更新与梯度信号和梯度的运行平均值进行对比，根据这两个值是否一致来调整步长大小。如果是，则当前的更新方向更加可靠，从而步长可以增大。谷歌还发现了一种简单的学习率衰减方案即线性余弦衰减（linear cosine decay），它可以带来更快的收敛。 神经优化器搜索发现了在小规模卷积网络上可超越常用优化器的多种优化器。部分优化器可以很好地迁移到其它的任务上，谷歌发现 PowerSign 和 AddSign 将最先进的 ImageNet 移动尺寸模型的 top-1 和 top-5 精度提高了 0.4 %。它们在谷歌的神经机器翻译系统中也能良好运行，在英德翻译任务中的 BLEU 值提高了 0.7。 神经优化器搜索不仅能提高机器学习模型的性能，还可能带来新的可解释优化器方程和更多发现。 原文链接：https://research.googleblog.com/2018/03/using-machine-learning-to-discover.html 
59,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740005&idx=4&sn=41c6a368fe03c76cc60d019300ec38d1&chksm=871ad15bb06d584d57ddd1c74394aa07abd942fd7daf57ccf1ff388dc5f35da33c5591c0c8da&scene=27,入门 | 我们常听说的置信区间与置信度到底是什么？,TowardsDataScienceR Dima Shulga 机器学习本质上是对条件概率或概率分布的估计，而这样的估计到底有多少是置信度？这里就涉及到统计学里面的置信区间与置信度，本文简要介绍了置信区间这一核心概念，它有助于我们从直观上理解评价估计优劣的度量方法。 本文讨论了统计学中的一个基本术语 ：置信区间。我们仅以一种非常友好的方式讨论一般概念，没有太多花哨的统计术语，同时还会使用 Python 完成简单的实现！尽管这个术语是非常基础的，但我们有时很难完全理解置信区间到底是什么，为什么我们需要它。 假设你想知道美国有多少人热爱足球。为了得到 100％ 正确的答案，你可以做的唯一一件事是向美国的每一位公民询问他们是否热爱足球。根据维基百科，美国有超过 3.25 亿的人口。与 3.25 亿人谈话并不现实，因此我们必须通过问更少的人来得到答案。 我们可以通过在美国随机抽取一些人（与更少人交谈）并获得热爱足球的人的百分比来做到这一点，但是我们不能 100％ 确信这个数字是正确的，或者这个数字离真正的答案有多远。所以，我们试图实现的是获得一个区间，例如，对这个问题的一个可能的答案是：「我 95％ 相信在美国足球爱好者的比例是 58％ 至 62％」。这就是置信区间名字的来源，我们有一个区间，并且我们对它此一定的信心。 非常重要的是我们的样本是随机的，我们不能只从我们居住的城市中选择 1000 人，因为这样就不能很好地代表整个美国。另一个不好的例子是，我们不能给这 1000 个随机用户发 Facebook 消息，这样我们就会得到美国 Facebook 用户的喜爱趋势，因为并不是所有的美国公民都使用 Facebook。 因此，假设我们随机抽取了 1000 个美国人的样本，我们发现，在 1000 人中有 63% 的人喜欢足球，我们能假设（推断）出整个美国人口的情况吗？ 为了回答这个问题，我希望我们以一个不同的方式来看待它。假设我们知道（理论上）美国人的确切比例，假设它是 65％，那么随机挑选 1000 人只有 63％ 的人喜欢足球的机会是多少？让我们用 Python 来探索这个问题！ 在这段代码中，我创建了一个表示 3.25 亿人的 NumPy 数组，对于每个人，如果他/她喜欢足球，那么我会存储 1，否则就是零。我们可以通过计算它的平均值来得到数组中的百分比，实际上它是 65％。 现在，让我们取几组容量为 1000 个样本的试验，看看得到的百分比是多少： 对于每组样本，我们获得了不同的值，但直觉（和统计理论）表示，大量样本的平均值应该非常接近真实百分比。让我们这样试试！我们取很多样本，然后看看会发生什么： 我们创建了 10K 个样本，检查了每个样本中热爱足球的人的百分比，然后取平均值，我们得到了 64.98％，这非常接近于实际值 65％。让我们画出我们得到的所有值： 这里你看到的是我们得到的所有样本值的直方图，这个直方图的一个很好的性质是它和正态分布非常相似。正如我所说的，我不想在这里使用太多的统计术语，但假设如果我们这样做了很多次（无限次），我们将得到一个非常接近正态分布的直方图，我们可以知道该分布的参数。用更简单的话来说，我们会知道这个直方图的形状，所以我们可以精确地知道在任意数值范围内有多少个样本。 下面是一个例子，我们会多次运行这个模拟（试图达到无穷大）： 首先，我们可以看到直方图的中心（平均值）接近 65％，正如我们所预期的，但我们可以通过查看直方图来得到更多信息，例如，我们可以说，一半样本都大于 65％，或者我们可以说大约 25％ 的样本大于 67％，甚至可以说（大致）只有 2.5％ 的样本大于 68％。 在这一点上，很多人可能会问两个重要的问题：「我怎样才能取得无数的样本？」和「它对我有什么帮助？」。 让我们回到我们的例子，我们抽取了 1000 人的样本，得到了 63％，我们想知道，随机抽样的 1000 人中有 63％ 的足球爱好者的概率是多少。使用这个直方图，我们可以说有（大概）25％的概率，我们会得到一个小于或等于 63％ 的值。该理论告诉我们，我们实际上并不需要得到无限的样本，如果我们随机选择 1000 人，只有 63％ 的人喜欢足球是可能发生的。 实际上，为了找到不同数值范围或区间的概率，我们需要知道或至少估计总体分布的标准差。因为我们想把事情变得简单一点，因此现在先不讨论它。 让我们回到现实和真正的问题，我不知道美国足球爱好者的实际比例，我只抽取了一个样本，得到了 63％，这对我有什么帮助？ 所以，我们不知道在美国热爱足球的人的实际比例。我们所知道的是，如果我们从总体分布取无数个样本，它将如下所示： 这里 μ 是总体分布的平均值（我们例子中足球爱好者的实际百分比），σ 是总体分布的标准差。 如果我们知道这一点（并且我们知道标准差），我们可以说约 64％ 的样本会落在红色区域，或者 95％ 以上的样品会落在图中的绿色区域之外： 如果我们在之前假设的实际百分比 65％ 上使用该图，那么 95％ 以上的样本将在 62％ 和 68％ 之间（+ - 3）。 当然，距离是对称的，所以如果样本有 95% 落在在实际百分比 -3 和 +3 之间，那么真实百分比落在样本百分比 -3 和 +3 之间的概率为 95％。 如果我们抽取一个样本，得到了 63％，那么我们可以说我们 95％ 确信实际比例在 60％（63-3）和 66％（63 + 3）之间。 这就是置信区间，区间为 63 + -3，置信度为 95％。 我希望大家现在对置信区间有更好的理解，但这个介绍忽略了一些重要的技术性的部分。有很多文章包含了这些部分，因此读者可继续阅读相关的材料加强理解。 
60,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740005&idx=5&sn=445df39753f67104d71e86c8e900feda&chksm=871ad15bb06d584da3361ceb371f36476d916b30596d576051c39f0c849adfb4978adf086bdf&scene=27,学界 | UCSB提出变分知识图谱推理：在KG中引入变分推理框架,"arXiv 推理知识图谱中缺失的连接已经吸引了研究界的广泛关注。在本论文中，加州大学圣塔芭芭拉分校的王威廉等研究者在知识图谱推理中引入了变分推理框架，并将路径搜索和路径推理紧密结合从而进行联合推理，这种方法提升了知识图谱推理模型的稳定性。 自动推理（Automated reasoning）作为计算系统根据观察到的证据做出新推论的一种能力，已经引起了很多研究团体的关注。近年来，人们对为复杂推理任务设计机器学习算法的兴趣一浪高过一浪，尤其在大型知识图谱（KGs）方面，数不清的实体和连接让传统基于逻辑的算法面临巨大挑战。具体来说，我们将研究定位于这种大型知识图谱多跳（multi-hop）推理的情境，目标是设计一个自动推理模型，以完善大型知识图谱中现有实体间缺失的连接。例如，若知识图谱中包含「总统」（贝拉克·奥巴马，美国）及其配偶（米歇尔，贝拉克·奥巴马）这两个实体，我们要让机器来自动完善隐藏其中的连接（米歇尔，美国）。实现该任务的系统在解答复杂问题的应用中必不可少。 为了处理多跳连接推测的问题，人们提出过各种各样的方法。一些诸如 PRA（Path Ranking Algorithm，路径排序算法）(Lao et al., 2011; Gardner et al., 2014, 2013) 的早期研究使用可重复启动的有界深度（bounded-depth）随机游走来获取路径。最近 DeepPath (Xiong et al., 2017) 和 MINERVA (Das et al., 2017) 将路径搜索问题设置为一个马尔可夫决策过程（MDP）并利用强化学习（RL）将预期的返回值最大化。与我们的工作同时进行的另一项工作是「推理链」(Chain-of-Reasoning，Das et al., 2016) 和复合推理 (Compositional Reasoning，Neelakantan et al., 2015) 的研究，即以 PRA 学习到的多跳链为输入来推断其关系。 这里我们将 KG 推理任务设置为两个子步骤，即「路径搜索」和「路径推理」。我们发现大多数相关研究只专注于其中一步，且其主要缺陷在于忽视了两个步骤间的交互。更确切地说，DeepPath (Xiong et al., 2017) 和 MINERVA (Das et al., 2017) 可以被理解为「路径搜索」步骤的优化，而复合推理 (Neelakantan et al., 2015) 和推理链 (Das et al., 2016) 可被认为是「路径推理」这一步的优化。DeepPath 经训练以用来使两个给定实体间的路径搜索更有效率，但无法得知实体对之间的联系存在与否；相对地，给定一对查询实体，MINERVA 会学习到达目标节点，但又无法得知搜索路径的质量。相比来说，推理链和复合推理仅仅是推断给定关系的路径，但无法得知路径搜索程序的过程。缺乏交互使得模型无法理解更多样化的输入，而且对噪声和对抗样本非常敏感。 为了提高现有 KG 推理模型的稳定性并处理更多有噪声的环境，我们打算从潜变量图模型的视角，将两个步骤结合为一个整体。这个图模型在给定实体对的条件下将路径视为离散的潜变量，将关系看作显变量，这样路径搜寻模块可以被视为使用先验分布来推断 KG 中的底层连接。与之相对，路径推理模块可被视为把底层连接划分为不同类别的似然分布。在这一假设下，我们引入近似后验，并设计了一个变分自编码 (Kingma and Welling, 2013) 算法以最大化下界。这个变分架构将两个模块紧密结合为统一的整体，并对其同时进行训练。通过积极合作和交互，路径搜索可以考虑到搜索到路径的价值并使用更有意义的路径。与此同时，路径推理模块会收到路径搜索模块传来的多样性路径，从而更好地归纳未知的情境。我们的贡献在于以下三点： 在 KG 推理中引入了变分推理框架，将路径搜索和路径推理紧密结合从而进行联合推理。 成功地在训练中加入反面样本，同时提高了现有 KG 推理模型的稳定性。 本文的模型可以扩展到大型 KG，并在两项任务中得到最高水平的结果。 论文的其它部分结构如下：在第 2 部分我们将概述 KG 嵌入、多跳推理以及变分自编码的相关研究；在第 3 部分描述我们的变分知识推理工具 DIVA；第 4 部分展示了试验结果；第 5 部分为结论。 论文：变分知识图谱推理 论文链接：https://arxiv.org/abs/1803.06581 推理知识图谱中缺失的连接已经吸引了研究界的广泛关注。在本文中，我们处理了一类包含推理给定实体对间关系的实际查询任务。我们将这类问题设计为一个概率图模型下的推理问题并试图从变分推理的视角解决它。为了建立查询实体对的关系模型，我们假设 KG 中存在潜变量（所有连接这两个节点的路径集合），即它们之间的联系。但由于大型 KG 内的关联问题难以解决，我们提出用变分推理来使 ELBO 最大化。更确切地说，我们的框架（DIVA）由三个模块组成——后验近似、先验（路径搜索）以及似然估计（路径推理）。通过变分推理方法，我们成功将三者紧密结合为一个统一的架构，同时对其联合优化以实现 KG 推理。随着伴随子模块间的积极交互，DIVA 可以更好地处理噪声并应付更复杂的推理情境。为评估我们的方法，我们基于 NELL-995 和 FB15k 数据集执行了连接推理任务试验，而在两个数据集下的表现都达到了很高的水准。 "
61,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740005&idx=2&sn=4d0ebf36f2334f5dbe742b4a22e85f87&chksm=871ad15bb06d584daca67f95a9ded51330d2c1e36d4acbce56a496e4558068267a89f147af86&scene=27,业界 | Waymo捷豹发布全球首辆全自动驾驶电动汽车 I-PACE（附视频）,当地时间 3 月 27 日上午，谷歌旗下自动驾驶公司 Waymo 宣布推出全世界第一台全自动驾驶电动车。 这款自动驾驶电动车基于捷豹的首款纯电动汽车 I-pace，原型车将在 2018 年底上路进行公开测试，并于 2020 年开始商业叫车服务。 同时，Waymo 还与捷豹路虎达成了长期合作协议。Waymo 和捷豹路虎的工程师将协同工作，在造车初期就着手向自动驾驶汽车的方向打造，而非让汽车离开了流水生产线后再进行装配改造。 公司计划，在自动驾驶电动车项目启动的前两年内生产 20000 辆汽车，目标每天能够提供 100 万次旅程服务。但目前尚不清楚将在哪个城市率先落地。 事实上，捷豹路虎此次与 Waymo 达成合作并不让人意外。在其位于考文垂的总部附近，这家英国汽车制造商已经进行过 44 英里自动驾驶技术行驶里程的测试。去年夏天，捷豹路虎还向 Lyft 注资 2500 万美元。根据协议，捷豹路虎会给 Lyft 的测试运营提供一部分车辆。 发布会现场，捷豹路虎全球 CEO Ralf Speth 表示：「每家车厂都想推出能够给行业带来变革的颠覆性产品，但是这非常难以实现的。今天我们之所以推出捷豹 I-pace 自动驾驶汽车，就是希望能够做到这一点，而 Waymo 就是帮助我们实现梦想的合作伙伴。」 这一次，Waymo 直接将特斯拉 Model X 作为自己的竞争对手。 作为全自动驾驶电动汽车的「原形」，捷豹 I-pace 采用小尺寸跑车型 SUV 设计（长宽高 4682*2011*1565），续航里程达 500 公里。不过，个头比它大的 Model X，其最高续航里程达到了 552 公里。 但就出行服务而言，I-pace 的这个数字已能满足大部分短途需求。可以说，Waymo 自动驾驶车队能够提供的出行服务，又多了一种类型和选择。 Waymo 公司 CEO John Krafcik 表示，Waymo 已经是一家独立的公司，将把重点从研究转向技术的落地与部署。 迄今为止，Waymo 的自动驾驶汽车已在美国的道路上进行了超过 500 万英里的实际道路驾驶，另有 40 亿英里的模拟环境驾驶，这个数字是任何其他公司所无法企及的。 「Waymo 的自动驾驶汽车拥有全世界最丰富的自动驾驶经验，以及最强大的自动驾驶 AI 软件。」John Krafcik 如是说。 但同样不可忽视的是，2016 年 10 月，特斯拉已经表示将在其生产的所有新电动汽车上安装硬件，以使其可以采用全自动驾驶技术。特斯拉还运用「影子」模式收集数据来判断自动驾驶是否比人为驾驶安全。而马斯克也曾表示，特斯拉要在 2019 年前实现全自动驾驶。 现在问题来了，究竟谁将赢得这场全自动驾驶电动汽车的竞赛？ 
62,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740005&idx=3&sn=947d138f9edf1421a134b901e884383b&chksm=871ad15bb06d584dd5d23029567fd197b1793034c41e9567b49cf30ff5b664b31dc9c4192c96&scene=27,教程 | 在Keras上实现GAN：构建消除图片模糊的应用,"Sicara Blog 2014 年，Ian Goodfellow 提出了生成对抗网络（GAN），今天，GAN 已经成为深度学习最热门的方向之一。本文将重点介绍如何利用 Keras 将 GAN 应用于图像去模糊（image deblurring）任务当中。 Keras 代码地址：https://github.com/RaphaelMeudec/deblur-gan 此外，请查阅 DeblurGAN 的原始论文（https://arxiv.org/pdf/1711.07064.pdf）及其 Pytorch 版本实现：https://github.com/KupynOrest/DeblurGAN/。 生成对抗网络简介 在生成对抗网络中，有两个网络互相进行训练。生成器通过生成逼真的虚假输入来误导判别器，而判别器会分辨输入是真实的还是人造的。 训练过程中有三个关键步骤： 使用生成器根据噪声创造虚假输入； 利用真实输入和虚假输入训练判别器； 训练整个模型：该模型是判别器和生成器连接所构建的。 请注意，判别器的权重在第三步中被冻结。 对两个网络进行连接的原因是不存在单独对生成器输出的反馈。我们唯一的衡量标准是判别器是否能接受生成的样本。 以上，我们简要介绍了 GAN 的架构。如果你觉得不够详尽，可以参考这篇优秀的介绍： 。 Ian Goodfellow 首先应用 GAN 模型生成 MNIST 数据。而在本教程中，我们将生成对抗网络应用于图像去模糊。因此，生成器的输入不是噪声，而是模糊的图像。 我们采用的数据集是 GOPRO 数据集。该数据集包含来自多个街景的人工模糊图像。根据场景的不同，该数据集在不同子文件夹中分类。 你可以下载简单版：https://drive.google.com/file/d/1H0PIXvJH4c40pk7ou6nAwoxuR4Qh_Sa2/view 或完整版：https://drive.google.com/file/d/1SlURvdQsokgsoyTosAaELc4zRjQz9T2U/view 我们首先将图像分配到两个文件夹 A（模糊）B（清晰）中。这种 A&B 的架构对应于原始的 pix2pix 论文。为此我创建了一个自定义的脚本在 github 中执行这个任务，请按照 README 的说明去使用它： https://github.com/RaphaelMeudec/deblur-gan/blob/master/organize_gopro_dataset.py 训练过程保持不变。首先，让我们看看神经网络的架构吧！ 生成器 该生成器旨在重现清晰的图像。该网络基于 ResNet 模块，它不断地追踪关于原始模糊图像的演变。本文同样使用了一个基于 UNet 的版本，但我还没有实现这个版本。这两种模块应该都适合图像去模糊。 其核心是应用于原始图像上采样的 9 个 ResNet 模块。让我们来看看 Keras 上的代码实现！ ( , ) ( , ) 该 ResNet 层基本是卷积层，其输入和输出都被添加以形成最终的输出。 按照计划，9 个 ResNet 模块会应用于输入的上采样版本。我们在其中添加了从输入到输出的连接，并对结果除以 2 以保持标准化输出。 这就是生成器的架构！让我们继续看看判别器怎么做吧。 判别器 判别器的目标是判断输入图像是否是人造的。因此，判别器的体系结构是卷积以及输出单一值。 最后一步是构建完整的模型。本文中这个生成对抗网络的特殊性在于：其输入是实际图像而非噪声。因此，对于生成器的输出，我们能得到直接的反馈。 让我们一起看看，如何利用两个损失函数来充分利用这种特殊性。 损失函数 我们在两个级别提取损失函数：生成器的末尾和整个模型的末尾。 前者是一种知觉损失（perceptual loss），它直接根据生成器的输出计算而来。这种损失函数确保了 GAN 模型面向一个去模糊任务。它比较了 VGG 第一批卷积的输出值。 而后者是对整个模型的输出执行的 Wasserstein 损失，它取的是两个图像差异的均值。这种损失函数可以改善生成对抗网络的收敛性。 训练过程 第一步是加载数据并初始化所有模型。我们使用我们的自定义函数加载数据集，同时在我们的模型中添加 Adam 优化器。我们通过设置 Keras 的可训练选项防止判别器进行训练。 然后，我们启动 epoch 并将数据集分成不同批量。 最后，根据两种损失，我们先后训练判别器和生成器。我们用生成器产生虚假输入，然后训练判别器来区分虚假输入和真实输入，并训练整个模型。 你可以参考如下 Github 地址查看完整的循环： https://www.github.com/raphaelmeudec/deblur-gan 材料 我使用了 Deep Learning AMI（3.0 版本）中的 AWS 实例（p2.xlarge）。它在 GOPRO 数据集上的训练时间约为 5 小时（50 个 epoch）。 图像去模糊结果 上面的输出是我们 Keras Deblur GAN 的输出结果。即使是在模糊不清的情况下，网络也能够产生更令人信服的图像。车灯和树枝都会更清晰。 其中的一个限制是图像顶部的噪点图案，这可能是由于使用 VGG 作为损失函数引起的。 希望你在这篇「基于生成对抗网络进行图像去模糊」的文章中度过了一段愉快的阅读时光！ 论文：DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks  论文地址：https://arxiv.org/pdf/1711.07064.pdf 摘要： 我们提出了一种基于有条件 GAN 和内容损失函数的运动去模糊的端到端学习方法——DeblurGAN。在结构相似性测量和视觉外观方面，DeblurGAN 达到了业内最先进的技术水平。去模糊模型的质量也以一种新颖的方式在现实问题中考量——即对（去）模糊图像的对象检测。该方法比目前最佳的竞争对手速度提升了 5 倍。另外，我们提出了一种从清晰图像合成运动模糊图像的新方法，它可以实现真实数据集的增强。 模型、训练代码和数据集都可以在以下地址获得：https://github.com/KupynOrest/DeblurGAN。 "
63,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650740005&idx=1&sn=37ac00d05be6a6e339e5c7b729a59524&chksm=871ad15bb06d584d4e30f5310fc3f7bcfdb04ab6be44cd433491bc62f97493b29250370633ef&scene=27,英伟达刚刚发布全球最大GPU：GTC2018黄仁勋演讲核心内容都在这了,"昨天，第九届年度 GPU 技术大会（GTC）在加州圣何塞 McEnery 会议中心正式开幕。在刚刚结束的 Keynote 演讲中，英伟达创始人兼首席执行官黄仁勋宣布了该公司在芯片、AI 平台、自动驾驶上的一系列新动作。在本文中，机器之心对其演讲的核心内容做了梳理。 正如黄仁勋所说的，今天的发布会有关于：「Amazing science, amazing graphics, amazing products and amazing AI.」 核心内容： 新一代服务器级 GPU：搭载英伟达 RTX 技术的 GPU Quadro GV100，以及「世界最大的 GPU」 NVIDIA AI 平台：TensorRT 4 等技术，多种重大改进 推出 DRIVE Constellation 自动驾驶仿真系统 图注：黄仁勋 Keynote 演讲总结 在今天的 GTC 大会 Keynote 中，黄仁勋首先宣布推出搭载 NVIDIA RTX 技术的 Quadro GV100 GPU，首次向数以百万计的艺术家和设计师提供实时光线追踪技术。 黄仁勋表示，结合强大的 Quadro GV100 GPU，NVIDIA RTX 能够在运行专业设计及内容创作类应用程序的同时，实现实时的计算密集型光线追踪。 Quadro GV100 具有 32GB 内存，且可借助 NVIDIA NVLink 2 互联技术，通过并联两块 Quadro GPU 扩展至 64GB，在所有适用于此类应用的平台中其性能最高。 在性能方面，GV100 基于 NVIDIA Volta GPU 架构，可提供每秒 7.4 万亿次浮点运算的双精度性能、每秒 14.8 万亿次浮点运算的单精度性能、以及每秒 118.5 万亿次浮点运算的深度学习性能。NVIDIA RTX 内置的 NVIDIA OptiX AI-denoiser 可实现实时的 AI 去噪，英伟达表示且其性能相当于采用 CPU 时的 100 倍。 而后，如同往届，黄仁勋对英伟达 AI 平台做了介绍，公布了其中的一系列重要进展，包括全新 Tesla V100 32GB GPU 的 2 倍内存、革命性的 NVSwitch 结构、以及全面的软件堆栈推动性能提升、深度学习工作站 DGX-2 成为首款性能高达每秒 2 千万亿次浮点运算的深度学习系统、发布深度学习引擎 TensorRT 4 等。英伟达表示，相较于六个月前发布的上一代产品 DGX-1，其深度学习工作负载性能实现了 10 倍提升。 在大会上，黄仁勋宣布，新版的 Tesla V100 内存扩容了一倍。「5 年前 AlexNet 在 ImageNet 上展示了突破性的能力，」黄仁勋说道，「它有 8 层，数百个参数。而今天我们能够看到数百层的神经网络，内含数十亿参数，深度学习模型经过五年的发展，体量扩大了 500 倍。」 而这样的计算需求可由「世界上最大的 GPU」DGX-2 进行处理，它是由 16 块 32GB 内存的 Tesla V100 计算卡通过 NVSwitch 进行连接（显卡间的通信速度是 PCI 的 20 倍，每秒 300Gbyte）所组成的，共拥有 2000TFPLOS 的 Tensor Core 算力，售价 39.9 万美元。NVSwitch 是今天黄仁勋宣布的全新的 GPU 互联结构。 DGX-2 是首款能够提供每秒两千万亿次浮点运算能力的单点服务器，具有 300 台服务器的深度学习处理能力，占用 15 个数据中心机架空间，而体积则缩小 60 倍，能效提升 18 倍。 而后，黄仁勋宣布了英伟达在 AI 推理上的一系列动作。黄仁勋表示，基于在数据中心、汽车应 用、以及包括机器人和无人机等嵌入式设备领域中，诸如语音识别、自然语言处理、推荐系统、 以及图像识别等新功能的支持，面向深度学习推理的 GPU 加速正在获得越来越多的关注。 「我们需要超级计算机来帮助自己寻找更高效的能源存储方法，探索地球的内部，预测未来的自然灾害，以及模拟微观世界的变化。」黄仁勋说道。 英伟达宣布推出新版 TensorRT 推理软件 TensorRT 4，并将 TensorRT 集成至谷歌的 TensorFlow 框架。 英伟达表示，TensorRT 4 可用于快速优化、验证及部署在超大规模数据中心、嵌入式与汽车 GPU 平台中经过 训练的神经网络。相比 CPU，针对计算机视觉、神经网络机器翻译、自动语音识别、语音合成 与推荐系统等常见应用，该软件最高可将深度学习推理的速度加快 190 倍。而且为了进一步精简开发，英伟达与谷歌的工程师已将 TensorRT 集成至 TensorFlow 1.7，使得在 GPU 上运行深度学习推理应用更加容易。 此外，英伟达还宣布了面向 Kubernetes 的 GPU 加速，以促进企业在多云 GPU 集群上的推理部署。英伟达将针对开源社群强化 GPU 性能，以支持 Kubernetes 生态系统。 自动驾驶一直是 GTC 大会的重要部分，今天，英伟达展示了一套用于使用照片级真实感模拟，基于云的自动驾驶汽车测试系统。 该系统被称为 NVIDIA DRIVE Constellation，是一种基于两种不同服务器的计算平台。第一台服务器运行 NVIDIA DRIVE Sim 软件，用以模拟自动驾驶汽车的传感器，如摄像头、激光雷达和雷达。第二台服务器搭载了 NVIDIA DRIVE Pegasus AI 汽车计算平台，可运行完整的自动驾驶汽车软件堆栈，并能够处理模拟数据，这些模拟数据如同来自路面行驶汽车上的传感器。 要实现自动驾驶汽车的量产部署，我们需要一种能够在数十亿英里的行驶中进行测试和验证的解决方案，以实现足够安全性和可靠性。黄仁勋介绍说，DRIVE Constellation 可以将视觉计算和数据中心方面的专业知识相结合以实现这一目标。借助虚拟现实技术，测试者可通过对数十亿英里的自定义场景和极端情况进行测试，从而提高算法的稳定性，而花费的时间和成本仅为实际道路测试的一小部分。 此外，英伟达还推出了机器人开发平台 ISSAC 等工具。同时宣布与 ARM 展开合作。两家公司正在合作将开源的 NVIDIA 深度学习加速器 NVDLA 架构集成到 Arm 的 Project Trillium 平台上，以实现机器学习。此次合作将使物联网芯片公司能够轻松地将 AI 集成到自己的设计中，并帮助它们将智能化且价格低廉的新产品带给全球数十亿的消费者。 英伟达 GTC 大会从 2009 年开办以来，越来越受到人们的关注。而随着人工智能的火热，GPU 价值也水涨船高。而本届 GTC 相比于第一届，参会人数增加了近 10 倍，火热程度也超乎以往。但遗憾的是，今天的发布仍然围绕商用计算设备进行，不像国内外众多媒体猜测的那样会发布新一代 Geforce 显卡。或许，众多玩家还要继续等待一段时间。 "
64,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739982&idx=1&sn=271fe04e0eea989cb4c55d1c96e7964e&chksm=871ad170b06d5866148ecf0e54b09691fb7ec6f9387dd83847fcb90ee9249ad9a2c9d3fef874&scene=27,活在实验室还是实现霸权？揭开当前量子计算技术进展之谜,IEEE Spectrum 随着谷歌要在今年实现「量子霸权」等新闻的出现，社交网络上最近出现了一个热门的话题：当前的量子计算技术前沿是什么水平？量子计算和人工智能一样，是目前人类科技发展的重要方向。在摩尔定律逐渐失效的今天，科技巨头和创业公司无不想使量子计算成为主流，但成功与否依然未知。本文作者探访了数位一线研究人员，试图为你揭开当前人类量子技术水平的谜团。 值得注意的是，其中大多数人都对目前量子计算技术的进展持谨慎态度，正如其中一位科学家所说的：「我不认为那些鼓吹量子计算机将很快能够解决现实世界问题，或实现商用化的人是完全诚实的。」 你一定知道「薛定谔的猫」，那只同时处在生和死的状态的猫。现在我们来认识一下「薛定谔的科学家」，他们同时处在一种既高兴又惊恐的怪异状态之中。 薛定谔的著名思想实验再次以新的形式出现，因为量子研究者正处在孜孜追求的成功的风口浪尖：打造一台传统计算机无法匹敌的量子计算机。数年来他们坚持与认为量子计算机只不过是科学幻想的唱反调的人论战，现在他们终于有了自我祝贺的资格。 但是同时他们也在抨击媒体的炒作，后者过分夸大了量子计算的进展。比如，《时代》杂志 2014 年 2 月 17 日量子计算专题中，编辑在封面上写到：「无限机」（the Infinity Machine）如此具有革命性以至于其可解决人类一些最复杂的问题。自此之后，媒体的炒作之风一发不可收拾。 科罗拉多大学波尔得分校的量子计算研究员 Graeme Smith 解释了现今这一领域面对的难题，他说：「过去你在这一领域工作，如果你告诉每个人量子计算未来大有前景，那么你一定是个乐观主义者；现在情况改变了，当有人讲量子计算机很快会解决所有问题时，我和同僚们简直无法相信。大家争先恐后地声明量子计算机的用途，这看上起像极了一场恶性竞争。」 目前激动人心的原因是，就在今年某个时候，量子计算有望取得一个里程碑式的成果。在谷歌和 IBM 研究小组的领导之下，科学家预计实现「量子霸权」。这意味着该系统能解决传统现有计算机没有内存或处理能力来解决的问题。 尽管标题党们一再宣称量子计算的到来不可避免，但其成就相比炒作会打折扣。首先，谷歌用以运行以展示量子霸权的算法并未做出任何实际重要的事情：超出目前任何传统计算机的计算能力的问题。 构建人们实际关心的、可解决实际问题的量子计算机需要长年的研究。谷歌和 IBM 的量子计算工程师说道，确实，能够解决最棘手的计算问题的量子计算机可能还要再等数十年。 即使这样，实际上该领域没人会期望量子计算机取代传统计算机——尽管随着摩尔定律失效，人们普遍相信量子计算时代呼之欲出。目前所有量子计算机的设计都是将其与传统计算机配对，执行无数的预处理和后处理步骤。更重要的是，考虑到让量子计算机工作的软硬件开销，现在许多可以在传统计算机上快速执行的日常编程任务实际上在量子计算机上可能运行得更慢。 曾在 NIST 工作多年，后来加入微软雷德蒙德研究院的量子研究员 Stephen Jordan 说：「我并不认为有人会希望量子计算机取代传统计算机。」而且，量子计算机很可能只对现有计算机无法处理的、回报巨大的特定计算工作有帮助。 量子计算机的想法最早可追溯到诺贝尔奖得主、物理学家 Richard Feynman 在 1981 年的一次演讲，其中他设想了通过亚原子粒子的特有属性建模其他亚原子粒子行为的可能性。曾工作于 AT&T 贝尔实验室、现在 MIT 的 Peter Shor 在 1994 年的论文《Algorithms for Quantum Computation: I Discrete Logarithms and Factoring》中提出了更好的设想：如果可以打造一台量子计算机，找到大数的质因子，就可以破解常用的公钥加密系统。这样一台计算机将从根本上瓦解互联网。 这引起了很多人的关注，特别是涉及加密的美国安全机构，他们很快开始投资量子硬件研究；在过去的二十年，政府开销达数十亿美元。现在量子技术更加接近商业化，风投资本也开始行动，这一现象与目前的量子炒作程度很相关。 那么，量子计算机到底是怎么工作的？ 给出一个扼要而易懂的解释并非易事，这就是为什么 2016 年 4 月加拿大总理 Justin Trudeau 成了极客英雄。在一次新闻发布会现场（后来在网上迅速传播开来），Trudeau 解释道：「传统的计算机只有 1 或 0，是二值系统；而量子态允许更复杂的信息被编码进单一比特。」 量子计算机的主要构件模块是量子比特（qubit），任何量子性质，例如电子能级、自旋或光子的量子态等都可以用来表征量子比特，只要系统可以将其隔离并控制它们。一个量子比特只有两个状态，而 n 个量子比特最多可以表示 2 的 n 次方个状态。 例如，为了运行一个特定程序，某些量子计算机使用电磁波脉冲序列来操控量子比特，每个脉冲都具有确定的频率和确定的持续时长。这些脉冲就是量子程序的指令（门操作）。每个指令都导致未被测量的量子比特的状态以特定方式进行演化。 这些脉冲操作不仅仅在一个量子比特上进行，而是在所有的量子比特上进行，通常每个量子比特或每个集群的量子比特接收不同的脉冲指令。量子计算机的量子比特通过纠缠相互作用，纠缠使这些量子比特的状态互相关联。在这里最重要的是，对量子比特的状态的相继改变可以用于执行有用的计算。 一旦量子程序完成执行——数千甚至上百万个激光脉冲的作用——量子比特将被测量以输出计算的最终结果。测量操作使得每个量子比特变成 0 或 1，即量子力学中著名的波函数坍缩。 这是量子计算机开发中需要直接面对的工程问题，不仅仅是因为量子比特必须与外界隔离（哪怕只有轻微的干扰），至少在完成计算后的输出结果阶段也是非常重要的。这个困难也导致了直到最近几年，最大规模的量子计算机也不过一二十个比特，并且只能运行最简单的算法。 由于噪声的包围，量子比特容易出现错误。为了解决这个问题，量子计算机需要额外的量子比特作为备份。如果一个量子比特失效了，系统将根据备份比特来将出错的比特恢复为合适的状态。 这种纠错方法在经典计算机里也存在。但在量子系统中用于纠错的备份比特的数量要显著多于经典计算机。工程师以此来评估可靠的量子计算机的标准，每个实用的量子比特可能需要 1000 个或更多的备份比特。由于很多高级算法都需要数千个量子比特来初始化，从而量子比特的总数量（包括纠错的备份比特）将很容易达到数百万个。 与此相比， 才包含 72 个量子比特，这些量子比特的实用价值取决于它们的出错率。 谷歌的量子计算机研发由来自加州大学圣芭芭拉分校的一支被同时聘用的团队所领导。在去年的 11 月，IBM 宣布开发出了 50-qubit 的量子计算机。这两个公司，以及 Rigetti Computing、英特尔（近期开发了 49-qubit 阵列），他们研发的量子计算技术都依赖于特殊设计的超导电路。这些芯片必须被保持在相当低的温度，需要复杂的冷却设备来维持运作。 有一种完全不同的量子硬件架构，其中的量子粒子即离子悬浮在室温运行的系统中。马里兰州大学园区的创业公司 IonQ 由杜克大学的物理学家 Jungsang Kim 和马里兰大学的 Christopher Monroe 成立，正在开发一台使用这种方法的量子计算机，他们使用的是镱离子。 微软选择探索第三个方向，即拓扑量子计算，它在理论上很有潜力，但尚未出现真正可工作的硬件。 所有这些系统，与近年来最受公众熟知的量子相关的计算平台即加拿大的 D-Wave 系统，都没有多少相似之处。虽然一些著名公司如谷歌和大众汽车已经购买了 D-Wave，但是量子研究社区中很多人都对此类设备抱有怀疑态度。那些科学家怀疑 D-Wave 是否能做经典计算机不能做的事，以及它们是否获得了任何的量子加速。 谷歌-IBM-Rigetti 的超导量子计算方向目前在硬件开发上处于领先地位，但目前尚不清楚哪种形式的硬件将被证明是最先进的，也许三个方向将共存。对于量子编程研究者而言，他们不关心哪种设计将胜出，只要有量子比特可以用就够了。 量子计算还有很多谜团，其中一个就是量子计算机的量子比特数能增长多快。通过传统的计算机技术，摩尔定律长期以来一直确保计算机芯片的晶体管数量每两年翻一番。但由于量子力学对电子行为的限制，摩尔定律已经失效了。很多工程师预期在中期未来，我们将被限制在少数量子比特的技术水平上，可能在未来数百年都将如此。因为量子霸权的基本证明可能也无法提供任何有用的结果，并且成熟的系统还需要很多年才能实现，工程师正集中精力开发可用在近期的一般规模的量子系统的算法。 初步共识：虽然惊喜总是可能的，但进步将是循序渐进的。 「我不认为那些鼓吹量子计算机将很快能够解决现实世界问题，或实现商用化的人是完全诚实的，」加州大学圣芭芭拉分校的物理学家 Wim van Dam 说。 自从 MIT 的 Shor 开发了他的第一个大数因式分解算法的 20 年来，量子计算已和密码学密切相关。但是关于互联网加密系统被破坏的担忧近年已有所缓和，部分是因为量子研究社区意识到能大规模运行 Shor 算法的量子计算机还远未出现，部分是因为「后量子加密」技术是可以不受任何形式的量子攻击所影响的。即使到现在，NIST 仍在评估多种后量子加密基础建设的候选方案。 与其对加密技术过于担忧，研究者近期更关心使用量子计算机来对原子和分子建模，这正是费曼对量子计算的最初洞见。用于模拟物理和化学系统的算法在 NIST 的 Quantum Algorithm Zoo 中是最数值化的部分，其价值是难以估计的，研究者说。想象一下，有一天当我们用量子计算机模拟出室温超导体的时候，世界将变成什么样子。 这里也一样，应该避免不合理的炒作。马里兰大学的物理学家和计算机科学家 Andrew Childs 预测，第一代量子计算机仅能求解相对简单的物理和化学问题。「用这些有限的量子比特，你可以回答凝聚态物理中一些较简单的人类也可能解答的问题，但对于高温超导的理解，将需要非常多的量子比特。」   虽然研究者反对过分乐观，他们也不排除量子计算的突破将使计算机的效率大大提高。越多的程序员将带来越好的算法，这也是 IBM 为什么将其量子计算机上线云平台的原因。 「我可以在这块白板上写下地球上每个量子算法研究者的名字，这才是我们的问题。」来自伯克利量子计算公司的 Chad Rigetti 断言。「我们需要在算法开发上取得更多的进展，为成千上万的学生提供开发算法的机器，这样才能促进量子计算领域的发展。」 在他们看来，目前的研究者们对这个新兴领域以及其中潜在的令人惊奇的发现非常感兴趣，并乐在其中。 五台量子计算机一览 谷歌 谷歌使用超导量子处理器构建量子计算机，例如上图中的将 22 个量子比特按两行排列的设计。 IBM 这个 16-qubit 的超导处理器支持着 IBM 的公开量子计算云平台，帮助人们探索量子计算。 英特尔 今年 1 月份，英特尔发布了 49-qubit 的超导量子计算芯片，称为 Tangle Lake。 IonQ 2016 年，IonQ 展示了用激光来操控镱离子的 5-qubit 量子计算机（Shantanu Debnath）。 Rigetti Rigetti 是由加州大学伯克利分校创立的，近期开始了 19-qubit 超导处理器芯片的开发。 
65,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739982&idx=2&sn=526066f35c7c06af5258223791fe07e1&chksm=871ad170b06d586666d17e7b8059b1baac359dd829e3684a1af3013d48baf33f67cce3d283a0&scene=27,专栏 | 为什么只用摄像头和光学雷达是不够的：我们能从Uber的自动驾驶车致死事件中学到什么,"3 月 18 日星期天晚十点左右，Uber 的一辆自动驾驶 SUV 在美国亚利桑那州坦佩市的街道上造成了一起交通致死事故。坦佩市的警方证实，在事故发生时，该 SUV 处于自动驾驶模式并撞上了一名推着自行车横穿马路的女士。这名女士在医院抢救无效后去世。坦佩市警察局莫尔，在看过由 Uber SUV 车载摄像头记录的行车视频之后表示，该名女士「从阴影中直接走到了路中间」。Uber 的自动驾驶系统和车内的安全员都没有注意到她的出现。在撞上受害者的过程中，Uber SUV 并没有减速。该名女士被认为是第一位因自动驾驶而去世的行人。 但这其实并不是第一起自动驾驶致人死亡的事故。将近两年以前，在美国佛罗里达州，一辆特斯拉汽车撞上了一辆半挂式卡车，造成了特斯拉司机的死亡。根据事后特斯拉发布的消息，这一辆特斯拉 Model S 正开启着自动驾驶模式，沿着高速公路行驶。与此同时，一辆半挂式卡车从垂直方向的高速公路路过。在明亮天空的光照下，特斯拉的自动驾驶系统和司机都没有看到半挂式卡车的白色拖车部分，因此没有踩下刹车。特斯拉汽车的挡风玻璃直接撞上了拖车部分的底座，导致了司机的死亡。 就如同 Uber 在事故后发表的推特所说，我们都对遇难者家属表示深切哀悼。但同时，我们也应该进一步了解，Uber、特斯拉、以及其它采用自动驾驶技术的公司的车辆，为何没能避免事故的发生。这样，我们才能进一步完善新技术，并避免再次发生类似的事故。 为了回答这一问题，我们先要了解现有的自动驾驶技术是如何感知周围环境和检测行人的。 2.1. 现有的自动驾驶技术如何感知周围环境 Uber、特斯拉以及谷歌所采用的自动驾驶系统，主要是基于摄像头和光学雷达（即 Lidar）。接下来，我们以 Uber 的自动驾驶系统（如图一所示）为例进行分析。 Uber 的自动驾驶 SUV 搭载了若干个成像系统，可以应对常规任务和紧急任务。   车顶 Lidar。如图 1 所示，SUV 车顶的桶装设备就是一个 Lidar。它每秒会生成若干个反应车辆周边情况的三维图像（即 point cloud，点云）。它通过发射红外线脉冲，并收集反射信号，来获得它与周围物体间的距离信息，从而实现检测静止和运动的物体的目的。 车前雷达。与 Lidar 类似，雷达通过发送无线电波并接收反射信号来测量距离。相较于 Lidar 的红外线光信号，雷达的无线电波的抗干扰性更强，能抵抗雨雪和大雾天气的影响。但雷达的辨率更低，工作距离也更小。 长短距光学摄像头。Lidar 和雷达在测距和定位方面表现出色，但在其它一些任务领域则力不从心，例如识别交通信号和分辨颜色方面。这类任务是由可见光摄像头以及其背后一系列复杂的计算机视觉算法所负责。Uber SUV 摄像头会检测前车刹车（红色刹车灯）、红绿灯、行人等特征图形图案。特别是在 SUV 的前向，Uber 采用了多个不同种类摄影头提取不同角度的图像信息，以获得行车环境的全面信息。这一摄像头系统，是对人类视觉系统的模拟和延伸。 2.2. Uber 的自动驾驶 SUV 为何没有检测到行人 2.2.1. 摄像头有其固有缺陷 行人检测是摄像头系统需要负责的主要任务之一。很明显，在事故发生时，Uber 的摄像头系统并没有有效地完成其任务。为了分析 Uber 的摄像头为何没有检测到行人，我们仔细查看了 Uber SUV 摄像头所拍摄的事故视频。 在视频中，在事故发生的当晚，车辆所行驶的道路上，有一部分道路是被阴影所覆盖的。推着自行车的女士从道路的左边开始穿越道路（我们以 Uber SUV 行进方向为正向）。她横穿道路的路线，恰好处在阴影区域内。起初，SUV 的摄像头在阴影区域的方向，除了一片昏暗以外什么也看不到。直到 SUV 的车前灯照射到了这名女士，摄像头才发现了她的存在。然而此时，SUV 离这名女士只有几米远，并在以 65 公里每小时的速度行驶中。无论是自动驾驶系统或是车内的安全员都来不及做出反应，最终造成了事故的发生。我们认为，在视频所体现出来的光照环境下，即便是一名专心开车的司机，也很可能无法避免事故的发生。（也有一些美国的网友指出，故事街道的光照条件并不像视频所显示的那么昏暗。但可以确定的是，阴影区域的光照条件，的确其它路段的光照条件差许多。）   然而，这只是机器和人类视觉系统的固有缺陷所造成的悲剧的其中一件。在另一件自动驾驶致死的事故中，特斯拉自动驾驶车所搭载的摄像头以及司机，都没能在明亮的天空的映衬下，注意到前方的卡车。与 Uber SUV 所经历的昏暗的环境所不同，Tesla 轿车的摄像头处于十分耀眼的光照环境中。从这两个事故中，我们可以总结出基于视觉的自动驾驶技术的固有缺陷。 缺陷 1：视觉系统在较差的光照条件下（例如过暗或过亮），表现不稳定。 图 3 给出了一些视觉系统在较差光照条件下表现不稳定的例子。     在我们开车时，我们也会经常遇到另一类情况。当车辆经过某些路段时，行车视野很可能被部分遮挡。遮挡物可以来自自然环境，例如，树木、树篱和灌木丛等，也可以来自人造环境，例如其它大型车辆、指示牌、围墙和围栏等。通过这类路段时，司机需要十分的注意来往车辆行人。不幸的是，事故还是时有发生。图 4 给出了这一情况的几个例子，中文俗称鬼探头。     由此，我们总结出基于视觉的自动驾驶技术的第二个固有缺点。 缺陷 2：视觉系统会被障碍物所遮挡，因而对被遮挡区域的潜在危险一无所知。 从更广泛的意义上说，现有的基于视觉的自动驾驶技术试图复制一个类似人类的司机。因此，它们无法从根本上克服人类司机及其视觉系统的固有缺陷。 2.2.2. Lidar 不适合行人检测 除了摄像头以外，Uber SUV 还搭载了光学雷达系统。这一系统即便在完全黑暗的条件下，也能正常工作。那么为什么 Uber SUV 还是没能及时检测到行人呢？其根本原因在于，光学雷达并不是为行人检测所设计的。它之所以不能及时检测到行人，是因为它如下所述的局限性。 光学雷达的英文 Lidar，是 Light Detection And Ranging 的缩写，即光学检测和测距。Lidar 本身是一种测绘技术。它通过发射脉冲激光并分析反射信号，来测量发射源到某一目标距离。商用的 Lidar 善于测距并检测测距目标大致的形状。但是它们并不善于实时地分辨物体（例如识别远处的汽车或自行车）。其原因有以下几点。1) Lidar 没有办法获得物体宝贵的颜色信息；2) Lidar 的分辨率比较有限，特别是对于远处物体的分辨率较低（激光光束将会太发散，以至于无法形成有效的图像或点云）；3) 相对于高速的车辆环境来说，Lidar 的刷新率不够高。 基于以上局限性，我们尝试着还原了 Uber SUV 的光学系统在事故发生时的工作状态。（以下对于 Lidar 的分析都是基于我们的理解和假设。实际情况究竟是怎么样，会在 Uber 分析并公布 Lidar 系统记录之后，有一个更加清晰的还原。）当推着自行车的女士进入对向车道时，Uber 的 Lidar 的确检测到了一些反射信号。但是由于前文所述的局限性，Lidar 并不能识别这究竟是一名自行车骑手、一辆汽车、一棵树、一个交通指示牌、或是其它什么物体。在行车过程中，对向车道存在物体是完全正常的现象。最常见的例子是另一辆相向行驶的车辆。因此，即便 Lidar 检测到了有物体的存在，也没有减速或刹车的必要，除非是 SUV 的车载摄像头发现了行人或者交通信号。不幸的是，Uber SUV 的车载摄像头并没有发现推自行车的女士。因此，当该名女士突然进入 SUV 所行驶的车道时，她已经离 SUV 很近了。这时，Lidar 发现 SUV 前面出现了障碍物，但为时已晚。 如前文所述，仅仅是模拟人类的视觉系统，并不能实现可靠的行人检测，也无法完全避免类似 Uber 和 Tesla 所经历的这类事故。如果我们想实现从根本上优于人类司机的自动驾驶系统，那么我们必须为其配备人类所不具备的能力。例如，如果自行车骑手能够以某种主动的方式，将她的存在通知给 Uber SUV，或者是，如果半挂式卡车能够主动地将其位置和尺寸告知 Tesla 自动驾驶车，那么这些事故都可以避免，或至少不会造成人员死亡。但问题是，我们现在能够以经济有效的方式实现这一任务么？ 答案是肯定的。 车载通信技术（即 Vehicle-to-Everything, V2X）正是为类似的安全性任务而设计的。以车辆专用短程通信技术（即 Dedicated Short-Range Communications, DSRC）为例，该技术支持所有的交通参与者以每秒 10 到 20 次的频率交互其动态信息。这些信息包括位置、速度、加速度、行进方向、以及其它交通相关信息。这样一来，即便视野不清晰或是视线被遮挡，司机或者自动驾驶车都能够及时了解周围所有的交通情况。更重要的是，无线信号传播能力，能将司机或自动驾驶车的感知能力，从视觉所及的范围，提升到视觉所不及的几公里之外。 接下来，让我们以 Uber SUV 事故场景为例，分析一下 V2X 技术能如何避免事故的发生并拯救人们的生命。 场景 1：人-车通信 (Vehicular-to-Pedestrian, V2P) 在这个场景中（如图 5 所示），一名自行车骑手 B 和一辆汽车 V1 都加载了嵌入式的 V2P 通信模块。当骑手 B 要横穿马路时，车辆 V1 能够预测到 B 将会出现在 V1 的行车路线上。做到这一点，只需要车辆 V1 对骑手 B 最近几秒内的位置、速度、加速度和方向信息，进行分析和预测，即可描绘出骑手 B 过去、现在以及将来的行进路线。而以上这些信息，骑手 B 都会主动地周期性地通过 V2P 的无线信道，与车辆 V1（以及周围其它的交通参与者）进行共享。于是，当骑手 B 刚刚开始横穿马路是，车辆 V1 便及时预测到两者的行进路线有可能交汇，从而及时降速并避免了可能发生的事故。   场景 2：车-车通信 (Vehicular-to-Vehicle, V2V) 也许有人会说，自行车骑手有可能不愿意佩戴 V2P 设备（尽管 V2X 芯片，例如高通的 QCA6584A，已经做到了毫米级的尺寸）。在这种情况下，我们也可以依靠 V2V 通信，实现多车辆视野共享，从而为每一辆汽车提供更清晰全面的视野。在图 6 所示的场景中，道路上有一名未佩戴 V2P 设备的骑手 B，和两辆装载了 V2V 设备的车辆 V1 和 V2。在骑手 B 横穿马路时，V1 的摄像头没能在昏暗的光线下及时识别出 B。而没有携带任何设备的骑手也将她的存在无法告知 V1。幸运的是，这次我们有车辆 V2 的帮助。车辆 V2 从另一个方向驶向现场，V2 的摄像头和 Lidar 能更好的检测到骑手 B 的存在。车辆 V2 及时地通过 V2V 通信，将自己的视野分享给 V1。于是，V1 能够从分享的视野中发现 B 的存在，从而及时制动来避免事故的发生。   场景 3：车-基础设施通信 (Vehicular-to-Vehicle, V2V) 当然，也有可能光照条件如此恶劣以至于任何车辆的摄像头都没有检测到骑手 B，或是 V1 是路上唯一的车辆。在这种场景中（如图 7 所示），我们还可以利用沿路放置的交通传感器，来辅助完成行人检测。（读者可参考这一链接了解更多关于交通传感器的介绍： http://www.windmill.co.uk/vehicle-sensing.html）这类传感器，或者被部署在各类交通设施上（例如红绿灯、路灯和交通指示牌），或者被单独部署在路边。当某一个传感器感应到了骑手 B 横穿马路的行为和位置后，它会将这一信息通过 V2I 无线信道广播给周围所有的车辆。这样一来，车辆 V1 也能及时做出相应的反应，从而避免事故的发生。 以上三类场景，只包含了 2 到 3 个交通参与者，都只描述了 V2X 车载通信最基础的应用场景。在实际的应用场景中，一公里的范围内，就会有成千上万的交通参与者，直接地实时地分享海量的交通安全信息。重要的信息内容将被提取出来，通过多跳或者回程数据网络，一步步集中到整个城市的交通控制系统中去。这就意味着，在 V2X 的支持下，我们能在不同的粒度和层级上 (即从单个车辆的粒度，到某个路口、某片街区的层级，直到整个城市的级别)，调度并协调交通。 4.1. 什么是 V2X？ 援引自维基百科:「V2X 车载通信，是在车辆和任何会被该车辆所影响的实体之间分享信息的技术。它包含了 V2I 车-基础设施通信，V2V 车-车通信，V2P 人-车通信，V2D 车-设备通信和 V2G 车-电网通信。」 援引自美国交通部：「V2V 系统可以避免 79% 的各类交通事故。具体地说，V2V 系统可以避免 81% 的轻型车辆事故以及 71% 的重型车辆事故。V2I 系统可以避免 26% 的各类交通事故。具体地说，V2I 系统可以避免 27% 的轻型车辆事故以及 15% 的重型车辆事故。将 V2V 和 V2I 系统整合起来，可以避免 83% 的轻型车辆事故以及 72% 的重型车辆事故。」 用我们的话说：V2X 是一个强大的平台。它能将车辆，司机，自动驾驶系统，行人，交通基础设施，路边传感器，交通管理部分以及其它各类交通实体，联通整合在一起。V2X 所使用的无线电波，不受光照条件影响，并能轻易地穿透或绕过障碍物，从而在各类交通参与者之间分享有价值的交通信息。这些信息会在覆盖整个交通系统的巨大的数据网络中流通，使得信息的获取者能够了解到更大范围的交通情况。由此，人类司机或自动驾驶系统可以更早更快地对周围的交通情况作出反应。更重要的是，V2X 使得各类交通参与者能够主动地相互协作，让每一个人和物都能积极地为更安全更高效的交通做出自己的贡献。 4.2. V2X 标准化 – DSRC 与 LTE-V 之争 各类车辆和设备是由不同的厂家所生产的。为了让本不兼容的它们能都有次序地、高效地、公平地相互通信，我们需要建立通信标准来规范它们的信号发送和接收行为。现如今，V2X 领域存在着两大通信标准，即 DSRC（车载专用短程通信）和 LTE-V（长期演进技术-车辆通信）。其中，DSRC 是由电气电子工程师学会（即 IEEE）制定，并且有主要车辆生产商支持的标准。而 LTE-V 是由第三代合作伙伴计划（即 3GPP）通过拓展 LTE 而制定的。 DSRC 的标准化流程可以追溯至 2004 年。当时，IEEE 在其 802.11 无线局域网（即 Wireless Local Area Networks, WLAN）标准系列下，开始制定新的车载通信标准。这一标准即是 IEEE 802.11p。在 2007 年左右，IEEE 802.11p 标准已经趋于稳定。于是，IEEE 又开始着手制定 1609.x 系列标准，以做为 V2X 的安全性框架。差不多同一时间，美国汽车工程师协会（即 Society of Automotive Engineers, SAE）从汽车工业的需求出发，也开始制定关于 V2V 应用的标准，并将其称为 DSRC。而 DSRC 所采用的通信标准即是 IEEE 802.11p 和 1609.x。现在，人们将 DSRC 和相应的下层标准统称为 DSRC。 LTE-V 是 3GPP 在 2017 年新发布的。与 IEEE 802.11 无线局域网标准不同，LTE-V 是一组基于蜂窝通信网络的 V2I 和 V2V 的通信物理层协议。 现有的研究表明，相较于 DSRC，LTE-V 拥有更大的带宽，因而能更好地支持非安全性应用，例如文件下载和互联网连接。然后，LTE-V 的通信延时较大，阻碍了它在安全性相关的场景中的应用。DSRC 在碰撞预警等安全性相关的场景中的表现，优于 LTE-V。   表 1 在各个方向上对 DSRC 和 LTE-V 进行了对比。 表 1: DSRC 和 LTE-V 的对比 综上所述，要通过 V2X 升级现有的自动驾驶系统并提升其安全性能，DSRC 是首选。 4.3. 进一步认识 DSRC 在美国和其它许多国家，各方都在积极地发展和部署 DSRC。2014 年 2 月，美国交通部首度承诺它将大力支持 DSRC 在轻型车辆上的应用。从那时开始，美国国会，美国交通部，IEEE 以及各个大的车企，都在积极地推荐 DSRC 的立法工作。值得一提的是，美国公路交通安全管理局要求在不久的将来，车辆都需要以 DSRC 做为 V2V 车辆安全标准。车辆将会通过 DSRC 发送和接收基础安全信息（即 Basic Safety Messages, BSMs）。这一要求得到了绝大多数车企的支持。只有少部分手机和 Wi-Fi 领域的企业表示了反对。大规模部署 DSRC 的时间表如图 9 所示。   DSRC 要点： 专属带宽：位于 5.9GHz 频带的一段 75MHz 的带宽被划为 DSRC 专属的交通安全频谱。这与一些常见的其它通信协议有所不同。例如，Wi-Fi、蓝牙和 Zigbee 就是共享开放的 2.4Ghz 频带。 短距离通信：DSRC 的目标通信范围在 1 千米之内。相对于蜂窝通信和卫星通信来说，其通信距离较短。 75MHz 的带宽被分为 7 个频道，如图 10 所示。   图 10. DSRC 频道的划分 每辆车都会在信道 172 中，以每秒 10 到 20 次的频率，交互 DSRC 基础安全信息。紧急信息则会在信道 184 中，以更高的优先级进行传播。 每一条基础安全信息都包含两部分信息。第一部分信息是强制性信息，包括位置、速度、方向、角度、加速度、制动系统状态和车辆尺寸。第二部分是可选信息，例如防抱死系统状态、历史路径、传感器数据、方向盘状态等。图 11 展示了基础信息的具体格式细节。   5.1. DSRC 的挑战所在 相较于其它传统的通信技术（例如 Wi-Fi 和蜂窝通信），DSRC 技术所工作的交通环境是动态而多变的。因此，DSRC 技术面临着几个特殊的挑战。 快速变化的环境。比方说，但车辆快速地穿行于不同的路段时，所经历的交通参与者的密度和通信拓扑结构是随时变化的。同时，交通参与者的特性和类型也会在几秒钟内变得完全不同，例如车辆在经过人行横道的人群时。 激烈的通信冲突。在繁忙时段的某些路口，参与车辆和行人的数量会急剧上升，导致小范围内剧烈的信道竞争（DSRC 通信节点是基于 CSMA 的方式，来争夺通信信道的使用权）。 5.2. 我们的方法 – OnCAR 为了面对这些挑战并保证 DSRC 优良的通信性能，我们需要一种方法来自适应地调整 DSRC 的通信行为，以应对快速变化的通信环境。DSRC 通信行为，是由众多的参数所共同决定的。这些通信参数包括，数据率（调制和编码的组合），传输功率，竞争窗口大小，目标传输范围等。同时，DSRC 的性能也是由众多的性能指标来共同描述的。这些性能参数包括有效吞吐量（即每秒有多少有效数据被成功接收）、数据包传输效率（即发送出去的数据包当中，有百分之多少的数据包能被成功接收）、端到端延时（即成功接收的数据包从发送端到接收端所经历的传输延时）等。除了 DSRC 本身的通信性能之外，自动驾驶技术也可以在应用层或数据层，对 DSRC 提出应用性能需求指标。这些各层次的指标，在某些程度上，是相互排斥的。优化一个指标，可能会导致另一个指标的下滑。同时，通信参数和性能指标之间，有着复杂的相互联系。改变某一个通信参数，会造成多个性能指标的变动。而优化某一个性能指标时，也需要同时控制多个通信参数。同时，通信参数内部和性能指标之间，也有着复杂的耦合和互动关系。因此，我们需要在实时变化的交通环境中，同时协调多个通信参数，以满足各类性能指标的不同需求。 为了完成这一复杂的任务，我们提出了名叫 OnCAR 的方法 (文章链接: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7524434)。相较于其它现存的自适应方法，OnCAR 能更好地联合优化 DSRC 的各项性能指标。其原因如下： OnCARC 采用了先进的多输入多输出 (Multiple-Input Multiple-Output, MIMO) 控制模型，因而能同步地调节控制多个 DSRC 通信参数，来联合优化多项 DSRC 性能指标。(请注意 OnCAR 的 MIMO 模型是指多控制变量和多控制目标，与通信领域常说的多数据流 MIMO 不是同一概念。) OnCAR 具有在线机器学习 (online machine learning) 的能力，能够根据交通环境的变化，实时更新 MIMO 控制模型。 OnCAR 的结构如图 12 所示。   在大的框架上看，OnCAR 是由两条控制回路所构成的。其中，前馈回路则是用于提升控制的速度，而反馈回路是用于细粒度的性能优化。具体地说，前馈回路会主动收集周围的环境信息，例如车辆密度和信干噪比等，并将这些信息输入预设的经验模型中，得到一个包含各类 DSRC 参数的基础控制向量。同时，反馈链路会评估一个由各项性能指标的变化量组成的多阶成本函数，以此来分析近几次参数控制的优劣，从而在线地更新控制策略。基于更新后的控制策略，反馈链路会生成一个增量向量，用以在细粒度上调整前馈链路所给出的基础控制向量。 OnCARD 的反馈链路的在线机器学习能力，是由图 13 所示的在线控制器所实现的。这一控制器所应用了的在线机器学期算法，是递归最小二乘法 (Recursive Least Squares, RLS, https://goo.gl/sy92cZ)。通过应用这一算法，OnCAR 在线地学习并更新从多参数到多指标的复杂的映射关系，并将学习结果传递给增量控制器，以不断更新 DSRC 通信指标，达到自适应优化 DSRC 性能的目的。   5.3. 性能评估 由于篇幅的限制，具体的 OnCAR 的性能评估请参见文章: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7524434 和链接: https://sites.google.com/site/xichenmcgill/cameras-and-lidars-are-not-enough。这里，我们只给出最终的性能评估结果。 通过收集美国各地交通部门 (例如加州的伯克利和圣地亚哥的交通安全局) 的交通数据，我们建立了一个数据驱动的性能评估平台。评估结果显示，OnCAR 能够 i) 将 DSRC 的可靠性提升至少 23.7%（以 PDR 为可靠性指标），ii) 将 DSRC 的有效吞吐量提高至少 30.1%, iii) 将 DSRC 的公平性提升至少 40.1%（以 PDR 的变异系数, 即 coefficient of variation, 为指标）。 5.4. 应用 OnCAR 实现交通自动化 以上的性能评估，只是基于 DSRC 本身的通信性能。为了进一步展示 OnCAR 增强型 DSRC 对于自动驾驶和自动交通的支持和提升，我们实现了一个名叫 VSmart（链接: https://sites.google.com/site/xichenmcgill/vsmart）的测试平台。基于此平台，我们录制了一系列演示视频，以展示 OnCAR 增强型 DSRC 是如何支持并提升自动驾驶和自动交通的。 这一系列演示视频展示了以下几种先进的自动交通安全应用。欢迎观看。 自适应巡航控制 (Adaptive Cruise Control, ACC): 视频链接：https://youtu.be/QQuSGSC6SlM 变道辅助 (Lane Change Assist, LCA): 视频链接: https://youtu.be/1hNIIUnp6AM 路口自动交通调度 (Autonomous Intersection Management): 视频链接: https://youtu.be/Rh4HQjjc3nM  仅仅采用摄像头和 Lidar，是不足以保证自动驾驶车的行车安全的。 V2X 技术，特别是 DSRC 技术，能够辅助现有的自动驾驶技术，有力地提高其安全性能。 我们的 OnCAR 方法具有实时自适应和在线学习的能力，从而能更好地让 DSRC 服务于自动驾驶和交通自动化。 7. 致谢 我们想要特别感谢徐学鹏、向乔博士和孔令和博士，在搭建 VSmart 平台和录制演示视频时所提供的大力支持。 作者简介 陈熙博士，本科和硕士毕业于上海交通大学，博士毕业于加拿大麦吉尔大学 McGill University，现任 Nuance 公司自然语言处理研究科学家 (NLP Research Scientist)。他参与了多项先进技术的研究工作，其中就包括 i) 应用 V2X 通信提升自动驾驶的安全性，并实现地面交通控制和调度的自动化，和 ii) 整合传统的控制理论和先进的在线机器学习技术，来全面提升 V2X 通信系统的性能。他所提出的 OnCAR 方法被美国通用汽车公司研究部门所认可，并有可能被搭载至通用汽车新一代的 V2X 设备上。同时，他也参与了若干工业界黑科技的开发和部署项目。例如，使用普通的家用 Wi-Fi 信号，对没有佩戴任何设备 (包括手机和各类传感器) 的用户，进行定位、手势识别和身份认证。他所提出的 AutoFi 方法，可以应用现有的家用 Wi-Fi 信号，实现室内定位和导航、智能家居控制、入侵者检测、老年人智能护理等功能。这一方法被智能家居初创公司 Aerial.ai 所采用，并做为技术基石，成功地帮助该公司获得了天使轮和 A 轮的投资。该方法现已经被广泛地部署于 Aerial.ai 的客户端设备当中。 刘学教授本科和硕士毕业于清华大学，博士毕业于美国 Univ. of Illinois at Urbana-Champaign (UIUC)，现任麦吉尔大学计算机系正教授和特聘讲席教授 (Chair Professor)。同时他还担任全球最大的在线交友 App—Tinder 的首席科学家，领导机器学习、人工智能算法和系统的研发。他与多家世界著名高校、公司和研究院建立了紧密和良好的合作，其中包括微软 (Microsoft)、惠普 (HP)、IBM、通用汽车 (GM)、庞巴迪 (Bombardier)、博世 (Bosch) 及多家高科技创业公司。刘学教授获得过多项国际科研奖项和荣誉，包括多次国际顶级学术会议的最佳论文奖、加拿大计算机学会 2014 年杰出青年计算机科学家奖、麦吉尔大学汤姆林森科学家奖、加拿大 MITACS 2017 年杰出创新领袖奖, 以及由 IBM Lotus Notes 的缔造者、前任微软首席软件架构师（Chief Software Architect）雷·奥兹先生 (Ray Ozzie) 所设立的 Ray Ozzie 奖学金等。刘教授还担任多家国际著名学术期刊的副主编和编委，并参与组织举办过 40 多个国际知名会议和研讨会。他在国际顶级学术会议和期刊上累计发表过 280 余篇论文，有上万的引用率，并获得过多项美国专利。 刘学教授实验室介绍： McGill University 麦吉尔大学是世界著名一流大学。有很多著名的校友，包括多位加拿大和其他国家的领导人、诺贝尔奖获得者、普利策奖获得者、商界领袖、知名运动员和奥运会冠军、演员、音乐家、艺术家、宇航员、以及著名的科学家和学术领袖，其中包括现任斯坦福大学校长 Marc Tessier-Lavign、英属哥伦比亚大学校长 Santa Ono 和剑桥大学副校长 Stephen Toope 等。McGill University 所在城市—蒙特利尔是一座富有浓郁学术、文化、创业和时尚气息的大都市，并被评选为 2017 年世界最适合学生的城市 (「World's Best City for Students」)，也被誉为当今世界的人工智能中心。坐落于蒙特利尔市中心的麦吉尔大学计算机学院不但培育了深度学习三大巨头之一的 Yoshua Bengio（现任蒙特利尔大学教授），还先后吸引了 Google Deepmind、Facebook AI Research、RBC 等大公司投资建立联合人工智能实验室。蒙特利尔有世界多家高科技公司，如 Google、Microsoft、Facebook、Element AI、 Nuance、 EA、 Ubisoft、 Airbnb、 Shopify、 Morgan Stanley、 Bombardier、 Expedia、华为、Nokia、Ericsson 等。刘教授领导的 Cyber-Physical Systems (CPS) 实验室主要从事人工智能、机器学习、大数据、计算机网络和系统系统在物联网、自动驾驶、社交网络，绿色能源等领域的相关研究工作。CPS 实验室同时充分利用学校的人工智能研究实力，与多个高科技公司、实验室、和初创公司合作，致力于培养硕士生、博士生、博士后的理论研究与应用问题相结合的研究和领导能力，并为学生创造大量的实习与工作机会。 "
66,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739982&idx=3&sn=6378e68d3a764c256fa7225fd4ce47cb&chksm=871ad170b06d5866c818721f03b83f6e0f071b5b43c67578498c2e1663c8bc97338603370baf&scene=27,深度 | BAIR提出MC-GAN，使用GAN实现字体风格迁移,"BAIR 近日，BAIR 发布博客提出 MC-GAN（Multi-Content GAN），可以快速生成相同风格的字体。 文本是二维设计中的一个显著视觉元素。艺术家投入了大量时间设计在视觉上与其他元素的形状和纹理相兼容的字形。这个过程需要大量劳动，艺术家们通常只设计标题或注释所必需的字形子集，这使得设计完成后文本很难更改，或者很难把看到的字体实例迁移到自己的项目中。 早期字形合成研究集中在轮廓的几何建模上，局限于特定的字形拓扑上（例如，不能应用到装饰字体或者手写体），并且不能与图像输入一起使用。随着深度神经网络的兴起，研究者研究了从图像进行字形建模的问题。另一方面，合成与局部观察相一致的数据在计算机视觉和图形学中是一个有趣的问题，例如，多视角图像生成、补全图像中的缺失区域，以及三维形状的生成。字体数据是一个典型的例子，它提供了文字样式和内容的清晰分解。 条件生成对抗网络（cGANS）[1] 的最新进展在许多生成应用中取得了成功。然而，它们只有在相当特定的领域中才能发挥最佳效果，无法适应通用领域或多领域迁移。类似地，当被直接用来生成字体时，cGAN 模型会生成严重的失真。例如，给定下面五个字母： 条件生成对抗网络模型无法以同一种风格生成 26 个字母： 用于 Few Shot 字体风格迁移的多内容生成对抗网络 我们设计了多内容 GAN 架构 [2] 来为每个观察到的字符集（只具备少量观察到的字形）重新训练定制的魔法网络，而不是为所有可能的字体装饰训练单个网络。该模型考虑沿着信道的内容（即 A-Z 字形）和沿着网络层的样式（即字形装饰），将给定字形的样式迁移到模型未见过的字形的内容。 多内容 GAN 模型包括用于预测粗糙字形形状的堆叠 cGAN 架构和用于预测最终字形颜色和纹理的装饰网络（ornamentation network）。第一个网络称为 GlyphNet，用于预测字形掩码；第二个网络称为 OrnaNet，用于微调从第一个网络生成字形的颜色和装饰。每个子网络遵循条件生成对抗网络（cGAN）的结构，并修改该结构以达到使字形风格化或装饰预测的特定目的。 网络架构 下面是 GlyphNet 示意图，它从一组训练字体中学习字体流形的一般形状。GlyphNet 的输入和输出是字形的堆栈，其中每个字母分配有一个通道。在每个训练迭代中，x_1 包含一个由 y_1 字形组成的随机子集，剩余输入通道被清零。 通过这种新颖的字形堆栈设计，跨网络信道学习不同字形之间的相关性得以实现，并实现风格自动迁移。下图通过结构类似性（SSIM）指标在包含 1500 个字体样例的随机集合上展示了这种相关性。计算每个生成字形与其真实字形之间的结构相似性时，当一次观察到一个字母时，共发现了 25 种分布。这些曲线图显示了当观察到字母 β（蓝色）时生成字母 α 的分布 α|β 与当给出除 β 以外的任何其它字母（红色）时生成字母 α 的分布 α|β。在生成 26 个字母中的每一个字母时，两个信息量最大的给定字母和两个信息量最少的给定字母的分布如图所示。例如，从图的第五行来看，与其他字母相比，字母 F 和 B 在生成字母 E 方面是最有信息贡献的，而 I 和 W 是信息最少的。其他例子中，O 和 C 是构造 G 的最具指导性的字母，R 和 B 是生成 P 的最具指导性的字母。 因此，对于任何仅有几个观察字母的期望字体，预训练的 GlyphNet 要生成全部的 26 个 A-Z 字形。但是我们应该如何迁移装饰呢？第二个网络 OrnaNet 采用这些生成的字形，在简单的整形变换（reshape transformation）和灰度信道重复之后（在下图中用 T 表示），使用条件 GAN 架构生成具备期望颜色和装饰的输出。OrnaNet 的输入和输出是批量的 RGB 图像，而不是堆栈（其中每个字母的 RGB 信道是其对应的灰度字形的重复）。OrnaNet 中的多个正则化器会惩罚风格化字母掩膜与其对应字形形状的偏差。 结果 下面，我们将使用单个单词给出的字体样式演示例句。 此外，以下是 OrnaNet 预测的逐步改进： 参考资料 [1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. ""Image-to-Image Translation with Conditional Adversarial Networks."" CVPR 2017. [2] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, and Trevor Darrell. ""Multi-Content GAN for Few-Shot Font Style Transfer."" CVPR 2018. 论文链接：https://arxiv.org/abs/1712.00516 GitHub 链接：https://github.com/azadis/MC-GAN "
67,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739899&idx=3&sn=a368d8c934d1e3c78ad5b50bef32e1c0&chksm=871ad0c5b06d59d36a25897848f7cae01bfe8d0705190ab75b042190a8e7dd2f0f22b11e9204&scene=27,教程 | 没错，纯SQL查询语句可以实现神经网络,"Medium 我们熟知的SQL是一种数据库查询语句，它方便了开发者在大型数据中执行高效的操作。但本文从另一角度嵌套SQL查询语句而构建了一个简单的三层全连接网络，虽然由于语句的嵌套过深而不能高效计算，但仍然是一个非常有意思的实验。 在这篇文章中，我们将纯粹用SQL实现含有一个隐藏层（以及带 ReLU 和 softmax 激活函数）的神经网络。这些神经网络训练的步骤包含前向传播和反向传播，将在 BigQuery 的单个SQL查询语句中实现。当它在 BigQuery 中运行时，实际上我们正在成百上千台服务器上进行分布式神经网络训练。听上去很赞，对吧？ 也就是说，这个有趣的项目用于测试 SQL 和 BigQuery 的限制，同时从声明性数据的角度看待神经网络训练。这个项目没有考虑任何的实际应用，不过最后我将讨论一些实际的研究意义。 我们先从一个基于神经网络的简单分类器开始。它的输入尺寸为 2，输出为二分类。我们将有一个维度为 2 的单隐层和 ReLU 激活函数。输出层的二分类将使用 softmax 函数。我们在实现网络时遵循的步骤将是在  Karpathy’s CS231n 指南（https://cs231n.github.io/neural-networks-case-study/）中展示的基于 SQL 版本的 Python 示例。 该模型含有以下参数： 输入到隐藏层 W: 2×2 的权重矩阵（元素： w_00, w_01, w_10, w_11） B: 2×1 的偏置向量（元素：b_0, b_1） 隐藏到输出层 W2: 2×2 的权重矩阵(元素： w2_00, w2_01, w2_10, w2_11) B2: 2×1 的偏置向量(元素：b2_0, b2_1) 训练数据存储在 BigQuery 表格当中，列 x1 和 x2 的输入和输出如下所示（表格名称：example_project.example_dataset.example_table） 如前所述，我们将整个训练作为单个 SQL 查询语句来实现。在训练完成后，通过 SQL 查询语句将会返回参数的值。正如你可能猜到的，这将是一个层层嵌套的查询，我们将逐步构建以准备这个查询语句。我们将会从最内层的子查询开始，然后逐个增加嵌套的外层。 前向传播 首先，我们将权重参数  W 和 W2 设为服从正态分布的随机值，将权重参数 B 和 B2 设置为 0。 W 和 W2 的随机值可以通过 SQL 本身产生。为了简单起见，我们将从外部生成这些值并在 SQL 查询中使用。用于初始化参数的内部子查询如下： 请注意，表格 example_project.example_dataset.example_table 已经包含了列  x1、 x2 和 y。模型参数将会被作为上述查询结果的附加列添加。 接下来，我们将计算隐藏层的激活值。我们将使用含有元素 d0 和 d1 的向量 D 表示隐藏层。我们需要执行矩阵操作 D = np.maximum(0, np.dot(X, W) + B)，其中 X 表示输入向量（元素 x1 和 x2）。这个矩阵运算包括将权重 W 和输入 X 相乘，再加上偏置向量 B。然后，结果将被传递给非线性 ReLU 激活函数，该函数将会把负值设置为 0。SQL 中的等效查询为： 上面的查询将两个新列 d0 和 d1 添加到之前内部子查询的结果当中。 上述查询的输出如下所示。 这完成了从输入层到隐藏层的一次转换。现在，我们可以执行从隐藏层到输出层的转换了。 首先，我们将计算输出层的值。公式是：scores = np.dot(D, W2) + B2。然后，我们将对计算出来的值用 softmax 函数来获得每个类的预测概率。SQL 内部的等价子查询如下： 首先，我们将使用交叉熵损失函数来计算当前预测的总损失。首先，计算每个样本中正确类预测概率对数的负值。交叉熵损失只是这些 X 和 Y 实例中数值的平均值。自然对数是一个递增函数，因此，将损失函数定义为负的正确类预测概率对数很直观。如果正确类的预测概率很高，损失函数将会很低。相反，如果正确类的预测概率很低，则损失函数值将很高。 为了减少过拟合的风险，我们也将同样增加 L2 正则化。在整体损失函数中，我们将包含 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)，其中 reg 是超参数。在损失函数中包括这一函数将会惩罚那些权重向量中较大的值。 反向传播 接下来，对于反向传播，我们将计算每个参数对于损失函数的偏导数。我们使用链式法则从最后一层开始逐层计算。首先，我们将通过使用交叉熵和 softmax 函数的导数来计算 score 的梯度。与此相对的查询是： 在上文中，我们用  scores = np.dot(D, W2) + B2 算出了分数。因此，基于分数的偏导数，我们可以计算隐藏层 D 和参数 W2，B2 的梯度。对应的查询语句是： 同理，我们知道  D = np.maximum(0, np.dot(X, W) + B)。因此，通过 D 的偏导，我们可以计算出 W 和 B 的导数。我们无须计算 X 的偏导，因为它不是模型的参数，且也不必通过其它模型参数进行计算。计算 W 和 B 的偏导的查询语句如下： 最后，我们使用 W、B、W2 及 B2 各自的导数进行更新操作。计算公式是 param = learning_rate * d_param ，其中learning_rate 是参数。为了体现 L2 正则化，我们会在计算 dW 和 dW2 时加入一个正则项 reg*weight。我们也去掉如  dw_00, correct_logprobs 等缓存的列，它们曾在子查询时被创建，用于保存训练数据(x1, x2 及 y 列) 和模型参数（权重和偏置项）。对应的查询语句如下： 这包含了正向和反向传播的一整个迭代过程。以上查询语句将返回更新后的权重和偏置项。部分结果如下所示： 为了进行多次训练迭代，我们将反复执行上述过程。用一个简单 Python 函数足以搞定，代码链接如下： https://github.com/harisankarh/nn-sql-bq/blob/master/training.py。 因为迭代次数太多，查询语句嵌套严重。执行 10 次训练迭代的查询语句地址如下： https://github.com/harisankarh/nn-sql-bq/blob/master/out.txt 因为查询语句的多重嵌套和复杂度，在 BigQuery 中执行查询时多项系统资源告急。BigQuery 的标准 SQL 扩展的缩放性比传统 SQL 语言要好。即使是标准 SQL 查询，对于有 100k 个实例的数据集，也很难执行超过 10 个迭代。因为资源的限制，我们将会使用一个简单的决策边界来评估模型，如此一来，我们就可以在少量迭代后得到较好的准确率。 我们将使用一个简单的数据集，其输入 X1、X2 服从标准正态分布。二进制输出 y 简单判断   x1 + x2 是否大于 0。为了更快的训练完 10 个迭代，我们使用一个较大的学习率 2.0（注意：这么大的学习率并不推荐实际使用，可能会导致发散）。将上述语句执行 10 个迭代得出的模型参数如下： 我们将使用 Bigquery 的函数 save to table 把结果保存到一个新表。我们现在可以在训练集上执行一次推理来比较预测值和预期值的差距。查询语句片段在以下链接中： https://github.com/harisankarh/nn-sql-bq/blob/master/query_for_prediction.sql。 仅通过十个迭代，我们的准确率就可达 93%（测试集上也差不多）。 如果我们把迭代次数加到 100 次，准确率高达 99%。 优化 下面是对本项目的总结。我们由此获得了哪些启发？如你所见，资源瓶颈决定了数据集的大小以及迭代执行的次数。除了祈求谷歌开放资源上限，我们还有如下优化手段来解决这个问题。 创建中间表和多个 SQL 语句有助于增加迭代数。例如，前 10 次迭代的结果可以存储在一个中间表中。同一查询语句在执行下 10 次迭代时可以基于这个中间表。如此，我们就执行了 20 个迭代。这个方法可以反复使用，以应对更大的查询迭代。 相比于在每一步增加外查询，我们应该尽可能的使用函数的嵌套。例如，在一个子查询中，我们可以同时计算 scores 和 probs，而不应使用 2 层嵌套查询。 在上例中，所有的中间项都被保留直到最后一个外查询执行。其中有些项如  可以早些删除（尽管 SQL 引擎可能会自动的执行这类优化）。 多尝试应用用户自定义的函数。如果感兴趣，你可以看看这个 BigQuery 的用户自定义函数的服务模型的项目（但是，无法使用 SQL 或者 UDFs 进行训练）。 意义 现在，让我们来看看基于深度学习的分布式 SQL 引擎的深层含义。 BigQuery、Presto  这类 SQL 仓库引擎的一个局限性在于，查询操作是在 CPU 而不是 GPU 上执行的。研究 blazingdb 和 mapd 等基于 GPU 加速的数据库查询结果想必十分有趣。一个简单的研究方法就是使用分布式 SQL 引擎执行查询和数据分布，并用 GPU 加速数据库执行本地计算。 退一步来看，我们已经知道执行分布式深度学习很难。分布式 SQL 引擎在数十年内已经有了大量的研究工作，并产出如今的查询规划、数据分区、操作归置、检查点设置、多查询调度等技术。其中有些可以与分布式深度学习相结合。如果你对这些感兴趣，请看看这篇论文（https://sigmodrecord.org/publications/sigmodRecord/1606/pdfs/04_vision_Wang.pdf），该论文对分布式数据库和分布式深度学习展开了广泛的研究讨论。 希望你如我一般享受其中！请在下方分享你的高见。 原文链接：https://towardsdatascience.com/deep-neural-network-implemented-in-pure-sql-over-bigquery-f3ed245814d3 "
68,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739899&idx=4&sn=77cc8c1ef6161b09302e7cad16d65e67&chksm=871ad0c5b06d59d36e4976e7637535571f7d4f2b49410aa200622266a849ab5308c2103c1e2d&scene=27,CVPR 2018 | 腾讯AI Lab提出新型损失函数LMCL：可显著增强人脸识别模型的判别能力,"arXiv 深度卷积神经网络 (CNN) 已经推动人脸识别实现了革命性的进展。人脸识别的核心任务包括人脸验证和人脸辨识。然而，在传统意义上的深度卷积神经网络的 softmax 代价函数的监督下，所学习的模型通常缺乏足够的判别性。为了解决这一问题，近期一系列损失函数被提出来，如 Center Loss、L-Softmax、A-Softmax。所有这些改进算法都基于一个核心思想： 增强类间差异并且减小类内差异。腾讯 AI Lab 的一篇 CVPR 2018 论文从一个新的角度研究了这个问题，并设计了一个新的损失函数，即增强边缘余弦损失函数 (LMCL)。更具体地说，通过对特征向量和权向量的 L2 归一化，把 softmax 损失函数转化为余弦损失函数，这样做消除了半径方向的变化，并在此基础上引入了一个余弦边缘值 m 来进一步最大化所学习的特征在角度空间的决策边界。因此，采用这种归一化和增强余弦决策边界的方法，能够更有效地起到最大化类间差异和最小化类内差异的作用。作者在最权威的人脸公开测试集上进行了实验评估，这些测试集包括 MegaFace Challenge、YouTube Faces (YTF) 和 Labeled Face in the Wild (LFW)，取得了极其优异的表现，表明了新方法的有效性。 深度卷积神经网络（CNN）的近期进展已经显著提升了多种计算机视觉任务的当前最佳表现，使得深度 CNN 成为了计算机视觉领域主导的机器学习方法。人脸识别是最常见的计算机视觉任务之一，通常包含两个子任务：人脸验证和人脸辨识；其中人脸验证是比较两张人脸以确定它们是否来自同一主体，而人脸辨识是根据人脸图库识别人的身份。这两个任务都涉及到三个阶段：人脸检测、特征提取、分类。深度 CNN 可以提取整齐干净的高层面特征，这使得其可凭借相对简单的分类网络实现优越的表现：通常情况下是后面跟着 softmax 的多层感知器网络。但是，近期的研究发现传统的 softmax 不足以最大化在分类任务上的判别能力。 为了实现更好的判别表现，研究界已经进行了很多研究。所有这些研究在最大化判别能力上都具有一个共同的思想：最大化类间差异且最小化类内差异。 相比于 [1,11,13] 提出的欧几里德边缘（Euclidean margin），角边缘（angular margin）更好，因为角的余弦与 softmax 具有固有的一致性。但是，进一步看，似乎直接在两个不同的类之间引入余弦边缘（cosine margin）会更为自然。此外，余弦的公式与常用于人脸识别的相似度度量是匹配的。从以上角度看，余弦边缘提供了一种用于提升余弦相关的判别信息的直接方法，要优于欧几里德边缘或角边缘。 在这篇论文中，我们通过对特征向量和权重向量的 L2 归一化，把 softmax 损失函数转化为余弦损失函数，从而消除了半径方向上的变化，并在此基础上引入了一个余弦边缘值 m 来进一步最大化所学习的特征在余弦角度空间中的决策边界。具体而言，我们发明了一种巧妙的算法，称为增强边缘余弦损失函数 (LMCL)，其以归一化后的特征为输入，可通过最大化类间余弦边缘来学习高度判别性的特征。 图 1：我们提出的 CosFace 框架。在训练阶段，使用不同类之间的增强边缘学习判别性的人脸特征。在测试阶段，首先将测试数据输入 CosFace 来提取人脸特征，然后再将这些特征用于计算余弦相似度分数以执行人脸验证和人脸辨识。 基于 LMCL，我们开发了一种精巧的深度模型 CosFace，如图 1 所示。在训练阶段，LMCL 引导卷积网络使用增强余弦边缘来学习特征。在测试阶段，卷积网络提取出人脸特征，用以执行人脸验证或人脸辨识。我们的贡献总结如下： 我们接受了最大化类间差异和最小化类内差异的思想，提出了一种全新的损失函数 LMCL，可用于为人脸识别学习高度判别性的深度特征。 根据 LMCL 所带来的超球面特征分布，我们提供了一个合理的理论分析。 在 LFW、YTF 和 Megaface 等流行的人脸数据库上，我们提出的方法在大多数基准上都优于之前的最佳表现。 在本章节中，我们将首先详细介绍我们提出的 LMCL。然后我们将给出 LMCL 与其它损失函数的比较，以表明其优越性。然后我们将描述 LMCL 中所使用的特征归一化技术，以阐明其有效性。最后，我们将给出对所提出的 LMCL 的理论分析。 增强边缘余弦损失函数 (LMCL) 形式上，LMCL 的定义为： 使其满足： 与不同损失函数的比较 我们比较了我们的方法（LMCL）与 Softmax、NSL 和 A-Softmax 的决策边界，如图 2 所示。 图 2：在两个类别上，不同损失函数的决策边界的比较。虚线表示决策边际线，灰色区域是决策边界。 在特征上的归一化 在我们提出的 LMCL 中，归一化方案的目的是推导余弦损失函数的形式和消除半径方向上的变化。和 [3] 中仅归一化权重向量不同，我们的方法是同时归一化权重向量和特征向量。因此，其特征会分布在一个超球面上，其中缩放参数（scaling parameter）s 控制着半径大小。 对 LMCL 的理论分析 图 5：在 LFW 和 YTF 上，具有不同边缘参数值 m 的 CosFace 的表现（%） 表 2：我们提出的 LMCL 与人脸识别社区当前最佳的损失函数的比较。这个表格中的所有方法都使用了同样的训练数据和同样的 64 层 CNN 架构。 表 3：在 LFW 和 YTF 数据集上的人脸验证表现（%）。#Models 表示评估方法中所使用的模型的数量。 表 4：在 Megaface Challenge 1 (MF1) 上的人脸辨识和人脸验证评估。 表 5：在 Megaface Challenge 2 (MF2) 上的人脸辨识和人脸验证评估。 论文：CosFace：用于深度人脸识别的增强边缘余弦损失（CosFace: Large Margin Cosine Loss for Deep Face Recognition） 论文地址：https://arxiv.org/abs/1801.09414  在深度卷积神经网络（CNN）的发展的推动下，人脸识别已经取得了革命性的进展。人脸识别的核心任务涵盖人脸验证和人脸辨识，都涉及到人脸特征判别。但是，深度 CNN 的传统 softmax 损失通常缺乏判别能力。为了解决这个问题，最近有 Center Loss、L-Softmax、A-Softmax 等一些损失函数被提了出来。所有这些改进算法都基于同一个思想：最大化类间差异并且最小化类内差异。在这篇论文中，我们设计了一种全新的损失函数增强边缘余弦损失函数 (LMCL)，从不同的角度实现了这一想法。具体而言，我们通过对特征向量和权向量的 L2 归一化，把 softmax 损失函数转化为余弦损失函数，这样做消除了半径方向的变化，并在此基础上引入了一个余弦边缘值 m 来进一步最大化所学习的特征在角度空间的决策边界。由此，通过归一化和余弦决策边界的最大化，可实现类间差异的最大化和类内差异的最小化。我们将我们使用 LMCL 训练得到的模型称为 CosFace。为了测试我们的方法，我们在 MegaFace Challenge、YouTube Faces (YTF) 和 Labeled Face in the Wild (LFW) 等最流行的公开域人脸识别数据集上进行了大量实验评估。我们在这些基准实验上实现了当前最佳的表现，这证明了我们的方法的有效性。 "
69,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739866&idx=3&sn=7bd925c1fc8b7976afc8b995e1039822&chksm=871ad0e4b06d59f23704020849ba54a837c57252bf2a8aa34c3da00e1802d79e0827e3cef877&scene=27,资源 | 开放Python书籍：一本短小精悍的初学者入门指南,GitHub     如何快速熟悉 Python 编程一直是很多初学者的疑问，我们经常考虑要不要系统地从头开始啃 Python 教程，但这种方法不仅非常枯燥且很难坚持。因此，对于很多入门读者，更好的方式是学习基础的 Python 编程，然后在实践中完善代码技巧。本文介绍了一本非常精炼的 Python 免费书籍，它不仅有基础知识，同时每一章节还有非常多的练习与源代码。 项目地址：https://github.com/joaoventura/full-speed-python 该书是使用实践方法教授基础的 Python 编程语言。其方法相当基础：在简要介绍每个主题之后，读者要解答练习题来巩固知识点。这些练习题都是非常经典的任务，它们可以让我们在短时间内熟悉 Python 的使用。 本书在第二章介绍了 Python 在各种操作系统上的安装并依次介绍了最基本的数据结构、数值和字符串、函数和模块、循环、字典和最后的类与目标。该书每一章都有非常多的配套练习题，且整本书非常短小精悍（22 页）。在该 GitHub 项目的发布页中，作者提供了 PDF 和 Epub 两种阅读版本和源代码。 目录 1 前言 2 安装 2.1 Windows 上的安装 2.2 macOS 上的安装 2.3 Linux 上的安装 3 基本数据类型 3.1 数值型习题 3.2 字符串型习题 3.3 列表型习题 4 模块和函数 4.1 数学模块习题 4.2 函数习题 4.3 递归函数 4.4 递归函数习题 5 迭代和循环 5.1 循环习题 5.2 while 循环习题 6 字典 6.1 字典习题 6.2 子字典习题 7 类 7.1 类习题 7.2 类继承 7.3 类继承习题 Python 的安装其实非常简单，方法也非常多，本书介绍的安装都是不使用 pip 等包管理工具的方法，其它方法还可以安装 Anaconda 等集成开发环境。 第三章介绍了基本的数据类型与数据结构，如下展示了字符串与列表的基本使用： 字符串执行乘积运算可以复制多次，以下展示了列表的简单索引方法。 第四章介绍了基本的模块与函数。首先在机器学习中，模块的导入与调用时非常重要的，因为很多时候科学计算库与模型框架能节省很多时间。以下简单地导入了一个数学模块： 当我们重复使用一个过程时，我们可以将这个过程定义为函数，以在主体函数中重复调用。 第五章介绍了迭代与循环，这是在训练过程中不可缺少的部分。这一部分主要描述了 for 循环与 while 循环，其中 for 循环经常用于列表循环。 根据个人经验，我们感觉 while 循环在机器学习中使用得比 for 循环少。大家在迭代训练训练或处理列表时基本上都是使用的 for 循环，以下展示了简单的 while 循环。 第 6 章介绍了字典，它在机器学习中是非常重要的数据结构。例如我们在 TensorFlow 中使用占位符定义输入数据，那么我们就需要使用字典馈送输入数据与对应的标注。 最后一章介绍了面向对象编程的基础，即类和对象。在机器学习中，使用类来定义模型是非常常见的方法，如果希望简单的看懂模型源代码，那么类与对象的知识就必不可少了。 以上就是该书籍的简要介绍，读者可具体查看该 GitHub 项目，并下载书籍与源代码。 
70,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739899&idx=2&sn=29d6cbaea2a076f3fb275c1661c90337&chksm=871ad0c5b06d59d3814a9dcd0fd6ce557f7bde280f223da7e86186470d8b5239b242d63b8de1&scene=27,深度 | 语义分割网络DeepLab-v3的架构设计思想和TensorFlow实现,"Medium 深度卷积神经网络在各类计算机视觉应用中取得了显著的成功，语义分割也不例外。这篇文章介绍了语义分割的 TensorFlow 实现，并讨论了一篇和通用目标的语义分割最相关的论文——DeepLab-v3。DeepLab-v3 是由谷歌开发的语义分割网络，近日，谷歌还开源了该系列的最新版本—— 。 GitHub 地址：https://github.com/sthalles/deeplab_v3 语义分割 常规的图像分类深度卷积神经网络拥有相似的结构。这些模型以图像作为输入，并输出一个代表图像类别的数值。 通常，分类深度卷积神经网络有 4 种主要运算。卷积、激活函数、池化以及全连接层。传递一张图片，通过一系列这些运算会输出一个包含每个类别标签的概率的特征向量。请注意，在这种设定下，我们是对图片的整体进行分类。也就是说，为一张图像分配一个标签。 与图像分类不同的是，在语义分割中我们将对图像中的每一个像素作出分类。所以，对每个像素而言，模型需要将它归类为预定义的类别之一。换言之，语义分割是在像素级别理解图像。 请记住，语义分割不会区分目标实例。因此，如果我们有同一个类的两个目标，它们最终将具有相同的类别标签。实例分割是区分同一类实例的问题。 语义分割与实例分割的区别。(中) 虽然它们是相同的目标，但它们被分类为不同的目标（实例分割）。(右) 相同的目标，相同的类别（语义分割）。 然而，常规的深度卷积神经网络 (如 AlexNet 和 VGG ) 并不适用于密集预测的任务。首先，这些模型包含许多用于减小输入特征的空间维度的层。结果，这些层最终产生缺乏清晰细节的高度抽象的特征向量。第二，全连接层在计算过程中具有固定的输入规模和松散的空间信息。 作为一个例子，试想通过一系列的卷积来传递图像，而不是使用池化和全连接层。我们将每次卷积都设置成步长为 1，padding 为「SAME」。通过这种处理，每一次卷积都保留了输入的空间维度。我们可以堆叠很多这种卷积，并最终得到一个分割模型。 用于密集预测的全卷积神经网络。请注意，不存在池化层和全连接层。 这个模型可以输出形状为 [W,H,C] 的概率张量，其中 W 和 H 代表的是宽度和高度，C 代表的是类别标签的个数。在第三个维度上应用最大化函数会得到形状为 [W,H,1] 的张量。然后，我们计算真实图像和我们的预测的每个像素之间的交叉熵。最终，我们对计算结果取平均值，并且使用反向传播算法训练网络。 然而，这个方法存在一个问题。正如前面所提到的，使用步长为 1，padding 为「SAME」，保留了输入的维度。但是，那样做的后果就是模型会极其耗费内存，而且计算复杂度也是很大的。 为了缓解这个问题，分割网络通常会有三个主要的组成部分：卷积层、降采样层和上采样层。 图像语义分割模型的编码器-解码器结构。 在卷积神经网络中实现降采样的常用方式有两个：通过改变卷积步长或者常规的池化操作。一般而言，降采样的目标就是减少给定特征图的空间维度。因此，降采样可以让我们在执行更深的卷积运算时不用过多地考虑内存。然而，这样一来在计算的时候会损失一些特征。 值得注意的是，这个架构的第一部分看上去类似于普通的分类深度卷积神经网络。不同的是，其中没有设置全连接层。 在第一部分之后，我们就得到了形状为 [W, H, D] 的特征向量，其中 W,H,D 分别是特征张量的宽度、高度和深度。注意，这个压缩向量的空间维度比原始输入更加少，但是更紧致。 顶部：VGG-16 网络的原始形式。要注意的是，堆叠的卷积层的顶部有三个全连接层。 底部：VGG-16 网络中用 1x1 的卷积代替全连接层。这种改变可以让网络输出粗略的热图。 此时，常规的分类深度卷积神经网络会输出一个包含每个类别概率的密集（非空间）向量。取而代之，我们将这个压缩向量输入到一系列上采样层中。这些上采样层的作用就是重建与输入维度相同的输出向量。 通常，上采样都是基于步长置换卷积（strided transpose convolution）完成的。这些函数从深而窄的层变成浅而宽的层。这里，我们使用置换卷积将特征向量的维度增加到期望的结果。 在大多数论文中，分割网络的这两个部分被称作编码器和解码器。简而言之，第一部分将信息「编码」为压缩向量来代表输入。第二部分（解码器）的作用是将这个信号重建为期望的输出。 有很多基于编码器—解码器结构的神经网络实现。FCNs、SegNet，以及 UNet 是最流行的几个。 模型架构 与大多数编码器—解码器架构设计不同的是，Deeplab 提供了一种与众不同的语义分割方法。Deeplab 提出了一种用于控制信号抽取和学习多尺度语境特征的架构。 Deeplab 把在 ImagNet 上预训练得到的 ResNet 作为它的主要特征提取网络。但是，它为多尺度的特征学习添加了一个新的残差块。最后一个 ResNet 块使用了空洞卷积（atrous convolution），而不是常规的卷积。此外，这个残差块内的每个卷积都使用了不同的扩张率来捕捉多尺度的语境信息。 另外，这个残差块的顶部使用了空洞空间金字塔池化 (ASPP，Atrous Spatial Pyramid Pooling)。ASPP 使用了不同扩张率的卷积来对任意尺度的区域进行分类。 为了理解 Deeplab 的架构，我们需要着重注意这三个部分。(i)ResNet 架构，(ii)  空洞 卷积，(iii)  空洞 空间金字塔池化（ASPP）。接下来我们将逐一介绍这几个部分。 ResNets ResNet 是一个非常流行的深度卷积神经网络架构，它赢得了 ILSVRC 2015 分类任务挑战赛的冠军。它的主要贡献之一就是提供了简化深度学习模型训练的框架。 在 ResNet 的原始形式中，它含有 4 个计算模块。每个模块包含不同数量的残差单元。这些单元以特别的形式执行一系列的卷积运算。同样，每个模块都夹杂了最大池化操作来减少空间维度。 原始论文提出了两种残差单元：基线块和瓶颈块。 基线块包含两个 3x3 的卷积，卷积中使用了 BN（批归一化）和 ReLU 激活函数。 残差模块。左：基线块；右：瓶颈块。 第二个是瓶颈块，它包括三个堆叠的部分，用一系列的 1x1、3x3 和 1x1 的卷积代替了之前的设计。两个 1x1 的卷积操作被用来减少和恢复维度。这使得中间的 3x3 的卷积可以在一个密度相对较低的特征向量上进行操作。此外，每个卷积之后、每个非线性 ReLU 之前都应用了 BN。 为了有助于澄清这个问题，我们将这一组操作定义为一个输入为 x 的函数 F——F(x)。 在 F(x) 中的非线性变换之后，这个单元将 F(x) 的结果和原始输入 x 相结合。这种结合是通过对两个函数求和得到的。原始输入 x 和非线性函数 F(x) 合并带来了一些优势。它使得前面的层可以访问后面层的梯度信号。换句话说，跳过 F(x) 上的操作允许前面的层访问更强的梯度信号。这种类型的连接已经被证明有助于更深网络的训练。 当我们增加模型容量时，非瓶颈单元也表明有助于准确率的提高。然而，瓶颈残差单元具有一些实际优势。首先，它们在几乎相同数量的参数下可执行更多的计算。第二，它们与非瓶颈单元的计算复杂度相似。 在实际中，瓶颈单元更适合于训练更深的模型，因为它们需要的训练时间和计算资源更少。 在我们的实现中，我们将使用完全预激活残差单元（full pre-activation Residual Unit），与标准瓶颈单元的唯一区别在于 BN 和 ReLU 激活函数的放置顺序。对于完全预激活，BN 和 ReLU（按此顺序）出现在卷积之前。 不同的 ResNet 构建模块块体系架构。最左边：原始 ResNet 模块；最右边：改进的完全预激活版本。 正如《Identity Mappings in Deep Residual Networks》中所展示的一样。完全预激活单元要优于其他的变体。 注意，这些设计之间的唯一区别是卷积堆栈中 BN 和 ReLU 的顺序。 空洞 卷积 空洞 卷积（或者扩张卷积）是具有一个因子的常规卷积，这个因子使得我们能够扩展滤波器的视野。 以 3×3 卷积滤波器为例。当扩张因子等于 1 时，它的行为类似于标准卷积。但是，如果将扩张因子设置为 2，则它具有扩大卷积核的效果。 理论上，它是这样工作的：首先，根据扩张率对卷积滤波器进行扩张。然后，它用零填充空白空间，创建稀疏的类似滤波器。最后，使用扩张的滤波器进行常规卷积。 因此，大小为 3x3、扩张率为 2 的卷积将使其能够覆盖 5x5 的区域。然而，因为它的作用就像一个稀疏的过滤器，只有原始的 3 x3 单元将执行计算并生成结果。 以类似的方式，扩张因子为 3 的常规 3×3 的卷积能够得到对应的 7×7 区域的信号。 这种效果允许我们控制计算特征响应的分辨率。此外， 空洞 卷积在不增加参数数量或计算量的情况下增加了更大范围的语境信息。 Deeplab 还表明，必须根据特征图的大小来调整扩张率。他们研究了在小特征图上使用大扩张率的结果。 给小特征图设置更大的扩张率的副作用。对于 14×14 的输入图像，使用扩张率为 15 的 3×3 卷积，其结果和常规的 1×1 卷积类似。 当扩张率非常接近特征图的尺寸时，一个常规的 3×3 的 空洞 滤波器的效果与标准的 1×1 卷积是一样的。 换句话说， 空洞 卷积的效率依赖于对扩张率的选择。由于这一原因，理解神经网络中的输出步长（output stride）的概念是很重要的。 输出步长反映输入图像大小与输出特征图大小的比率，它定义了输入向量在通过网络时经受的信号抽象程度。 输出步长为 16，图像大小为 224x224x3 时，输出特征向量比输入图像的维度小 16 倍，变成了 14x14。 此外，Deeplab 还讨论了不同输出步长对分割模型的影响。Deeplab 认为过强的信号抽象不利于密集预测任务。总之，具有较小输出步长 (较弱信号抽象) 的模型倾向于输出更精细的分割结果。然而，使用较小的输出步长训练模型需要更多的训练时间。 Deeplab 还展示了两种输出步长（8 和 16）设置下的结果。和预期的一样，步长等于 8 能够产生稍微好一些的结果。在这里，出于实际原因，我们选择了 16 为输出步长。 此外，由于 空洞 卷积块没有实现降采样，所以 ASPP 也运行在相同的特征响应大小上。因此，它允许使用相对较大的扩张率从多尺度的语境中学习特征。 新型 空洞 残差块包含三个残差单元。三个单元都总共拥有三个 3×3 的卷积块。在多重网格（multigrid）方法的启发下，Deeplab 为每个卷积设置了不同的扩张率。总之，多重网格为三个卷积中的每个卷积定义了不同的扩张率。 在实际中： 对于 block 4，当输出步长是 16，多重网格为（1,2,4）的时候，这三个卷积的扩张率分别是（2,4,8）。 空洞 空间金字塔池化 空洞 空间金字塔池化（ASPP）的思想是提供具有多尺度信息的模型。为了做到这一点，ASPP 添加了一系列具有不同扩张率的 空洞 卷积。这些扩张率是被设计用来捕捉大范围语境的。此外，为了增加全局的语境信息，ASPP 还通过全局平均池化（GAP）结合了图像级别的特征。 这个版本的 ASPP 包含 4 个并行的操作。它们分别是一个 1×1 的卷积以及三个 3×3 的卷积（扩张率分别是（6,12,18））。正如我们前面所提及的，现在，特征图的标称步长（nominal stride）是 16. 在原始实现的基础上，我们使用 513 x513 的裁剪尺寸进行训练和测试。因此，使用 16 的输出步长意味着 ASPP 接收大小为 32 x32 的特征向量。 此外，为了添加更多全局语境信息，ASPP 结合了图像级别的特征。首先，它将 GAP 应用于从最后一个 空洞 块输出的特征上。其次，所得特征被输入到具有 256 个滤波器的 1x 1 卷积中。最后，将结果进行双线性上采样到正确的维度大小。 最后，各个分支的特征都被通过连接操作结合成一个单独的向量。然后使用另一个 1×1（采用 BN，和 256 个滤波器）的卷积对这个输出进行卷积。 ASPP 之后，我们将结果输入到另一个 1×1 的卷积中去生成最终的分割逻辑。 实现细节 这个实现用 ResNet-50 作为特征提取器，Deeplab_v3 采取了以下网络配置： 输出步长=16 为新的 空洞 残差块（block 4）使用固定的多重网格 空洞 卷积率（1,2,4） 在最后一个 空洞 卷积残差块之后使用扩张率为（6,12,18）的 ASPP。 将输出步长设置为 16 有利于可持续地快速训练。与另一个输出步长 8 相比，输出步长为 16 使得 空洞 残差块处理的特征图比步长为 8 时处理的特征图小四倍。 将多重网格扩张率应用于 空洞 残差块内部的 3 个卷积。 最终，ASPP 中的三个并行的卷积得到了不同的扩张率——（6,12,18）。 在计算交叉熵损失函数之前，我们将分割逻辑调整为输入图像的大小。正如论文中所指出的，为了保持分辨率细节，调整分割逻辑的大小比调整真实标签的大小更好。 基于原始的训练过程，我们使用一个 0.5 到 2 之间的随机因子对每个图像做了扩展。此外，我们还对缩放后的图像做了随机的左右翻转。 最终，我们为训练和测试裁剪了 513 x513 大小的图像。 为了实现残差网络 block4 中具有多重网格的 空洞 卷积，我们仅仅改变了 resnet_utils.py 文件中的以下这段代码： 训练 为了训练网络，我们决定使用来自于《Semantic contours from inverse detectors》的扩增版的 Pascal VOC 数据集。 训练数据由 8252 张图像组成。训练集有 5623 张，验证集有 2299 张。为了使用原始的 VOC2012 验证数据集来测试模型，我们从验证集中删去了 558 张图像。这 558 张图片也出现在官方的 VOC 验证集中。此外，我还添加了来自 VOC 2012 训练集中的 330 幅图像，它们既没出现在 5623 张训练集中，也没出现在 2299 张的验证集中。最后，8252 张图像中的 10%（大约 825 张图像）用来验证，其余的图像留着训练。 注意，这与原始论文是不一样的：这次实现没有在 COCO 数据集上预训练。此外，论文中描述到的一些训练和评估技术也没有用到。 结果 模型能够在 PASCAL VOC 验证集上得到良好的结果。 像素准确率：大约 91% 平均准确率：大约 82% 均交并比（mIoU）：大约 74% 频权交并比（FWIoU）:大约 86% 以下是 PASCAL VOC 验证集的图像分割的结果。 结论 语义分割无疑是计算机视觉领域中最流行的领域之一。Deeplab 提供了一个传统编码器-解码器体系架构的替代方案。它提倡在多范围的语境中使用 空洞 卷积学习特征。 原文链接：https://medium.freecodecamp.org/diving-into-deep-convolutional-semantic-segmentation-networks-and-deeplab-v3-4f094fa387df "
71,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739866&idx=4&sn=dc5dc8b25beda7803295d7e0623c1cac&chksm=871ad0e4b06d59f2aafc872fdd934916ffdf8be96efdbc4648cc70d00116169dcaa2b5650de6&scene=27,CVPR 2018 | 商汤科技提出GeoNet：用无监督学习感知3D场景几何,"arXiv     有效的无监督学习方法能缓解对有标注数据的需求，无监督学习技术与视觉感知领域的结合也有助于推动自动驾驶等高价值技术的发展。近日，商汤科技的一篇 CVPR 2018 论文提出了一种可以联合学习深度、光流和相机姿态的无监督学习框架 GeoNet，其表现超越了之前的无监督学习方法并可比肩最佳的监督学习方法。本论文的作者为 Zhichao Yin 和 Jianping Shi。机器之心对该论文进行了简要介绍，更多详情请参阅原论文。 理解视频中的 3D 场景几何是视觉感知领域内的一项基本主题。其中包括很多经典的计算机视觉任务，比如深度恢复、流估计、视觉里程计（visual odometry）。这些技术有广泛的工业应用，包括自动驾驶平台、交互式协作机器人以及定位与导航系统等。   传统的根据运动恢复结构（SfM：Structure from Motion）方法是以一种集成式的方式来解决这些任务，其目标是同时重建场景结构和相机运动。在稳健的和鉴别式的特征描述系统、更有效的跟踪系统以及更好地利用形义层面的信息等方面最近已经取得了一些进展。尽管如此，容易受到异常值和无纹理区域故障的影响的问题仍然还未完全消除，因为它们本质上依赖于高质量的低层面特征对应。   为了突破这些局限，研究者将深度模型应用到了每个低层面子问题上，并且相对于传统方法实现了显著的增益。其中主要的优势来自于大数据，这有助于为低层面的线索学习获取高层面的形义对应，因此相比于传统方法，即使在不适定（ill-posed）的区域，深度模型也能取得优良的表现。   尽管如此，为了保证在更一般的场景中也能表现优良，深度学习通常需要大量基本真值数据（groundtruth data）。在大多数情况下，需要基于昂贵的激光的设置和差分 GPS，这就限制了对大规模数据的获取。此外，之前的深度模型大都是为解决单个特定任务而设计的，比如深度、光流、相机姿态等。它们没有探索这些任务之间固有的冗余性（redundancy），这可以通过几何规律根据 3D 场景构建的本质性质来形式化。 最近已有工作研究了将这些问题与深度学习一起形式化。但所有这些工作都存在固有的局限性。比如，它们需要大量激光扫描的深度数据来进行监督学习，需要立体相机作为获取数据的额外设备或不能明确处理非刚性（non-rigidity）和遮挡问题。   在这篇论文中，我们提出了一种无监督学习框架 GeoNet，可用于根据视频联合估计单眼深度、光流和相机运动。我们的方法基于 3D 场景几何的本质性质。直观的解释就是大多数自然场景都由刚性的静态表面组成，即道路、房屋、树木等。它们投射在视频帧之间的 2D 图像运动完全由深度结构和相机运动决定。同时，行人和车辆等动态目标通常存在于这样的场景中，而且通常具有大位移和扰乱性（disarrangement）的特点。   由此，我们使用了深度卷积网络来理解上述直观现象。具体来说，我们的范式使用了一种 分治策略（divide-and-conquer strategy）。我们设计了一种全新的二级式级联架构来适应地处理场景刚性流和目标运动。因此这个全局的运动域（motion field）可以逐步得到细化，让我们的整个学习流程变成一种分解的且更易于学习的形式。由这种融合的运动域引导的视图合成损失（view synthesis loss）可以为无监督学习实现自然的正则化。图 1 给出了预测示例。 第二个贡献是我们引入了一种全新的自适应几何一致性损失（geometric consistency loss）来克服纯视图合成目标中未包含的因素，比如遮挡处理和照片不一致问题。通过模仿传统的前向-反向（即向左-向右）一致性检查，我们的方法可以自动滤除可能的异常值和遮挡。预测一致的地方会在无遮挡区域中的不同视图之间得到强化，而错误的预测则会被平滑处理掉，尤其是被遮挡的区域。   最后，我们在 KITTI 数据集的全部三项任务上全面地评估了我们的模型。我们的无监督方法的表现优于之前的无监督方法，并且可媲美监督方法的结果，这体现了我们的范式的有效性和优势。 GeoNet 概述   我们提出的 GeoNet 能以一种无监督的方式通过 3D 场景几何的本质性质来感知 3D 场景几何。特别需要指出，我们分别使用了刚性结构重建器和非刚性运动定位器来分开学习刚性流和目标运动。我们采用了图像外观相似度来引导无监督学习，这无需任何标注成本就能泛化到无限多的视频序列上。   图 2 给出了我们的 GeoNet 的概览图。它包含两个阶段：刚性结构推理阶段和非刚性运动细化阶段。第一个推理场景布局的阶段由两个子网络构成，即 DepthNet 和 PoseNet。深度图和相机姿态分别经过回归处理后再融合到一起，得到刚性流。此外，第二个阶段通过 ResFlowNet 实现，用于处理动态目标。ResFlowNet 学习得到的残差非刚性流再与刚性流相结合，就推导出了我们的最终流预测。因为我们的每个子网络的目标都是解决一个特定的子任务，因此复杂的场景几何理解目标就分解成了一些更简单的目标。我们将不同阶段的视图合成用作我们的无监督学习范式的基本监督。   最后但并非不重要的是，我们会在训练期间执行几何一致性检查，这能显著提升我们的预测一致性并得到出色的表现。 论文：GeoNet：密集的深度、光流和相机姿态的无监督学习（GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose） 论文链接：https://arxiv.org/abs/1803.02276 我们提出了 GeoNet，这是一种可以从视频中联合学习单眼深度、光流和自我运动估计的无监督学习框架。这三个分量可以根据 3D 场景几何的本质性质而组合到一起，通过我们的框架以一种端到端的方式联合学习得到。具体而言，该框架可以根据单个模块的预测提取几何关系，然后可以将这些几何关系组合成一个图像重建损失，可用来分别推理静态和动态的场景部分。此外，我们还提出了一种自适应几何一致性损失，用以提升模型对异常值和非朗伯区域的稳健性，这能有效地解决遮挡和纹理模糊问题。我们在 KITTI 驾驶数据集上进行了实验，结果表明我们的方法能在所有三项任务上实现当前最佳的结果，表现优于之前的无监督方法，并可与监督方法媲美。 "
72,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739899&idx=1&sn=1da672e06e17f1041de9967c1e924370&chksm=871ad0c5b06d59d3327418ac72f8cd6b75894bfb66a18492ec77f24898c9467686276af44ac6&scene=27,AlphaGo背后的力量：蒙特卡洛树搜索入门指南,"int8 Blog 我们都知道 DeepMind 的围棋程序 AlphaGo，以及它超越人类的强大能力，也经常会听到「蒙特卡洛树搜索」这个概念。事实上，蒙特卡洛树搜索是在完美信息博弈场景中进行决策的一种通用技术，除游戏之外，它还在很多现实世界的应用中有着广阔前景。本文中，我们会以 AlphaGo 为例子，对这一方法进行详细介绍。 长久以来，学术世界一直认为计算机在围棋这个复杂游戏上达到超越人类的水平是几乎无法实现的。它被视为人工智能的「圣杯」——一个我们原本希望在未来十年挑战的遥远里程碑。在国际象棋上，「深蓝」曾在 20 多年前实现了自己的目标，而其后数年，没有一个围棋引擎能够打败人类顶尖棋手。围棋及其引发的「数字混沌」是如此令人着迷，以至于人们一度将其想象为人类「对抗」计算机的最后壁垒。 然而正如我们所知，2016 年 DeepMind 推出的人工智能围棋程序 AlphaGo 结束了这一局面，它在当年 3 月份的系列比赛中以 4：1 的比分击败了来自韩国的前世界冠军李世石。AlphaGo 证明了世人对于虚拟和现实世界的怀疑是错误的。而在短短一年之后，新一代围棋程序  ，这无疑宣告了人类在围棋上和计算机的差距已经越来越远。 作为今天最被人们所熟知的人工智能系统（没有之一），AlphaGo/Zero 是一个多种计算方法的集合体，人类工程学的杰作，其核心组件包含： 蒙特卡洛树搜索——内含用于树遍历的 PUCT 函数的某些变体 残差卷积神经网络——其中的策略和价值网络被用于评估棋局，以进行下一步落子位置的先验概率估算。 强化学习——通过自我对弈进行神经网络训练 在本文中，我们只着重于介绍蒙特卡洛树搜索（MCTS/Monte Carlo Tree Search）。这个算法很容易理解，而且也在游戏人工智能领域外有很多应用。 目录 1 介绍 1.1 有限两人零和回合制游戏 1.2 如何表征一个游戏 1.3 什么是最有潜力的下一步？简要介绍极小极大（minimax）算法和 alpha-beta 修剪算法 2 蒙特卡洛树搜索——基本概念 2.1 模拟——AlphaGo 和 AlphaZero 2.2 博弈树的展开节点、完全展开节点和访问节点 2.3 反向传播：将模拟结果传播回去 2.4 关于节点的统计学 2.5 博弈树遍历 2.6 树的置信上限 2.7 终止蒙特卡洛树搜索 3 总结 蒙特卡洛树搜索是由前里尔第三大学助理教授 Rémi Coulom 在围棋程序 Crazy Stone 中首先引入的方法——后者是第一个在围棋上达到职业五段水平的计算机程序。 从最直观的角度来看，蒙特卡洛树搜索有一个主要目的：给出一个「游戏状态」并选择「胜率最高的下一步」。在本文中，我会试图解释蒙特卡洛树搜索的大多数细节，其中我们也会不时回顾 AlphaGo/Zero，并试图解释那些在 DeepMind AI 程序系列中使用的 MCTS 变体。 有限两人零和回合制游戏 蒙特卡洛树搜索运行的框架/环境是「游戏」，其本身是一个非常抽象的广义术语，所以在这里我们只针对于一种游戏类型：有限两人零和回合制游戏——这听起来或许有点复杂，不过其实很简单，让我们来分析一下： 「游戏」意味着处理「互动情况」。互动意味着有玩家会参与进来（一个或多个） 「有限」表示在任何时间点上，玩家之间都有有限的互动 「两人」有限游戏，顾名思义 「回合制」表示玩家按照一定顺序进行游戏——轮流出招 最后「零和游戏」——这意味着游戏双方有着相反的目标，换句话说：在游戏的任何终结状态下，所有玩家获得的总和等于零。有时这样的游戏也被称为严格竞争博弈 我们可以轻易验证围棋、国际象棋或井字棋是有限两人零和回合制游戏。的确，它们都是两个玩家，游戏可选的下一步也是有限的，且游戏是严格竞争的——两名玩家会进行对抗（游戏的所有输出之和为零）。 Notes：请注意，为了简化本教程，我们只专注于可能场景的某系子集，蒙特卡洛树搜索是一个应用广泛的工具，适用于两人有限零和游戏以外。更为全面的概述请参阅：http://mcts.ai/pubs/mcts-survey-master.pdf 如何表征一个博弈 形式上，一个博弈由一系列的基本数学实体表征。在一本 PhD 级别的博弈论书中你可以找到这样的定义： 定义 1. 一个博弈的扩展式可以用一个多元组来定义： 从计算机编程的角度来看形式化的定义可能难以理解，但幸运的是，我们可以使用一种著名的数据结构以简单的形式来表征一个博弈：博弈树。 博弈树是一种树结构，其中每一个节点表征博弈的确定状态。从一个节点向其子节点的转换被称为一个行动（move）。节点的子节点数目被称为分支因子（branching factor）。树的根节点表征博弈的初始状态。我们还区分了博弈树的端节点（terminal nodes），即没有子节点的节点，表示博弈无法再继续进行。端节点的状态可以被评估，并总结博弈的结果。 在上图的井字棋博弈树（部分展示）的例子中： 在顶部，你可以看到树的根节点，其表征了井字棋博弈的初始状态，即空白棋盘（标记为绿色）； 任何从一个节点向另一个节点的转换被称为一个行动； 井字棋的分支因子是变化的，它依赖于树的深度； 从一个根节点到一个端节点的树遍历表征了单个博弈过程。 博弈树是一种递归的数据结构，因此当你选择了一个最佳行动并到达一个子节点的时候，这个子节点其实就是其子树的根节点。因此，你可以在每一次（以不同的根节点开始），将博弈看成由博弈树表征的「寻找最有潜力的下一步行动」问题的序列。在实践中很常见的是，你不需要记住到达当前状态的路径，因为它在当前的博弈状态中并不重要。 什么是最有潜力的下一步行动？简要介绍极小极大（minimax）策略和 alpha-beta 剪枝算法 再次提醒，我们的最终目标是在给定博弈状态的前提下，利用博弈树寻找最有潜力的下一步行动。但这究竟是什么意思呢？ 这个问题并没有直接的答案。首先你不能提前知道对手的策略，对手可能是个高手，也可能是个菜鸟。假定在国际象棋中，你知道对手是个业余爱好者（数学家会说，你的对手使用的是混合策略），你可以使用简单的策略来尝试欺骗对手并快速获得胜利。但很明显，同样的策略在面对强大的对手时将适得其反。 如果你完全不了解对手，那么你可以使用一种非常保守的策略即极小极大算法，在假定你的对手执行最佳行动的前提下，最大化你的收益，也可以说在各种获得最小收益的策略中选择有最大收益的策略。这种方法以放弃最优策略为代价，从而最小化了风险，因此它是一种非常保守的方法。在 A 和 B 的两人有限零和序列博弈中（其中 A 尝试最大化其收益，而 B 尝试最小化 A 的收益），极小极大算法可以用以下的递归形式来描述： 其中： v_A 和 v_B 分别是玩家 A 和玩家 B 的效用函数（效用=收益）； move 是一个函数，它在给定当前状态 s_i 和在该状态的动作 a_i 下，生成下一个博弈状态（当前节点的子节点之一）； eval 是一个评估最终博弈状态（在端节点处）的函数； s hat 是任意的最终博弈状态（即一个端节点）； 右下方式子的负号表示该博弈是一个零和博弈。 简单来说，给定状态 s，并假定对手尝试最小化你的收益，你希望找到能最大化收益的动作 a_i。这正是该算法被称为极小极大的原因。我们需要做的就是展开整个博弈树，并反向传播由递归形式的规则得到的值。 上图中的博弈树展示了极小极大算法中的最佳行动选择过程。白皇后希望博弈的结果尽可能的黑暗（冷色，奖励值=像素强度），而黑皇后希望博弈的结果尽可能的明亮（暖色）。每一个层级的选择都是极小极大判断的最优结果。我们可以从底部的终端节点开始，其中的选择是很明显的。黑皇后将总是选择最明亮的颜色，然后白皇后将寻找最大的奖励并选择到达最暗颜色的路径，等等。这正是基础的极小极大算法的执行过程。 极小极大算法的最大弱点是它需要展开整个博弈树。对于有高分支因子的博弈（例如围棋或国际象棋），该算法将导致巨大的博弈树，使得计算无法进行。 那么有什么解救的办法吗？其中一个方法是仅在确定的阈值深度 d 内展开博弈树，但是我们无法保证在阈值深度 d 处的任何节点是否端节点。因此我们一个函数来评估非终端博弈状态。这对于人类来说很自然：即使博弈仍在进行，你也可能通过观察围棋或国际象棋的棋盘预测胜者。例如，对以下棋局可以很容易知道结束棋局的走法。 另一种克服博弈树规模过大问题的方法是通过 alpha-beta 剪枝算法来修剪博弈树。alpha-beta 剪枝是提升版的极小极大算法，它以极小极大算法的形式遍历博弈树，并避免某些树分支的展开，其得到的结果在最好的情况下等于极小极大算法的结果。alpha-beta 剪枝通过压缩搜索空间提高搜索效率。 极小极大算法和 alpha-beta 修剪算法已经是相当成熟的解决方案，目前已被用于多个成功的博弈引擎例如 Stockfish——AlphaZero 的主要对手之一。 在蒙特卡洛树搜索算法中，最优行动会通过一种新颖的方式计算出来。顾名思义，蒙特卡洛树搜索会多次模拟博弈，并尝试根据模拟结果预测最优的移动方案。 蒙特卡洛树搜索的主要概念是搜索，即沿着博弈树向下的一组遍历过程。单次遍历的路径会从根节点（当前博弈状态）延伸到没有完全展开的节点，未完全展开的节点表示其子节点至少有一个未访问到。遇到未完全展开的节点时，它的一个未访问子节点将会作为单次模拟的根节点，随后模拟的结果将会反向传播回当前树的根节点并更新博弈树的节点统计数据。一旦搜索受限于时间或计算力而终止，下一步行动将基于收集到的统计数据进行决策。 下面有一些关于上述蒙特卡洛树搜索过程的关键问题，它们有助于我们的理解： 什么是展开或未完全展开的博弈树？ 在搜索过程中，向下遍历是什么意思？如何选择访问的下一个子节点？ 什么是模拟？ 什么是反向传播？ 反向传播回的统计数据是什么，在展开博弈树结点更新的是什么? 最后的行动策略到底是如何选择的？ 下面，我们将依次解决这些问题，因而能对蒙特卡洛树搜索有一个清晰的理解。 模拟 首先我们会关注于模拟，它并不会过多依赖于其它术语的定义。模拟即单次博弈策略，它是一系列从当前节点（表示博弈状态）开始，并在计算出博弈结果后结束于端节点。模拟是一个在随机博弈初始点开始评估近似计算的博弈树节点。那在模拟中如何选择行动呢？ 在模拟中，行动可以通过 rollout 策略函数选择： 该函数将输入一个博弈状态，并产生下一次行动的选择。在实践中，该函数会设计为允许很多次模拟快速进行，一般默认的 rollout 策略函数可以是服从均匀分布的随机采样。 Alpha Go 和 Alpha Zero 中的模拟 在 Alpha Go Lee 叶 S_L 的评估中，它会采用以下两个分量的加权和： 带有自定义快速 rollout 策略的标准 rollout 评估 z_L，它是一个带有人工特征的浅层 softmax 神经网络。 称之为价值网络的 13 层卷积网络 v_0 从 Alpha Go 自我对抗中抽取 30mln 不同位置进行训练，并最后预测评估位置。 Deepmind 的工程师在 Alpha Zero 中更进一步，他们根本不会执行模拟，他们会使用 19 层 CNN 残差网络直接评估当前节点。 最简单的模拟形式只是在给定博弈状态下的随机行动序列。模拟总会产生一个评估，对于博弈来说，该评估就是胜利、失败或平局等结果，但通常任何值都可以是模拟的合理结果。 在蒙特卡洛树搜索模拟中，我们始终会从一个前面没访问的节点开始，因此下面会介绍关于访问节点的意义。 博弈树的展开节点、完全展开节点和访问节点 现在我们需要思考人类是如何考虑围棋或象棋等博弈的。给定一个根节点并加上博弈的规则，那么博弈树的其余部分就已经隐含表示出来了。我们可以遍历它而不需要将整棵树储存在内存中。在最初的根节点中，它是完全未展开的，我们处于博弈的初始状态，其余所有节点都没有被访问。一旦我们需要执行一个行动，我们就会思考采用该行动后会产生怎样的结果，因此访问一个节点后，需要分析该节点位置与带来的效用。 蒙特卡洛树搜索也是采用相同的特性构建博弈树。所有节点可以分为访问或未访问，那么一个节点的访问到底指的是什么？一般而言，如果模拟将该节点作为初始节点，这就意味着它至少评估了一次，那么它就可以视为已访问节点。如果某节点的所有子节点都是已访问节点，那么它就可视为完全展开的节点，相对而言也就存在未完全展开的节点。 在实践中，搜索开始时，根节点的所有子节点都未被访问。然后一个节点被选中，第一个模拟（评估）就开始了。 请注意：模拟过程中 rollout 策略函数选择的节点并未被访问。它们仍然是未被访问状态，即使 rollout 经过它们，只有模拟开始的那个节点是被访问的状态。 反向传播：将模拟结果传播回去 当初次访问节点的模拟结束后，其结果会反向传播至当前博弈树的根节点。模拟开始的节点被标注为已访问。 反向传播是从子节点（模拟开始的地方）遍历回根节点。模拟结果被传输至根节点，反向传播路径上的每个节点的统计数据都被计算／更新。反向传播保证每个节点的数据都会反映开始于其所有子节点的模拟结果（因为模拟结果被传输回博弈树的根节点）。 节点的统计数据 反向传播模拟结果的目的是更新反向传播路径（包括模拟起始的节点）上所有节点 v 的总模拟奖励 Q(v) 以及总访问次数 N(v)。 Q(v) 即总模拟奖励是节点 v 的一项属性，在最简单的形式中是通过考虑的节点得到的模拟结果的总和。 N(v) 即总访问次数是节点 v 的另一项属性，表示节点 v 位于反向传播路径上的次数（即它对总模拟奖励做出了多少次贡献）。 每个被访问节点都会保存这两个值，一旦完成了确定次数的模拟之后，被访问的节点就保存了它们被利用/探索（expolited/explored）的信息。 换句话说，当你查看任意节点的统计数据时，这两个值将反映该节点的潜在价值（总模拟奖励）和它被探索的程度（总访问次数）。高奖励的节点是很好的可利用候选，而那些访问次数少的节点也可能是有价值的（因为它们尚未得到很好的探索）。 我们还缺少一块拼图。如何从一个根节点到达一个未访问节点，来启动一次模拟呢？ 博弈树遍历 在搜索最开始的时候，由于我们还没有进行任何模拟，所以先选择未被访问的节点。在每个未被访问的节点上进行单次模拟，结果被反向传播至根节点，然后根节点即被认为经过了完全展开。 但是接下来怎么做呢？现在我们如何从完全展开的节点导向未被访问的节点呢？我们必须遍历被访问节点的层，目前没有很好的继续进行的方式。 为了在路径中找到下一个节点，以通过完全展开的节点 v 开始下一次模拟，我们需要考虑 v 所有子节点 v_1, v_2, …, v_k 的信息，以及节点 v 本身的信息。现在我们来思考一下可用信息有哪些： 当前节点（蓝色）是完全展开的，因此它必须经过访问，以存储节点数据：它及其子节点的总模拟奖励和总访问次数。这些值是为了最后一部分：树的置信上限（UCT）做准备。 树的置信上限 UCT 是一个函数，使我们在被访问节点中选择下一个要遍历的节点，这也是蒙特卡洛树搜索的核心函数： UCT 最大的节点就是蒙特卡洛树搜索遍历过程中选择的节点。让我们来看看 UCT 函数如何运行： 首先，该函数为节点 v 的子节点 v_i 而定义，它包括两个组件：第一个组件是 ，又叫做 exploitation 组件，可以理解为赢／输率，总模拟奖励（simulation reward）除以总访问次数，即节点 v_i 的胜率评估结果。我们当然更想遍历具备高赢率的节点。 为什么不仅仅使用 exploitation 组件呢？因为我们会在搜索开始时很快结束对取得单次获胜的节点的贪婪探索。 简单示例： 假设我们仅使用 exploitation UCT 组件开始蒙特卡洛树搜索。从根节点开始，我们对所有子节点进行一次模拟，然后下一步仅访问那些模拟结果至少有一次是赢的节点。第一次模拟结果不幸失败的节点会立刻被舍弃。 因此我们还要有第二个 UCT 组件 exploration。exploration 组件支持未被探索的节点，这些节点相对来说更少被访问（N(v_i) 较低）。我们来看一下 UCT 函数 exploration 组件的形状：随着节点访问量的增加而递减，给访问量少的节点提供更高的被选中几率，以指引 exploration 探索。 最终，UCT 公式中的参数 c 控制蒙特卡洛树搜索中 expolitation 和 exploration 组件之间的权衡。 UCT 函数中的一个重要标志是：在竞争性游戏中，其 exploitaion 组件 Q_i 的计算通常与在节点 i 处行动的玩家有关，这意味着在遍历博弈树时，玩家视角根据被遍历的节点而变化：对于任意连续节点，玩家视角都是相反的。 终止蒙特卡洛树搜索 现在我们了解了实现蒙特卡洛树搜索所需要的所有因素，但还有一些问题需要回答。首先，我们什么时候可以终止 MCTS？答案是：看情况。如果你构建一个游戏引擎，那么你的「思考时间」有限，计算能力也有限。因此最安全的选择是只要资源允许，就可以一直运行 MCTS。 一旦 MCTS 过程结束，最好的一步通常是具备最高访问量 N(v_i) 的一步，因为它的奖励值评估结果最好（评估的值必须很高，因为它被探索的频率也最高）。 在使用蒙特卡洛树搜索走了一步之后，你的选择节点就变成了对手下一步的起始游戏状态。一旦他走了一步，你就可以执行蒙特卡洛树搜索，从表示对手选择游戏状态的节点开始。之前的 MCTS round 数据可能仍然在你现在考虑的新分支以内。这就可以重新使用数据而不是从头构建新的树，事实上这就是 Alpha Go / Alpha Zero 创造者所做的。 现在，我们来回顾一下蒙特卡洛树搜索的简单定义，并将其封装进伪代码： 你可以看到它缩减至非常少的函数，这些函数对任何游戏都有效，不只是围棋或象棋。你可以在这里找到蒙特卡洛树搜索用于井字棋（Tic-Tac-Toe）的实现示例：https://github.com/int8/monte-carlo-tree-search。 希望本文对大家有所帮助。 "
73,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739866&idx=2&sn=1653be668016b07828266e48424b2fee&chksm=871ad0e4b06d59f29c3195b438eff96506511c9aa191976a69306fbd1d24c2ff65938703fc78&scene=27,教程 | 如何使用JavaScript实现GPU加速神经网络,Towards Data Science    本文作者 Sebastian Kwiatkowski 介绍了使用 JavaScript 实现 GPU 加速神经网络的四个项目：deeplearn.js、Propel、gpu.js 和 Brain.js。 根据 GitHub Octoverse 2017 报告，JavaScript 是过去一年中 GitHub 最流行的编程语言。根据 pull requests 的数量，JavaScript 的体量与 Python、Java 以及 Go 语言的总和相当。 JavaScript 已经征服了 Web，并在服务器、移动电话、桌面和其他平台上取得了进展。 与此同时，GPU 加速的使用已经远远超出了计算机图形学的范围，它现在已经成为机器学习的一个组成部分。 训练深层神经网络是一个计算密集型过程，深度神经网络在机器智能的许多重要领域得到了当前最优结果。 本文着眼于这些趋势的持续融合，并概述了将 GPU 加速的神经网络引入 JavaScript 的一些项目。 概述 本文列出的所有项目都是正被社区积极维护的，它们在 GitHub 上有着数千 stars，并且通过 NPM 或 CDN 进行分发。 它们都是通过 WebGL 在浏览器中实现 GPU 加速的，如果没有合适的显卡，则返回到 CPU 模式。 本概述不包含旨在运行现有模型（尤其是使用 Python 训练的模型）的库。 最后，有 4 个项目被列入清单。 尽管 deeplearn.js 的特征集是面向神经网络的，但是它也可被看作是一个通用的机器学习框架。Propel 是一个用于科学计算的库，提供自动微分功能。gpu.js 提供了在 GPU 上运行 JavaScript 函数的便捷方式。Brain.js 是一个较老的神经网络库的延续，它使用 gpu.js 来完成硬件加速。 Deeplearn.js Deeplearn.js  是以上四个项目中最流行的，被描述为「用于机器智能的硬件加速 JavaScript 库」。它由 Google Brain 团队和一个超过 50 位贡献者的社区共同支持。两位主要作者是 Daniel Smilkov 和 Nikhil Thorat. deeplearn.js 中卷积层的定义 deeplearn.js 是仿照 TensorFlow 用 TypeScript 写成的。deeplearn.js 支持由 Google Brain 主要开源项目提供的一个功能子集。API 基本上拥有 3 个部分（API 地址：http://www.deeplearnjs.org/docs/api/index.html）。 第一部分包括用来创建、初始化以及变换张量的函数（http://www.deeplearnjs.org/docs/api/index.html#Tensors-Creation），用类似数组的结构来保存数据。 第二部分提供了在张量上执行的操作（http://www.deeplearnjs.org/docs/api/index.html#Operations-Arithmetic），包括基本的数学运算、规约（reduction）、正则化以及卷积。对循环神经网络的支持目前还处于初级阶段，但是已包括 LSTM 单元的堆叠（http://www.deeplearnjs.org/docs/api/index.html#dl.multiRNNCell）。 API 的第三部分围绕模型训练展开。所有流行优化器，从随机梯度下降到 Adam 都包含在其中。不过，目前 reference 中提及的损失函数只有交叉熵损失函数。 API 其他部分用来进行环境设置和资源管理。 可以通过 headless-gl（https://github.com/stackgl/headless-gl0）在 node.js 中实现 GPU 加速的实验（参见 issue #49，https://github.com/PAIR-code/deeplearnjs/issues/49）。 项目网站有很多优秀的 demo（http://www.deeplearnjs.org/index.html#demos），包括使用循环神经网络进行钢琴演奏、用来构建模型的可视化界面，以及基于 SqueezeNet（一个使用较少参数的图像分类器）的 webcam 应用。 PropelJS PropelJS 被描述为「可微分编程的 JavaScript」。这份工作由主要作者 Ryan Dahl 和 Bert Belder 以及其他 11 位贡献者完成。 自动微分（AD）是这个项目的核心，它使得我们无需手动指定导数。给定一个由支持的张量运算定义的函数 f(x)，它的梯度函数可以使用 grad（http://propelml.org/docs/#grad）得到。多变量的情况可以使用 multigrad 完成（http://propelml.org/docs/#multigrad）。 除了自动微分之外，目前尚不清楚该项目的方向。虽然网站上提到其目标是成为「类似 numpy 的基础架构」，但该项目目前仍在开发中，并且包含与神经网络（http://propelml.org/docs/#conv2d）和计算机视觉（http://propelml.org/docs/#imread）相关的功能。npy 文件的内容可以通过 load 函数（http://propelml.org/docs/#load）进行解析，并作为张量使用。 在浏览器环境中，PropelJS 利用了 deeplearn.js 中的 WebGL 功能。对于节点中的 GPU 加速，该项目则使用了 TensorFlow 的 C API。 gpu.js 虽然我的大部分经验是使用 CUDA 而不是 WebGL，但我可以证明 GPU 编程的耗时性。因此，当我遇到 gpu.js 时，我感到非常意外。该项目在 GitHub 上拥有约 5700 个 stars，在知名度方面与 deeplearn .js 相当，共有 18 位贡献者。Robert Plummer 是主要作者。 使用 gpu.js 进行矩阵乘法运算，相当于 GPU 编程中的 Hello World！ 在当前语境中，内核是在 GPU 而不是 CPU 上执行的函数。使用 gpu.js，内核可以用 JavaScript 的子集（https://github.com/gpujs/gpu.js#creating-and-running-functions）编写。然后编译代码并在 GPU 上运行。几周前，gpu.js 支持基于 OpenCL 的 Node.JS（https://github.com/mikeseven/node-opencl/issues/55）。 数字和最多具有三维的数组被用作输入和输出。除了基本的数学运算之外，gpu.js 还支持局部变量、循环和 if/else 语句。 为了实现代码重用并允许更多模块化设计，你们可以注册自定义函数 ( https://github.com/gpujs/gpu.js#adding-custom-functions #），然后从内核代码中使用。 在内核的 JavaScript 定义中，this 对象提供线程标识符，并存储在实际内核里是常量、在外部是动态变量的值。 该项目专门研究加速 JavaScript 函数，并不试图提供神经网络框架。为此，我们可以求助一个依赖 gpu.js 的库。 Brain.js Brain.js 继承自 harthur/brain（https://github.com/harthur/brain），一个可以回溯至 2010 年的 repo。 共有近 30 人对这两个 repo 做出了贡献。 对 GPU 加速神经网络的支持基于 GPU.js，这可以算得上该项目近期最重要的进展了。 除了前馈网络之外，Brain.js 还包括三种重要 RNN 类型的实现（https://github.com/BrainJS/brain.js#neural-network-types）：经典 Elman 网络、LSTM，以及具备门控循环单元的近期网络。 该 repo 包含的 demo 处于早期阶段。源代码中还有另外两个演示 ( https://github.com/BrainJS/brain.js/tree/develop/examples)，其中一个 demo 涉及检测用 ASCII 码绘制的字符。 针对机器学习的加速 JavaScript 库有很多有趣的应用。 在线课程可以将与机器学习或 GPU 计算相关的练习直接集成到 web 应用程序中。学生不必跨不同的操作系统和软件版本去设置单独的开发环境。 许多基于神经网络的 demo 可以更容易地部署，并且不再需要服务器端 API。 对机器学习感兴趣的 JavaScript 开发者可以充分利用他们的专业技能，在集成问题上花费更少的时间。 此外，客户端上的可用计算资源应该被更好地利用。毕竟，并非所有的显卡都一直用于虚拟现实和挖矿。 需要说清楚，我现在并不主张将本文中提到的库用于任务关键型神经网络。Python 生态系统仍然是大多数应用程序的首选。 然而，过去 12 个月取得的进展确实令人鼓舞。一年前既没有 deeplearn.js，也没有 Propel。彼时 gpu.js repo 中的活动水平相对较低，Brain.js 也不支持 GPU 加速。 随着时间的推移，这些项目将在某些方面与已建立的框架发生竞争，并催生出 JavaScript 完美适合的全新应用。 原文链接： https://towardsdatascience.com/gpu-accelerated-neural-networks-in-javascript-195d6f8e69ef 
74,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739866&idx=5&sn=3472aa505aa01dfee941fc6379ee5097&chksm=871ad0e4b06d59f2e4a8a541610a1980302a26ee74b0f38a5584deee2571165ee27f15c6880d&scene=27,报名 | 你的公司是AWS创业大赛要找的爆点项目吗？,专家告诉你，AI 创业的爆点在哪里。 这几年，人工智能可以说是热得发烫。   有不少先行者已经选择投身人工智能大潮中随之翻腾起伏，也有不少尚未下定决心的观望者站在岸边跃跃欲试。   与两三年前不同，经过了初期的行业发酵，现在可供创业者们选择的算法框架和平台芯片的数目和质量都有所提升，技术似乎也不再是阻拦那些在人工智能领域里打拼的团队在圈内崭露头角的门槛。   相反，对业务的理解、行业脉络的认知以及风口机会的把握，则成为了每一个创始团队成员最为关注的事情：   到底如何切入这些所谓的技术热门落地场景？沿着业务路径走下去是会赚得盆满钵满还是血本无归？那些所谓的风口究竟是昙花一现还是终将成为大势？   3 月 21 日，由 Amazon Web Services（AWS）中国以及启迪之星联合主办的「AWS 智能之星」创业大赛开幕式上，人工智能行业内的知名投资人、孵化器负责人、专家学者以及领域内的优秀创业者针对这些问题展开了讨论，分享了有关人工智能领域的创业热点的探索以及如何让人工智能初创公司加速成长的经历。   这是一个见仁见智的问题。   在启迪之星创投总经理刘博看来，人工智能是一个有机的体系，创业者要看自己在有机整体里自己的核心竞争力在哪里，那么哪里就是爆点。   她举了一个例子。启迪之星投资的速感科技在创业初期是由两位做算法的硕士组成。刘博把这个团队刚开始创业的想法形容为「很天真」。当时这个团队想的是，自己的算法在比赛中取得过最好成绩，那么能不能依托这个算法做一个可以为大家服务的跟随机器人。   「事实上，这个链条太长了。而且到底跟随机器人有没有人用？会不会有人买单？」刘博解释道，这里需要处理的信息太多了，需要把各个模块之间的曲线缩短。为此，速感科技迅速找到了适合自己技术落地的最佳场景，包括为前不久刚刚过会的科沃斯机器人提供算法模块，也与小米等公司达成了合作。   人工智能算法离不开优质的数据，而离开了具体场景，数据则无法聚焦。有了一个合适的落地场景才最大化构建自己的算法能力。   「很多创业者面临的挑战都来自于对自我不充分的认知，很多人觉得自己有最好的技术指标就一定能拿下厂商，但不是这样的。」刘博说道。速感科技的算法模块成本低于同类竞品，因而可以通吃扫地机器人这块市场。而对一个行业来说，有了场景就有了数据，那么算法就很难被超越了。「算法是怎么优化的？数据是怎么获取的？获取到数据后有没有理解到获取数据背后的商业途径？这些都是对创业者来说最大的挑战。」   星瀚资本创始人杨歌对于这个问题也有着自己的看法：「从底层的技术层到中间的模块层再到上层的应用层，哪一条线能最快跑通那里就是爆点。」这也是星瀚资本的一个投资逻辑。   他认为，机器人是最缓慢的，因为机器人的链条太长。不仅要有具备听觉、视觉等依赖算法的应用模块，还有不断迭代硬件模块。   「机器人这样的应用时间太长了，而太长了就见不到钱。」杨歌说道，要找最短的链条，去看哪些是可以直接变现的。而拥有大量数据的金融领域就是首选，其次是利用人工智能可以实现智能优化的仓储物流领域。 那么，发掘了行业爆点后，创业公司应该如何跳过创业途中的那些坑，从容迎接创业挑战？   对此，杨歌给出的答案是，创业者可能要从可行性、鲁棒性和协同性这三个方面考虑。   可行性指的是，很多创业者发现有价值的场景后，却在进入场景后遇到数据量太少，算法完全不行的情况。这就意味着，创业一开始的选择就发生的偏差。   鲁棒性可以理解为适应性。无论是芯片还算法，你所做的能否适应很多场景。人工智能发展的速度非常之快，同时数据结构以及模型算法也在不断变化。不能因为其他层次的设计出现变化而无法适应的情况。   「能用硬件解决的，不用软件解决，要尽量为软件提供一个最好的鲁棒性。用算法解决的不要通过选择场景来改变。一层一层地把鲁棒性扩展到最大，这样你的价值才能得到最大的体现。」杨歌说道。   最后一点也就是协同性。大多数对产业精通的人对数据、区块链、人工智能不是很精通，可是很多算法工程师却完全不理解商业场景。那么如何把市场需求和行业需求转化为技术语言需求，再把技术转化结果反馈到市场就是需要协同性解决的问题。   英伟达半导体有限公司橱窗加速计划中国区经理黄庆春表示，成功的初创企业还要具备以下三个要素：   首先，团队做的是一个长期项目，而非短期项目。其次，初创企业中的核心人物要有一定的行业积累和沉淀。第三，要有一个开放的心态。 
75,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739866&idx=1&sn=19fb68ccc56d68aa55016a77387ad27b&chksm=871ad0e4b06d59f22cc69c1a619ff9fef12182b40bcafa1fcee745415cbd987d2875c0b99a74&scene=27,专访MIT教授Tomaso Poggio：表达、优化与泛化——数学视角里的深度学习,"   三月，受腾讯 AI Lab 学术论坛邀请，机器之心在深圳采访了深度学习理论研究著名学者 Tomaso Poggio。他以平直易懂的语言介绍了自己的「长篇系列工作」，也谈了谈他对理论指导实践以及仿生学指导深度学习算法发展等观点的看法。 Tomaso Poggio 的知名度，有相当一部分来源于他异常出色的导师身份：DeepMind 创始人及 CEO Demis Hassabis 和 Mobileye 创始人及 CTO Amnon Shashua，都是他的学生。这两家公司一个创造出了击败了围棋世界冠军、重新定义这个项目的 AlphaGo，另一个将辅助驾驶系统装进了全球超过 1500 万辆车里，制造了世界上第一款能在终端进行深度神经网络推理的量产车型的系统。Poggio 本人不仅鼓励他的学生们以创业的形式将深度学习带进现实世界，也亲身投入指导了这两家公司的早期创立。 然而在学术界，Poggio 的知名度更多来自于他的深度学习理论研究。他的论文非常好辨认——命名方式简单粗暴如同长篇系列小说的就是他，《深度学习理论 II》，《深度学习理论 IIIb》……  这个编号系统来自他对深度学习理论问题进行的拆分：在 Poggio 看来，深度学习理论研究问题分为三类： 第一类是表达（representation）问题：为什么深层网络比浅层网络的表达能力更好？ 第二类是优化（optimization）问题：为什么 SGD 能找到很好的极小值，好的极小值有什么特点？ 第三类是 泛化（generalization）问题：为什么参数比数据还多，仍然可以泛化、不过拟合？ 对于每一类问题，他都尝试以应用数学工具为基础，通过举出能够用数学语言进行描述的例子然后给出解释的方式，用理论推导（也辅以一定的实验验证）来说明自己的观点。 早在 2002 年，Poggio 和著名数学家 Steve Smale 就合著了一篇论文 [1]，总结了那些经典学习理论，它们的共同点是，都相当于具有单一隐藏层的网络。Poggio 是这样解释他研究「表达」的初衷：「当时我们就提出了一个问题：为什么大脑具有很多层？为什么当传统理论告诉我们使用单层网络的时候，大脑的视觉皮层其实在用许多层解决这一问题？」 毫无疑问，目睹了深度网络的成功后，同样的问题再一次被摆上了台面。 Poggio 认为，事实上无论是深层网络还是单层网络，都能相当不错地近似任意连续函数——这也是上世纪 80 年代的学者们通常忽略多层网络而采用单层网络的原因。但是，问题的核心在于表达清楚一个函数所需要的维度：单层网络需要的单元数非常多，甚至比宇宙中的原子数还要多。这就是数学上所说的「维度灾难」： 。 为了跳出维度灾难，过去的数学家尝试假设方程的光滑性：他们发现，维度灾难取决于「维度除以光滑性」。而深度学习给出了针对一类特定函数的独特方法：如果近似的对象是一个组合函数，换言之，是一个函数嵌套函数的大函数，那么 。换言之，无论维度多大，深度网络都能够摆脱维度灾难来拟合这个函数。 现在，能够被深度神经网络很好地拟合的数据都具有组合函数的特点。以图像为例，想要分类一张图像，并不需要同时观察左上角和右下角两个相距甚远的像素，只需要观察每一小块，再将它们组合在一起。有了这种「组合」（compositional）的性质，当卷积神经网络被用来刻画图像，甚至不需要参数共享的帮助，就能轻易摆脱维度灾难。 而那些现在还不能被神经网络刻画得很好的数据，通常都不是组合函数。但是知道「组合函数和神经网络很配」还远远不够，Poggio 说，「作为计算机科学学者，甚至作为数学家，我们能不能进一步解释一下组合函数，给出一些比『它是组合的』更明确的性质，从而更好地理解神经网络的工作机制。这对于我来说也是一个非常有趣的、希望更多研究者投入精力的开放问题。」 优化的致胜：取之不尽的参数和性质漂亮的 SGD 解线性方程组的时候，如果未知量的数量大于方程数，我们将收获很多解。 拿神经网络处理数据的时候，以图像举例子，包含 6 万张训练数据的 CIFAR 数据集，通常会用一个包含数十万乃至上百万参数的神经网络进行处理——一个教科书般的过参数化（overparameterization）例子。 将神经网络近似看成一个多项式——把每个具有非线性的小单元都用一个单变量多项式替代，得到一个由数十万乃至上百万多项式组成的大多项式，此时，想要在 CIFAR 上获得 0 训练误差，就转化成了一个解 6 万个线性方程的问题。根据贝祖定理（Bézout's theorem），此时的解的数量比宇宙中的原子数量还多。另外，参数多于数据量带来了「退化」（degenerate）性质：每一个解都对应着一个无限大的解集。 因此， 。 而众所周知，随机梯度下降（SGD）的特性就是会以较高的概率倾向于停留在退化的谷地里，即，停留在全局最优解上。二者的结合，就让神经网络的优化变得轻松：确定有全局最优、有很多，它们的特征明显，很容易被优化算法找到。 过参数化是优化的福音，同时也是泛化的噩梦。在经典机器学习里，随着优化进行，测试错误率会呈现一条先减后增的 U 型曲线，尤其是模型规模与数据规模不匹配的时候，后半段的过拟合是十分可怕的。 然而在深度学习里，泛化错误率却经常呈现一个「下降，然后停住不动」的态势，即使不是零，也能保持在一个相当低的水准上。 Poggio 对此的解释是：这是深度学习所进行的任务与所用的损失函数之间的特定组合带来的美好化学反应。 具体来说，就是大多数神经网络都是用来解决分类问题（而不是回归问题）的，错误率通常以 0-1 损失计算，而目标函数却通常是交叉熵。 这种差异是因为 0-1 损失函数是好的效果衡量指标，却并不适合做优化的目标函数。拿手写数字分类器举例，神经网络分类器最后是通过 softmax 转 hardmax 来选择分类类别的，这也就意味着，即使模型认为一张「1」的图像是「1」的概率只有 30%，但只要这 30% 是所有 10 个可能性中最高的，模型仍然会将这张图像分类为「1」。一个信心水平只有 30% 的模型，即使分类正确，也远称不上一个好模型，需要继续优化。但是，如果选用 0-1 损失函数作为目标函数，只要分对了，该样本的损失就是 0 了，没办法计算梯度，也自然没办法进行反向传播来优化参数。 选用交叉熵做损失函数就没有这个烦恼，你可以一直优化到信心水平无限接近 100%。 而交叉熵函数与 0-1 损失这对组合的奇妙之处在于，即使 。 几个月前，芝加哥大学的 Srebro 组的工作 [2] 证明了：对于单层线性网络来说，如果数据集噪声较小、可分，那么即使交叉熵过拟合了，分类误差也不会过拟合。 「这是一个非常优美的，角度独特的工作。在此之上，我们用微分方程动力系统理论的工具证明了，在全局最小值附近，深度网络表现得就像一个线性网络。因此，我们可以将 Srebro 工作的结果用在深度学习上，说明即使神经网络分类器的交叉熵过拟合了，分类器本身也不会过拟合。」 交叉熵的这一性质是最小平方误差（least square error）等其他损失函数所不具备的，拥有这一性质的最简单的损失函数是指数误差（exponential loss）。而当我询问究竟是交叉熵的哪些特质让它拥有了如此特别的性质，是否和它的不对称性有关，Poggio 表示这仍然是一个有待讨论的问题。 以上就是 Poggio 的「深度学习理论三部曲」的内容概要了，详情请参阅 [3-7]。 除了他的工作本身，我们也和他聊了一些关于深度学习理论工作的其他问题： 平坦的极小值意味着好的泛化能力吗？一个观点转变 关于极小值的形状与泛化之间的关系，Poggio 说，他的观点转变了：「确实有学者在工作中表示，平坦是有利于泛化的。大概一年多以前我也曾经发表过类似的观点，但是我现在不再这么认为了。」 在关于优化的研究中，Poggio 证明了平坦确实会让优化过程变得容易，平坦的最小值也有更大可能是全局最小值。「但是我不觉得它和泛化之间有直接的联系，起码现在没有。如今对于泛化能力的研究，依赖于分类问题、依赖于损失函数的选择，却不依赖于平坦。Bengio 兄弟两人都参与的一篇论文就证明了，陡峭的极小值也是可以泛化的 [8]，因为你完全可以通过改变不同层的参数，在不改变网络的输入输出关系的前提下，让一个平坦的极小值变得陡峭。」 另外，他也认为完全平坦的极小值是不存在的，起码对于现在这种以多项式网络为基础添加非线性的神经网络来说，是不存在的。「我们都知道，一旦多项式在一个解集上都为 0，那么这个多项式处处为 0，因此，我不觉得存在完全平坦的极小值了。」 致力于应用深度学习算法的工程师们最经常对深度学习的理论研究者提出的一个问题就是：「你的工作很棒，但请问这能如何帮助我训练我的模型？」了解更多的理论知识当然具有启发意义，但是理论研究范围广阔且往往十分艰深，究竟哪些理论研究有助于应用开发者，应用开发者应该了解理论到何种程度？ 机器学习里的无免费午餐定理（No Free Lunch Theorem），也就是 Wolpert 在 1996 和 1997 年发表的两篇著名论文里 [9, 10] 所提到的，学习算法之间没有先验区别，对于任何两个算法 A 和 B 来说，都存在一样多的两堆目标，对一堆目标来说 A 的检验误差比 B 高，对另一堆来说 B 的检验误差比 A 高。Poggio 援引了无免费午餐定理到理论研究中：不存在一个适用于所有问题的算法，类似地，也很难给出一个普适性正确的理论陈述。 「理论通常给出的是通常情况或最坏情况的分析，他们给出建议，告诉你应该做/不做什么，以避免最坏情况的发生。但是理论无法告诉你，对于一个特定案例来说，最佳方案是什么。」 而他对今天的深度学习应用者的建议是，小心过拟合。 「在过去几十年的机器学习发展史中，我们学到的一课是，如果你的数据集没有大到排除过拟合可能性，那么在一个特定数据集上的最佳方法通常是过拟合的、无法扩展到其他数据集上的。并不是说学者们『偷看』了验证集测试集，而是当一个社区的学者都在用不同的方法进行试错，那么一段时间后的最佳做法通常是过拟合了这个特定数据集的。」 「我曾经是一名物理学研究者，在我的学生时代，最普遍的经验法则是，如果你想建立一个参数为 n 的模型，那么至少要有规模为 2n 的数据，如果你想从统计的角度得出一些结论，更为推荐的数据规模是 10n。然而现在的深度学习研究者倾向于对所有问题都套用有数百万参数的模型。我们得出的『深度学习模型不受过拟合困扰』的论证只适用于特定问题（分类）、且要求数据集质量良好（可分），因此深度学习研究者应该对过拟合持有更谨慎的态度。」 人是很好的学习者，既不需要数百万数据，也不需要数据有标签，而这部分取决于我们与生俱来的、写在基因里的先验知识。然而，关于先天与后天（nature versus nurture）的争论从未停止。 「模型需要多少先验，是一个不能一概而论的、没有简单答案的问题。」Poggio 总结道，「理论研究的目的是找到能够做出特定预测所需的先验的下限。」 他以回归问题举例，「对于给定一些数据点来恢复一条曲线的任务来说，如果你什么都不告诉我，那么除非给我曲线上的所有点，否则我基本上什么也做不了。 是一个必须的先验，但这还不够。我起码需要类似 （smothness）这样的性质，才能进行预测。而最重要的还是数据量，样本复杂度和先验之间，存在一个权衡取舍的关系。」 三十年前，「深度学习之父」Geoffrey Hinton 用利于优化且计算高效的「反向传播」将整个领域带入了高速发展，而近年来，他则致力于寻找比反向传播更有可能在仿生学（bionics）上成立的结构。 MIT 对于深度学习的研究素来与脑神经科学结合紧密，Poggio 是如何看待这一问题的呢？ 「我认为从生物学上完成反向传播并非完全不可能（not impossible），只能说，根据我们现在对神经元以及信号传递机制的了解，可能性不大（unlikely）。然而我认为真正不可能的是对所有样本的标注。」 「因此一个有趣的研究课题是，大脑是如何『绕开』标注的。例如，一个有趣的假设是，我们的视觉系统是以学习给图像『填色』来进行预训练的，它接收到了颜色信息，却只给其他视觉皮层以黑白的灰度信息，以此训练一个能够预测颜色的网络。在这个设定下，你不需要『神谕』（oracle）来告诉你真实的颜色是什么，你是有这部分信息的，只不过通过把它藏起来而建立了一个可以进行优化的模型。」 「类似的假设还有，大脑在不断地预测下一帧影像并进行优化等等。而能够预测颜色的、预测下一帧影像的视觉系统，是不是能够更好地进行其他视觉任务呢？是不是能够利用更少的数据就能学会识别物体呢？这都是有趣的开放问题，而且一旦得到答案后，将对深度学习产生巨大的推动。」 Reference 1. Cucker, F., & Smale, S. (2002). On the mathematical foundations of learning. Bulletin of the American mathematical society, 39(1), 1-49. 2. Neyshabur, B., Tomioka, R., Salakhutdinov, R., & Srebro, N. (2017). Geometry of optimization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071. 3. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., & Liao, Q. (2017). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503-519. 4. Liao, Q., & Poggio, T. (2017). Theory of Deep Learning II: Landscape of the Empirical Risk in Deep Learning. arXiv preprint arXiv:1703.09833. 5. Zhang, C., Liao, Q., Rakhlin, A., Miranda, B., Golowich, N., & Poggio, T. (2018). Theory of Deep Learning IIb: Optimization Properties of SGD. arXiv preprint arXiv:1801.02254. 6. Poggio, T., Kawaguchi, K., Liao, Q., Miranda, B., Rosasco, L., Boix, X., ... & Mhaskar, H. (2017). Theory of Deep Learning III: explaining the non-overfitting puzzle. arXiv preprint arXiv:1801.00173. 7. Zhang, C., Liao, Q., Rakhlin, A., Sridharan, K., Miranda, B., Golowich, N., & Poggio, T. (2017). Theory of deep learning iii: Generalization properties of sgd. Center for Brains, Minds and Machines (CBMM). 8. Dinh, L., Pascanu, R., Bengio, S., & Bengio, Y. (2017). Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933. 9. Wolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390. 10. Wolpert, D. H., & Macready, W. G. (1997). No free lunch theorems for optimization. IEEE transactions on evolutionary computation, 1(1), 67-82. "
76,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739820&idx=3&sn=cedea425dabca94b6a1258af847c31e2&chksm=871ad012b06d590474aa55c1e3667c665af3531b014b37c07d17c141fe419639e499d05cac3b&scene=27,资源 | Pandas on Ray：仅需改动一行代码，即可让Pandas加速四倍,"本文中，来自 UC Berkeley 的 Devin Petersohn 发布文章介绍了其参与的项目 Pandas on Ray，使用这款工具，无需对代码进行太多改动即可加速 Pandas，遇到大型数据集也不怕。作者还对 Pandas on Ray、Pandas 进行了对比评估。机器之心对此文进行了编译介绍。 项目链接：https://github.com/ray-project/ray 最近，我和一位使用 100 多 TB 生物数据的朋友讨论了数据科学库的一些局限性。当面临这种规模的数据时，Pandas 成了最受喜爱的工具；然而，当你开始处理 TB 级别的基因数据时，单核运行的 Pandas 就会变得捉襟见肘。如果我们拥有更多的处理器核，或者要打开数十 TB 规模的文件时，我们希望 Pandas 运行得更快。目前，Apache Spark 是最高性能的分布式选择了，但是如果未对 Pandas 代码做出足够多的修改，你无法使用 Apache Spark 运行 Pandas 代码。 大规模数据科学任务向来都是丢给分布式计算专家来做的，或者至少是熟悉此类概念的人员。大多数分布式系统的设计者给用户提供了调节「旋钮」，并留下了大量的系统配置。因此，高系统性能需要用明显更加陡峭的学习曲线来折中。大多数现有用户可能只是想让 Pandas 运行得更快，并不希望在特定的硬件环境中优化他们的工作流。在我的案例中，我想在 10KB 和 10TB 的数据上使用相同的 Pandas 脚本，并且希望 Pandas 在处理这两种不同量级的数据时速度一样快（如果我有足够的硬件资源的话）。为了完成这些目标，我们开启了一个 Pandas on Ray 项目。 我们对系统进行了初步测评，Pandas on Ray 可以在一台 8 核的机器上将 Pandas 的查询速度提高了四倍，而这仅需用户在 notebooks 中修改一行代码。我们为现在的 Pandas 用户设计了该系统，旨在帮助他们的程序运行得更快，并且无需大量代码改动就能够进行更好的扩展。这项工作的最终目标就是在云环境中使用 Pandas。 简介 Pandas on Ray 是 DataFrame 库的早期阶段，DataFrame 库封装了 Pandas，并且透明地分配数据和计算。使用 Pandas on Ray，用户不需要知道他们的系统或集群有多少个核心，也不需要指定如何分配数据。事实上，在 Pandas on Ray 上体验可观的加速时，用户可以继续使用之前的 Pandas notebook，甚至是在同一台机器上。仅仅需要按照下面描述的修改 import 语句。一旦修改了 import 语句，你就可以像使用 Pandas 一样使用 Pandas on Ray 了。 Pandas on Ray 主要针对的是希望在不切换 API 的情况下提高性能和运行速度的 Pandas 用户。我们正在积极实现与 Pandas 所有 API 的对等功能，并且已经实现了 API 的一个子集。我们会介绍目前进展的一些细节，并且给出一些使用示例。 使用的数据集 标普 500 股市数据：29.6MB（https://www.kaggle.com/camnugent/sandp500/data） 导入 Pandas on Ray Waiting for redis server at 127.0.0.1:21844 to respond... Waiting for redis server at 127.0.0.1:41713 to respond... Starting local scheduler with the following resources: {'GPU': 0, 'CPU': 8}. ====================================================================== View the web UI at http://localhost:8890/notebooks/ray_ui62630.ipynb?token=bcf6d5b6cb9c2c478207f025384869100d7a25dcc27d7a56 ====================================================================== Ray 将根据可用内核的数量进行自动初始化。现在你可以开始运行 Pandas 命令，它们将被并行化。 我们也可以开始检查数据。让我们来看一下坐标轴。 [RangeIndex(start=0, stop=619040, step=1), Index(['date', 'open', 'high', 'low', 'close', 'volume', 'Name'], dtype='object')] 让我们运行一个简单的数据查询（just for fun），看看有多少天是以正收益结束的。 0 2013-02-13 1 2013-02-15 2 2013-02-26 3 2013-02-27 4 2013-03-01 5 2013-03-04 6 2013-03-05 7 2013-03-06 8 2013-03-07 9 2013-03-11 Name: date, dtype: object Number of positive days: 2232790 Ratio of positive days to total days: 0.5152655724993538 我不喜欢使用默认索引，那么让我们来看一下「date」是不是一个好的索引。 0 2013-02-08 1 2013-02-11 2 2013-02-12 3 2013-02-13 4 2013-02-14 5 2013-02-15 6 2013-02-19 7 2013-02-20 8 2013-02-21 9 2013-02-22 Name: date, dtype: object 看上去是个正确的选择，因为我可能希望基于日期查询。让我们修改一下 DataFrame 中的索引，以便设置基于日期的查询。 [Index(['2013-02-08', '2013-02-11', '2013-02-12', '2013-02-13', '2013-02-14',  '2013-02-15', '2013-02-19', '2013-02-20', '2013-02-21', '2013-02-22',  ...  '2018-01-25', '2018-01-26', '2018-01-29', '2018-01-30', '2018-01-31',  '2018-02-01', '2018-02-02', '2018-02-05', '2018-02-06', '2018-02-07'],  dtype='object', name='date', length=619040), Index(['open', 'high', 'low', 'close', 'volume', 'Name'], dtype='object')] 我们可以查询数据来收集更多的信息。我们可以找到股票收益为正的日期。 这个小例子旨在演示一些 Pandas 操作，这些操作作为并行实现可在 Pandas on Ray 上找到。下面，我们会展示一些性能对比，以及我们可以利用机器上更多的资源来实现更快的运行速度，甚至是在很小的数据集上。 分布式转置是 DataFrame 操作所需的更复杂的功能之一。在以后的博客中，我们将讨论我们的实现和一些优化。目前，转置功能相对粗糙，也不是特别快，但是我们可以实现一些简单优化来获得更好的性能。 date   2013-02-08 2013-02-11 2013-02-12 2013-02-13 2013-02-14 2013-02-15  \ open        15.07      14.89      14.45       14.3      14.94      13.93    high        15.12      15.01      14.51      14.94      14.96      14.61    low         14.63      14.26       14.1      14.25      13.16      13.93    close       14.75      14.46      14.27      14.66      13.99       14.5    volume    8407500    8882000    8126000   10259500   31879900   15628000    Name          AAL        AAL        AAL        AAL        AAL        AAL    date   2013-02-19 2013-02-20 2013-02-21 2013-02-22    ...     2018-01-25  \ open        14.33      14.17      13.62      13.57    ...          78.47    high        14.56      14.26      13.95       13.6    ...          79.38    low         14.08      13.15       12.9      13.21    ...         78.345    close       14.26      13.33      13.37      13.57    ...          79.25    volume   11354400   14725200   11922100    6071400    ...        2327262    Name          AAL        AAL        AAL        AAL    ...            ZTS    date   2018-01-26 2018-01-29 2018-01-30 2018-01-31 2018-02-01 2018-02-02  \ open        79.49      79.81      78.44      78.49      76.84      77.53    high        80.13      79.95      78.69      78.77      78.27      78.12    low         79.38      79.11      77.91      76.54      76.69      76.73    close       80.09      79.18      78.35      76.73      77.82      76.78    volume    2532808    2662383    3808707    4136360    2982259    2595187    Name          ZTS        ZTS        ZTS        ZTS        ZTS        ZTS    date   2018-02-05 2018-02-06 2018-02-07   open        76.64      72.74       72.7   high        76.92      74.56         75   low         73.18      72.13      72.69   close       73.83      73.27      73.86   volume    2962031    4924323    4534912   Name          ZTS        ZTS        ZTS   [6 rows x 619040 columns] 接下来，我们要比较一下 Pandas 和 Ray on Pandas。尽管我们目前还没有支持完整的 Pandas 功能 API，但是我们展示了一些初步的基准测试，证明我们的方法是有潜力的。我们会在以下对比中做到尽可能的公平。需要注意的是，我们没有在 Pandas on Ray 上做任何特殊的优化，一切都使用默认设置。还需要注意的是，Ray 使用了 eager execution，因此我们无法进行任何查询规划，也无法掌握计算给定工作流的最佳方法。 所用的数据集 全球健康数据：1.79GB（https://www.kaggle.com/census/international-data/data） 首先我们要检查加载一个 CSV 文件所需的时间。这个文件相对较大（1.7GB），所以使用 Pandas 和使用 Pandas on Ray 的加载时间会有所不同。 Pandas on Ray: CPU times: user 48.5 ms, sys: 19.1 ms, total: 67.6 ms Wall time: 68 ms Pandas: CPU times: user 49.3 s, sys: 4.09 s, total: 53.4 s Wall time: 54.3 s 我们看到的结果是：Pandas on Ray 的加载速度大约是 Pandas 的 675 倍。尽管这些数字令人印象深刻，但是 Pandas on Ray 的很多实现将工作从主线程转移到更异步的线程。文件是并行读取的，运行时间的很多改进可以通过异步构建 DataFrame 组件来解释。让我们将所有线程的结果汇总到一起，看看它需要多长时间。 Pandas on Ray: CPU times: user 2.59 s, sys: 2.52 s, total: 5.11 s Wall time: 9.09 s Pandas: CPU times: user 16 ms, sys: 240 ms, total: 257 ms Wall time: 256 ms 这里我们可以看到，如果我们使用 [:] 运算符将所有的数据收集到一起，Pandas on Ray 速度大约是之前的 1/36。这是因为并行化。所有的线程以并行的方式读取文件，然后将读取结果串行化。主线程又对这些值进行去串行化，这样它们又变得可用了，所以（去）串行化就是我们在这里看到的主要开销。熟悉 Spark 的人可能会记得，这类似于一个.collect() 调用。它使任务不再并行执行，将它们转移动单独的线程中。所以，尽管它读取文件更快，但是将这些片段重新组合在一起的开销意味着 Pandas on Ray 应该不仅仅被用于文件读取。让我们看一下文件加载完成后索引会发生什么。 Pandas on Ray: CPU times: user 12 µs, sys: 1 µs, total: 13 µs Wall time: 16 µs Pandas: CPU times: user 4 µs, sys: 0 ns, total: 4 µs Wall time: 7.15 µs RangeIndex(start=0, stop=3058280, step=1) 请注意，两种方法都在缓存.index 调用的结果，所以我们调用一次 .index 之后看到的是原始时间，再一次调用的时候看到的是缓存访问时间。Pandas on Ray 大约慢了 10 µs，但是维持一个分布式索引的复杂度更高。这显示了底层 Ray 基础架构的效率，它能够快速检索数据。 现在让我们尝试加速一次示例查询，看看 Pandas 和 Pandas on Ray 的性能对比。 Pandas on Ray: 100 loops, best of 3: 4.14 ms per loop Pandas: The slowest run took 32.21 times longer than the fastest. This could mean that an intermediate result is being cached. 1 loop, best of 3: 17.3 ms per loop 在这次 timeit 调用中，我们看到 Pandas on Ray 的速度大约是 Pandas 的 4 倍。这是在一台 8 核的机器上运行的，由于开销的因素，加速并不是特别完美。尽管如此，通过仅仅修改 import 语句，原始 Pandas 上的运行时间和 Pandas on Ray 上的运行时间还是有显著差别的。 DataFrame 库 Dask 提供可在其并行处理框架上运行的分布式 DataFrame，Dask 还实现了 Pandas API 的一个子集。一般来说，目前 Dask 在绝大多数操作上都比 Pandas on Ray 快一些。Dask 为 Pandas 用户提供精细调整的定制，而 Pandas on Ray 则提供一种以最少的工作量实现更快性能的方法，且不需要多少分布式计算的专业知识。Pandas on Ray 针对的不是目前的 Dask（或 Spark）用户，而是希望在无需学习新 API 的情况下提升现有和未来工作负载的性能和可扩展性的 Pandas 用户。在 columnar operation 上，Dask 比 Pandas on Ray 快，但是它需要一些超出传统 Pandas 之外的知识。 Dask 中存在两个主要的差别，而 Pandas on Ray 则尝试解决这两个差别： 1. 用户需要一直意识到：数据是分布式的，计算是懒惰的。 2. 多线程和多进程之间的权衡是可扩展性和性能之间的权衡。 数据科学家应该用 DataFrame 来思考，而不是动态的任务图 Dask 用户一直这样问自己： 我什么时候应该通过 .compute() 触发计算，我什么时候应该调用一种方法来创建动态任务图？ 我什么时候应该调用 .persist() 将 DataFrame 保存在内存中？ 这个调用在 Dask 的分布式数据帧中是不是有效的？ 我什么时候应该重新分割数据帧？ 这个调用返回的是 Dask 数据帧还是 Pandas 数据帧？ 使用 Pandas 的数据科学家不一定非得是分布式计算专家，才能对数据进行高效分析。Dask 要求用户不断了解为计算而构建的动态任务图。此外，默认情况下，懒惰计算使每个熟悉的 Pandas 调用返回一个意外的结果。这些差异为 Dask 提供了更好的性能配置，但对于某些用户来说，学习新 API 的开销太高。 使用 Pandas on Ray 的时候，用户看到的数据帧就像他们在看 Pandas 数据帧一样。 我们要速度，也要扩展性 Dask 默认是以多线程的模式运行的，这意味着一个 Dask 数据帧的所有分割部分都在一个单独的 Python 进程中。尽管多线程模式让一些计算变得更快，但是一个单独的 Python 进程并不能利用机器的多个核心。 或者，Dask 数据帧可以以多进程模式运行，这种模式能够生成多个 Python 进程。然而，如果一个 Python 进程需要将一个小的 Pandas 数据帧发送到另一个进程，则该数据帧必须通过 Pickle 进行串行化处理，然后在另一个进程中进行去串行化处理，因为这两个进程没有共享内存。串行化、拷贝以及去串行化，这三步会带来高性能损失。即使这个解决方案可以扩展到多个核心，但是高昂的通信成本会对整体性能造成影响。 如上图所示，由于串行化和拷贝操作，Dask 的多进程模式损伤了 read_csv 操作的性能。 Pandas on Ray 既可以以多线程模式运行，也可以以多进程模式运行。Ray 的默认模式是多进程，因此它可以从一台本地机器的多个核心扩展到一个机器集群上。至于通信方面，Ray 使用共享内存，并且通过 Apache Arrow 实现零拷贝串行化，显著降低了进程之间的通信代价。 使用 Pandas on Ray，你的 Pandas 工作流可以同时实现快速运行和可扩展性。 read_csv 案例研究 在 AWS m5.2x 大型实例（8 个虚拟核、32GB 内存）上，我们使用 Pandas、Ray 和 Dask（多线程模式）进行了 read_csv 实验。 我们采用了从 60KB 到 2GB 大小不等的四个数据集： 泰坦尼克数据集：60KB（https://www.kaggle.com/c/titanic/data） Yelp 数据集：31MB（https://www.kaggle.com/c/titanic/data） Kiva Loan 数据集：187MB（https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding/data） NYC Parking Tickets 数据集：2GB（https://www.kaggle.com/new-york-city/nyc-parking-tickets/data） 结果显示 Ray 的性能是快速且可扩展的，在多个数据集上都优于 Dask。 注：第一个图表明，在像泰坦尼克数据集这样的小数据集上，分发数据会损害性能，因为并行化的开销很大。 MAX 案例研究 为了查看逐行操作和逐列操作时三者的对比结果，我们继续在相同的环境中进行实验。 除了在最小的文件上 Pandas 是最快的以外，Pandas on Ray 的逐行操作速度大约是 Pandas 和 Dask 的三倍。在逐列操作上，它大约慢了 2.5 倍，这是因为目前的 Pandas on Ray 实现尚未针对 columnar operation 进行优化。值得注意的是，Dask 的惰性计算和查询执行规划不能在单个操作中使用。 通常情况下，Pandas on Ray 是异步运行的，但是出于实验目的，我们强制执行同步，以便对 Pandas 和 Dask 进行正确的评估。 我们已经开始构建 Pandas on Ray，这是一个仅更改 import 语句就可以使 Pandas 工作流并行化的库。到今天为止，我们已经在大约 45 天内实现了 Pandas DataFrame API 的 25%。目前，我们仅在单个节点上加速 Pandas，但很快我们将具备在集群环境中运行 Pandas 的功能。 如果您想试用 Pandas on Ray，请按照 readthedocs 文档说明（http://ray.readthedocs.io/）从源代码开始构建。此处使用的代码目前位于 Ray 的主分支上，但尚未将其转换为发布版本。  原文链接：https://rise.cs.berkeley.edu/blog/pandas-on-ray/ "
77,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739820&idx=2&sn=243a2cfd0f8e02859dc930b990046271&chksm=871ad012b06d590455badafde83ece9b08eb3bd962f6f7442a36e2594866da2e268fe40cbcdc&scene=27,INTERFACE | 从技术到产品，搜狗为我们解读了神经机器翻译的现状,3 月 12 日，搜狗正式在线上平台发布了「旅行翻译宝」。这款随身翻译设备结合了搜狗神经网络机器翻译、语音识别、图像识别等多项技术，不仅支持语音、图像翻译等多种翻译模式，还提供中英日韩俄德等 18 种语言互译。 在深度学习快速发展的今天，机器翻译系统的能力究竟达到了什么样的水平？机器翻译是否已经可以代替人类翻译？3 月 17 日，机器之心与搜狗共同举办的 INTERFACE 线下分享中，搜狗语音交互技术中心研发总监陈伟、搜狗 IOT 事业部产品负责人李健涛，从技术和产品两个方面为我们回答了这些令人感兴趣的问题，解读了搜狗，以及业内目前机器翻译技术的现状。 在活动结束之后，我们对本次分享内容进行了整理： 搜狗语音翻译技术 分享者：搜狗语音交互技术中心研发总监陈伟 陈伟：语音这块分语音识别、语音合成和机器翻译的积累。我将重点介绍搜狗如何把复杂的在线语音翻译技术放到离线上面的，我相信这件事大家会很感兴趣。 搜狗语音翻译技术架构 在移动时代，我们更多地通过输入法表达信息，搜索获取信息。而到了智能时代，知音 OS 是怎么获取信息呢？通过深智引擎获得，因此搜狗的人工智能战略是两块，一块是自然交互，一块是知识计算。我们团队目前是在自然交互，人和机器之间，人更好的在机器输入信息，同时机器有很好的反馈。速记翻译笔设备不是拍脑袋想出来的，本身的产品思考已经非常成熟，技术上我们一直没有落下，紧跟产品思维在做。 搜狗这段时间在人工智能上面的能力持续提升，一个是在语音识别方面，处于行业领先，准确率 97% 的水平；另外，我们在 2016 年 11 月份首发机器同传技术，另外是 2017 年 5 月份我们参加了国际顶级机器翻译评测 WMT，获得了中英机器翻译全球第一。我们也一直在推动机器翻译技术的进步，因为去年我们跟今日头条和创新工场一起推动人工智能挑战赛，当时参与的人群比较多，当时我们在业内开放出 1000 万条精标口语翻译数据集，目前很多学术机构和公司使用的都是目前我们开放出来的数据集，这也是业内开放出来最大的高质量评测语料机器翻译数据集。除此之外，搜狗在知识问答上过去几年也取得了很大成绩。   把这些单点技术拿出来看我们会形成好的产品，2016 年开始我们一直在推搜狗机器同传，往技术层面来看，它其实就是语音翻译技术，把搜狗语音识别和搜狗机器翻译、语音合成打通。这是一个简单的机器框图，同传整个口译技术里面最难的技术层面，相当于人不断讲话过程中，屏幕和耳机要实时听到机器翻译的声音，语音不会停，我们做的过程中要不断去判断语音断点，找到语音之前断句的位置，然后做语音识别。语音识别之后拿到的句子非常多，不同语句混合在一起，需要有文本断句的能力，我们判断出这句话是完整的句子送到机器翻译，把结果通过语音合成方式，最后构成机器同传的完整能力。 我们需要加强的单点能力是把语音识别、机器翻译、语音合成的能力提升。连接语音识别和机器翻译之间文本断句能力是我们过去一直在加强的功能，有三个方面：内容顺滑、句子划分、输出判断。举一个例子，这个例子比较简单，他说「呃，我想去找你吃饭，不知道你有有没有空」。我们要把语气词去掉，否则会影响用户观感，做了顺滑以后会把语气词删掉，还有重复词去掉，这样的东西带到翻译里会极大降低翻译体验。 此外，我们经常讲的关于口头禅的事情，比如有人经常说「在这里、在那里、那就是说」等等。做了顺滑之后我们会得到相对来说语义完整，没有太多冗杂的句子。多个句子连在一起，如何找到断句的位置，需要在文本上做断句，通过神经网络模型，把句子划分开，上一个句子就变成了：我想去找你吃饭，不知道你有没有空。 在我想去找你吃饭后面加上标点符号。是不是把整个句子送过去就可以？这样是不合理的，为什么？我们做的过程中在实时做语音翻译处理，用户可能想说我想去找你吃饭，不知道你有没有空，我们一般来说会判断一下到底哪些句子需要送到后台做翻译，哪些需要等一等，等来了之后拼凑成完整语句再进行翻译。输出判断的时候，优先把我想去找你吃饭进行翻译，后面会有句子加进来。因此同传的时候会做很多容错以及判断，同传这件事情是很强的单点技术融合在一起，把更多单点技术串连起来非常大的系统工程。 搜狗语音产品发展历程 简单回顾一下搜狗在语音产品上面的进展，2016 年 11 月 24 号，第三届乌镇世界互联网大会上同传，在屏幕上投屏，这个系统有什么特点呢？第一，它是同传的，真正对于人工口译行业，他们一般叫同传和交传，同传概念是讲话过程中，人工同传把听到的语音翻译成对应的目标端语言，还有交传，说完了翻译才会做翻译，比如新闻发布会上，总理讲完了，翻译针对总理讲的东西做翻译，那是交传。我们首发的时候是同传模式，整个服务基于在线，使用两项技术：语音识别和机器翻译。2017 年我们逐渐把产品放到搜狗产品上面，目前搜狗翻译产品日均请求量已达 720 万次，随着技术成熟逐渐上线，产品带来了大量数据积累，这方便了我们不断进行技术迭代。2017 年第四届乌镇互联网大会上，我们发布了机器同传 2.0，把语音合成能力加进去。因为现场有人工同传箱，我们把机器合成的声音连到人工同传箱，人们戴上耳机以后可以选择 3-4 个频道。 整个 2017 年，机器同传支持了 200 多场同传服务，极客公园创新大会上我们发布了英译中机器同传，之前很多是中译英，但台下如果很多是中国观众的话不是刚需，反过来英译中对于国内大会来说非常重要，大家可以实时的看到英译中的效果。 搜狗的翻译硬件布局 搜狗在语音翻译硬件上的布局，在于今年 1 月 24 号发布的搜狗旅行翻译宝和搜狗速记翻译笔。技术特点拆解来看，搜狗旅行翻译宝主要是搜狗离线语音翻译，交传模式，我讲完以后你会看到我的播报声音，并不是同传的，用到的技术有搜狗的语音识别、机器翻译和语音合成。搜狗速记翻译笔是同传模式，语音识别和机器翻译。所以这两个产品略有区别。 现有技术进步肯定会提到深度学习对我们这个行业，对 AI 技术带来极大的提升，怎么来看带来的提升呢？或者从哪些因素分析呢？一般来说分三个特点：1）数据的变迁；2）算法的变迁；3）运算的变化。 从数据角度来看，其实现在数据对于工业界而言是非常大的护城河，工业界因为有自己的产品，因为有更多人力财力支持，我们可以获得更多的数据。现在已经达到数十万的量级，真正工业级商业机器翻译系统都是过亿语料规模，以前做语音合成语料库大概需要 10 个小时，从录音室出来的精标数据，现在这个量已经扩展到几百小时，甚至上千小时，数据规模逐渐变迁。   为什么说数据有用呢？上图是搜狗语音识别从 2012 年上线到现在的数据变化情况，早期上线之前，我们通过 Google 接口，收取大量用户数据，我们没有完全做语音识别系统，而是做了热启动，最早期的语音识别系统。当时上线用到的数据量也就 500 小时左右，随着数据级变化，从早期接近 40% 的错误率一直到 2016 年是 4.8%，现在已经是 3% 以内了。现在搜狗输入法上面每天请求数 PV 是 3 亿次左右，差不多总体语音总时长是 26 万小时，虽然这个数据并不代表搜狗马上可以拿 26 万小时的内容做训练，但是它的确会给我们带来更多的可能性，当你有大量数据的时候要不要做更多数据挖掘，我们可以通过机器半自动方式挑选数据，让我们的技术获得不断提升。 我们有一个自己的平台，叫 EVA，来自日本动漫的名字（新世纪福音战士），这个平台更多研究如何去做训练，因为当你的数据越来越多的时候，算法越来越复杂的时候，怎么样在短时间内获得更好的模型就成为需要优先考虑的问题了。底层基础设施有不同硬件，RDMA 是高速率的路由器。再上面，我们怎么能够更好的做自动配置，以及更灵活的调度，所以我们用了分布式系统，更好的调度各种各样训练服务。再上层，基于目前已有的系统上面的设计，把很多算法放上去，比如现在常用的 CNN、RNN、CTC、LSTM/GRU。再往上就是应用，图像识别、语音识别、机器翻译这样的技术。 现在的大多数其他开源平台在服务器端还是比较成熟的，但在终端——手机端和硬件端的运算能力，如何拿到好的模型来做推断？我们团队内部意识到，针对特定业务，做推断这件事情一定是强定制的，不可能有通用运算库，针对所有模型和任务都可以算得非常快。因此，搜狗团队在自己的深度学习平台内部孵化出一个工具，试图解决在已有 ARM 的 CPU 等计算硬件的条件下，如何进行更高效的运算的问题。 这一挑战分两个部分：任务调度的事情和高性能计算，针对目前我的逻辑和业务做更多的运算定制，这些东西支持了目前我们在语音识别、机器翻译、语音合成上各种运算任务，这些任务会逐渐放到搜狗对内对外很多产品上，比如今天看到的搜狗旅行翻译宝，包括之前手机端的很多业务，另外车机，搜狗在家，主要依赖于目前在云端基于我们的平台，以及在手机终端上比较强的运算定制能力，保证这件事情是打通的。 我们把自己的库与 ARM 的 ACL 对比，目前我们已有的库平均性能加速比 1.62 倍，在搜狗自有任务下加速比将近 4 倍。自有硬件可以保证你基于已有架构做更好的定制。如果你想在手机上跑起来，首先要对模型做更大的裁剪，在相对容忍的时间内跑起来，如果在自有硬件上，由于有很强的运算能力，我们可以把高品质模型和能力放入设备。 自 2010 年以后，深度学习技术变革了整个语音识别性能，2010 年之前，我当时学的语音专业，找工作非常难，2010 年之后忽然发现深度学习技术使用起来之后，错误率急速下降 30%，从实验室-可商用中间摇摆的状态到快速使用的状态。我们发现深度学习技术不只变革了语音识别，也变革了 AI 行业。比如现在做语音做图像，它用到的底层结构基本类似。 语音为例，2010 年之后，搜狗团队做过 DNN，做过 CNN，做过 LSTM 和简单的 RNN，我们也尝试把 CNN 做的很深，比如我们团队做 50 多层 CNN 结构，尝试了 seq2seq 等结构。我们也尝试做一些变化，比如用 CTC 结构代替之前的 cost funtion，保证它能够更多的端到端，而不要把很多东西做的太复杂了。我们尝试在 LSTM 经典的基于序列建模方式上，把它简化，因此就会有 SRU 和 QRNN 的尝试，所以在算法方面有很多的变化。 如何把多个模型结构融合在一起，形成多模型融合在一起的复合结构？比如我们现在做的是 LS-BLSTM，不同的特点，提升整体在语音识别上的效果。 语音合成的合成前端，包括分词也使用了神经网络结构，合成后端是端到端神经网络系统。这里给大家分享一些小的差异点。语音合成现在已经比较成熟了，得到的参数合成效果现在较之前有很大的提升，我们能不能做更多的事情，能不能使用少量语音做更大的合成？我们拿林志玲 6 分钟训练数据合成，或者做迁移学习，变到其他风格上面，比如她只是讲话，能不能让她去讲贯口，或者唱首歌？ 我们可以实现个性化定制，也可以称之为情感迁移、风格迁移。6 分钟林志玲的声音，合成了让机器像她一样讲话，怎么针对已有风格让它迁移到这个风格上面，我们团队在做这样一些事情。 搜狗的机器翻译技术 聊聊搜狗在机器翻译上的工作，目前的框架是去年我们获得 WMT 冠军时的框架，采用 encoder attention 加 decoder 技术，采用 layer norm 加速收敛。当时我们做了比较多的深层 RNN-NMT 模型，另外做了很多融合，在后面结果的筛选上，提升候选重排序，还有神经网络语言模型。RNN-NMT 已经是 2017 年的技术，甚至 2017 年上半年的技术，现在技术变化非常快。 2017 年，Facebook 提出了基于卷积神经网络（CNN）的 NMT，之后 Google 的论文《Attention is all you need》则提出了更先进的机器翻译技术。我们在 2017 年 7 月份上线了基于 transformer 的机器翻译系统，这个系统最大的问题在于它的解码器很慢，搜狗很快解决了解码器的问题，形成了自有的 Transformer 框架，新系统较原生系统提升了 8 倍，机器评分好了 3 个 BLEU 以上。在人工评测上，和竞品对比，我们发现目前这个框架比其他系统都要好很多。   为什么搜狗能这么快上线？主要原因是我们把解码器这件事情做好了，我们找一个小的测验级测试，TensorFlow 版本解码器是 691 毫秒，我们自己的解码器是 78 毫秒，加速比是 9 倍。目前有一些基于 transformer 的竞品开始上线，他们在做在线的时候，我们在今年 1 月份发布的旅行翻译宝用到的框架已经是离线的 transformer 了，我们认为我们的翻译产品是领先其他竞品一个代差的。 在离线产品上，我们的语音识别、语音合成、机器翻译效果媲美在线水平。 我们的提升有三个维度：翻译模型压缩至原模型的 1/35，现在大家拿到最新版本搜狗旅行翻译宝已经压缩到了 1/48。再是实时响应，最后是模型精度接近无损，基本上中英一致。这里引用了一句话，是我比较喜欢的科学家 Alex Graves，说「what is possible in principle is not always what is simple in practice」。做的过程中踩了很多坑，碰到很多问题，真正想把东西做到离线设备上，并且有好的体验，其实是很难的事情。首先从硬件设计上，最早期我们设计旅行翻译宝的时候我们就在做麦克风双阵列设置，这块描述图比双麦用到的算法多很多，这是完整麦克风阵列算法图，实际上对于我们旅行翻译宝上面用到的技术主要是三块： 1）波束形成，我首先知道你在哪，麦克风阵列一定角度指向你，在角度内的声音会做到语音增强，角度外的做到语音抑制； 2）环境降噪，我们做噪声抑制，我们叫 NS； 3）自动增益，这块主要针对远场拾音，当我和对方距离不是特别远，几十公分还好，进到麦克风里的声音很小，我怎么把它进一步放大。所以主要做了这三方面。 围绕着这三个维度给大家讲一下我们当时在开发产品时的心得。 模型压缩，我们发布的时候压缩到 1/35，现在到了 1/48，怎么做的呢？目标是怎么降低离线模型存储大小，我们毕竟希望推出一款离线设备——这就存在一定的硬件限制，我们希望把硬件需求降下来。大家能想到的就是精细的模型结构设计，减少模型的参数数量，另外是针对特别大的矩阵试图做一些 SVD 的分解。另外是量化存储和运算，现在的大部分神经网络是 32 位的，但实际上我们可以做一些量化，尝试做半精度，甚至到 8 位存储。这样的话，只要你做一次量化比特降低，就会使得你实际存储降低。这里只是模型存储降低了，我能不能把内存降低？需要做完量化运算，比如针对 16 位做运算，针对 8 位做运算，这样的话运算需求的内存也会降低。 还有参数矩阵共享，之前云端一个模型各有各的参数，没有内存和存储上的限制。但在移动设备上，因为有很多硬件上存储和内存限制，对于翻译而言，我们希望在嵌入向量（embedding）、识别语言模型和翻译之间进行参数共享，通过这种方式把存储进一步降低。还有模型裁剪，2017 年上半年我们和 MIT 助理教授，深鉴科技联合创始人韩松合作，他们致力于开发压缩后模型在 FPGA 上的运算，我们和他们共同研究了语音识别模型的深度压缩。我们在 FPGA 行业最顶级会议上发布了一篇论文，对语音识别模型压缩的工作做了总结。我们已经做到把语音模型在无损情况下压缩到原来的 20% 以下，再小就不太行了。怎么裁剪？一般剪两种，对于模型来说是凸处，即剪权重；再是剪神经元，剪完之后需要做在训练，要保证精度拉回来，所以有很多这样的工作。 运算的加速，目标是能够压缩语音翻译的延迟。我讲完话以后，马上听到翻译的合成声音控制在百毫秒级，我们的目标是 1 秒以内。为了让模型处理速度变快，我们希望在输入输出上做批量运算和批处理，另外是运算策略优化，我们希望能每个时刻都在做测算，或者跳帧处理，低帧率操作，保证运算速度提上去。在工程上，针对任务的运算定制，基于 EVA 平台的定向优化，还有任务调度，我们有很多任务。旅行翻译宝有识别翻译和合成三个大任务，里面还有很多小任务，比如在逻辑上，做路径搜索，做前端文本还是做后面的参数预测，有很多任务，我们把它拆解成小的任务，每个子任务评估目前的任务难度以及目前运算瓶颈在哪，把它分别调度到不同的运算器件上。旅行翻译宝上面有一个小的 ARM GPU 和一个大的 ARM A72 CPU 核心，及一个小的 ARM A53 CPU 核心，它们分别算哪个任务是需要提前做好调度的，我们做了一些优化策略。 我们还对很多任务进行了强定制，如一些循环、向量化的并行，以及针对内存、缓存、寄存器的优化等等。在定向优化之后，设备的运行内存快了 3 倍，解码速度快了 5 倍，这是非常重要的。这就是我们为什么要做硬件的原因——很多事情只有在可控的硬件以及有更多自由度的硬件上才可以做更多优化策略。 我有了好的速度，有了好的压缩，但仍然要保证离线翻译的效果。我们的目标是能够媲美在线的效果，在这种情况下，我需要明确设备的使用场景到底是什么，比如旅行翻译宝面向出行，场景数据要做更多优化和定制。另外是知识提纯，所有的方法都有一个老师，有一个学生。老师干什么？不用考虑运算速度和硬件限制，只要有好的性能就可以了，学生做什么？能不能从老师那里学到更多知识，让自己和老师一样有学问，大致的思路是这样。因此，有老师和学生以后，他们俩共同维护一个目标函数，让他们输出概率分布尽可能保持一致，这是整个知识提纯非常基础的想法。   我列出来的只是一篇论文，截了一张图，给大家一个主观感觉，基于不同层级的机器翻译层级，在词一级就可以做提纯了，有完全针对句一级的，也有混合在一起的，这个方法不仅限于机器翻译。另外，在语音识别上我们也在使用，比如这张图是我们在语音识别上声音的，我们用 50 层 CNN 的结构，得到语音识别模型，基于这个模型我用它做老师，训练离线语音识别模型。这是非常大的 transformer 在线翻译模型，用这个老师教离线的小学生去学知识。   最后我们做到了在线和离线模型效果保持一致。 今天的时间有限，我们只与大家分享了一些感兴趣的技术内容，感谢大家能够来到现场学习，也欢迎大家对我们的技术多提意见，也希望大家能够随时来我们团队参观访问和加入，谢谢！ 分享者：搜狗 IOT 事业部首席产品经理李健涛 李健涛：欢迎大家来到搜狗，今天我们给大家做一些分享。我们刚刚发布了翻译的硬件产品，可能很多人会好奇：你们做这款产品是出于什么目的，背后怎么思考的，今天就这个话题展开，我们要进行一场答疑解惑。 本次的分享内容有关我们做这个产品背后的思考，包含几方面的问题： 1）搜狗为什么发力翻译？现在业内出现了一些翻译类的产品，按理说，手机上的翻译 APP 很早以前就已经出现了，为什么现在几家公司都会做翻译机这样的产品，为什么大家发力翻译产品，包括 Google 也在大力推动自己的翻译产品。 2）为什么做硬件？人们普遍认为，手机上已经有很好的翻译 APP，我们应该不需要一款硬件产品。 3）除了能够语音对话翻译之外，为什么费劲拍照翻译？ 4）为什么要有屏幕，简简单单成本还低一点，便宜一点，为什么要有屏？ 5）翻译这件事情的未来会是什么样的，再往前演进会怎么样？ 今天围绕着这几个话题，我们来讲讲自己对这些事情的思考。 搜狗为什么发力翻译 搜狗作为一家互联网公司，在用户规模上和可利用的用户数据上规模非常大，我们的数据主要在于两个方面：搜索和输入法，这也是搜狗两个核心引擎。它们都是围绕语言做处理的——搜索也是帮助人们搜索互联网世界各种语言，输入法帮助人们输入语言，让表达和获取信息更简单。作为人工智能的重要组成部分，我们必须考虑未来 AI 技术发展的时候，怎么让人工智能能力对语言进行处理，人类区别于动物最重要的地方就在于人类有语言，从有语言和文字开始，人和动物就有了巨大的不同，人工智能如果能够类比人的话，它也具备对语言的处理能力。 在这里又分为两个方面：在听觉上——包括能不能听得清、听得懂，包括说得出，这里涉及语音识别、语音合成等等，相应技术稍后陈伟会介绍其中细节；在视觉上——能不能看得清、看得懂，包括把你想表达的东西合成出来，因为涉及到 OCR 等等技术。 这些技术最后会应用在三个核心方面，分别是：对话、问答、翻译。问答可以理解，是搜索再往前演进的必然发展方向，搜索现在是输入一个你想查询的词，获得 10 万条结果，算法会对结构进行排序，你可以在这些结果里找到自己想要的。问答做到的是你给计算机一个问题，它会给你一个答案，而这个答案就是你想要的，这是搜索必然要发展的方向——问答。对话是让人和机器之间、人与人之间能够顺畅无阻地进行交流，理解彼此背后的意思，包括上下文等等。 最后就是翻译。为什么翻译是目前大家都在发力的事情？因为随着神经网络技术发展，现在的机器翻译技术已经到了技术成熟的临界点，我们已经真正可以将这些技术应用在生活中了。在以前，各种翻译类产品使用更多的是统计学翻译：只是简单地按照语法结构、单词拆分做字面上的翻译。现在，我们能够做到通过神经网络来进行翻译，它可以翻译的更准，真正能够达到人们日常生活中翻译的诉求。相对来说，AI 问答和对话技术的发展程度还差那么一点点，目前离真正的应用可能还有一定的距离，所以翻译是搜狗首先选择去发力的方向。 如果大家对圣经中的故事有些了解的话，有个巴别塔的故事：上帝造人，人们希望造通天之塔找到上帝，就开始建造巴别塔，上帝不想人类干成这样的事情，就创造了语言，让不同人群无法协作，最终只好放弃了巴别塔的建造。如果通过机器技术，真正打消人们彼此之间跨语言的障碍，这会是一件非常伟大的事情，不亚于人类学会使用电能。它能够真正解决人们之间协作的问题，包括信息沟通获取的问题，这也是为什么搜狗愿意在翻译这件事情上持续投入，包括技术和产品方面，甚至做硬件产品的原因。 翻译的本质是解决人们之间跨语言交流问题，跨语言问题有三种典型场景：1）外文检索，可以看到外文世界的内容。目前国内所有搜索引擎能够接触到的中文信息只占全世界信息的 10%，这意味着 90% 的信息因为语言障碍是无法使用的。2）在线交流，当你和外国人交流的时候也会存在语言障碍。3）境外旅游场景，现在中国人境外游的频次和市场规模每年都在扩大。 外文检索，可以让你输入中文，收到外文世界的搜索结果，前段时间搜狗跨语言搜索上了英文，最近上了日文、韩文的，那些喜欢海淘的、网购的，愿意追星的，想在 Twitter 上交流的等等都可以。输入法方面，输入中文，发给对方的可以是英文、日文、韩文，这意味着你可以和外国朋友去聊天。我们也推出了 toB 产品，机器同传，现在很多大会上也都有应用，台上嘉宾演讲，屏幕上实时把演讲的中文内容英文内容识别出来，形成会议纪要。 搜狗为什么要做硬件 另外，境外旅游，为什么不做一个 APP 就 OK 呢？境外旅游有两个很重要的痛点： 第一，网络连接问题，很多国家和地区的网络是不如中国发达的，你想找个 wifi，尤其美国 wifi 不免费，即使有网络，租个当地的卡，或者连个当地的 wifi，进行翻译的话，数据通信链路依然要跑到国内的机器，消耗时间很长，这个体验会非常糟糕。所以国外翻译本身网络带来的速度感觉会非常差。 第二，翻译准确度问题，很多用户用各种翻译 APP 时候经常会吐嘈的点，大部分是翻译不准，关键在于它的识别不靠谱，有很多噪音，识别能力就会受到影响。这些都是目前手机 APP 难以解决，但又必须解决的问题。问题的一方面原因是翻译能力需要是离线的，不在云端，把这么一个复杂神经网络模型想办法放进小小的机器里，是一件非常有挑战的事情。 另一方面则是需要 MIC 阵列，需要让这样的设备具备把噪音降掉的能力，需要这样一些独有的设备去解决人们在嘈杂环境里面识音的问题，而这个是手机也做不到的。有人可能会问，你为什么不在手机 APP 上做离线神经翻译？因为离线本身对机器计算性能要求非常高，目前主流手机达不到离线计算要求，可能高端手机，新的旗舰手机勉强可以，但主流手机很难达到这样的计算能力要求。   我们今天需要这样一种特别的设备：它具备很强的离线计算能力，以及识音的能力，让我们可以做到离线语音翻译，可以即说即翻。它需要能够处理 18 种语言，毕竟现在国人出游不只是去英语世界，包括日韩，也是国人出游典型的目的地。 除了语言翻译之外，为什么会有拍照翻译？其实语言的处理两方面：声音的、图像的。尤其对于中国人来说，我们有着含蓄的文化，往往很多时候不太愿意去说，而更愿意自己解决问题，能够看，解决看不懂这个问题，这个往往更有意义。这个频次甚至更高，包括你点菜的时候，走路的时候，看路牌等等，其实很多时候你是看不懂的。这也是我们为什么在机器里面加入拍照翻译，真正解决人们看不懂的问题，加入实景 OCR，同样也是离线，让这样小小的机器可以离线识别你看到的问题，并且把它翻译成你懂的语言。 这里也有很多挑战，包括复杂的背景，复杂的版式，因为和平常 OCR 拍普通文章不一样，面对实景有很多复杂情况，包括复杂字体，刚才我看后面摆的菜单，里面有一些花体字，这个挑战非常大。另外，实景拍照光线的问题，角度的问题，包括你拍的菜单可能是扭曲的等等，这里也有很大的技术挑战。这条路我们依然在往前推进摸索，目前还没有到完美的程度。 作为一个独有的产品，只是翻译的话还是不够，我们在翻译宝中还加入了如汇率的计算、多国的时间、当地紧急联系电话等等实用小工具，让你真的在出国的时候可以拿这么一个设备，就可以解决你在出游中面对的绝大部分问题。后面我们也会考虑在里面加入导航的一些能力，目的地的景点推荐能力，包括到了某一个景区里面，某一些景物的介绍能力等等，因为很多国外你去参观博物馆，看到《蒙娜丽莎》，却看不懂英语法语的介绍，不知道它的背景故事，我们希望针对旅游场景的能力也结合进来。 对于屏幕的问题，我们也回访过很多用户，人们总是担心你识别的准不准，在对话过程中，你不必要等对方说完了机器给你读出来，因为读的速度慢，如果翻译完给你文字看这个效率更高，所以这都让有屏幕这件事情变得非常有意义。待机的时间基本上也够一周出游时间，3.1 寸的触摸屏体积也很小，随便揣在口袋里就可以走，很方便。   翻译的未来 面向未来，翻译再往前演进，未来会是什么样子？我们把这件事情拆解一下，上面是软的，下面是硬的。软的两条路径，上面是语音这条路径，下面是图像路径。通过语音识别翻译，然后合成，播放出来。图像进行图像识别、翻译，再把图像合成，给人去看，这是从软的层面。从硬的层面相对应的需要具备拾音能力，计算能力，播放能力，图像方面需要采集的能力，计算处理能力，显示的能力。 面向未来我们需要考虑的是：手机处理能力会随着时间的发展越来越强，虽然现在手机计算能力没那么强，没办法做到离线快速实时的翻译，未来随着时间的发展，也许两三年后的主流手机就可以支持这样的计算。本身播放和显示方面也是手机的优势，但你会发现，在前两件事情上，不管是从手机能力上还是使用体验上，都不是最佳的解决方案，包括拾音，本身手机就不是面向远场拾音的设备，从技术上讲，半米就算远场，识别就已经非常有挑战了，手机天然不具备优势，现在 iPhone 有三个麦克风，只能针对近场识别，有意消掉远场噪音，很长一段时间手机面对远场拾音都不会作为重要的发力点，这是手机所缺失的。 另外，手机实时性速度体验很重要。让我们去思考一下，最极致的体验是什么，我们希望能够立即听到翻译好的语言，让看不懂的文字随时变成我看得懂的文字，实时的感觉很需要，这种实时感觉也是作为手机形态产品所不适合的，你不可能一直举着手机到处看到处听。所以未来演进的形态分别是耳机、眼镜这些产品，戴着耳机和眼镜可以实时的听、看，这本身对计算能力和硬件的拾音采集等等都会带来极大的挑战。 搜狗的使命是让表达和获取信息更简单，在翻译这件事情我们希望能够让跨国表达和获取信息更简单，我们也会在这条路上持续走下去，而且会保持行业的领先，不管是技术层面的，产品层面的，持续在这条领域里面去发力。 今天大体是这些，谢谢大家！  
78,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739820&idx=5&sn=9b08713d256206fa383fc02296e19f53&chksm=871ad012b06d590452125067bc969bb3804dcec1ac06ef0e9777f11ea8899b5755e222572059&scene=27,CVPR 2018 | 新型语义分割模型：动态结构化语义传播网络DSSPN,"arXiv   近日，来自 CMU、Petuum 等机构的研究者提出一种新型语义分割模型动态结构化语义传播网络 DSSPN，通过将语义概念层次明确地结合到网络中来构建语义神经元图。实验证明 DSSPN 优于当前最优的分割模型。 引言 随着卷积神经网络的不断进步，目标识别和分割作为计算机视觉的主要研究方向取得了巨大的成功。然而，目前使用更深、更宽网络层的分割模型 [24,5,40,37,22] 在对分割注释有限的大型概念词汇的识别方面表现欠佳。原因在于它们忽略了所有概念的固有分类和语义层次。例如，长颈鹿、斑马和马同属于有蹄类动物，这个大类描绘了它们的共同视觉特征，使得它们很容易与猫/狗区分开来。此外，由于专业水平和应用目的不同，语义分割的目标概念集本质上可以开放化和高度结构化，以适应特定的任务/数据集。然而，一些技术还通过在最终预测分数上采用复杂图形推断 [7]、层级损失 [31] 或词嵌入先验 [39] 来探索视觉识别的语义层次。它们的损失约束只能间接地将视觉特征引导为可被层次感知，与通用的 CNN 模型相比，结果难以得到保证，甚至往往会得到更差的结果。 此外，模型语义层次的缺乏也阻碍了对一次性解决所有概念分割的通用分割模型的研究。现有研究 [24,5,40,37] 通常致力于训练特定任务的模型，因为数据集之间存在标签差异且数据集的注释有限。这种方法很大程度上限制了模型的泛化能力，并且偏离了通过结合概念层次来识别并关联所有概念的人类感知。如果想通过充分利用具备不同标签集的注释来改进一项任务，那么以前的模型必须移除分类层，并且仅共享中间表征。学习通用分割模型的目标与最近将不同视觉任务 [18, 36]  或多模态任务 [17] 整合进一个模型的研究有一定关联，这些研究通常使用几个固定的、具备特定损失函数的网络架构来整合所有的任务。 这篇论文旨在将语义概念层次显式地集成到动态网络优化中，称为动态结构化语义传播网络  (Dynamic-Structured Semantic Propagation Network，DSSPN)。本着课程学习（curriculum learning）的精神 [2] 逐步提高目标难度，并利用以前学过的知识学习新的细粒度的概念，DSSPN 首先遵从语义概念层次结构逐步构建语义神经元图（semantic neuron graph），其中每个神经元负责分割单词层次结构中一个概念的区域。然后每个神经元学得的特征被传播到它的子神经元中进一步学习特征，以便识别更细粒度的概念。对于每个图像或数据集，DSSPN 在激活的语义神经元子图上执行动态结构语义传播，且子图只选择当前概念及其 ancestors。得益于有序语义网络模块和动态优化策略的优点，DSSPN 学得的视觉表征可在不同概念之间自然地嵌入丰富的语义相关性。这种显式的神经元定义机制使得这篇论文提出的 DSSPN 是一种语义可解释的动态网络架构，同时具备优秀的内存和计算效率。 图 1. 根据语义概念层次结构显式构建动态网络结构的 DSSPN。基本的卷积特征被传播到动态结构化语义神经元图中，用于层级的像素级识别。在训练过程中，DSSPN 只激活语义神经元的一个子图（语义神经元涉及每个图像的目标标签），引起动态结构化的前馈传播和反向传播。这意味着 DSSPN 只需要在训练过程中将具备同一父神经元的易混淆概念进行层级分类。为了厘清概念，这里只显示部分语义神经元。 不同于只将每个神经元的父神经元的特征传递进来，受 DenseNets [16] 影响，本研究引入了一种新型密集语义增强神经块，它紧密地集成了所有 ancestor 神经元的特征来深化学习每个神经元的特征表示。通过将所有 ancestor 神经元习得的知识广播到每个子神经元中，DSSPN 可以更有效地利用语义相关性和继承性来进行特征学习。正如最近的信息瓶颈理论 [35] 所解释的那样，深层网络往往倾向于通过瓶颈来汲取信息，并只保留与目标最相关的特征。这种密集的语义连接可以通过显性地强制 ancestor 神经元保存用于识别更细粒度概念的鉴别特征，从而缓解更深层上的信息损失情况。 请注意，DSSPN 在训练期间会激活每个样本的动态计算图。为了提高可扩展性，研究者提出了动态批量优化方案，通过配置用于在每一步中学习不同神经模块的样本的动态数量优化一个 batch 中的多个计算图。 研究者在四个流行的语义分割数据集（即 Coco-Stuff [4]、ADE20k [41]、Cityscape [6] 和 Mapillary [27]）上进行了实验，证明了将 DSSPN 整合进当前最优的基础分割网络的高效性。因此，研究者展示了其动态结构化传播机制是实现分割大量内在结构化概念所需的语义可解释性方式的有效方法。此外，实验表明，在多个模型上学习统一的 DSSPN 模型能够提升性能，优于常见的利用多个领域注释的精细调整（fine-tuned）方案。 图 2. 密集语义增强模块。对于每个激活的语义神经元 v_i，它级联整个路径中的神经元特征（橙色虚线箭头）以获得增强表征 h_i，然后将 h_i 通过动态像素级预测层 P_i，以区分其子节点。每个模块的输出维度（例如 48）和像素级预测层的输出维度（例如 27）显示在括号中。 图 3. DSSPN 可以学习一个统一的分割模型，以适应不同的注释策略。为了对具有不同标签粒度的多种注释进行训练，DSSPN 激活每个图像的动态结构语义传播图。例如，Ade20k 只为所有动物类别标注单个「动物类别」标签，而 Cocostuff 精心分类每个细粒度概念，例如猫或大象。对应目标标签的语义神经元未激活（灰色实心圆圈）。因此它充分利用了语义层次中共享的概念模式和概念关系。为简洁起见，此图只显示了目标标签及其 ancestor 概念。 表 1. 在 ADE20K 验证集 [41] 上进行的现有语义分割模型对比（％）。 PSPNet (101)+DA+AL [40] 使用了其他数据增强方案和辅助损失函数。从 [39] 可知，「Conditional Softmax (VGG) [31]」、「Word2Vec(VGG) [10]」和「Joint-Cosine (VGG) [39]」表明现有方法也尝试了层级分类。 图 4. 在 Coco-Stuff 数据集上的视觉对比。对于每个图像，研究者按序显示其真值标注、「DSSPN (ResNet-101)」的预测和「DSSPN (ResNet-101) Universal」的预测。 论文：Dynamic-structured Semantic Propagation Network 论文链接：https://arxiv.org/abs/1803.06067 摘要： 语义分割的语义概念层级仍处于探索阶段，因为将结构推断整合进密集预测仍然低效且优化过程复杂。数据集之间的标签不尽相同和建模语义相关性的缺乏使得之前的研究必须针对每个任务调整相应的模型。这极大地限制了分割模型泛化至开放集合概念词汇和注释的能力。本论文提出一个动态结构语义传播网络（DSSPN），它通过将语义概念层次明确地结合到网络中来构建语义神经元图。每个神经元表示用于识别特定实体类型的实例化模块，实体类型包括大类（例如食物）或特定概念（例如披萨）。在训练阶段，DSSPN 理论上通过仅仅激活每个图像的神经元子图来执行动态结构化神经元计算图。我们提出了一种密集的语义增强神经模块，将所有 ancestor 神经元学到的知识传播到每个细粒度子神经元中以学习特征。这种语义可解释结构的另一个优点是可以在每一步选择性地为每个注释激活不同的神经元子图，进而在不同数据集上同步学习一个统一模型。我们在四个公开语义分割数据集（即 ADE20K、COCO-Stuff、Cityscape 和 Mapillary）上进行了大量实验，证明 DSSPN 优于当前最优的分割模型。此外，我们还展示了一个通用的分割模型，它在不同的数据集上进行联合训练，性能优于利用多领域知识的常见精细调整方案。 "
79,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739820&idx=4&sn=9e9be7acb53bdc4b81982ab4c3da91d2&chksm=871ad012b06d590480dde749cbc127fda09fb53645946f32d791203155199b805cc8ca31f5cf&scene=27,教程 | 如何使用LSTM在Keras中快速实现情感分析任务,TowardsDataScience 本文对 LSTM 进行了简单介绍，并讲述了如何使用 LSTM 在 Keras 中快速实现情感分析任务。 长短期记忆网络通常被称为 LSTM，它是由 Hochreiter 和 Schmiduber 提出的，被广泛地应用在语音识别、语言建模、情感分析和文本预测中。在深入 LSTM 之前，我们首先应该理解对 LSTM 的需求，这个可以通过 RNN 在实际应用中的缺陷来解释。所以我们就从 RNN 开始吧。 我们人类在看电影的时候，理解任何事件的时候每次都不是从零开始的，我们会从电影中最近发生的事中学习。但是，传统的神经网络是无法从之前的事件中学习的，因为这些信息没有从一步传递到下一步。相反，RNN 从与之紧接的前一步中学习。 例如，电影中有这么一幕：一个人在篮球场。那么我们会在未来的画面中即兴联想篮球活动：有人在奔跑、跳跃很可能被打上打篮球的标签，而一个人坐着观看很可能是观众在观看球赛。 常见的 RNN 如上所示：其中 X(t) 是输入，h(t) 是输出，A 是一个在循环中从前一步获得信息的神经网络。一个单元的输出被传送到下一个单元，信息也被传递了。 但是，有时候我们需要网络不仅仅能够从最近的过去信息中学习。假设我们要预测下面这句话中空格里面的内容：「David，一个 36 岁的男子，住在旧金山。他有一位女性朋友 Maria。Maria 在纽约一家著名的餐馆当厨师，最近他在一次校友会上遇到了他。玛丽亚告诉他，她总是对_ _ 有热情」。这里，我们希望网络能够从「厨师」（cook）中学习，预测出「烹饪」（cooking）一词。然而我们期望预测的内容和期望被预测的位置之间存在距离，这被称作长期依赖。任何比三元语法更大的东西我们都称之为长期依赖。不幸的是，RNN 在这种情况下并不会成功。 在训练 RNN 的过程中，信息在循环中一次又一次的传递会导致神经网络模型的权重发生很大的更新。这是因为每次更新中的误差梯度都会积累起来，因此会导致一个不稳定的网络。在极端情况下，权值可能会变得过大以至于溢出并形成一个非数值（NaN)。网络层之间的梯度（值大于 1）重复相乘导致梯度爆炸，而小于 1 的梯度重复相乘会造成梯度消失。 上面提到的 RNN 的缺陷促使科学家发明了一种 RNN 模型的新变体，它就是长短期记忆（LSTM）。LSTM 可以解决这个问题，因为它使用了门机制来控制记忆过程。 让我们来理解 LSTM 的结结构，并将它与 RNN 进行对比： 图中使用的符号含义如下： a) X：信息 b) +：增加信息 c) σ：Sigmoid 层 d) tanh：tanh 层 e) h(t-1)：上一个 LSTM 单元的输出 f) c(t-1)：上一个 LSTM 单元的记忆 g) X(t)：当前输入 h) c(t)：新更新的记忆 i) h(t)：当前输出 为了克服梯度消失问题，我们需要一个二阶导数在到达 0 之前能够持续很长范围的函数。tanh 函数就是满足这一属性的合适函数。 Sigmoid 可以输出 0 或 1，因此它可用来遗忘或者记住信息。 信息通过很多此类 LSTM 单元传递。LSTM 有三个主要组成部分，正如上图中所标记出来的： LSTM 具备一种特殊的结构，能够让网络忘记不必要的信息。Sigmoid 层以 X(t) 和 h(t-1) 为输入，并且决定旧输出的哪一部分应该被删除（通过输出 0）。在我们的例子中，当输入是「他有一位女性朋友 Maria」时，「David」的性别可以被忘记了，因为这里的主语已经变成「Maria」了。这个门被称作「遗忘门」f(t)。遗忘门的输出是 f(t)*c(t-1)。 下一步就是利用 cell 状态的新输入 X(t) 做决策并存储信息。Sigmoid 层决定哪个新信息应该被更新或者被忽略。tanh 层从新输入中创建一个新向量，向量的值是所有可能的值。然后这两个值相乘来更新新的 cell 状态。然后这个新记忆和旧的记忆 c(t-1) 加起来得到 c(t)。在我们的例子中，对于新的输入「他有一位女性朋友 Maria」，Maria 的性别就会被更新。当输入是「Maria 在纽约一家著名餐厅当厨师，最近他在一次校友会上遇到了她」（Maria works as a cook in a famous restaurant in New York whom he met recently in a school alumni meet）时，像「famous」、「school alumni meet」这些词可以被忽略，而「cooking」、「restaurant」以及「New York」这样的词将被更新。 最后，我们需要决定输出什么。sigmoid 层决定了我们要输出 cell 状态的哪一部分。然后，我们使 cell 状态通过 tanh 层来生成所有可能的值，并将它与 sigmoid 门的输出相乘，所以我们只输出想要输出的部分。在我们的例子中，我们想要预测空格中的单词，模型可以从记忆中得知它是一个与「cook」相关的词，因此它就可以很容易地回答这个词是「cooking」。我们的模型不是从瞬时依赖中学习这个答案，而是从长期依赖中学到的。 我们可以看到，经典 RNN 和 LSTM 的结构存在很大的差异。在 LSTM 中，我们的模型学会了在长期记忆中保存哪些信息，丢掉哪些信息。 这里，我在 Yelp 开放数据集（https://www.yelp.com/dataset）上使用 Keras 和 LSTM 执行情感分析任务。 下面是数据示例。 我使用 Tokenizer 将文本进行向量化，在限制 Tokenizer 仅仅使用前 2500 个常用词之后，把文本转换成整数序列。我使用 pad_sequences 将生成的整数序列转换成 2 维的 numpy 数组。 接下来，我构建自己的 LSTM 网络，该网络具备以下超参数： 1. embed_dim：嵌入层把输入序列编码成维度为 embed_dim 的密集向量序列。 2. lstm_out：LSTM 网络将向量序列转换成一个 lstm_out 向量，它包含整个序列的信息。 其他超参数和卷积神经网络类似，例如 dropout、batch_size。 我使用 softmax 作为激活函数。 现在，我在训练集上训练我的模型，然后在验证集上检验准确率。 在仅仅 1 个 epoch 之后，我就得到了 86% 的准确率，而这只是在一个小型数据集（包括所有行业）上运行。 下一步的工作： 1. 我们可以过滤特定的行业（如餐厅），并使用 LSTM 做情感分析。 2. 我们可以使用更大的数据集和更多的 epoch，来得到更高的准确率。 3. 我们可以使用更多隐藏密集层来提升准确率。我们也可以调整其他的超参数。 当我们期望模型能够从长期依赖中学习的时候，LSTM 优于其他模型。LSTM 遗忘、记忆和更新信息的能力使得它领先 RNN 一步。  参考资料和有用的资源 我的 GitHub repo：https://github.com/nsinha280/lstm-on-Yelp-review-data 理解 LSTM：http://colah.github.io/posts/2015-08-Understanding-LSTMs/ RNN 和 LSTM 新手指引：https://deeplearning4j.org/lstm.html 探索 LSTM：http://blog.echen.me/2017/05/30/exploring-lstms/ 关于 LSTM 的研究论文：http://www.bioinf.jku.at/publications/older/2604.pdf 原文链接： 
80,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739820&idx=1&sn=260173be8cecc3a10d7b455922c10e0c&chksm=871ad012b06d5904e65e96d0cfa236d0b6c7ce5683dd8d29514e4cbe420dc5b1ccd2786eea2c&scene=27,这是一份「不正经」的深度学习简述,TowardsDataScience 作为人工智能领域里最热门的概念，深度学习会在未来对我们的生活产生显著的影响，或许现在已经是了，从 AlphaGo 到 iPhone X 上的人脸识别（FaceID），背后都有它的身影。关于深度学习，我们能够看到很多优秀的介绍、课程和博客，本文将列举其中的精华部分，而且，你会发现这是一篇「不一样」的文章。 不一样在哪儿呢？可能是本文没有按照「正常」的深度学习博客结构：从数学讲起，然后介绍论文、实现，最后讲应用。我希望用讲故事的方式来介绍深度学习，这可能要比只介绍信息和公式要更加平易近人一些。 有时候，把自己的思考过程记录下来非常重要。 目前，深度学习（Deep Learning）是数据科学、AI、技术和人类生活中重要的一部分，它值得我们去关注。你不能简单地说：「深度学习就是往神经网络中添加一个层，哇，神奇！」。不，不是这样。我希望读完本文后，大家会对深度学习有不一样的认识。 我根据多篇论文和其他文章的内容绘制了这份时间线，旨在使大家看到深度学习不只是神经网络。在它的发展过程中出现了真正的理论进步、软件和硬件进展。 深度学习已经出现很久了，那么为什么它直到最近 5-7 年才闻名于世，并迅速发展起来呢？ 如前所述，直到 21 世纪初，我们仍然缺乏训练非常深层神经网络的可靠途径。现在，随着多个简单却重要的理论、算法进步，硬件发展（大部分是 GPU，现在是 TPU）和数据的指数级增长和积累，深度学习快速发展，并改变我们做机器学习的方式。 深度学习也是非常活跃的研究领域，今天，众多研究者们仍在寻找最好的模型、网络拓扑、最好的超参数优化方法等等。要想像其他活跃的科学领域一样紧跟研究成果很难，但是并非不可能。 Hofer 等人在论文《Deep Learning with Topological Signatures》如此介绍拓扑和机器学习： 近期代数拓扑方法仅在机器学习社区出现，最显著的是，它出现在术语「拓扑数据分析」（topological data analysis，TDA）下面。TDA 帮助我们从数据中推断出相关拓扑和几何信息，因此它提供了一种看待多种机器学习问题的新型、有益的视角。 对我们来说很幸运的是，有很多人在帮助我们理解和消化此类信息，比如吴恩达的课程、一些相关博客等等。 参考阅读： 机器之心专访吴恩达，深度学习课程项目 Deeplearning.ai 正式发布 吴恩达 Deeplearning.ai 课程学习全体验：深度学习必备课程（已获证书） 入门 | 吴恩达 Deeplearning.ai 全部课程学习心得分享 资源 | 吴恩达 deeplearning.ai 第四课学习心得：卷积神经网络与计算机视觉 资源 | 吴恩达 deeplearning.ai 五项课程完整笔记了解一下？ 这对我来说有些奇怪或者不寻常，因为正常情况下你必须花费一段时间（甚至好多年）才能消化论文或期刊中那么多艰深、前沿的信息。当然，现在大部分科学领域从论文到一篇博客解读的时间越来越快，虽然我认为深度学习还有一些不一样的感觉。 机器学习领域中的大多数人都认为，几十年来深度学习论文中的每个最新思想（具体来说是指神经网络或算法的新型拓扑结构和配置）都是机器学习中的最棒思想（要知道深度学习是机器学习的子领域）。 我在本文中用了很多次「学习」（learning）这个词，那么「学习」究竟是什么意思呢？ 在机器学习中，「学习」是指为你正在分析和研究的数据自动搜索更好的数据表征的过程（记得，这并不是让机器来学习）。 「表征」（representation）一词在这个领域中特别重要，那什么是「表征」呢？「表征」就是观察数据的方式。 举个例子，如下图所示，假设问题是画出一条直线将图中的蓝色圆和绿色三角形分开： 在《深度学习》这本书中，作者解释道：我们使用笛卡尔坐标系来表征数据，这时该问题不可解。 难道就没办法了吗？当然不是。如果我们采用不同的方式来表征数据，使得可以用直线分离不同的数据类型。这种方法在数学中已经出现了好几百年。在这个例子中我们需要的仅仅是一次坐标变换。通过坐标变换，我们得到了问题的解： 现在我们就可以画出一条直线来分离数据： 因此在这个例子中，我们通过手动探索并选择了能获得更好的表征方式的变换。但是，假如我们能开发一个系统或程序来自动搜索不同的表征（在这个例子中是坐标变换），然后确定新方法的分类准确率的计算方式，这时候就变成了机器学习。 这一点很重要，深度学习是使用不同类型神经网络的表征学习，通过优化网络的超参数来获得对数据的更好表征。 而没有深度学习中的突破性研究，这一切也将不可能出现，这里我列出几个经典案例： 1：反向传播 参考阅读： 被 Geoffrey Hinton 抛弃，反向传播为何饱受质疑？（附 BP 推导） A theoretical framework for Back-Propagation——Yann Lecun：http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf 2：更好的初始化网络参数。 需要记住的是：初始化策略需要根据所使用的激活函数来选择。 参考阅读： 「深度学习的权重初始化」——Coursera：https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks How to train your Deep Neural Network：http://rishy.github.io/ml/2017/01/05/how-to-train-your-dnn/ 斯坦福大学 CS231n Convolutional Neural Networks for Visual Recognition：http://cs231n.github.io/neural-networks-2/#init 3：更好的激活函数。 这意味着，可以更快地逼近函数，从而实现更快的训练。 参考阅读： 一文概览深度学习中的激活函数 神经网络中激活函数的作用 4：Dropout：防止过拟合等问题。 Learning Less to Learn Better—Dropout in (Deep) Machine learning：https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5 Geoffrey Hinton 等人的「Dropout: A Simple Way to Prevent Neural Networks from Overfitting」：http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer 5：卷积神经网络（CNN） 参考阅读： 一文看懂卷积神经网络 Yann LeCun 等人的「Gradient-Based Learning Applied to Document Recognition」：http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf 6：残差网络（ResNet） 参考阅读： 孙剑等人的论文「Deep Residual Learning for Image Recognition」：https://arxiv.org/abs/1512.03385v1 论文「Residual Networks of Residual Networks: Multilevel Residual Networks」：https://arxiv.org/abs/1608.02908 7：基于区域的 CNN ，可用于目标检测等。 参考阅读： 论文「Rich feature hierarchies for accurate object detection and semantic segmentation」：https://arxiv.org/abs/1311.2524v5 先理解 Mask R-CNN 的工作原理，然后构建颜色填充器应用 从论文到测试：Facebook Detectron 开源项目初探 8：循环神经网络（RNN）与 LSTM 参考阅读： LSTM 入门必读：从入门基础到工作方式详解 在调用 API 之前，你需要理解的 LSTM 工作原理 从 90 年代的 SRNN 开始，纵览循环神经网络 27 年的研究进展  9：生成对抗网络（GAN） 参考阅读： Ian Goodfellow 等人的 GAN 论文：https://arxiv.org/abs/1406.2661v1 机器之心 GitHub 项目：GAN 完整理论推导与实现，Perfect！ 10：Geoffrey Hinton 近期提出的 Capsule 浅析Geoffrey Hinton最近提出的Capsule计划 Capsule 官方代码开源之后，机器之心做了份核心代码解读 当然，还有很多其它的重要成果。我认为正是以上所列举的研究给我们带来了重要的理论和算法上的突破，并改变了世界，推动了深度学习的革命。 如何入门深度学习？ 深度学习的入门并不容易，但我会尽我所能指导你完成这一阶段。参考以下学习资源，但记住，你需要的不仅仅是观看视频和阅读论文，还需要不断地理解、编程、写代码、经历失败，然后成功。 -1. 请先学习 Python 和 R 语言：） 0. 学习吴恩达的深度学习课程 Siraj Raval 的视频 ：Siraj Raval 非常 amazing，他可以用风趣易懂的方式来解释复杂的概念。你可以在 YouTube 上关注他的个人频道，其中这两个视频非常棒： François Chollet 的两本书： Deep Learning with Python Deep Learning with R 深度学习是数据科学家应该学习的最重要的工具和理论之一。我们很幸运，有那么多深度学习方向的研究、软件、工具和硬件被开发出来。 深度学习的计算成本很昂贵，即使在理论、软件和硬件有所进展的情况下，我们也需要大数据和分布式机器学习的发展来提升深度学习的性能和效率。为此，人们开发出了分布式框架（Spark）和深度学习库（TensorFlow、PyTorch 和 Keras）。 参考阅读： 学习了！谷歌今日上线基于 TensorFlow 的机器学习速成课程（中文版） 三天速成！香港科技大学 TensorFlow 课件分享 四天速成！香港科技大学 PyTorch 课件分享 分布式 TensorFlow 入坑指南：从实例到代码带你玩转多机器深度学习 分布式 Keras（Keras+Spark）：https://github.com/cerndb/dist-keras 在谷歌云上运行 TensorFlow 和 Spark：https://cloud.google.com/blog/big-data/2017/11/using-apache-spark-with-tensorflow-on-google-cloud-platform 正如之前所说的，深度学习领域最重要的里程碑之一就是 TensorFlow 的创建与开源。 TensorFlow 是一个使用数据流图进行数学计算的开源软件库，图中的节点表示数学运算，而图的边表示在节点之间通信的多维数据阵列（张量）。 上图是广义相对论黎曼张量中的张量运算。 张量，从数学定义上看，就是简单的数或函数的阵列，根据坐标变换的特定规则进行变换。 但是在机器学习和深度学习领域中，张量是向量和矩阵在更高维度上的泛化形式。TensorFlow 将张量表示为基础数据类型的 n 维数组。 我们在深度学习中广泛使用张量，但是你不必成为这方面的专家，只需要稍微了解就足够了。 参考阅读： 教程 | 深度学习初学者必读：张量究竟是什么？ 现在你已经了解了我之前提到的突破和编程框架（如 TensorFlow 或 Keras），那么你应该明白自己需要了解和使用深度学习的哪些方面了。 但是目前我们使用深度学习取得了什么成就？下面我列举了一些（来自 François Chollet 的书）： 接近人类水平的图像识别能力； 接近人类水平的语音识别能力； 接近人类水平的手写体转录能力； 机器翻译水平提高； 文本转语音水平提高； 数字助手，如 Google Now 或 Amazon Alexa； 接近人类水平的自动驾驶能力； 广告定向投放水平提高； 网页搜索结果优化； 自然语言问答能力提高； 超越人类的围棋水平。 参考阅读： 30 AMAZING APPLICATIONS OF DEEP LEARNING：http://www.yaronhadad.com/deep-learning-most-amazing-applications/ 关于深度学习的未来，我认为 GUI 和 AutoML 是深度学习不久后能够达到的。不要误会，我喜欢写代码，但是我也认为以后我们写的代码数量会减少。我们不能重复浪费那么多时间一遍一遍写同样的东西，因此我认为这两个功能（GUI 和 AutoML）将帮助数据科学家提高生产力，解决更多问题。 参考阅读： 业界 | 李飞飞、李佳宣布发布 Cloud AutoML：AI 技术「飞入寻常百姓家」 专栏 | 自动选模型+调参：谷歌 AutoML 背后的技术解析 在简单的 GUI 中完成这些任务的最好免费平台之一是 Deep Cognition。其简单的拖放界面可以帮助你轻松设计深度学习模型。Deep Learning Studio 具备先进的 AutoML 功能（几乎可以一键完成），可以为你的自定义数据集设计深度学习模型。 该平台还是免费的～ 我的意思是，这个领域的发展实在太迅速，现在我们已经可以使用简单的 GUI 来学习本文中涉及的所有复杂和有趣的概念。 我喜欢这个平台的原因是，你不需要安装任何东西就可以写代码，用命令行或 Notebook 来使用 TensorFlow、Keras、Caffe、MXNet 等。  其它有趣的深度学习应用： http://skejul.com/ https://www.microsoft.com/en-us/seeing-ai/ https://dialogflow.com/ 
81,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739772&idx=5&sn=7be3ddaef15686974df6bb9cfcb49466&chksm=871ad042b06d595499e663b25f6d5d743af4cccc195ab69f47eb4a0829330778b8dc586d1f05&scene=27,CVPR 2018 | 中国科学院大学Oral论文：使用鉴别性特征实现零样本识别,arXiv 在将于今年六月举办的 CVPR 2018 会议上，中国科学院大学、英国邓迪大学和中国科学院脑科学与智能技术卓越创新中心的一篇 Oral 论文提出了一种使用鉴别性特征学习零样本识别的方法。和人工智能领域的很多新研究成果一样，该研究实现了当前最佳。机器之心对该研究进行了编译介绍。 近年来，零样本学习（ZSL：zero-shot learning）已经在目标识别任务中得到普及应用。传统的目标识别方法是通过将图像标签分配到训练集中见过的一个类别来预测目标实例的存在，零样本学习则与传统方法不同，其目标是识别之前从未见过的新类别中的目标实例。因此，在 ZSL 任务中，在训练集中见过的类别和测试集中没见过的类别是不相交的。 通常而言，见过和没见过的类别都要提供类别描述信息（比如用户定义的属性标注、类别的文本描述、类别名的词向量等）；某些描述信息是各个类别共有的。这些描述信息通常被称为辅助信息或语义表征。在本研究中，我们关注的是使用属性的 ZSL 的学习。 如图 1 所示，典型 ZSL 方法的一个通用假设是：存在一个共有的嵌入空间，其中有一个映射函数 ，定义这个函数的目的是对于见过或没见过的类别，衡量图像特征 φ(x) 和语义表征 ψ(y) 之间的相容性（compatibility）。W 是所要学习的视觉-语义映射矩阵。现有的 ZSL 方法主要侧重于引入线性或非线性的建模方法，使用各种目标和设计不同的特定正则化项来学习该视觉-语义映射，更具体而言就是为 ZSL 学习 W。 到目前为止，映射矩阵 W 的学习（尽管对 ZSL 很重要）的主要推动力是视觉空间和语义空间之间对齐损失的最小化。但是，ZSL 的最终目标是分类未见过的类别。因此，视觉特征 φ(x) 和语义表征 ψ(y) 应该可以被区分开以识别不同的目标。不幸的是，这个问题在 ZSL 领域一直都被忽视了，几乎所有方法都遵循着同一范式：1）通过人工设计或使用预训练的 CNN 模型来提取图像特征；2）使用人类设计的属性作为语义表征。这种范式存在一些缺陷。 第一，图像特征 φ(x) 要么是人工设计的，要么就是来自预训练的 CNN 模型，所以对零样本识别任务而言可能不具有足够的表征能力。尽管来自预训练 CNN 模型的特征是学习到的，然而却受限于一个固定的图像集（比如 ImageNet），这对于特定 ZSL 任务而言并不是最优的。 第二，用户定义的属性 ψ(y) 是语义描述型的，但却并不详尽，因此限制了其在分类上的鉴别作用。也许在 ZSL 数据集中存在一些预定义属性没有反映出来的鉴别性的视觉线索，比如河马的大嘴巴。另一方面，如图 1 所示，「大」、「强壮」和「大地」等被标注的属性是很多目标类别都共有的。这是不同类别之间的知识迁移所需的，尤其是从见过的类别迁移到没见过的类别时。但是，如果两个类别（比如豹和虎）之间共有的（用户定义的）属性太多，它们在属性向量空间中将难以区分。 第三，现有 ZSL 方法中的低层面特征提取和嵌入空间构建是分开处理的，并且通常是独立进行的。因此，现有研究中很少在统一框架中考虑这两个组分。 为了解决这些缺陷，我们提出了一种端到端的模型，可以同时在视觉空间和语义空间中学习用于 ZSL 的隐含的鉴别性特征（LDF）。具体而言，我们的贡献包括： 一种级联式缩放机制，可用于学习以目标为中心的区域的特征。我们的模型可以自动识别图像中最具鉴别性的区域，然后在一个级联式的网络结构中将其放大以便学习。通过这种方式，我们的模型可以专注于从以目标为焦点的区域中学习特征。 一种用于联合学习隐含属性和用户定义的属性的框架。我们将隐含属性的学习问题形式化为了一个类别排序问题，以确保所学习到的属性是鉴别性的。同时，在我们模型中，鉴别性区域的发掘和隐含属性的建模是联合学习的，这两者会互相协助以实现进一步的提升。 一种用于 ZSL 的端到端网络结构。所获得的图像特征可以调整得与语义空间更加兼容，该空间中既包含用户定义的属性，也包含隐含的鉴别性属性。 我们提出的方法的框架如图 2 所示。注意，原则上该框架包含多个图像尺度，但为描述清楚，这里仅给出了有 2 个图像尺度的情况作为示例。在每个图像尺度中，网络都由三个不同组分构成：1）图像特征网络（FNet），用于提取图像表征；2）缩放网络（ZNet），用于定位最具鉴别性的区域，然后将其放大；3）嵌入网络（ENet），用于构建视觉信息和语义信息关联在一起的嵌入空间。对于第一个尺度，FNet 的输入是原始尺寸的图像，ZNet 负责生成放大后的区域。然后到第二个尺度，放大后的图像区域成为 FNet 的输入，以获得更具鉴别性的图像特征。 我们提出的 LDF 模型在两个有代表性的 ZSL 基准上进行了评估，即：Animals with Attributes（AwA）和 Caltech-UCSD Birds 200-2011（CUB）。 论文：用于零样本识别的隐含特征鉴别式学习（Discriminative Learning of Latent Features for Zero-Shot Recognition） 论文地址：https://arxiv.org/abs/1803.06731  摘要：零样本学习（ZSL）的目标是通过学习图像表征和语义表征之间的嵌入空间来识别未曾见过的图像类别。多年以来，在已有的研究成果中，这都是学习对齐视觉空间和语义空间的合适映射矩阵的中心任务，而学习用于 ZSL 的鉴别性表征的重要性却被忽视了。在本研究中，我们回顾了已有的方法，并表明了为 ZSL 的视觉和语义实例学习鉴别性表征的必要性。我们提出了一种端到端的网络，能够做到：1）通过一个缩放网络自动发现鉴别性区域；2）在一个为用户定义属性和隐含属性引入的扩增空间中学习鉴别性语义表征。我们提出的方法在两个有挑战性的 ZSL 数据集上进行了大量测试，实验结果表明我们提出的方法的表现显著优于之前最佳的方法。 
82,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739772&idx=3&sn=1d1a6cadfa042064928cab3191942df7&chksm=871ad042b06d59548a5406b8e45d05b48671e7cfd77c7633f6fb557ef783a2fb76cacbecf7a0&scene=27,专栏 | 新手入门？一步一步教你如何安装PaddlePaddle,不久之前，机器之心联合百度推出 PaddlePaddle 专栏，为想要学习这一平台的技术人员推荐相关教程与资源。在解析过   框架之后，从这篇文章开始上手，安装 PaddlePaddle。 环境 Windows 系统的安装 在 Windows 上安装 Docker 容器 在 Windows 上安装 Ubuntu 使用 pip 安装 使用 Docker 安装 从源码编译生成安装包 在本地编译生成安装包 在 Docker 编译生成安装包 编译 Docker 镜像 测试安装环境 最后提示 项目代码 参考资料 系统：Ubuntu 16.0.4（64 位） 处理器：Intel(R) Celeron(R) CPU  内存：8G PaddlePaddle 目前还不支持 Windows，如果读者直接在 Windows 上安装 PaddlePaddlePaddle 的话，就会提示没有找到该安装包。如果读者一定要在 Windows 上工作的话，笔者提供两个建议：一、在 Windows 系统上使用 Docker 容器，在 Docker 容器上安装带有 PaddlePaddle 的镜像；二、在 Windows 系统上安装虚拟机，再在虚拟机上安装 Ubuntu。 在 Windows 上安装 Docker 容器 首先下载 Docker 容器的工具包 DockerToolbox，笔者使用这个安装包不仅仅只有 Docker，它还包含了 VirtualBox 虚拟机，使用者工具包我们就不用单独去安装 VirtualBox 虚拟机了，DockerToolbox 的官网下载地址：https://docs.docker.com/toolbox/toolbox_install_windows/ 下载之后，就可以直接安装了，双击安装包，开始安装  选择安装路径，笔者使用默认的安装路径  然后安装所依赖的软件，因为笔者之前在电脑上已经安装了 git，所以在这里就不安装了，其他都要勾选  这一步不用修改什么，让程序为我们创建一个桌面快捷键  最后就可以安装了，等待一小段时间即可  到这里就安装完成了  安装完成之后，如果直接启动 Docker 的话，有可能可能会卡在这里，因为还有下载一个 boot2docker.iso 镜像，网速比较慢的话就可能一直卡在这里。所以我们还要镜像下一步操作 在下载 DockerToolbox 的时候，这个工具就已经带有 boot2docker.iso 镜像了。并且存在 DockerToolbox 安装的路径上，笔者的路径是： C:\Program Files\Docker Toolbox\boot2docker.iso 我们把这个镜像复制到用户目录\.docker\machine\cache\，如笔者的目录如下： 复制完成之后，双击桌面快捷方式 Docker Quickstart Terminal，启动 Docker，命令窗口会输出以下信息： 最后看到 Docker 的 logo 就表示成功安装 Docker 容器了 到这就可以使用 Docker 来安装 PaddlePaddle 了，具体请看本文章中关于 Docker 使用 PaddlePaddle 部分 在 Windows 上安装 Ubuntu 在 Windows 上在 Ubuntu 就要先安装虚拟机，虚拟机有很多，笔者使用的是开源的 VirtualBox 虚拟机，VirtualBox 的官网：https://www.virtualbox.org/ 安装完成 VirtualBox 虚拟机之后，进入到 VirtualBox 虚拟机中点击新建，创建一个系统  选择分配的内存，我这里只是分配了 2G，如果正式使用 PaddlePaddle 训练模型，这远远不够，读者可以根据需求分配内存  创建一个虚拟硬盘  选择默认的 VDI 硬盘文件类型  这里最好是选择动态分配硬盘，这样虚拟机会根据实际占用的空间大小使用电脑本身的磁盘大小，这样会减少电脑空间的占用率的。如果是固定大小，那么创建的虚拟机的虚拟硬盘一开始就是用户设置的大小了。 这里就是选择虚拟硬盘大小的，最后分配 20G 以上，笔者分配 30G，应该够用。 然后选择刚才创建的 Ubuntu 系统，点击设置，这系统中取消勾选软驱，然后点击存储，选择 Ubuntu 镜像，笔者使用的是 64 位 Ubuntu 16.04 桌面版的镜像  最后就可以启动安装 Ubuntu 了。选择我们创建的 Ubuntu 系统，点击启动，进入到开始安装界面，为了方便使用，笔者选择中文版的 为了安装之后不用在安装和更新应用，笔者勾选了安装 Ubuntu 时下载更新，这样在安装的时候就已经更新应用了  然后是选安装的硬盘，因为我们使用的自己创建的整一个硬盘，所以我们可以直接选择青春整个硬盘并安装 Ubuntu，这里就不用考虑分区和挂载问题了  选择所在的位置，这没什么要求的，笔者随便选择一个城市  然后是选择键盘的布局，通常的键盘布局都是英语（美国） 创建 Ubuntu 的用户名称和密码  最后就是安装了，这个安装过程可能有点久，耐心等待  安装完成之后就可以在 Windows 系统上使用 Ubuntu 系统了，我们再使用 Ubuntu 来学习和使用 PaddlePaddle 做深度学习了。最好安装完成之后，把在存储中设置的 Ubuntu 镜像移除  在本篇文章之后部分都是在 Ubuntu 上操作，我们都可以使用 Ubuntu 这虚拟机来完成。 如果读者使用的是 Windows 10，可以使用 Windows 系统自带的 Linux 子系统，安装教程可以看我之前的文章 Windows10 安装 Linux 子系统。 如果你还没有在 pip 命令的话，首先要安装 pip，要确保安装的 pip 版本是大于 9.0.0 的，否则可能无法安装 paddlepaddle。 安装 pip 命令如下： 安装之后，还有看一下 pip 的的版本 pip --version，如果版本低于 9.0.0，那要先升级 pip，先要下载一个升级文件，命令如下： 下载完成之后，可以使用这个文件安装最新的 pip 了 安装 pip 就可以动手安装 paddlepaddle 了。如果权限不够，请在 root 下执行命令 现在就测试看看 paddlepaddle 有没有，在 python 的命令终端中试着导入 paddlepaddle 包： 如果没有报错的话就证明 paddlepaddle 安装成功了。 为什么要使用 Docker 安装 paddlepaddle 呢，Docker 是完全使用沙箱机制的一个容器，在这个容器安装的环境是不会影响到本身系统的环境的。通俗来说，它就是一个虚拟机，但是它本身的性能开销很小。在使用 Docker 安装 paddlepaddle 前，首先要安装 Docker，通过下面的命令就可以安装了： 安装完成之后，可以使用 docker --version 查看 Docker 的版本，如果有显示，就证明安装成功了。可以使用 docker images 查看已经安装的镜像。 一切都没有问题之后，就可以用 Docker 安装 paddlepaddle 了，命令如下： 在这里不得不说的是，这个安装过程非常久，也许是笔者的带宽太小了。安装完成后，可以再使用 docker images 命令查看安装的镜像，应该可以 看到类似这样一个镜像，名字和 TAG 会相同，其他信息一般不同 我们的硬件环境都有很大的不同，官方给出的 pip 安装包不一定是符合我们的需求，比如笔者的电脑是不支持 AVX 指令集的，在官方中没找到这个的安装包（也行现在已经有了），所以我们要根据自己的需求来打包一个自己的安装包。 在本地编译生成安装包 1. 安装依赖环境  在一切开始之前，先要安装好依赖环境，下面表格是官方给出的依赖环境 1.1 安装 GCC  一般现在的 Ubuntu 都是高于个版本了，可以使用 gcc --version 查看安装的版本。比如笔者的是 4.8.4，如果你的是版本是低于 4.8.2 的就要更新一下了 1.2 安装 CMake  先要从官网下 CMake 源码 解压源码 依次执行下面的代码 查看是否安装成功，cmake --version，如果正常显示版本，那已经安装成功了。 1.3 安装 pip  关于安装 pip9.0.0 以上的版本，在上面的使用 pip 安装部分已经讲了，这里就不在熬述了  1.4 安装 numpy  安装 numpy 很简单，一条命令就够了 顺便多说一点，matplotlib 这个包也经常用到，顺便安装一下 1.5 安装 SWIG  执行下面代码安装 SWIG，安装成功之后，使用 swig -version 检查安装结果 1.6 安装 Go  官方说可选择，那看情况吧，如果像安装安装吧，笔者顺便安装了，就一条代码的事情，老规则 go version 到这里，依赖环境就已经安装好了，准备安装 paddlepaddle。 2. 首先要在 GitHub 上获取 paddlepaddle 源码 3. 然后输以下命令 经过长久的 make 之后，终于生成了我们想要的安装包，它的路径在 Paddle/build/python/dist 下，比如笔者在该目录下有这个安装包 paddlepaddle-0.11.0-cp27-cp27mu-linux_x86_64.whl，你的命名可能不是这个。之后就可以安装了，使用 pip 安装： 这个我们就已经安装了 paddlepaddle，现在就测试看看 paddlepaddle 有没有安装成功了，在 python 的命令终端中试着导入 paddlepaddle 包： 如果没有报错的话就证明 paddlepaddle 安装成功了。 在 Docker 编译生成安装包 使用 Docker 就轻松很多了，有多轻松，看一下便知 。 1. 首先要在 GitHub 上获取 paddlepaddle 源码 2. 切入到项目的根目录下 3. 生成安装包  下面一行代码，提醒一下，这个过程非常长，一定要有耐心，顺便把编译测试关了，减少空间 同样会在 Paddle/build/python/dist 下生成一个安装包，这对比在本地生成的安装包，是不是要简单很多，没错这就是 Docker 强大之处，所有的依赖环境都帮我们安装好了，现在只要安装这个安装包就行了： 同样我们要测试看看 paddlepaddle 有没有安装成功了，在 python 的命令终端中试着导入 paddlepaddle 包： 如果没有报错的话就证明 paddlepaddle 安装成功了。 如果你比较喜欢使用 Docker 来运行你的 paddlepaddle 代码，但是有没有你想要的镜像，这是就要自己来制作一个 Docker 镜像了，比如笔者的电脑是不支持 AVX 指令集的，还只有 CPU，那么我就要一个不用 AVX 指令集和使用 CPU 训练的镜像。好吧，我们开始吧  1. 我们要从 GitHub 下载源码： 2. 安装开发工具到 Docker image 里 有可能它不能够命名为 paddle:dev，我们可以对他从重新命名，ID 要是你镜像的 ID 镜像名:TAG 3. 编译 # 这个编译要很久的，请耐心等待 安装完成之后，使用 docker images 查看刚才安装的镜像。 我们就使用官方给出的一个例子，来测试我们安装 paddlepaddle 真的安装成功了  1. 创建一个记事本，命名为 housing.py，并输入以下代码： 2. 执行一下该代码  在本地执行代码请输入下面的命令 在 Docker 上执行代码的请输入下面的代码 -v 命令是把本地目录挂载到 docker 镜像的目录上，-w 设置该目录为工作目录，-p 设置端口号，使用到的镜像是在使用 Docker 安装部分安装的镜像 docker.paddlepaddle.org/paddle 3. 终端会输出下面类似的日志 如果没有成功运行该代码，报错信息如下，说明安装的 paddlepaddle 版本过低，请安装高版本的 paddlepaddle 有很多学习者会出现明明安装完成 PaddlePaddle 了，但是在 PaddlePaddle 的时候，在初始化 PaddlePaddle 这一行代码出错 这个多数是读者的电脑不支持 AVX 指令集，而在 PaddlePaddle 的时候，安装的是支持 AVX 指令集的版本，所以导致在初始化 PaddlePaddle 的时候报错。所以在安装或者编译 PaddlePaddle 安装包时，要根据读者电脑本身的情况，选择是否支持 AVX 指令集。查看电脑是否支持 AVX 指令集，可以在终端输入以下命令，输出 Yes 表示支持，输出 No 表示不支持。 项目代码 GitHub 地址:https://github.com/yeyupiaoling/LearnPaddle 参考资料 http://paddlepaddle.org/ https://pip.pypa.io/en/stable/ http://www.runoob.com/ http://www.linuxidc.com/Linux/2016-12/138489.htm https://www.jianshu.com/p/c6264cd5f5c7 
83,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739772&idx=2&sn=fadf2d0a8bbe269d2c6b3890d5321eab&chksm=871ad042b06d5954fbaac6e3022f461a44e24a2fc49f6f77f376f011cea08262efcc87071aa8&scene=27,深度 | DeepMind提出神经元删除法：通过理解每个神经元来理解深度学习,近日，DeepMind 发表博客介绍其对神经网络可解释性的最新研究成果。受神经科学启发，他们通过删除神经元来探索其对网络性能的影响。研究发现，和过去的经验直觉相反，选择性神经元（如「猫神经元」）对于网络的泛化能力并不重要。而某些行为难以理解的非选择性神经元却是不可或缺的。此外，作者还对比了泛化好和记忆好的网络对删除操作的响应行为。 深度神经网络由很多独立的神经元组成，这些神经元以一种复杂而反直觉的方式结合，从而完成一系列的挑战性任务。这一复杂性保证了神经网络的效力，但也使其成为了一个令人困惑且不透明的黑箱。 理解深度神经网络的工作原理对于解释其决策、构建更强大的系统来说至关重要。比如，想象一下，如果不了解每个齿轮之间的协作原理，那么制造一个钟表该有多么困难。探索独立神经元的作用，尤其是那些可以轻松解释的神经元，可以帮助我们理解神经科学和深度学习中的神经网络。 我们的论文  On the importance of single directions for generalization 将很快出现在 ICLR 2018 上，它使用一种受到数十年神经科学实验成果启发的方法来决定深度神经网络中小批神经元的重要性，以及更易解释的神经元对网络计算是否更重要，从而 探索破坏带来的影响。 我们通过删除单个神经元和神经元集群来测量破坏网络造成的性能影响。实验得出了两个出人意料的结果: 尽管许多早先的研究集中探讨容易解释的单个神经元 (如「猫神经元」或深度网络中只对猫的图像有反应的神经元)，但我们发现这些可解释的神经元并不比激活行为难以解释的困惑神经元更重要。 与只能对以前看过的图像进行分类的网络相比，能对未看过的图像进行正确分类的网络在神经元删除时表现出了更强的适应性。换句话说，泛化良好的网络比记忆良好的网络对单方向的依赖要小得多。 「猫神经元」也许更易解释，但并不重要 在神经科学和深度学习中，人们已经广泛分析了只对单一输入类别的图像（比如狗）作出积极回应的易于解释的神经元（「选择性」神经元）。在深度学习中，这导致了对猫神经元、情感神经元和括号神经元的重要性强调；在神经科学中则是 Jennifer Aniston 神经元，等等。然而，相比于具有低选择性、高度困惑性和难以解释的行为的绝大多数神经元，这些少量的高选择性神经元的相对重要性依然不得而知。 相对于那些对图像集做出似乎随机性的积极和消极回应的令人困惑的神经元，带有清晰回应模式（比如只对狗积极回应，对其他一切消极回应）的神经元更易解释。 为了评估神经元的重要性，我们测量了当删除神经元时神经网络在图像分类任务上的表现是如何改变的。如果一个神经元非常重要，删除它的后果应该很严重，并导致网络性能锐减；而当删除一个不重要的神经元时则影响较小。神经科学家也惯常地执行类似的实验，尽管他们无法达到实验所需的细粒度精确度，但应用于人工神经网络则毫无难度。 删除操作对简单神经网络的影响的概念图。颜色越深，表明神经元越活跃。尝试单击隐藏层神经元对它们进行删除，并查看输出神经元活跃度的变化（原网页）。请注意，仅删除一个或两个神经元对输出的影响很小，而删除大多数神经元则影响很大，并且某些神经元的重要度高于其他神经元！ 出人意料的是，我们发现选择性和重要性之间没有什么关系。换句话说，「猫神经元」并不比困惑神经元更重要。这一发现与神经科学最近的研究成果相呼应，后者已经证明，困惑神经元实际上可以提供相当多的信息，除了那些最容易解释的神经元之外，我们还应研究其他神经元，只有这样才能理解深度神经网络。 虽然「猫神经元」可能更具可解释性，但它们相对于没有明显偏好的困惑神经元并没有更强的重要性。可以尝试点击上图（原网页）来查看重要性和可解释性的几种可能关系（正相关、负相关或不相关）。 尽管可解释的神经元在直觉上更易理解（「它喜欢狗」），但它们并不比没有明显偏好的令人困惑的神经元重要。 泛化能力更好的网络更不容易崩坏 只有当系统能泛化到新的场景中时，该系统才能称得上是智能的。例如，一个图像分类网络仅能分类它见过的特定的狗的图像，而对于同一只狗的不同图像则无能为力，该网络就是无用的。近期一篇由 Google Brain、Berkeley 和 DeepMind 合作的论文《Understanding deep learning requires rethinking generalization》表明深度网络可以简单地记住训练过的每张图像，而不是像人类一样学习（例如，理解「狗」的抽象概念）。（参见： ） 然而，关于神经网络学习到的解的泛化能力是由什么因素造成的，至今仍未得到清晰的解答。通过持续删除越来越大的神经元集群，我们发现泛化能力更好的网络对于删除操作更具鲁棒性（相对于仅在训练过程中记忆图像的网络而言）。换句话说，泛化能力更好的网络的性能更不容易崩坏（虽然仍可能遭遇崩坏）。 随着被删除的神经元集群越来越大，泛化能力好的网络的性能下降显著慢于在训练中记忆的网络。 通过这种方式来测量网络的鲁棒性，我们可以评估网络是否使用记忆能力在「作弊」。理解网络记忆过程中的变化，可以帮助我们建立泛化能力更好、更不依赖于记忆的网络。 神经科学启发的分析方法 综上，这些发现表明使用实验神经科学启发的技术可以帮助我们理解人工神经网络。通过这些方法，我们发现高度选择性的独立神经元并不比非选择性的神经元更加重要，而泛化能力更好的网络相对于简单地记忆的网络，对独立神经元的依赖性更小。这些结果暗示我们，独立神经元的重要性可能小得多。 通过解释所有神经元在任务中的角色，而不仅仅是那些更好解释的神经元，我们希望能更好地理解神经网络的内部工作原理，并通过这种理解构建更智能和更通用的系统。 论文：ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION 论文链接：https://arxiv.org/abs/1803.06959Despite 尽管有能力记忆大规模的数据集，深度神经网络通常也能获得良好的泛化性能。然而，关于神经网络学习到的解的泛化能力是由什么因素造成的，至今仍未得到清晰的解答。此外，人们曾强调过探索单个方向的微调属性（被定义为一个单元或多个单元的线性组合的激活值对一些输入的反应）的方法，但其重要性也未被评估过。在本文中，我们将这些探究方向连接起来，并证明网络对单个方向的依赖性可以很好地预测其泛化性能（通过让网络在不同比例的损坏标签的数据集上训练，让网络在未修改标签的数据集上训练并集成，进行不同的超参数试验以及多项训练试验）。dropout 仅能在一定程度上将这个量正则化，批量归一化却隐含地会减弱单一方向的依赖性，这部分是因为减少了独立单元的类别选择性。最后，我们发现类别选择性并不能很好地预测任务重要性。这不仅意味着网络可通过减少选择性来最小化对独立单元的依赖性，从而提高泛化能力；还表明独立地选择的单元对于强大的网络性能可能不是必须的。 原文链接： https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/ 
84,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739772&idx=4&sn=8bf5f04f3ee60688bd0d29167d4ca594&chksm=871ad042b06d59540c4db2389da1a8229da75ba9878501cf2b98e3ec67956e6fc0957bcae583&scene=27,入门 | 无需基础知识，使用JavaScript构建你的第一个神经网络,"ITNEXT 随着新技术和新工具的出现，构建神经网络已不再是一件需要大量机器学习相关知识的工作了。本文将会教你以 JavaScript 库 Brain.js 为基础，构建并训练自己的神经网络。 （如果你只想直接上手，请跳过这部分......） 首先，本文作者还不是神经网络或机器学习方面的专家。坦率的说，我仍然对人工智能的大部分内容感到困惑。但希望这能够鼓励到正在读这篇文章，并急切想尝试机器学习的初学者们。 机器学习是近年来在人们眼前时不时会出现的新概念，这让我不禁对自己说：「看起来这很酷，但是我不确定我是否想在接下来几个月的时间里学习线性代数和微积分……」 然而，和很多的开发人员一样，我对 JavaScript 很感兴趣，偶尔也会寻找在 JS 中实现机器学习的例子，结果却发现大量的文章和 StackOverflow 的帖子都在告诉我们对于机器学习来说 JS 是一种多么糟糕的语言。诚然，目前这种观点还是事实。然后我就有所动摇了，认为他们是对的，我应该回头去验证格式输入并等待 CSS 网格的启动。 但是后来我发现了 Brain.js，这让我大吃一惊。它在是如何被人们雪藏的？！这个库的文档写得非常好且易于遵循，在上手大约 30 分钟之内，我就建立并训练了一个神经网络。事实上，如果你想直接跳过整篇文章，仅仅在 Github 上阅读该库的教程，请便。它真的很棒：https://github.com/BrainJS/brain.js 也就是说，接下来的内容不是关于深入研究神经网络的隐藏输入层、激活函数或如何使用 TensorFlow 的教程。相反，这是一个简单的、入门级别的关于如何实现 Brain.js 的解释，这有点超出了文档的范围。 以下是我们将要做的事情的概述： 1. 创建你的起始文件 2. 决定你想要你的神经网络做什么 3. 设置 Brain.js 并搞清楚如何处理训练数据和用户输入 4. 收集一些训练数据 5. 运行神经网络 6. 利润？ 如果你希望直接下载此项目的可用版本，而不是按照文章进行操作，那么你可以在这里克隆 Github 存储库：https://github.com/lordpoint/neural-network-author-classifier 1 - 起始文件 创建一个新目录并在其中放置一个 index.html 样板文件。然后创建三个 JS 文件：brain.js、 training-data.js 和 scripts.js（或用于默认 JS 文件的任何通用项），随后将所有这些文件导入到 index.html 的底部文件中。 永远不要害怕观察文件结构 到目前为止都很容易。 现在，在这里获取 Brain.js 的源代码：https://raw.githubusercontent.com/harthur-org/brain.js/master/browser.js 将全部内容复制并粘贴到空的 Brain.js 文件中，点击 save 和 bam：完成 4 个文件中的 2 个。 2 -「我的目的是什么？」 接下来是有趣的部分：决定你的机器能学到什么。今天的机器学习模型可以解决无数的实际问题，例如，进行情感分析或图像分类等。我碰巧认为将文本作为输入的机器学习的应用程序非常有趣，因为你几乎可以在任何地方找到训练数据，而它们还有各种各样的潜在用途，所以我们将在这里使用的例子是一个处理文本分类的例子： 我们将训练一个模型，让它判定一条推特是由 Donald Trump（唐纳德·特朗普）还是 Kim Kardashian（金·卡戴珊）写的。 Ok，这可能不是最有用的应用程序。但是 Twitter 是机器学习素材的宝库，尽管它可能是无用的，但我们的推特作者识别器仍然会说明一个强大的观点。一旦它被训练完成，我们的神经网络将可以在看到它从未看过的推特后确定它是由 Donald Trump 还是 Kim Kardashian 所发出的，仅仅通过识别他们所写东西中的模式。为了做到这一点，我们将需要提供尽可能多的训练数据，以便将其复制/粘贴到我们的 training-data.js 文件中，然后我们可以看看我们是否能找到一些推特的作者。 3 - 建立和数据处理 现在剩下要做的就是在我们的 scripts.js 文件中建立 Brain.js，并提供一些在 training-data.js 文件中的训练数据。但是在此之前，让我们先从整个项目的高度上来看一下这些组件是如何工作的。 建立 Brain.js 非常简单，所以我们不会花费太多时间，但是有一些关于如何预测其输入数据被格式化的细节，我们应该先来看一下。我们先看一下文档汇中包含的建立示例（我在此略微做过修改），这些示例很好地说明了这一点： 首先，上面的例子实际上是一个可用的 AI（它看着给定的颜色，然后告诉你黑色文本还是白色文本在上面更清晰）。它可以说明 Brain.js 是多么容易使用。只是实例化、训练、然后运行而已。我的意思是，如果你将训练数据内联，将会是 3 行代码。非常简洁。 现在让我们来谈一下训练数据。在上面的例子中，除了训练数据整体 input: {}, output: {} 的格式，还有两个重要的事情需要注意。 首先，数据不需要长度相同。正如你在上面第 11 行看到的，只有一个 R 和 B 值通过，而另外两个输入通过了 R、G 和 B 的值。另外，值得一提的是，即使上面的例子将输入看作对象，你也可以使用数组。我提到这点很大程度上是因为我们将在项目中传递不同长度的数组。 第二，这些不是有效的 RGB 值。如果你想真的使用它们，每个都会变成黑色。这是因为输入值必须介于 0 和 1 之间才能使 Brain.js 和它们一起工作。因此，在上面的例子中，每种颜色都必须经过处理（可能只是经过一个将其除以 255 的函数进行处理——RGB 的最大值）以使其工作。之后我们也会做同样的事情。 3.1—encode() 所以如果我们想让神经网络接受推特（即字符串）为输入，我们需要通过一个类似的函数（下面称为 encode()）来处理它们，这将使字符串中的每一个字符变成一个介于 0 和 1 之间的值，并将其储存在一个数组中。幸运的是，Javascript 有一个将字符转化成 ASCII 的简单方法，叫做 charCodeAt()。所以我们将使用它并将输出除以 扩展的 ASCII 码的最大值：255（我们用扩展的 ASCII 码，以防遇到像é 或 ½ 这样的边缘情况），这将确保我们得到的值 <1。 3.2—processTrainingData() 另外，我们会将训练数据存储为纯文本，而不是我们最终将输入到人工智能的编码数据——稍后你将会看到这样做的好处。所以我们需要另一个函数（下面称为 processTrainingData()）将前面提到的编码函数应用到训练数据中，有选择地将文本转换为编码字符，并返回一组训练数据，这些数据将与 Brain.js 很好的工作。 下面是所有的代码（这会进入你的 『scripts.js』 文件中）： function encode arg x function processTrainingData data d function train data function execute 31 行，工作开始变多了。 在这里你会注意到在 train() 函数中的第 20 行有前面文档的示例中没有提到的（除了我们已经讨论过的两个辅助函数外），它将训练过的神经网络保存到一个叫 trainedNet 的全局变量中。这可以防止我们每次使用神经网络时不得不重新训练它们。一旦网络被训练并保存到变量中，我们可以将它称为函数，并传入编码输入（如 execute() 函数中的第 25 行）以使用机器学习模型。 好的，所以现在你的 index.html、brain.js、和 scripts.js 文件都完成了。现在我们需要的是将一些东西放入 training-data.js 中，准备好开始吧。 4 - 训练 下一个重要任务是进行训练。像我之前提到的，我们将所有推特存储为文本，并将它们编码为数值，这将使你在实际需要复制/粘贴训练数据时变得更加容易。没有必要的格式。只需要在文本中粘贴并添加一个新行。 input 把它添加到你的 『training-data.js』 文件中，我们就大功告成了！ 注意：虽然上面的例子只显示了每个人的 3 个样本，但我用了 10 个；我只是不希望这个样本占用太多空间。当然，你的神经网络的准确性会随着你提供的训练数据量的增加而成比例的增加，所以你可以随意使用比我更多或更少的数据，看看数据集体量是如何影响模型性能的。 5 - 执行 现在，运行新训练的神经网络只需要在 『script.js』 文件的底部额外添加一行调用 execute() 函数的指令，并传入一个 Trump 或 Kardashian 的推特。确保要有记录（console.log），因为我们还没有建立 UI。下面是一个来自 Kim Kardashian 的推特，它不在我的训练数据中（即神经网络之前没有见过这篇推特）： 然后在本地主机上拉起 index.html 页面，检查 console，然后...... 在这里！该网络正确识别了它之前从未见过的来自 Kim Kardashian 的推特，确定性为 86%。 现在让我们用 Trump 的推特再尝试一下： 结果是...... 再一次，一个从未见过的推特。再一次，正确识别！这次有 97% 的确定性。 6 - 受益 现在你有一个神经网络可以训练你想要的任何文本！你可以轻松地调整它来识别电子邮件或公司在线评论中的情绪，识别垃圾邮件，分类博客文章，确定信息是否紧急，或任何上千种不同的应用程序。尽管我们的推特识别器是无用的，但是它仍然说明了一个非常有趣的观点：像这样的神经网络可以执行类似根据写作方式识别作者这样的细致入微的任务。 所以，即使你并没有试图创建一个以机器学习为基础的应用或可用工具，这仍然是你开发工具中很好的经验。你永远不知道这些知识什么时候就能派上用场，甚至可能在未来开辟新的机会。 原文链接： https://itnext.io/you-can-build-a-neural-network-in-javascript-even-if-you-dont-really-understand-neural-networks-e63e12713a3 "
85,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739772&idx=1&sn=47d7c43c04f9074fb9d08ae630c8bf0a&chksm=871ad042b06d595427d63a2172fe58a2501f77b7951d11efa11f6ccc62529f6975a16347ba8b&scene=27,FAIR何恺明等人提出组归一化：替代批归一化，不受批量大小限制,"arXiv 自 Facebook 在 2017 年 6 月发布 以来，很多研究者都在关注如何使用并行训练来提高深度学习的训练速度，其研究所使用的批尺寸也呈指数级上升。近日，FAIR 研究工程师吴育昕、研究科学家何恺明提出了组归一化（Group Normalization）方法，试图以小批尺寸实现快速神经网络训练，这种方法对于硬件的需求大大降低，并在实验中超过了传统的批归一化方法。 批归一化（Batch Norm/BN）是深度学习中非常有效的一个技术，极大地推进了计算机视觉以及之外领域的前沿。BN 通过计算一个（迷你）批量中的均值与方差来进行特征归一化。众多实践证明，它利于优化且使得深度网络易于收敛。批统计的随机不确定性也作为一个有利于泛化的正则化项。BN 已经成为了许多顶级计算机视觉算法的基础。 尽管取得了很大的成果，BN 也会因为归一不同批尺寸的独特行为而有缺点。特别是，BN 需要用到足够大的批大小（例如，每个工作站采用 32 的批量大小）。一个小批量会导致估算批统计不准确，减小 BN 的批大小会极大地增加模型错误率（图 1）。结果导致，如今许多模型都使用较大的批训练，它们非常耗费内存。反过来，训练模型时对 BN 效力的极度依赖性阻碍了人们用有限内存探索更高容量的模型。 图 1：ImageNet 分类误差 vs. 批大小。这是在 ImageNet 训练集上用 8 个工作站（GPU）训练、在验证集上进行评估的 ResNet-50 模型。 计算机视觉任务（包括检测、分割、视频识别和其他基于此的高级系统）对批大小的限制更加严格。例如，Fast/er 和 Mask R-CNN 框架 [12, 46, 18] 使用的批大小为 1 或 2 张图像，为了更高的分辨率，其中 BN 通过变换为线性层而被「固定」[20]；在 3D 卷积视频分类中 [59, 6]，时空特征的出现导致时间长度和批大小之间的权衡。BN 的使用通常要求这些系统在模型设计和批大小之间作出妥协。 本文提出了组归一化（Group Normalization，GN）作为批归一化（BN）的替代。作者发现很多经典的特征例如 SIFT[38] 和 HOG[9] 是分组的特征并涉及分组的归一化。例如，一个 HOG 向量是多个空间单元的输出，其中每个单元由一个归一化的方向直方图表征。类似地作者提出了 GN 作为层将通道分组并在每个组中将特征归一化（见图 2）。GN 并没有利用批量的维度，它的计算是独立于批量大小的。 GN 在大范围的批量大小下都能表现得很稳定（见图 1）。当批量大小为 2 个样本时，在 ImageNet 训练的 ResNet-50 上，相比于 BN 的对应变体，GN 获得的误差率要小 10%。在常规的批量大小设置下，GN 获得的性能和 BN 相当（相差约 0.5%），并超越了其它的归一化变体 [3,60,50]。此外，虽然批量大小可能被改变，而 GN 的设置则可以从预训练阶段迁移到微调阶段。在 COCO 目标检测和分割任务的 Mask R-CNN 上，以及在 Kinetics 视频分类任务的 3D 卷积网络上，相比于 BN 的对应变体，GN 都能获得提升或者超越的结果。GN 在 ImageNet、COCO 和 Kinetics 上的有效性表明 GN 是 BN 的有力竞争者，而 BN 在过去一直在这些任务上作为主导的方法。 目前已有的优化方法包括层归一化（LN）[3] 和实例归一化（IN）[60]（如图 2 所示），它们也避免了在批量维度上的归一化。这些方法对于训练序列模型（RNN/LSTM）或生成模型（GAN）很有效。本文的实验研究表明，LN 和 IN 在视觉识别上的成功率都是很有限的，而 GN 则能获得更好的结果。相反地，GN 也许还能替代 LN 和 IN，在序列或生成模型上得到应用。这已经超越了本文的内容，但这些方向都是值得探索的。 图 2：归一化方法。每个子图展示了一个特征图张量，N 是批坐标轴，C 是通道轴，（H,W）是空间轴。通过计算蓝色像素值的和，这些像素被同样的平均值与方差归一化的。 视觉表征的各个通道其实并不完全独立。SIFT [38]、HOG [9] 和 GIST [40] 的经典特征都设计为按分组来表征，其中每一组通道由一些直方图（histogram）构成。这些特征通常通过在每个直方图或每个方向上执行分组归一化而得到处理。VLAD [29] 和 Fisher Vectors (FV) [43] 等高级特征同样也是分组的特征，其中每一组特征可以认为是关于集群（cluster）计算的子向量。 类似的，我们没必要将深度神经网络视为非结构化的向量。例如，对于网络的第一个卷积层 conv1，期望卷积核和其水平翻转在自然图像上呈现相似的卷积核反馈分布是合理的。如果 conv1 正好近似学习到这一对卷积核，或将水平翻转与其它转换设计到架构中 [11, 8]，那么我们可以将这些卷积核的对应通道一同归一化。 神经网络的较高层级会有更加抽象的特征，它们的行为也变得不那么直观。然而，除了方向（SIFT [38]、HOG [9] 或 [11, 8]）外，还有很多可以导致分组的因素，例如频率、形状、照明和纹理等。此外，它们的系数可以相互独立。实际上，神经科学中广泛接受的计算模型是在所有细胞反馈中执行归一化 [21, 51, 54, 5]，「具有各种感受野中心（覆盖视觉范围）和各种时空频率调谐」（p183, [21]）；这不仅发生在初级视觉皮层，同样可以发生在「整个视觉系统」[5]。受到这些研究工作地启发，我们为深度神经网络提出了一般分组归一化方法。 GN 可以通过 PyTorch [41] 和 TensorFlow [1] 中的几行代码轻松实现，二者均支持自动微分。图 3 是基于 TensorFlow 的代码。实际上，我们仅需要指定均值和方差的计算方式，恰当的坐标轴由归一化方法定义。 图 3：基于 TensorFlow 的组归一化 Python 代码。 图 4：批量大小为 32 张图像/GPU 时的误差曲线对比。图中展示了 ImageNet 训练误差（左）和验证误差（右）vs. 训练 epoch 数量。模型是 ResNet-50。 图 5：对批量大小的敏感度：BN（左）和 GN（右）在 ResNet-50 上的验证误差率，训练是以 32、16、8、4 和 2 张图像/GPU 的吞吐量进行的。 图 6：VGG-16 上 conv5 3 输出（在归一化和 ReLU 之前）的演进特征分布，以 {1, 20, 80, 99} 百分比位置的返回值进行结果展示。右侧表格显示了 ImageNet 验证误差（%）。模型是以 32 张图片/GPU 的吞吐量进行训练的。 表 4：在 COCO 数据集上的目标检测和分割结果，使用 Mask R-CNN（ResNet-50 C4）。BN*表示 BN 被冻结。 表 5：在 COCO 数据集上的目标检测和分割结果，使用 Mask R-CNN（ResNet-50 FPN 以及 4conv1fc 边框）。BN*表示 BN 被冻结。 图 7：Kinetics 中，输入长度为 32 帧的误差曲线。FAIR 的研究人员展示了 ResNet-50 I3D 分别应用 BN（左侧）和 GN（右侧）的验证误差率，批量大小为 8 和 4 clips/GPU。实验监控的验证误差率是通过和训练集相同的数据增强得到的 1-clip 误差率。 论文：Group Normalization 论文链接：https://arxiv.org/abs/1803.08494 批归一化（BN）是深度学习发展史中的一项里程碑技术，使得大量神经网络得以训练。但是，批量维度上的归一化也衍生出一些问题——当批量统计估算不准确导致批量越来越小时，BN 的误差快速增大，从而限制了 BN 用于更大模型的训练，也妨碍了将特征迁移至检测、分割、视频等计算机视觉任务之中，因为它们受限于内存消耗，只能使用小批量。在本论文中，我们提出了作为批归一化（BN）简单替代的组归一化（GN）。GN 把通道分为组，并计算每一组之内的均值和方差，以进行归一化。GN 的计算与批量大小无关，其精度也在各种批量大小下保持稳定。在 ImageNet 上训练的 ResNet-50 上，当批量大小为 2 时，GN 的误差比 BN 低 10.6%。当使用经典的批量大小时，GN 与 BN 相当，但优于其他归一化变体。此外，GN 可以自然地从预训练阶段迁移到微调阶段。在 COCO 的目标检测和分割任务以及 Kinetics 的视频分类任务中，GN 的性能优于或与 BN 变体相当，这表明 GN 可以在一系列不同任务中有效替代强大的 BN；在现代的深度学习库中，GN 通过若干行代码即可轻松实现。 "
86,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739707&idx=4&sn=f19a780da0e59387761b0968c124865a&chksm=871ad785b06d5e9377d06a0da1e802ba25c9001dcfcf12e47575f6acca6110f75ee375b34d02&scene=27,入门 | 从PCC到MIC，一文教你如何计算变量之间的相关性,"FreeCoderCamp 本文介绍了几个重要的变量相关性的度量，包括皮尔逊相关系数、距离相关性和最大信息系数等，并用简单的代码和示例数据展示了这些度量的适用性对比。 从信号的角度来看，这个世界是一个嘈杂的地方。为了弄清楚所有的事情，我们必须有选择地把注意力集中到有用的信息上。 通过数百万年的自然选择过程，我们人类已经变得非常擅长过滤背景信号。我们学会将特定的信号与特定的事件联系起来。 例如，假设你正在繁忙的办公室中打乒乓球。为了回击对手的击球，你需要进行大量复杂的计算和判断，将多个相互竞争的感官信号考虑进去。为了预测球的运动，你的大脑必须重复采样球的位置并估计它未来的轨迹。更厉害的球员还会将对手击球时施加的旋转考虑进去。最后，为了击球，你需要考虑对手的位置、自己的位置、球的速度，以及你打算施加的旋转。 所有这些都涉及到了大量的潜意识微分学。一般来说，我们理所当然的认为，我们的神经系统可以自动做到这些（至少经过一些练习之后）。 同样令人印象深刻的是，人类大脑是如何区别对待它所接收到的无数竞争信号的重要性的。例如，球的位置被认为比你身后发生的对话或你面前打开的门更重要。 这听起来似乎不值得一提，但实际上这证明了可以多大程度上学习从噪声数据中做出准确预测。 当然，一个被给予连续的视听数据流的空白状态机将会面临一个困难的任务，即确定哪些信号能够最好地预测最佳行动方案。 幸运的是，有统计和计算方法可以用来识别带噪声和复杂的数据中的模式。 一般来说，当我们谈到两个变量之间的「相关性（correlation）」时，在某种意义上，我们是指它们的「关系（relatedness）」。 相关变量是包含彼此信息的变量。两个变量的相关性越强，其中一个变量告诉我们的关于另一个变量的信息就越多。 你可能已经对相关性、它的作用和它的局限性有了一定了解。事实上，这是一个数据科学的老生常谈： 「相关性不意味着因果关系」 这当然是正确的——有充分的理由说明，即使是两个变量之间有强相关性也不保证存在因果关系。观察到的相关性可能是由于隐藏的第三个变量的影响，或者完全是偶然的。 也就是说，相关性确实允许基于另一个变量来预测一个变量。有几种方法可以用来估计线性和非线性数据的相关性。我们来看看它们是如何工作的。 我们将用 Python 和 R 来进行数学和代码实现。本文示例的代码可以在这里找到： GitHub 地址：https://gist.github.com/anonymous/fabecccf33f9c3feb568384f626a2c07 皮尔逊相关系数（PCC, 或者 Pearson's r）是一种广泛使用的线性相关性的度量，它通常是很多初级统计课程的第一课。从数学角度讲，它被定义为「两个向量之间的协方差，通过它们标准差的乘积来归一化」。 两个成对的向量之间的协方差是它们在均值上下波动趋势的一种度量。也就是说，衡量一对向量是否倾向于在各自平均值的同侧或相反。 让我们看看在 Python 中的实现： 协方差的计算方法是从每一对变量中减去各自的均值。然后，将这两个值相乘。 如果都高于（或都低于）均值，那么结果将是一个正数，因为正数 × 正数 = 正数；同样的，负数 × 负数 = 负数。 如果在均值的不同侧，那么结果将是一个负数（因为正数 × 负数 = 负数）。 一旦我们为每一对变量都计算出这些值，将它们加在一起，并除以 n-1，其中 n 是样本大小。这就是样本协方差。 如果这些变量都倾向于分布在各自均值的同一侧，协方差将是一个正数；反之，协方差将是一个负数。这种倾向越强，协方差的绝对值就越大。 如果不存在整体模式，那么协方差将会接近于零。这是因为正值和负值会相互抵消。 最初，协方差似乎是两个变量之间「关系」的充分度量。但是，请看下面的图： 看起来变量之间有很强的关系，对吧？那为什么协方差这么小呢（大约是 0.00003）？ 这里的关键是要认识到协方差是依赖于比例的。看一下 x 和 y 坐标轴——几乎所有的数据点都落在了 0.015 和 0.04 之间。协方差也将接近于零，因为它是通过从每个个体观察值中减去平均值来计算的。 为了获得更有意义的数字，归一化协方差是非常重要的。方法是将其除以两个向量标准差的乘积。 在 Python 中： 这样做的原因是因为向量的标准差是是其方差的平方根。这意味着如果两个向量是相同的，那么将它们的标准差相乘就等于它们的方差。 有趣的是，两个相同向量的协方差也等于它们的方差。 因此，两个向量之间协方差的最大值等于它们标准差的乘积（当向量完全相关时会出现这种情况）。这将相关系数限制在 -1 到 +1 之间。 箭头指向哪个方向？ 顺便说一下，一个定义两个向量的 PCC 的更酷的方法来自线性代数。 首先，我们通过从向量各自的值中减去其均值的方法来「集中」向量。 现在，我们可以利用向量可以看做指向特定方向的「箭头」的事实。 例如，在 2-D 空间中，向量 [1,3] 可以代表一个沿 x 轴 1 个单位，沿 y 轴 3 个单位的箭头。同样，向量 [2,1] 可以代表一个沿 x 轴 2 个单位，沿 y 轴 1 个单位的箭头。 类似地，我们可以将数据向量表示为 n 维空间中的箭头（尽管当 n > 3 时不能尝试可视化）。 这些箭头之间的角度 ϴ 可以使用两个向量的点积来计算。定义为： 或者，在 Python 中： 点积也可以被定义为： 其中 || x || 是向量 x 的大小（或「长度」）（参考勾股定理），ϴ 是箭头向量之间的角度。 正如一个 Python 函数： 我们通过将点积除以两个向量大小的乘积的方法得到 cos(ϴ)。 现在，如果你对三角学有一定了解，你可能会记得，余弦函数产生一个在 +1 和 -1 之间震荡的图形。 cos(ϴ) 的值将根据两个箭头向量之间的角度而发生变化。 当角度为零时（即两个向量指向完全相同的方向），cos(ϴ) 等于 1。 当角度为 -180°时（两个向量指向完全相反的方向），cos(ϴ) 等于 -1。 当角度为 90°时（两个向量指向完全不相关的方向），cos(ϴ) 等于 0。 这可能看起来很熟悉——一个介于 +1 和 -1 之间的衡量标准似乎描述了两个向量之间的关系？那不是 Pearson’s r 吗？ 那么——这正是它的解释！通过将数据视为高维空间中的箭头向量，我们可以用它们之间的角度 ϴ 作为相似度的衡量。 该角度 ϴ 的余弦在数学上与皮尔逊相关系数相等。当被视为高维箭头时，正相关向量将指向一个相似的方向。负相关向量将指向相反的方向。而不相关向量将指向直角。 就我个人而言，我认为这是一个理解相关性的非常直观的方法。 统计显著性？ 正如频率统计一样，重要的是询问从给定样本计算的检验统计量实际上有多重要。Pearson's r 也不例外。 不幸的是，PCC 估计的置信区间不是完全直接的。 这是因为 Pearson's r 被限制在 -1 和 +1 之间，因此不是正态分布的。而估计 PCC，例如 +0.95 之上只有很少的容错空间，但在其之下有大量的容错空间。 幸运的是，有一个解决方案——用一个被称为 Fisher 的 Z 变换的技巧： 像平常一样计算 Pearson's r 的估计值。 用 Fisher 的 Z 变换将 r→z，用公式 z = arctanh(r) 完成。 现在计算 z 的标准差。幸运的是，这很容易计算，由 SDz = 1/sqrt(n-3) 给出，其中 n 是样本大小。 选择显著性阈值，alpha，并检查与此对应的平均值有多少标准差。如果取 alpha = 0.95，用 1.96。 通过计算 z +(1.96 × SDz) 找到上限，通过计算 z - (1.96 × SDz) 找到下限。 用 r = tanh(z) 将这些转换回 r。 如果上限和下限都在零的同一侧，则有统计显著性！ 这里是在 Python 中的实现： 当然，当给定一个包含许多潜在相关变量的大数据集时，检查每对的相关性可能很吸引人。这通常被称为「数据疏浚」——在数据集中查找变量之间的任何明显关系。 如果确实采用这种多重比较方法，则应该用适当的更严格的显著性阈值来降低发现错误相关性的风险（即找到纯粹偶然相关的无关变量）。 一种方法是使用 Bonferroni correction。 小结 到现在为止还好。我们已经看到 Pearson's r 如何用来计算两个变量之间的相关系数，以及如何评估结果的统计显著性。给定一组未知的数据，用于开始挖掘变量之间的重要关系是很有可能的。 但是，有一个重要的陷阱——Pearson's r 只适用于线性数据。 看下面的图。它们清楚地展示了一种看似非随机的关系，但是 Pearson's r 非常接近于零。 原因是因为这些图中的变量具有非线性关系。 我们通常可以将两个变量之间的关系描绘成一个点云，分散在一条线的两侧。点云的分散度越大，数据越「嘈杂」，关系越弱。 然而，由于它将每个单独的数据点与整体平均值进行比较，所以 Pearson's r 只考虑直线。这意味着检测非线性关系并不是很好。 在上面的图中，Pearson's r 并没有显示研究对象的相关性。 然而，这些变量之间的关系很显然是非随机的。幸运的是，我们有不同的相关性方法。 让我们来看看其中几个。 距离相关性与 Pearson's r 有一些相似之处，但是实际上是用一个相当不同的协方差概念来计算的。该方法通过用「距离」类似物替代常用的协方差和标准差（如上所定义）的概念。 类似 Pearson's r，「距离相关性」被定义为「距离协方差」，由「距离标准差」来归一化。 距离相关性不是根据它们与各自平均值的距离来估计两个变量如何共同变化，而是根据与其他点的距离来估计它们是如何共同变化的，从而能更好捕捉变量之间非线性依赖关系。 深入细节 出生于 1773 年的 Robert Brown 是一名苏格兰植物学家。当布朗在显微镜下研究植物花粉时，注意到液面上有随机运动的有机颗粒。 他没有想到，这一观察竟使他名垂千古——他成为了布朗运动的（重新）发现者。 他更不会知道，近一个世纪的时间后爱因斯坦才对这种现象做出了解释，从而证实了原子的存在。同年，爱因斯坦发表了关于狭义相对论的论文（E=MC²），并打开了量子理论的大门。 布朗运动是这样一个物理过程：由于与周围粒子的碰撞，微小粒子随机运动。 布朗运动背后的数学原理可以被推广为维纳过程（Weiner process），维纳过程在数学金融中最著名的模型 Black-Scholes 中也扮演着重要的角色。 有趣的是，Gabor Szekely 在 20 世纪中期的研究表明，布朗运动和维纳过程和一个非线性关联度量相关。 让我们来看看如何由长度为 N 的向量 x 和 y 计算这个量。 1. 首先，我们对每个向量构建 N×N 的距离矩阵。距离矩阵和地图中的道路距离表非常类似——每行、每列的交点显示了相应城市间的距离。在距离矩阵中，行 i 和列 j 的交点给出了向量的第 i 个元素和第 j 个元素之间的距离。 2. 第二，矩阵是「双中心」的。也就是说，对于每个元素，我们减去了它的行平均值和列平均值。然后，我们再加上整个矩阵的总平均值。 上述公式中，加「^」表示「双中心」，加「-」表示「平均值」。 3. 在两个双中心矩阵的基础上，将 X 中每个元素的均值乘以 Y 中相应元素的均值，则可计算出距离协方差的平方。 4. 现在，我们可以用类似的办法找到「距离方差」。请记住，若两个向量相同，其协方差与其方差相等。因此，距离方差可表示如下： 5. 最后，我们利用上述公式计算距离相关性。请记住，（距离）标准差与（距离）方差的平方根相等。 如果你更喜欢代码实现而非数学符号，那么请看下面的 R 语言实现： 任意两变量的距离相关性都在 0 和 1 之间。其中，0 代表两变量相互独立，而接近于 1 则表明变量间存在依赖关系。 如果你不想从头开始编写距离相关方法，你可以安装 R 语言的 energy 包（https://cran.r-project.org/web/packages/energy/index.html），设计此方案的研究者提供了本代码。在该程序包中，各类可用方案调用的是 C 语言编写的函数，因此有着很大的速度优势。 物理解释 关于距离相关性的表述，有着一个更令人惊讶的结果——它与布朗关联（Brownian correlation）有着确切的等价关系。 布朗关联指的是两个布朗过程之间的独立性（或依赖性）。相互依赖的布朗过程将会表现出彼此「跟随」的趋势。 让我们用一个简单的比喻来把握距离相关性的概念——请看下图中漂浮在湖面上的小纸船。 如果没有盛行风向，那么每艘船都将进行随机漂流——这与布朗运动类似。 如果存在盛行风向，那么小船漂流的方向将依赖于风的强度。风力越强，依赖性越显著。 与之类似，无关变量可以被看作无盛行风向时随机漂流的小船；相关变量可以被看作在盛行风向影响下漂流的小船。在这个比喻中，风的强弱就代表着两个变量之间相关性的强弱。 如果我们允许盛行风向在湖面的不同位置有所不同，那么我们就可以引入非线性的概念。距离相关性利用「小船」之间的距离推断盛行风的强度。 置信区间？ 我们可以采取「重采样（resampling）」方法为距离相关性估计建立置信区间。一个简单的例子是 bootstrap 重采样。 这是一个巧妙的统计技巧，需要我们从原始数据集中随机抽样（替换）以「重建」数据。这个过程将重复多次（例如 1000 次），每次都计算感兴趣的统计量。 这将为我们感兴趣的统计量产生一系列不同的估计值。我们可以通过它们估计在给定置信水平下的上限和下限。 请看下面的 R 语言代码，它实现了简单的 bootstrap 函数： 如果你想建立统计显著性，还有另一个重采样技巧，名为「排列检验（permutation test）」。 排列检验与上述 bootstrap 方法略有不同。在排列检验中，我们保持一个向量不变，并通过重采样对另一个变量进行「洗牌」。这接近于零假设（null hypothesis）——即，在变量之间不存在依赖关系。 这个经「洗牌」打乱的变量将被用于计算它和常变量间的距离相关性。这个过程将被执行多次，然后，结果的分布将与实际距离相关性（从未被「洗牌」的数据中获得）相比较。 然后，大于或等于「实际」结果的经「洗牌」的结果的比例将被定为 P 值，并与给定的显著性阈值（如 0.05）进行比较。 以下是上述过程的代码实现： 最大信息系数（MIC）于 2011 年提出，它是用于检测变量之间非线性相关性的最新方法。用于进行 MIC 计算的算法将信息论和概率的概念应用于连续型数据。 深入细节 由克劳德·香农于 20 世纪中叶开创的信息论是数学中一个引人注目的领域。 信息论中的一个关键概念是熵——这是一个衡量给定概率分布的不确定性的度量。概率分布描述了与特定事件相关的一系列给定结果的概率。 为了理解其工作原理，让我们比较下面两个概率分布： 左侧是一个常规六面骰子结果的概率分布；而右边的六面骰子不那么均匀。 从直觉上来说，你认为哪个的熵更高呢？哪个骰子结果的不确定性更大？让我们来计算它们的熵，看看答案是什么。 不出所料，常规骰子的熵更高。这是因为每种结果的可能性都一样，所以我们不会提前知道结果偏向哪个。但是，非常规的骰子有所不同——某些结果的发生概率远大于其它结果——所以它的结果的不确定性也低一些。 这么一来，我们就能明白，当每种结果的发生概率相同时，它的熵最高。而这种概率分布也就是传说中的「均匀」分布。 交叉熵是熵的一个拓展概念，它引入了第二个变量的概率分布。 两个相同概率分布之间的交叉熵等于其各自单独的熵。但是对于两个不同的概率分布，它们的交叉熵可能跟各自单独的熵有所不同。 这种差异，或者叫「散度」可以通过 KL 散度（Kullback-Leibler divergence）量化得出。 两概率分布 X 与 Y 的 KL 散度如下： KL 散度的最小值为 0，仅当两个分布相同。 为了发现变量具有相关性，KL 散度的用途之一是计算两个变量的互信息（MI）。 互信息可以定义为「两个随机变量的联合分布和边缘分布之间的 KL 散度」。如果二者相同，MI 值取 0。如若不同，MI 值就为一个正数。二者之间的差异越大，MI 值就越大。 为了加深理解，我们首先简单回顾一些概率论的知识。 变量 X 和 Y 的联合概率就是二者同时发生的概率。例如，如果你抛掷两枚硬币 X 和 Y，它们的联合分布将反映抛掷结果的概率。假设你抛掷硬币 100 次，得到「正面、正面」的结果 40 次。联合分布将反映如下： P(X=H, Y=H) = 40/100 = 0.4 边缘分布是指不考虑其它变量而只关注某一特定变量的概率分布。假设两变量独立，二者边缘概率的乘积即为二者同时发生的概率。仍以抛硬币为例，假如抛掷结果是 50 次正面和 50 次反面，它们的边缘分布如下： P(X=H) = 50/100 = 0.5 ; P(Y=H) = 50/100 = 0.5 P(X=H) × P(Y=H) = 0.5 × 0.5 = 0.25 现在让我们回到抛硬币的例子。如果两枚硬币相互独立，边缘分布的乘积表示每个结果可能发生的概率，而联合分布则为实际得到的结果的概率。 如果两硬币完全独立，它们的联合概率在数值上（约）等于边缘分布的乘积。若只是部分独立，此处就存在散度。 这个例子中，P(X=H,Y=H) > P(X=H) × P(Y=H)。这表明两硬币全为正面的概率要大于它们的边缘分布之积。 联合分布和边缘分布乘积之间的散度越大，两个变量之间相关的可能性就越大。两个变量的互信息定义了散度的度量方式。 此处的一个重要假设就是概率分布是离散的。那么我们如何把这些概念应用到连续的概率分布呢？ 分箱算法 其中一种方法是量化数据（使变量离散化）。这是通过分箱算法（bining）实现的，它能将连续的数据点分配对应的离散类别。 此方法的关键问题是到底要使用多少「箱子（bin）」。幸运的是，首次提出 MIC 的论文给出了建议：穷举！ 也就是说，去尝试不同的「箱子」个数并观测哪个会在变量间取到最大的互信息值。不过，这提出了两个挑战： 要试多少个箱子呢？理论上你可以将变量量化到任意间距值，可以使箱子尺寸越来越小。 互信息对所用的箱子数很敏感。你如何公平比较不同箱子数目之间的 MI 值？ 第一个挑战从理论上讲是不能做到的。但是，论文作者提供了一个启发式解法（也就是说，解法不完美，但是十分接近完美解法）。他们也给出了可试箱子个数的上限。 至于如何公平比较取不同箱子数对 MI 值的影响，有一个简单的做法……就是归一化！这可以通过将每个 MI 值除以在特定箱子数组合上取得的理论最大值来完成。我们要采用的是产生最大归一化 MI 总值的箱子数组合。 最大的归一化互信息就是 X 和 Y 的最大信息系数（MIC）。我们来看看一些估算两个连续变量的 MIC 的代码。 以上代码是对原论文中方法的简化。更接近原作的算法实现可以参考 R package minerva（https://cran.r-project.org/web/packages/minerva/index.html）。 在 Python 中的实现请参考 minepy module（https://minepy.readthedocs.io/en/latest/）。 MIC 能够表示各种线性和非线性的关系，并已得到广泛应用。它的值域在 0 和 1 之间，值越高表示相关性越强。 置信区间？ 为了建立 MIC 估计值的置信区间，你可以简单地使用一个像我们之前介绍过的 bootstrap 函数。我们可以利用 R 语言的函数式编程，通过传递我们想要用作参数的函数来泛化 bootstrap 函数。 为了总结相关性这一主题，我们来测试下各算法在人工生成数据上的处理能力。 完整代码：https://gist.github.com/anonymous/fabecccf33f9c3feb568384f626a2c07 噪声函数 Pearson's r = - 0.05 距离相关性 = 0.157 MIC = 0.097 简单线性函数 Pearson's r =+0.95 距离相关性 = 0.95 MIC = 0.89 简单二次函数 Pearson's r =+0.003 距离相关性 = 0.474 MIC = 0.594 三次函数 Pearson's r =- 0.035 距离相关性 = 0.382 MIC = 0.484 圆函数 Pearson's r < 0.001 距离相关性 = 0.234 MIC = 0.218 "
87,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739707&idx=2&sn=feadded92a57db2aa890e6ad2f187cc8&chksm=871ad785b06d5e93a08200024b32c8d6c1b0939be4cd95940ae463a21943a7ced1e1f5b1e7d7&scene=27,教程 | 先理解Mask R-CNN的工作原理，然后构建颜色填充器应用,上年 11 月，matterport 开源了 Mask R-CNN 实现，它在 GitHub 已 fork1400 次，被用于很多项目，同时也获得了完善。作者将在本文中解释 Mask R-CNN 的工作原理，并介绍了颜色填充器的应用案例和实现过程。 代码（包括作者构建的数据集和已训练的模型）：https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon 什么是实例分割？ 实例分割是一种在像素层面识别目标轮廓的任务，相比其他相关任务，实例分割是较难解决的计算机视觉任务之一： 分类：这张图像中有一个气球。 语义分割：这些全是气球像素。 目标检测：这张图像中的这些位置上有 7 个气球。 实例分割：这些位置上有 7 个气球，并且这些像素分别属于每个气球。 Mask R-CNN Mask R-CNN 是一个两阶段的框架，第一个阶段扫描图像并生成提议（proposals，即有可能包含一个目标的区域），第二阶段分类提议并生成边界框和掩码。Mask R-CNN 扩展自 Faster R-CNN，由同一作者在去年提出。Faster R-CNN 是一个流行的目标检测框架，Mask R-CNN 将其扩展为实例分割框架。 Mask R-CNN 的主要构建模块： 1. 主干架构 主干网络的简化图示 这是一个标准的卷积神经网络（通常来说是 ResNet50 和 ResNet101），作为特征提取器。底层检测的是低级特征（边缘和角等），较高层检测的是更高级的特征（汽车、人、天空等）。 经过主干网络的前向传播，图像从 1024x1024x3（RGB）的张量被转换成形状为 32x32x2048 的特征图。该特征图将作为下一个阶段的输入。 代码提示：主干网络在 resnet_graph() 函数中。代码支持 ResNet50 和 ResNet101。 特征金字塔网络（FPN） 来源：Feature Pyramid Networks for Object Detection 上述的主干网络还可以进一步提升。由 Mask R-CNN 的同一作者引入的特征金字塔网络（FPN）是对该主干网络的扩展，可以在多个尺度上更好地表征目标。 FPN 通过添加第二个金字塔提升了标准特征提取金字塔的性能，第二个金字塔可以从第一个金字塔选择高级特征并传递到底层上。通过这个过程，它允许每一级的特征都可以和高级、低级特征互相结合。 在我们的 Mask R-CNN 实现中使用的是 ResNet101+FPN 主干网络。 代码提示：FPN 在 MaskRCNN.build() 中创建，位于构建 ResNet 的部分之后。FPN 引入了额外的复杂度：在 FPN 中第二个金字塔拥有一个包含每一级特征的特征图，而不是标准主干中的单个主干特征图（即第一个金字塔中的最高层）。选用哪一级的特征是由目标的尺寸动态地确定的。 2. 区域建议网络（RPN） 展示 49 个 anchor box 的简化图示 RPN 是一个轻量的神经网络，它用滑动窗口来扫描图像，并寻找存在目标的区域。 RPN 扫描的区域被称为 anchor，这是在图像区域上分布的矩形，如上图所示。这只是一个简化图。实际上，在不同的尺寸和长宽比下，图像上会有将近 20 万个 anchor，并且它们互相重叠以尽可能地覆盖图像。 RPN 扫描这些 anchor 的速度有多快呢？非常快。滑动窗口是由 RPN 的卷积过程实现的，可以使用 GPU 并行地扫描所有区域。此外，RPN 并不会直接扫描图像，而是扫描主干特征图。这使得 RPN 可以有效地复用提取的特征，并避免重复计算。通过这些优化手段，RPN 可以在 10ms 内完成扫描（根据引入 RPN 的 Faster R-CNN 论文中所述）。在 Mask R-CNN 中，我们通常使用的是更高分辨率的图像以及更多的 anchor，因此扫描过程可能会更久。 代码提示：RPN 在 rpn_graph() 中创建。anchor 的尺度和长宽比由 config.py 中的 RPN_ANCHOR_SCALES 和 RPN_ANCHOR_RATIOS 控制。 RPN 为每个 anchor 生成两个输出： anchor 类别：前景或背景（FG/BG）。前景类别意味着可能存在一个目标在 anchor box 中。 边框精调：前景 anchor（或称正 anchor）可能并没有完美地位于目标的中心。因此，RPN 评估了 delta 输出（x、y、宽、高的变化百分数）以精调 anchor box 来更好地拟合目标。 使用 RPN 的预测，我们可以选出最好地包含了目标的 anchor，并对其位置和尺寸进行精调。如果有多个 anchor 互相重叠，我们将保留拥有最高前景分数的 anchor，并舍弃余下的（非极大值抑制）。然后我们就得到了最终的区域建议，并将其传递到下一个阶段。 代码提示：ProposalLayer 是一个自定义的 Keras 层，可以读取 RPN 的输出，选取最好的 anchor，并应用边框精调。 3. ROI 分类器和边界框回归器 这个阶段是在由 RPN 提出的 ROI 上运行的。正如 RPN 一样，它为每个 ROI 生成了两个输出： 阶段 2 的图示。来源：Fast R-CNN 类别：ROI 中的目标的类别。和 RPN 不同（两个类别，前景或背景），这个网络更深并且可以将区域分类为具体的类别（人、车、椅子等）。它还可以生成一个背景类别，然后就可以弃用 ROI 了。 边框精调：和 RPN 的原理类似，它的目标是进一步精调边框的位置和尺寸以将目标封装。 代码提示：分类器和边框回归器已在 fpn_classifier_graph() 中创建。 ROI 池化 在我们继续之前，需要先解决一些问题。分类器并不能很好地处理多种输入尺寸。它们通常只能处理固定的输入尺寸。但是，由于 RPN 中的边框精调步骤，ROI 框可以有不同的尺寸。因此，我们需要用 ROI 池化来解决这个问题。 图中展示的特征图来自较底层。 ROI 池化是指裁剪出特征图的一部分，然后将其重新调整为固定的尺寸。这个过程实际上和裁剪图片并将其缩放是相似的（在实现细节上有所不同）。 Mask R-CNN 的作者提出了一种方法 ROIAlign，在特征图的不同点采样，并应用双线性插值。在我们的实现中，为简单起见，我们使用 TensorFlow 的 crop_and_resize 函数来实现这个过程。 代码提示：ROI 池化在类 PyramidROIAlign 中实现。 4. 分割掩码 到第 3 节为止，我们得到的正是一个用于目标检测的 Faster R-CNN。而分割掩码网络正是 Mask R-CNN 的论文引入的附加网络。 掩码分支是一个卷积网络，取 ROI 分类器选择的正区域为输入，并生成它们的掩码。其生成的掩码是低分辨率的：28x28 像素。但它们是由浮点数表示的软掩码，相对于二进制掩码有更多的细节。掩码的小尺寸属性有助于保持掩码分支网络的轻量性。在训练过程中，我们将真实的掩码缩小为 28x28 来计算损失函数，在推断过程中，我们将预测的掩码放大为 ROI 边框的尺寸以给出最终的掩码结果，每个目标有一个掩码。 代码提示：掩码分支网络在 build_fpn_mask_graph() 中。 和大多数图像编辑 app 中包含的过滤器不同，我们的过滤器更加智能一些：它能自动找到目标。当你希望把它应用到视频上而不是图像上时，这种技术更加有用。 训练数据集 通常我会从寻找包含所需目标的公开数据集开始。但在这个案例中，我想向你展示这个项目的构建循环过程，因此我将介绍如何从零开始构建一个数据集。 我在 flickr 上搜索气球图片，并选取了 75 张图片，将它们分成了训练集和验证集。找到图片很容易，但标注阶段才是困难的部分。 等等，我们不是需要数百万张图片来训练深度学习模型吗？实际上，有时候需要，有时候则不需要。我是考虑到以下两点而显著地减小了训练集的规模： 首先，迁移学习。简单来说，与其从零开始训练一个新模型，我从已在 COCO 数据集（在 repo 中已提供下载）上训练好的权重文件开始。虽然 COCO 数剧集不包含气球类别，但它包含了大量其它图像（约 12 万张），因此训练好的图像已经包含了自然图像中的大量常见特征，这些特征很有用。其次，由于这里展示的应用案例很简单，我并不需要令这个模型达到很高的准确率，很小的数据集就已足够。 有很多工具可以用来标注图像。由于其简单性，我最终使用了 VIA（VGG 图像标注器）。这是一个 HTML 文件，你可以下载并在浏览器中打开。标注最初几张图像时比较慢，不过一旦熟悉了用户界面，就能达到一分钟一个目标的速度。 VGG 图像标注器工具的用户界面 如果你不喜欢 VIA 工具，可以试试下列工具，我都测试过了： LabelMe：最著名的标注工具之一，虽然其用户界面有点慢，特别是缩放高清图像时。 RectLabel：简单易用，只在 Mac 可用。 LabelBox：对于大型标记项目很合适，提供不同类型标记任务的选项。 COCO UI：用于标注 COCO 数据集的工具。 加载数据集 分割掩码的保存格式并没有统一的标准。有些数据集中以 PNG 图像保存，其它以多边形点保存等。为了处理这些案例，在我们的实现中提供了一个 Dataset 类，你可以通过重写几个函数来读取任意格式的图像。 VIA 工具将标注保存为 JSON 文件，每个掩码都是一系列多边形点。 代码提示：通过复制 coco.py 并按你的需要修改是应用新数据集的简单方法，我将新的文件保存为 ballons.py。 我的 BalloonDataset 类是这样定义的： load_balloons 读取 JSON 文件，提取标注，然后迭代地调用内部的 add_class 和 add_image 函数来构建数据集。 load_mask 通过画出多边形为图像中的每个目标生成位图掩码。 image_reference 返回鉴别图像的字符串结果，以进行调试。这里返回的是图像文件的路径。 你可能已经注意到我的类不包含加载图像或返回边框的函数。基础的 Dataset 类中默认的 load_image 函数可以用于加载图像，边框是通过掩码动态地生成的。 为了验证我的新代码可以正确地实现，我添加了这个 Jupyter notebook：inspect_balloon_data.ipynb。它加载了数据集，并可视化了掩码、边框，还可视化了 anchor 来验证 anchor 的大小是否拟合了目标大小。以下是一个 good example。 来自 inspect_balloon_data notebook 的样本 代码提示：为了创建这个 notebook 我复制了 inspect_data.ipynb（这是为 COCO 数据集写的），然后修改了代码的初始部分来加载 Balloons 数据集。 这个项目的配置和训练 COCO 数据集的基础配置很相似，因此我只需要修改 3 个值。正如我对 Dataset 类所设置的，我复制了基础的 Config 类，然后添加了我的覆写： 基础的配置使用的是 1024x1024 px 的输入图像尺寸以获得最高的准确率。我保持了相同的配置，虽然图像相对较小，但模型可以自动地将它们重新缩放。 代码提示：基础的 Config 类在 config.py 中，BalloonConfig 在 balloons.py 中。 Mask R-CNN 是一个规模很大的模型。尤其是在我们的实现中使用了 ResNet101 和 FPN，因此你需要一个 12GB 显存的 GPU 才能训练这个模型。我使用的是 Amazon P2 实例来训练这个模型，在小规模的数据集上，训练时间不到 1 个小时。 用以下命令开始训练，以从 balloon 的目录开始运行。这里，我们需要指出训练过程应该从预训练的 COCO 权重开始。代码将从我们的 repo 中自动下载权重。 如果训练停止了，用以下命令让训练继续： 代码提示：除了 balloon.py 以外，该 repo 还有两个例子：train_shapes.ipynb，它训练了一个小规模模型来检测几何形状；coco.py，它是在 COCO 数据集上训练的。 inspect_balloon_model notebook 展示了由训练好的模型生成的结果。查看该 notebook 可以获得更多的可视化选项，并一步一步检查检测流程。 代码提示：这个 notebook 是 inspect_model.ipynb 的简化版本，包含可视化选项和对 COCO 数据集代码的调试。 现在我们已经得到了目标掩码，让我们将它们应用于颜色填充效果。方法很简单：创建一个图像的灰度版本，然后在目标掩码区域，将原始图像的颜色像素复制上去。以下是一个 good example： 代码提示：应用填充效果的代码在 color_splash() 函数中。detect_and_color_splash() 可以实现加载图像、运行实例分割和应用颜色填充过滤器的完整流程。 FAQ 环节 Q：我希望了解更多该实现的细节，有什么可读的？ A：按这个顺序阅读论文：RCNN、Fast RCNN、Faster RCNN、FPN、Mask RCNN。 Q：我能在哪里提更多的问题？ A：我们的 repo 的 Issue 页面：https://github.com/matterport/Mask_RCNN/issues 原文链接：https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46 
88,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739982&idx=4&sn=fdb36cf1487fccbaa2004d8cde8675a3&chksm=871ad170b06d5866c986b1d489c1d5b4a6bd413967adb06ffe5362fd41213bb51428155973c8&scene=27,学界 | 华盛顿大学推出YOLOv3：检测速度快SSD和RetinaNet三倍（附实现）,pjreddie 近日，来自华盛顿大学的 Joseph Redmon 和 Ali Farhadi 提出 YOLO 的最新版本 YOLOv3。通过在 YOLO 中加入设计细节的变化，这个新模型在取得相当准确率的情况下实现了检测速度的很大提升，一般它比 R-CNN 快 1000 倍、比 Fast R-CNN 快 100 倍。机器之心对论文进行了编译，实现和视频 demo 详见文中。 代码地址：https://pjreddie.com/yolo/. 1. 引言 有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。凭着上年余留的一点动力，我成功对 YOLO 做了一些升级。但实话讲，没什么超有趣的东西，只不过是些小修小补。同时我对其他人的研究也尽了少许绵薄之力。 于是就有了今天的这篇论文。我们有一个最终截稿日期，需要随机引用 YOLO 的一些更新，但是没有资源。因此请留意技术报告。 技术报告的优势在于其不需要介绍，你自然知道来由。因此简介的最后将为余文提供路标。首先我将介绍 YOLOv3 的结局方案；接着是其实现。我们也会介绍一些失败案例。最后是本文的总结与思考。 2. 解决方案 这一部分主要介绍了 YOLOv3 的解决方案，我们从其他研究员那边获取了非常多的灵感。我们还训练了一个非常优秀的分类网络，因此原文章的这一部分主要从边界框的预测、类别预测和特征抽取等方面详细介绍整个系统。 简而言之，YOLOv3 的先验检测（Prior detection）系统将分类器或定位器重新用于执行检测任务。他们将模型应用于图像的多个位置和尺度。而那些评分较高的区域就可以视为检测结果。 此外，相对于其它目标检测方法，我们使用了完全不同的方法。我们将一个单神经网络应用于整张图像，该网络将图像划分为不同的区域，因而预测每一块区域的边界框和概率，这些边界框会通过预测的概率加权。 我们的模型相比于基于分类器的系统有一些优势。它在测试时会查看整个图像，所以它的预测利用了图像中的全局信息。与需要数千张单一目标图像的 R-CNN 不同，它通过单一网络评估进行预测。这令 YOLOv3 非常快，一般它比 R-CNN 快 1000 倍、比 Fast R-CNN 快 100 倍。 最后，机器之心也尝试使用预训练的 YOLOv3 执行目标检测，在推断中，模型需要花 1s 左右加载模型与权重，而后面的预测与图像本身的像素大小有非常大的关系。因此，吃瓜小编真的感觉 YOLOv3 很快哦。 论文：YOLOv3: An Incremental Improvement 论文链接：https://pjreddie.com/media/files/papers/YOLOv3.pdf 摘要： 我们在本文中提出 YOLO 的最新版本 YOLOv3。我们对 YOLO 加入了许多设计细节的变化，以提升其性能。这个新模型相对更大但准确率更高。不用担心，它依然非常快。对于 320x320 的图像，YOLOv3 可以达到 22ms 的检测速度，获得 28.2mAP 的性能，与 SSD 的准确率相当但是速度快 3 倍。当我们使用旧版.5 IOU mAP 检测指标时，YOLOv3 是非常不错的。它在一块 TitanX 上以 51ms 的速度达到了 57.9 AP_50 的性能，而用 RetinaNet 则以 198ms 的速度获得 57.5 AP_50 的性能，性能相近但快了 3 倍。 
89,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739707&idx=5&sn=9d477bca61762f60647f0900126057e4&chksm=871ad785b06d5e93142061787f7348f30d25be89ce4b07794e72347696285db8a571cd7244c7&scene=27,AAAI 2018 | 美图联合中科院提出无监督类脑智能方法NOASSOM：可实现视频语义理解,"近日，美图云视觉技术部门与中科院自动化所共同合作研发，提出一种基于类脑智能的无监督的视频特征学习和行为识别的方法 NOASSOM (Hierarchical Nonlinear Orthogonal Adaptive-Subspace Self-Organizing Map based Feature Extraction for Human Action Recognition)，该方法不依赖于标签信息，可以自适应地、无监督地学到视频的特征表示，相关成果已发表在 AAAI 2018 上，并以 oral 的形式在大会上进行了报告。   视频语义理解一直是学术界的研究热点之一。近两年随着短视频领域的火爆发展，围绕短视频的业务场景应用也在增长，工业界应用场景都对视频内容理解提出了迫切的落地需求。与学术界用的确定性数据集不同，工业界业务产生的视频数据具有如下特点：首先，数据量大，每天都会有成千上百万的视频被上传；其次，内容未知，现实生活中的场景是很复杂的，尤其对于 UGC 内容，无法确定用户上传的视频中的主体和场景，行为更是无法预测；再次，时效性，在不同的时间段内视频的主题、场景以及行为是不同的，它可能会随着时间发生变化进行转移。因此，在这样的数据集上人工建立标签体系非常困难。NOASSOM 算法的提出有效解决了算法模型在训练过程中无标签输入的问题。   NOASSOM 是通过模拟视觉皮层中表面区域的结构来构建的，以数据驱动自组织更新，恢复基本视觉皮层中的神经元对输入刺激的反应。NOASSOM 是对 ASSOM 方法的改进。ASSOM 是一种特征提取方法，它可以从输入数据中学习统计模式，并对学到的模式进行自组织排列，从而进行特征表示。但是 ASSOM 只能处理有标签的数据，并且只对线性化的数据有效，无法胜任其他复杂情形。NOASSOM 的提出解决了 ASSOM 的这两个重要问题。首先，NOASSOM 通过引入一个非线性正交映射层，处理非线性的输入数据，并使用核函数来避免定义该映射的具体形式。其次，通过修改 ASSOM 的损失函数，使输入数据的每个样本可以独立地贡献于损失函数，而不需要标签信息。这样，NOASSOM 可以有效地、无监督地学习数据的统计模式和本征表示。图 1 示意了 NOASSOM 与 ASSOM 的网络结构区别。   NOASSOM 与 ASSOM 网络结构 ASSOM 由输入层、子空间层、输出层组成。NOASSOM 比 ASSOM 增加一个非线性正交映射层，用于实现输入层和子空间层的非线性正交映射。为保证映射后的子空间基向量仍然保持正交性，NOASSOM 采用正交约束的核函数： 输出层使用输入在子空间的投影表示： 使用投影残差构建损失函数： 原始的 ASSOM 的损失函数表示如下： 通过修改损失函数使每个样本独立地贡献于损失函数，而不必使用 Class-specific 的数据进行有监督训练。NOASSOM 使用随机梯度下降法对网络进行训练。 在每次迭代之后，重新对基向量进行正交化处理。算法流程图如下： 	   NOASSOM 论文进一步还提出一个层级的 NOASSOM 来提取高层的抽象特征，有效地描述视频中行为轨迹的表观和运动信息，构建了一个层级的 NOASSOM 结构提取视频中的局部行为特征，并使用 FISHER VECTOR 进行聚合编码，采用 SVM 进行分类，如图 2 所示。   训练得到的基向量的可视化结果如图 3 所示，左边是表观信息滤波器，右边是运动信息滤波器。可以看出表观信息滤波器可以学到一些类似边缘检测的滤波器，这样类型的滤波器对图像的水平边沿和垂直边沿能进行检测，从而提取良好的轮廓纹理信息。右边的运动信息滤波器学到了一些类似 Gabor 滤波器的滤波器，这样的滤波器对运动信息更加敏感，实现对运动信息进行良好的提取。   	   NOASSOM 中训练得到的基向量的可视化结果如图 2 所示，左边是表观信息滤波器，右边是运动信息滤波器。可以看出表观信息滤波器可以学到一些类似边缘检测的滤波器，这样类型的滤波器能对图像的水平边沿和垂直边沿进行检测，从而提取良好的轮廓纹理信息。右边的运动信息滤波器学到了一些类似 Gabor 滤波器学到的信息，这样的滤波器对运动信息更加敏感，实现对运动信息地鲁棒性提取。 NOASSOM 在国际公开大型数据集 UCF101, HMDB51 和小型数据集 KTH 上进行了评测，获得了 93.8%，69.3% 和 98.2% 的识别率。在 UCF101 和 HMDB51 上，分别超出使用手工特征的 iDt+HSV 基准方法 5.9% 和 8.2%，并且分别超出使用卷积神经网络模型的 iDt+CNN 方法 2.3% 和 3.4%，在 KTH 上超过 iDT+MBH 的基准方法 3.2% 以及基于 3D CNN 的方法 8.0%。公开数据集上的实验结果表明，这种方法优于之前基于手工特征的方法和大多基于深度特征的方法。此外，在小数据库上，性能更加优于基于 CNN 的方法。更多的技术细节和实验结果请参考原始论文。 	   NOASSOM 方法的独特优势在于，可以从大量没有标签的数据进行更加快速的训练，并且获得和其他基于有标签数据方法性能相当甚至更加优越的性能。基于这项技术的输出将被应用于美拍短视频多个业务场景中，如相似视频的推荐和大规模视频检索，基于短视频内容的用户聚类和画像，以及基于短视频内容的运营标签挖掘等等。 附： 美图云视觉技术部门，专注于文本、图像和视频等领域的视觉算法研发和平台构建。部门主导研发的 AI 视觉分析平台 DeepNet，提供检测、分类、语义理解、哈希、OCR 等多个方向的技术支撑，正在为美图各产品和业务，如美拍短视频运营、商业化广告、推荐业务、搜索业务和安全审核等提供算法支撑。视觉部门长期招纳视觉领域相关人才，方向不限，有意者请发简历至 lili.zhao@meitu.com。 "
90,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739982&idx=5&sn=99155166f66ad0ba36168a6b1c0fbfc7&chksm=871ad170b06d586605c95a8d7f945ad699f9cc3449796ea7b9ef8279d480543b5a73f0206844&scene=27,想看英伟达GTC黄仁勋演讲的视频直播？请锁定机器之心官网,"从 2009 年开始至今，英伟达 GPU 技术大会（GTC）已经成为了该公司以及整个业界一年一度的盛事。今年，英伟达将于 3 月 26 日至 29 日在加州圣何塞 McEnery 会议中心举办第九届年度 GPU 技术大会，英伟达创始人兼首席执行官黄仁勋也将于北京时间 28 日凌晨发表主题演讲，一系列有关新型 GPU 的新闻将会重磅公布。而我想说的是，本届 GTC，机器之心将现场直播搬到了国内，想第一时间见证新一代显卡的问世吗？请锁定机器之心官网。 机器之心 GTC 2018 专题链接： https://www.jiqizhixin.com/topics/GTC2018 据机器之心了解，自 2009 年首次举办以来，GTC 的参会人数增加了近 10 倍。今年的会场将占据整个面积为 373,000 平方英尺的会议中心。随着人工智能的快速发展，这一芯片领域的盛事也成为了机器学习社区的一大盛会：计算机视觉、自动驾驶、高性能计算等主题吸引了大批业内人士参与。 本届 GTC 大会，有以下 6 大亮点： 600 多场会议将覆盖自动驾驶汽车、高性能计算、机器人、医疗保健、智慧城市、数据中心和云计算、安防、生命科学、计算机和机器视觉、以及虚拟现实等领域。详情请查看 GTC 会议日程 (https://www.nvidia.com/en-us/gtc/topics/)。 2700 多家公司和知名高校代表参会，他们来自 Adobe、阿里巴巴、亚马逊、百度、宝马集团、第一资本、福特、通用电气、谷歌、摩根大通、洛克希德·马丁、梅奥医学中心、梅赛德斯 - 奔驰、微软、麻省理工学院、NASA、纽约大学、皮克斯、Salesforce、斯坦福、腾讯、丰田研究院、优步、VMware、沃尔玛和华特迪士尼幻想工程等。主要赞助商包括 Facebook、IBM、思科、戴尔 EMC、谷歌云、HPE、浪潮、联想和超微。 150 多家参展商将在会上展示最先进的技术，其中大部分技术为首次亮相，为参会者提供独特的实操机会，并能够与那些引领 AI 革命、将 VR 和自动驾驶汽车等变为现实的公司进行互动。 200 多家初创公司将展示旗下颠覆性技术。与会者可于太平洋时间 3 月 27 日星期二下午 4:30-5:30 在 NVIDIA 初创加速计划挑战赛决赛（Inception Awards Finale）上投票选出全球顶级 AI 初创公司。 60 多场实践动手培训课程将涵盖人工智能基础、自动驾驶汽车、CUDA 编程、基因组学 AI、嵌入式应用、OpenACC 和 VR 等主题。NVIDIA 深度学习学院认证讲师将使用最新的 AI 框架和软件开发工具包，为数千名数据科学家提供超过 100 小时的培训。 全球影响力大奖 (Global Impact Awards) 的两位获奖者将凭借其在利用 GPU 计算解决重要社会及人道主义问题方面的前沿成就，分别获得 20 万美元奖金。 除了以上精彩预告外，最扣人心弦的也许是英伟达将在此届 GTC 上发布的最新芯片了。 在大会开幕两个月前，就有媒体透漏，英伟达将会在 GTC 2018 大会上发布新的 GeForce GTX 系列游戏显卡，并且说是基于 Ampere（安培）架构，而非 GPU 路线图规划上的 Volta 架构。 此外，还有媒体放出另一「烟雾弹」：Turing 架构产品，多方证据表示这可能是面向挖矿的全新系列产品，擅长于虚拟货币「挖矿」，类似于 ASIC 矿机芯片。 因此，此届 GTC 可能会成为英伟达发布 GPU 产品最多的一届大会。而以上所有揣测，终将会在今晚黄老板的 Keynote 中揭晓。 锁定机器之心，第一时间见证英伟达的重大发布。 "
91,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739707&idx=1&sn=37b8403c86b7fd295d2fe7d00aa001fc&chksm=871ad785b06d5e93563511eecd728a18a00cda1161de2c6890e2f3d177f38d8c080181abbd77&scene=27,刚刚，ACM公布了2017年图灵奖得主：荣誉属于体系架构,"刚刚，美国计算机协会（ACM）宣布 John L. Hennessy 和 David A. Patterson 荣获 2017 年图灵奖。目前这两位学者都供职于谷歌，前者是谷歌母公司 Alphabet 的董事会主席，后者任谷歌杰出工程师，致力于研究机器学习和人工智能。图灵奖是计算机协会（ACM）于 1966 年设立的奖项，专门奖励对计算机事业作出重要贡献的个人，有「计算机界诺贝尔奖」之称。公告中写道，Hennessy 和 Patterson 对今天微处理器的基础贡献引领了移动和 IOT 领域的革命。 美国计算机协会（ACM）宣布：前斯坦福大学校长 John L. Hennessy 和加州大学伯克利分校退休教授 David A. Patterson 因其开创性地提出计算机架构设计和评估的系统、量化方法荣获 2017 年图灵奖，他们提出的方法对微处理器行业影响深远。Hennessy 和 Patterson 提出一种系统、量化的方法来构建更快、能耗更低的精简指令集计算机（RISC）微处理器。他们的方法是学界和业界数代架构师在多个项目中长期使用的原则。现在，每年生产的超过 160 亿微处理器中，99% 是 RISC 处理器，应用于智能手机、平板电脑和数十亿嵌入式设备中。 Hennessy 和 Patterson 将他们的见解写进了一本非常有影响力的书《计算机体系结构：量化研究方法》，现在这本书已经出到第六版，数代工程师和科学家采用和发扬他们的理念。他们的研究加强了我们建模和分析新型处理器架构的能力，极大地促进了微处理器设计方面的进展。 图灵奖有「计算机界诺贝尔奖」之称，奖金为 100 万美元，由谷歌公司赞助。图灵奖的名称取自英国数学家艾伦·图灵（Alan M. Turing），他奠定了计算机的数学基础和局限性。 ACM 主席 Vicki L. Hanson 称：「ACM 于 1966 年设立图灵奖，以表彰对计算领域持续且重大的技术贡献。Hennessy 和 Patterson 的研究工作无疑达到了这一标准。他们对基于 RISC 的节能处理器的贡献有助于实现移动和物联网革命。与此同时，他们的重要教科书《计算机体系结构：量化研究方法》影响了几代工程师和计算机设计师，并在过去的 25 年中推动了整个行业的创新步伐。」 微软集团的主要创始人比尔·盖茨同样称赞了 Hennessy 和 Patterson 研究工作的深远影响，称他们的贡献「已被证明是整个行业蓬勃发展的基石。」 MIPS 和 SPARC 的发展 尽管研究者从 1960 年代已经开始探索降低复杂度的架构（最为著名的是 IBM 801 项目），但 Hennessy 和 Patterson 领导的研究被认为牢固确立了 RISC 方法的可行性，使其概念广为流传，学界和业界也不例外。RISC 方法与当时流行的复杂指令集计算机（CISC）不同，它只需要一小组简单和计算机必须执行的一般指令，比复杂的指令集需要更少的晶体管，并减少了计算机必须执行的工作量。 Patterson 的伯克利团队创造了 RISC 这一术语，并于 1982 年构建和演示了他们的 RISC-1 处理器。RISC-1 原型机带有 44,000 个晶体管，性能优于带有 100,000 个晶体管的传统 CISC 设计。Hennessy 在 1984 年联合创建了 MIPS Computer Systems 公司，以商业化斯坦福团队的研究成果。之后，伯克利团队的研究成果在 Sun Microsystems 公司的 SPARC 微架构中实现了商业化。 尽管许多计算机架构师最初对 RISC 持怀疑态度，但 MIPS 和 SPARC 商业化努力的成功、RISC 设计的更低生产成本以及更多的研究进展使得 RISC 被广泛接受。到 1990 年代中期，RISC 微处理器已在整个领域占据主导地位。 开创性的教科书 Hennessy 和 Patterson 在他们 1990 年出版的教科书《计算机体系结构：量化研究方法》中提出了科学的新方法论。这本书影响了其后几代工程师，并通过向计算机架构社区传播其中重要思想大大提高了微处理器设计的进步速度。在这本书中，Hennessy 和 Patterson 鼓励架构设计师们仔细优化自己设计的系统，以适应不同的内存和计算需求。他们的研究也促使人们将研究方向从寻求单纯的性能提升转向设计架构时考虑能耗、散热，以及片外通信等问题。这本书具有开创性意义，因为它是第一本提供分析和科学框架的文本，为工程师和设计者评估微处理器设计的价值提供了方法和思路。 John L. Hennessy John L. Hennessy，2000-2016 年担任斯坦福大学校长。他还是斯坦福 Knight-Hennessy 学者计划的主任、思科系统公司的董事会成员、Gordon and Betty Moore 基金会董事会成员和 Alphabet 公司董事会主席。Hennessy 在维拉诺瓦大学获得电气工程学士学位，在纽约州立大学石溪分校获得计算机科学硕士及博士学位。 Hennessy 获得众多荣誉，包括 IEEE 荣誉奖章、ACM-IEEE CS Eckert-Mauchly 奖（与 Patterson 共享）、EEE John von Neumann Medal（与 Patterson 共享）、Seymour Cray 计算机工程奖以及美国艺术与科学学院颁发的创始人奖（Founders Award）。Hennessy 还是 ACM 和 IEEE 会士，同时还是美国国家工程院、美国国家科学院和美国哲学院院士。 David A. Patterson David A. Patterson 是谷歌的杰出工程师（Distinguished Engineer），也是 RISC-V Foundation 董事会副主席，他提出了一个开放、免费的指令集架构，通过开放的标准协作开启了处理器创新的新时代。从 1976 年到 2016 年，Patterson 一直担任加州大学伯克利分校计算机科学系教授，本、硕、博均毕业于加州大学洛杉矶分校计算机科学系。 Patterson 曾获得众多荣誉，包括 IEEEJohn von Neumann 奖章（和 Hennessy 共享）、ACM-IEEE CS Eckert-Mauchly 奖（和 Hennessy 共享）、ACM Karl V. Karlstrom 杰出教育家奖等。2004 年到 2006 年，Patterson 担任 ACM 主席。他是 ACM、AAAS、IEEE 三会会士，也被选为美国国家工程院院士、美国国家科学院院士。 附录：往届图灵奖得主名单 "
92,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739707&idx=3&sn=63a66b42cb7dfbaf36dca087bf4c1b95&chksm=871ad785b06d5e939e0176f9ea661d50b228f2e333f9253ee352bca80b136602970f1b14981a&scene=27,深度 | 变分自编码器VAE面临的挑战与发展方向,"变分自编码器（VAE）与生成对抗网络（GAN）一样，是无监督学习最具前景的方法之一。本文中，牛津大学统计系在读博士 Adam Kosiorek 从原理上向我们介绍了 VAE 目前面临的挑战。同时，文中也提出了对于该方法的几种改进方向。 隐变量模型 假设你希望通过一个定义在 x∈RD 上的概率分布来对整个世界建模，其中 p（x）表示 x 可能处于的状态。这个世界可能非常复杂，我们无法知道 p（x）的具体形式。为了解决这个问题，我们引入另一个变量 z∈Rd 来描述 x 的背景信息。例如 x 是一个图像，那么 z 就可以记录关于图像中可见物体的出现、数量、类型，以及画面的背景和光影条件的信息。这个新的变量使得我们可以将 p（x）表示为一个无限混合模型。 这是一个混合模型，因为对于 z 的任意可能取值，都引入另一个条件分布，并通过 z 的概率进行加权，最终得到 p（x）。 在这样的设定下，「给定 x 的观测值，隐变量 z 是什么」就成了一个非常有趣的问题。也就是说，我们希望知道后验分布 p(z∣x)。但是，z 和 x 之间可以呈现高度的非线性关系（比如，由一个多层神经网络实现），而且，D——我们观测值的维度，和 d——隐变量的维度，也可能非常大。由于边缘分布和后验分布都需要对（1）式积分求值，我们认为它们都是无法计算的。 我们可以通过蒙特卡罗抽样，根据 来估计（1）式，但由于 z 的空间可能非常大，我们可能需要上百万个 z 的样本，来得到一个可靠的估计。 在训练一个概率模型的时候，我们可以使用参数分布 - 它的参数由一个参数为θ∈Θ的神经网络来确定。现在，我们就可以使用极大似然估计来学习得到这些参数。 这里的问题是，我们无法最大化（1）式，因为我们无法估计它。为了解决这个问题，我们可以求助于重要抽样（importance sampling）。当我们需要对原始（名义分布）概率密度分布（pdf）估算一个期望值时，IS 使得我们可以从另一个不同的概率分布（建议分布）中抽样，然后将这些样本对名义分布求期望。用 qϕ(z∣x) 表示我们的建议分布 - 其中的参数由参数为 ϕ∈Φ的神经网络确定。我们可以得到： 根据重要性抽样的文献可知，最优的建议分布，与名义分布乘以某个函数成比例，其中这个函数的期望是我们希望估计的。在我们的设定下，「某个函数」就是 p（x|z）。根据贝叶斯定理，p(z∣x)=p(x∣z)p(z)/p(x)，我们可以看到，最优建议分布与后验分布成比例，显然，后验分布无法求解。 变分自编码器的诞生 幸运的是，事实上我们可以一箭双雕：通过一个学习到的建议分布来近似估计后验分布，我们可以有效的得到边缘分布 pθ(x) 的估计。在这里，我们无意间得到了一个自编码的设定。为了学习我们的模型，我们需要： pθ(x,z) - 生成模型，其中包含： pθ(x∣z) - 一个概率形式的解码器，以及 p(z) - 一个定义在隐变量上的先验分布 qϕ(z∣x) - 一个概率形式的编码器 为了近似估计后验分布，我们可以利用建议分布和后验分布之间的 KL 散度（可以理解为两个概率分布之间的距离），而且我们可以最小化这个结果。 这个时候，我们面临的新问题就是：为了计算 KL 散度，我们需要知道后验分布。并非没有可能，只要利用一点点代数运算，我们就能得到可以计算的目标函数。 我在第二行展开了对数项，在第三行使用了贝叶斯定理以及 pθ(x) 和 z 是独立的事实。最后一行中的 L(x;θ,ϕ) 是对数概率分布 pθ(x) 的下界 - 即通常所说的证据下界（ELBO）。我们通过整理可以得到： 只需要一个从建议分布中抽得的样本，我们就可以得到近似估计： 我们通过寻找最大化 ELBO 的ϕ和θ（通常使用随机梯度下降）来训练模型： 通过最大化 ELBO，我们或（1）最大化边缘分布，或（2）最小化 KL 散度，或同时完成。需要注意，ELBO 的近似估计是 f(x)=1、重要性权重为 w(x)=pθ(x,z)qϕ(z∣x) 的重要性抽样的期望的对数形式。 这个估计量有什么问题？ 如果你足够仔细的看重要性抽样，就能发现，对建议分布的支撑应该比对名义分布的支撑更广泛——应该同时避免估计量方差无限大和数值的不稳定性。在这里，最好是来优化 KL(p∣∣q) 的倒数——因为它有模式平均性质，而不是优化 KL（q∣∣p），来试图通过模式 q 去匹配找到一个最好的模式 p。这意味着我们需要从真实的后验分布中进行抽样，而这是很困难的。作为替代，我们可以使用 ELBO 的 IS 估计，作为重要性加权自编码器（IWAE）。这里的想法很简单：我们从建议分布中抽取 k 个样本，并从中计算出平均概率比，这里的每一个样本也叫「粒子」。 已经证明，这个估计量是在优化修正后的 KL 散度 KL(qIS∣∣pIS)，其中 qIS 和 pIS 的定义分别是： 尽管和原始分布看似接近，但 qIS 和 pIS 允许 q 和 p 中存在预想以外的小的变动。原始论文中证明，优化这个下界可以得到更好的生成模型。同时它也给出了一个近似后验分布 q 的熵更大的估计（更宽，更离散），并成功的超越了原始 KL 散度的模式匹配方法。还有一个有趣的结果，如果我们令粒子 K 的数量趋于无穷，我们就可以不需要推断模型 q。 IWAE（第一行）和 VAE（第二行）中 z 的后验分布。图像从 IWAE 论文中复现得到。 IWAE 有什么问题？ 重要性加权 ELBO，或 IWAE，推广了原始的 ELBO：对于 K=1，我们有 LK=L1=L。同时有 logp(x)≥Ln+1≥Ln≥L1。换言之，我们用来估计 LK 的粒子越多，它的结果就会越接近数据真实对数概率分布——即「界限越紧」。这意味着和原始 ELBO 的梯度相比，通过对 IWAE 求微分得到的梯度估计量可以帮助我们找到一个更好的梯度方向。除此之外，随着 K 的增加，梯度估计量的方差会相应收缩。 对于生成模型这些点非常好，但面对建议分布的时候，就会出现问题。随着 K 的增大，建议分布中参数的梯度的大小会趋于 0，而且比方差收敛得还要快。 令Δ(ϕ) 表示我们优化的目标函数（即 ELBO）在ϕ上的梯度的小批量估计。如果定义参数更新的信号-噪声比（SNR）如下： 其中 E 和 V 分别表示期望和方差。可以看出对于 pθ，SNR 随着 K 增加而增加，但对于 qϕ，SNR 随着 K 增加而减小。这里的结论很简单：我们使用的粒子越多，我们的推断模型效果就会越差。如果我们关心的是表示学习，我们就会遇到问题了。 更好的估计量 正如我们在最近的论文《Tighter Variational Bounds are Not Necessarily Better》中证明的，我们可以得到比 IWAE 更好的结果。思路是在推断和生成模型中使用不同的目标，通过这种方法，我们可以保证两个目标中都得到小方差非零梯度，最终得到更好的模型。 不同的训练目标在训练时期中信号-噪声比 在上图中，我们比较了建议分布 qϕ的参数ϕz 在更新中的 SNR。SNR 最高的 VAE 通过最优化 L1 来训练。SNR 最低的 IWAE 则通过最优化 L64。中间的三条曲线使用的是不同的组合：生成模型中使用的 L64，推断模型中使用的则是 L8 或 L1。在当前指标下，它们效果虽然没有 VAE 好，但训练出的建议分布和生成模型都比使用 VAE 或 IWAE 得到的好。 这里有一个令人惊讶的副作用：使用我们新的估计量训练的模型比使用 IWAE 本身训练的模型达到了更高的 L64 界限。为什么会这样？通过研究有效样本量（ESS）和数据的边缘概率分布的对数，似乎是最优化 L1，导致了性质最好的建议分布但是性质最差的生成模型。如果我们将一个好的建议分布和一个可以得出好的生成模型的目标结合在一起，我们应该可以得到这个目标的一个方差更小的估计，甚至因此可以得到更好的模型。请在这里查看我们论文的详情。 论文：Tighter Variational Bounds are Not Necessarily Better 论文地址：https://arxiv.org/abs/1802.04537 摘要：我们同时在理论和经验上证明，使用更紧的信息下界（ELBO）可能并不利于通过减少梯度估计量的信号-噪声比来学习推断网络的过程。我们的结果对目前广为应用的暗含假设：「更紧的 ELBO 是联立模型学习和推断摊销模式中更合适的变分目标」提出了质疑。根据我们的研究，我们提出了三个新的算法：偏重要性加权自编码器（PIWAE）、多层重要性加权自编码器（MIWAE）以及联合重要性加权自编码器（CIWAE）；在这三个算法中，标准的重要性自编码器（IWAE）都可以作为一个特殊情况。我们证明了这三个自编码器都可以在 IWAE 的基础上取得效果提升——即使我们使用的是 IWAE 中的目标来测试效果。进一步来说，和 IWAE 相比，PIWAE 可以同时提升推断网络和生成网络的效果。 "
93,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739632&idx=4&sn=b2633c77407d836b8203bc1202d22992&chksm=871ad7ceb06d5ed8c5f285b972fdbf8b16a26619a58beb8929c4a3febca725cad10e778bbdcb&scene=27,学界 | 新型实时形义分割网络ShuffleSeg：可用于嵌入式设备,"arXiv 表现优良的卷积神经网络往往需要大量计算，这在移动和嵌入式设备以及实时应用上是一个很不利的因素。近日，开罗大学和阿尔伯塔大学的研究者提出了一种能实现实时形义分割的框架 ShuffleSeg。这种方法能在保证分割准确度的同时显著降低对计算资源的需求。机器之心在本文中对该项目进行了简要编译介绍，相关研究的 TensorFlow 代码已发布在 GitHub 上。 论文地址：https://arxiv.org/abs/1803.03816 项目代码：https://github.com/MSiam/TFSegmentation  构建计算高效的卷积神经网络（CNN）仍然还是一个悬而未决的研究问题。在提升 CNN 的计算效率方面存在两种主要机制。第一种机制侧重于设计高效的模型，比如 GoogleNet、Xception、MobileNet 和近期的 ShuffleNet。另一种机制则是针对模型加速，具体方法包括对网络连接或通道进行剪枝或进行网络量化（network quantization）。之前在提升计算效率方面的研究主要侧重于图像分类和目标检测等最终任务。但以实时形义分割网络为目标的研究很少，尽管形义分割在机器人相关的应用中有许多益处。对计算高效的形义分割的需求是很显著的。 我们在本论文中提出了一种实时的形义分割网络，该网络基于 [4] 中提出的 ShuffleNet 单元。我们在本论文中将其称为 ShuffleSeg。ShuffleSeg 在其解码器中集成了 skip 连接来改善分割结果。我们的网络需要 2.03 GFLOPs，在计算效率上的表现优于之前最佳的需要 3.83 GFLOPs 的分割网络。尽管如此，ShuffleSeg 还是在 CityScapes 测试集基准上实现了可与最佳表现媲美的平均交并比（IoU）——58.2%。因此，我们的网络在速度和准确度之间实现了很好的平衡。这有望实现在嵌入式设备中的进一步部署应用。 实时形义分割在近期开始得到关注。Paszke et. al. [13] 引入了 ENet，这是带有一个瓶颈模块（bottleneck module）的高效轻量级分割网络。Chaurasia et. al. [14] 提出了 LinkNet 架构，其使用 ResNet18 作为编码器。LinkNet 实现了比 ENet 更优的平均 IoU。但 ENet 在计算效率上的表现更好。SegNet 和 FCN8s 等其它网络也没有重点关注计算效率，但在分割相关文献中被广泛使用。Badrinarayanan et. al. [15] 提出的 SegNet 是使用编码器-解码器架构进行端到端形义分割的早期尝试。Long et. al. [16] 首次尝试了以端到端的方式训练全卷积分割网络（FCN）。他还提出了 skip-net 方法，以在 FCN16s 和 FCN8s 架构的分割中使用更高分辨率的特征图。 就我们所知，之前在实时形义分割上的研究都没有利用分组卷积和通道混洗（channel shuffling）。我们在本研究中提出的 ShuffleSeg 是一种计算高效的分割网络。该网络的设计灵感源自 ShuffleNet，这是一种高效的分类和检测网络。ShuffleNet 单元使用了分组卷积来提升性能，而没有使用 1x1 卷积。只使用分组卷积对网络准确度不利，所以我们还使用了通道混洗来维持优良的准确度。这与 skip 架构组合在一起，可通过使用更高分辨率的特征图来改善我们的分割结果。ShuffleSeg 的代码将会公开发布。 这一节将详细介绍我们提出的用于形义分割的网络架构。该架构将分成两个主要模块进行解释：负责提取特征的编码模块，负责在网络中进行上采样以计算最终类别的概率图的解码模块。 解码器架构 我们提出的架构中所使用的编码器基于 ShuffleNet。我们主要从其中使用的分组卷积和通道混洗中受到了启发。[4,2,3] 表明深度上可分的卷积或分组卷积可以在降低计算成本的同时维持优良的表征能力。分组卷积的堆叠可能会导致出现一大主要瓶颈。输出通道将从有限的输入通道中导出。为了解决这个问题，[4] 中引入了信道混洗，这种方法也在 ShuffleSeg 的编码和解码部分都得到了良好的应用。 编码器架构 该分割网络的解码部分要执行转置卷积，以便上采样到输入的分辨率。受 UNet、FCN8s 和 Dilation Frontend 研究的启发，我们使用了不同的解码方法。我们比较的四种不同的解码方法为 (1) UNet、(2) SkipNet、(3) Dilation Frontend 8s、(4) Dilation 4s 实验 论文：ShuffleSeg：实时形义分割网络（ShuffleSeg: Real-time Semantic Segmentation Network） 实时形义分割对移动和机器人相关的应用而言非常重要。我们提出了一种计算高效的分割网络，我们称之为 ShuffleSeg。我们提出的架构基于其编码器中的分组卷积和通道混洗（channel shuffling），可用于提升性能。我们对不同的解码方法进行了剥离研究（ablation study），比较了 Skip 架构、UNet 和 Dilation Frontend。我们讨论了在速度和准确度权衡上的有趣见解。研究表明在解码方法中的 skip 架构能为实时性能目标提供最好的折中，同时还能通过为更准确的分割使用更高分辨率的特征图来提供足够的准确度。我们在 CityScapes 上评估了 ShuffleSeg，并且将其与当前最佳的实时分割网络进行了比较。它在 CityScapes 测试集上实现了可与其它方法媲美的 58.3% 的交并比（IoU），同时 GFLOPs 降低了 1 倍。ShuffleSeg 在 NVIDIA Jetson TX2 上的运行速度为每秒 15.7 帧，这让它在实时应用上有很大的应用潜力。 "
94,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739632&idx=5&sn=b397ff4d00b497b22af01860c1c5ad82&chksm=871ad7ceb06d5ed8fc4a18af8eef1a80236a33d9441b6dd63890a57097a10716f18c7359b635&scene=27,活动 |「智汇京东 • 开放共赢」2018 京东人工智能创新峰会报名启动,2018 京东人工智能创新峰会将在京举行 【峰会简介】 京东人工智能创新峰会由京东 AI 平台与研究部主办，京东大学 AI 学院承办，京东技术品牌发展部协办。此次峰会，京东高管和技术中坚力量将齐聚一堂，并特邀 AI 领域内重量级嘉宾，旨在为现场观众带来人工智能各领域最新进的资讯、最顶尖的科技、最前沿的观点。每一位参与者都将在此次峰会上体会到 AI 的奇幻魅力，现场还将有机会近距离接触 AI 领域内的大咖，同我们一道相信 AI，改变 AI，改变世界。此外，会上还将进行「Rosetta 平台发布」和「全球首届任务导向型多轮对话系统大奖赛启动仪式」。  最重磅  这里有最大咖的嘉宾 最有料  计算机视觉、语音语义、知识图谱 超全面的人工智能技术讲解满满干货带给你  最参与  开放式环境 自由的技术交流氛围 甚至让你与大咖之间零距离 【峰会时间地点】 2018 年 4 月 15 日 13:30-18:00 北京 朝阳区 光华路 【如何参与】 * 最终报名结果及具体地址将以 4 月 10 日电子门票为准 
95,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739632&idx=3&sn=dc86b4ede9b71b3af3df78d21cc5cd5a&chksm=871ad7ceb06d5ed8debde8936426c860fa52684a33b86432fc5f1a45f42c9986b2041ba19389&scene=27,前沿 | 机器学习助力医疗，通过数据分析发现近6000种新病毒,Nature 最近，研究者借助 AI 技术发现了近 6000 种前所未闻的新病毒，这一工作已在 3 月 15 号由美国能源部（DOE）组织的一场会议中展示，成为了一种探索发现数量巨大、种类繁多的病毒的新工具。 尽管从人类健康到垃圾降解，病毒的影响力无处不在，却很难被研究。科学家无法在实验室培植绝大多数病毒，确定其基因序列的尝试也多遭失败，因为它们的基因组极小，且进化迅速。 近年来，通过将取自不同环境的样本中的 DNA 进行排序，研究者已经获得了一些未知的病毒。为了确定目前存在的微生物，研究者搜集了已知病毒和细菌的基因特征，就如同文字处理器的「查找」功能会突出显示文档中包含特定字母的单词。但这一方法经常失败，因为病毒学家无法搜集他们不知道的东西。机器学习解决了这一问题，因为它可以发现海量数据中的潜在模式。机器学习算法解析数据，从中学习，接着自动分类信息。 南加州大学洛杉矶分校的计算生物学家 Jie Ren 说：「从前没有研究病毒的好方法，但现在就不同了，我们有了新工具。」 上周日，美国能源部联合基因组研究所（JGI）的计算生物学家 Simon Roux 训练计算机识别不常见的 Inoviridae 病毒家族的基因序列。这些病毒生存在细菌中并改变宿主的行为：比如，它们会使引起霍乱的细菌即霍乱弧菌的毒性变得更强。Roux 在 JGI 组织的会议上（加州，旧金山）展示了其研究，称在他的研究开始前已识别到的基因组种类不到 100 种。 Roux 展示了一个机器学习算法，该算法使用两个数据集：一个包含 805 个来自已知 Inoviridae 的基因序列，另一个包含 2000 个来自细菌和其他病毒的基因序列。算法可以找到一种方式来区分二者。 然后，Roux 向模型馈送大量宏基因组学数据集。计算机恢复了一万多种 Inoviridae 基因组，并将其分成不同种类的集群。这些集群之间的基因变异非常大，意味着 Inoviridae 可能有很多家族。 病毒学习 在另一项独立研究中，巴西圣保罗大学的生物信息学家 Deyvid Amgarten 应用机器学习来在城市动物园的天然肥料堆中寻找病毒。他将算法设计为可以搜索病毒基因组的几个可分辨特征，例如给定长度的 DNA 双链的基因密度。经过训练之后，计算机复原了几种可能是新型的基因组，Amgarten 说。他将这个结果在 JGI 会议上进行了展示。最后一步是学习这些病毒生成的蛋白质种类，然后检查哪些蛋白质将加速有机物的破坏。「我们希望改善肥料的质量。」他说。 Amgarten 是从去年报告的一项机器学习工具即 Ren 的团队开发的 VirFinder 中得到灵感的。VirFinder 被设计用于寻找 DNA 碱基组合，例如 DNA 双链中的 AT 或 GC。Ren 将算法应用到从健康人和肝硬化患者（由肝炎或慢性酒精中毒导致的疾病）的脸部取样的宏基因组。机器将取样的病毒完成了分组之后，该团队注意到样本中某些特定类型的基因组在健康人群中相对更少或更多，这意味着这些基因组相关的病毒可能导致了疾病。 Ren 的成果是一项很诱人的发现：生物医学研究者长期以来一直很困惑，到底是哪些病毒导致了那些疑难杂症，例如慢性疲劳综合症（也称为肌痛性脑脊髓炎），以及炎症性肠病。 康涅狄格州法明顿市的基因组医学杰克逊实验室的免疫学家 Derya Unutmaz 推测，病毒可能触发了一些毁灭性的反应，或者改变了人体微生物群系的细菌行为，从而使新陈代谢和免疫系统变得不稳定。 Unutmaz 说，通过应用机器学习，研究者可以发现患者体内仍被隐藏的病毒。此外，由于 AI 可以在大规模数据集中发现模式，该方法也许可以将病毒数据和细菌关联起来，然后和症状相关的蛋白质变化关联起来。「机器学习可以揭示出我们甚至从没思考过的问题。」Unutmaz 说。 
96,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739632&idx=2&sn=a929d1009773bd3f8ad286f2d11f908c&chksm=871ad7ceb06d5ed878394a350343574f3b1ceaa4ca3e9aa01b1aa8bff14f304feb53705e8760&scene=27,业界 | 英特尔开源nGraph编译器：从多框架到多设备轻松实现模型部署,"ai.intel 近日，英特尔的人工智能产品团队宣布开源 nGraph，这是一个面向各种设备和框架的深度神经网络模型编译器。有了 nGraph，数据科学家能够专注于数据科学研发，不需要担心如何将 DNN 模型部署到各种不同设备做高效训练和运行。 Github 地址：https://github.com/NervanaSystems/ngraph nGraph 目前直接支持 TensorFlow、MXNet 以及 neon，并可间接地通过 ONNX 支持 CNTK、PyTorch、Caffe2。用户能够在不同的设备上运行这些框架： 英特尔架构、GPU 和 英特尔 Nervana 神经网络处理器（NNP）。 为什么建立 nGraph 当深度学习框架作为模型训练和推断的工具首次出现时，在设计上是围绕 kernel 为特定设备优化。结果，把深度学习模型部署到其它更先进的设备时，会在模型定义暴露出许多细节问题，从而限制了其适应性和可移植性。 使用传统的方法意味着算法开发者面临把模型升级到其他设备时的沉闷工作。使一个模型能够在不同框架上运行也非常困难，因为开发者必须把模型的本质从对设备的性能调整中分离出来，并转化到新框架中的相似运算，最终在新框架上为优选的设备配置做必要的改变。 我们设计的 nGraph 库充分地减少了这些工程的复杂性。虽然通过该项目以及英特尔的 MKL-DNN 这样的库，能够为深度学习原语提供优化核，但仍有多种编译器启发式的方法能够带来进一步的优化。 nGraph 是如何工作的？ 安装 nGraph 库，并使用该库编写或编译一个框架来训练模型和执行模型推理。将 nGraph 指定为框架后端，以在任意支持的系统上用命令行运行该库。我们的中间表征（Intermediate Representation，IR）层可以处理所有的设备抽象细节，从而让开发者集中于数据科学、算法和模型的研究，不需要花费太多精力在写代码上。 从更加详细的角度来说： nGraph 核心创建了计算过程的一种强类型和设备无关的无状态图表征。图中的每一个节点或运算对应计算的一个步骤，其中每个步骤从 0 或更多张量的输入中生成 0 或更多张量的输出。我们的思想是 nGraph 运算可以作为深度学习框架中的复杂 DNN 操作的构建模块，且它能根据需要而衡量是高效编译和推导训练计算还是推断计算。 我们为每个支持的框架开发了框架桥梁（framework bridge）；它作为 nGraph 核心和框架之间的媒介起作用。目前我们已经开发了 TensorFlow/XLA、MXNet 和 ONNX 的框架桥梁。由于 ONNX 仅仅是一种交换格式，因此 ONNX 的桥梁将通过执行 API 进行增强。 在 nGraph 核心和多种设备之间工作的变换器有着类似的作用；变换器使用通用的和设备特定的图转换处理设备抽象。得到的结果是一个函数，可以从框架桥梁执行。变换器是可分配和可解除分配的，可按桥梁的方向读取和写入张量。我们目前已有英特尔架构、英特尔 NNP、英伟达 cuDNN 的变换器，并正积极开发着其它设备的变换器。 当前的性能 对于 Intel Architecture 上的框架的 MKL-DNN 优化，英特尔拥有大量的开发经验。我们借用了以前的工作带来的附加效益，使得通过 nGraph 为一个设备开发的优化方法可以为所有框架带来效益。框架开发者可以继续完善优化工作。例如，Intel Architecture 上的 TensorFlow 1.7+/XLA 的优化效果远远好于 Intel Architecture 上的 TensorFlow 1.3/XLA 的优化效果，因此随着更多工作投入到 Intel Architecture 上的 XLA，这种情况将会得到改善。 下图中展示了多个框架上的原始性能数据和优化性能数据，可以反映 nGraph 在 Intel Architecture 变换器上的优化带来的效益。在最新的 Intel Xeon Platinum 8180 处理器上，通过同时使用 MKLDNN v0.13，我们可以达到甚至超越之前已优化的框架的性能，例如 MXNet-MKLDNN-CPU（用 MKL-DNN 优化的 MXNet），以及 neon-MKLML-CPU（用 MKLML 优化的 neon）。我们还得到了比 TensorFlow XLA 编译器（TF-XLA-CPU）更好的性能，但在默认的 CPU 实现和 nGraph 上，还可以使用 XLA 做相当多的优化工作。 现状和未来工作 自今天起，nGraph 支持 6 个深度学习框架和三个计算设备。 支持的框架： 1.通过 nGraph 的框架独立表征直接支持的框架： TensorFlow MXNet neon 2.通过 ONNX 间接支持的框架： CNTK PyTorch Caffe2 支持的计算设备： 英特尔架构 (x86、Intel® Xeon® 和 Xeon Phi®) Intel® Nervana™ 神经网络处理器 (Intel® Nervana NNP) 英伟达 cuDNN (进行中) 我们将继续支持更多的设备、更多的图优化（例如特定设备的运算融合）、更好的工作进度表以及在运算 kernel 上更快的自定义。 论文：Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning 论文链接：https://arxiv.org/abs/1801.08058 深度学习（DL）社区每年都会发布非常多的拓扑结构实现。而在每一个新的拓扑结构中实现高性能计算仍然是非常大的挑战，因为每一个结构都需要一定的人力调整。这个问题由于框架和硬件平台的激增而变得越发复杂。目前我们称之为「直接优化」的方法需要在每个框架上进行深入的修改以在每一个硬件后端（CPU、GPU、FPGA 和 ASIC）提升训练性能，且要求 O(fp) 的复杂度；其中 f 为框架的数量，p 为平台的数量。虽然深度学习基元的优化核可以通过 MKL-DNN 等库提供支持，但目前有几种编译器启发的方式能实现进一步的优化。基于我们构建 neon（GPU 上的快速深度学习库）的经验，我们开发了 Intel nGraph，即一个用于在跨框架和硬件平台间简化深度学习的性能优化过程的开源 C++库。 该工具最初支持的框架包含 TensorFlow、MXNet 和 Intel neon 框架，最初支持的后端为 Intel Architecture 的 CPU、Intel(R) Nervana 神经网络处理器（NNP）和英伟达 GPU。目前支持的编译器优化包括高效内存管理和数据布局提取。在本论文中，我们描述了该工具的整体架构与核心组件。未来，我们希望 nGraph 的 API 支持扩展到更广泛的框架、硬件（包括 FPGA 和 ASIC）和编译器优化（训练 vs 推断优化、通过高效子图分割的多节点和多设备扩展、特定硬件的复合操作）。 "
97,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739632&idx=1&sn=6a5a2ab78854a9dc181e78d1a797a9ee&chksm=871ad7ceb06d5ed83a710e2677c7dace6c5ada697d10d842ae65d50bea471b4a29f6e2094bd7&scene=27,杨瓞仁教授代理港科大CSE系主任，杨强教授期满卸任,  据机器之心了解，原香港科技大学计算机科学与工程系主任杨强教授任职到期。2018 年 1 月份起，港科大 CSE 系由系里的杨瓞仁 (Dit-Yan Yeung) 教授代理系主任的职务。 从今年年初开始，机器之心观察到在香港科技大学计算机科学及工程学系的主页上，系主任已由杨强变更为杨瓞仁教授，杨强教授保留着讲座教授的职称。 而到三月份，杨瓞仁教授正式发布了公开信，宣布担任 CSE 系的代理系主任： 杨瓞仁（Dit-Yan Yeung）教授本科毕业于香港大学，1989 年于南加州大学（USC）获得计算机科学博士学位，曾任伊利诺伊理工大学（IIT）助理教授，后前来香港科技大学任教。杨瓞仁教授主要的研究方向是机器学习和计算机视觉。 以下为公开信内容（内容摘自公众号「清水湾区」）： 大家好！ 从今年 1 月份起，我非常有幸能够担任 CSE 系的代理系主任，为大家服务。我在港科大的 CSE 系已经工作了 20 多年了。我非常热爱港科大，特别是 CSE 系。上世纪 90 年代时，港科大还是一所鲜为人知的新的大学。当时我便放弃了在美国的教职而选择了来到港科大，我能深深地感觉到，我的选择是十分正确的。 CSE 系在前二十年里取得的成就一直被同行视为奇迹。数据显示，港科大 CSE 系在全球高校计算机系排名中位于前列。虽然我不是十分看重排名，但这些数据确确实实地表现出了我们的努力和成绩，让我们在短短二十年内便能名声大噪。 还记得当时吸引我来到港科大的，是港科大创始者们所描述的创校愿景：为香港、中国乃至亚太地区带来深远的社会影响。在现有的评判标准之外，我们应该问自己一个问题：如何在那些难以量化的方面提升我们的影响力。我认为以下几点值得我们关注： 1. 研究和教育之间的相互作用 大学，包括港科大这样的研究型大学，与研究机构有着本质上的差异。研究和教育是大学的核心，高质量的研究和教育相辅相成。因此，全球顶尖大学往往能同时在研究和教育、教学创新方面都表现得相当出色。实际上，大多数杰出的研究人员同时也是优秀的教师，他们能够激励并改变许多年轻人的人生。作为研究生我们必须懂得，做研究和发表论文只是漫长旅程的开端，在会议、或是面向公众发布研究报告都将更为重要。要做好这一点，我们必须重视提升在面对不同类型听众（无论专业与否）时的演讲技能。此外，无论是写论文还是准备课堂幻灯片，都绝不能马虎，需要像对待艺术品一样对待自己的任何作品，哪怕在格式或拼写错误等小缺陷上也丝毫不能怠慢。我们要努力成为一名学者，而不仅仅是一名研究人员，更不是一台写论文的机器。 2. 基础与应用研究之间的相互作用 基础研究和应用研究是互相依存的，尤其是在计算机科学领域。在稍大点的学系中，难免有人对基础研究更感兴趣，而另一些人更喜欢应用研究。虽然尊重个人偏好很重要，但学系也应该提供一个良好的学术环境，鼓励所有研究人员（包括教师和学生）在基础、应用研究方面加强合作，共同解决长期的重大挑战。我知道这说起来容易做起来难，但只有在意识上达成共识，我们才能有所成就。 3. 戒除从众心理 从众心理（羊群心理）十分普遍，特别是在学术领域。当决定选择什么专业或者研究领域时，许多学生会蜂拥到那些热门的领域。作为在机器学习领域工作了 20 多年的研究者，经历了这个领域的风风雨雨后，我对这方面已经十分淡然了。大多数研究者扎堆选择了少数研究领域，导致在其他领域工作的人变少，这对计算机系的发展是十分不利的。虽然大学或学系有时确实有必要更多地侧重一些领域，但我们需要非常小心，不能以牺牲在其他领域的投入作为代价。在这之间找到平衡点，不仅仅是对系主任的挑战，也是所有 CSE 系的成员们需要考虑的问题。 最后祝大家在 2018 年狗年里，万象更新，收获满满！ DY 2018 年 3 月 此外，目前港科大计算机科学及工程系的官网上已经出现了系主任的招聘广告，CSE 系正面向全球招聘新一任的系主任。 图注：在 2017 年 5 月底举行的机器之心 GMIS 全球机器智能峰会上，杨强教授做了《迁移学习的挑战和六大突破点》为主题的演讲。 除刚刚卸任港科大系主任职位之外，杨强还是第四范式的联合创始人、首席科学家。杨强教授在人工智能研究领域深耕三十年，是国际公认的人工智能全球顶级学者，ACM 杰出科学家，2017 ACM Fellow，两届「KDD Cup」冠军。 杨强是首位美国人工智能协会（AAAI）华人院士，AAAI 执行委员会唯一的华人委员，国际顶级学术会议 KDD、IJCAI 等大会主席，IEEE 大数据期刊等国际顶级学术期刊主编。杨强教授在数据挖掘、人工智能、终身机器学习和智能规划等研究领域都有卓越的贡献，是迁移学习领域的奠基人和开拓者，他发表论文 400 余篇，论文被引用次数超过 36000 次。 
98,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739594&idx=2&sn=633509b73be305f2a89214b510ac7328&chksm=871ad7f4b06d5ee2f37bee744e7cb0b994d4704a4fd5ef301998312a4305983a33952b39d823&scene=27,观点 | 为什么深度学习仍未取代传统的计算机视觉技术？,zbigatron 本文作者认为，深度学习只是一种计算机视觉工具，而不是包治百病的良药，不要因为流行就一味地使用它。传统的计算机视觉技术仍然可以大显身手，了解它们可以为你省去很多的时间和烦恼；并且掌握传统计算机视觉确实可以让你在深度学习方面做得更好。这是因为你可以更好地理解深度学习的内部状况，并可执行预处理步骤改善深度学习结果。 本文的灵感同样来自论坛中的一个常见问题： 深度学习已经取代了传统的计算机视觉吗？ 或是换种说法： 既然深度学习看起来如此有效，是否还有必要学习传统的计算机视觉技术？ 这个问题很好。深度学习确实给计算机视觉和人工智能领域带来了革命性的突破。许多曾经看似困难的问题，现在机器可以比解决的比人类还好。图像分类就是最好的印证。确实，如从前所述，深度学习有责任将计算机视觉纳入行业版图。 但深度学习仍然只是计算机视觉的一个工具，且显然不是解决所有问题的灵丹妙药。因此，本文会对此进行详细阐述。也就是说，我将说明传统的计算机视觉技术为何仍十分有用，值得我们继续学习并传授下去。 本文分为以下几个部分/论点： 深度学习需要大数据 深度学习有时会做过了头 传统计算机视觉将会提升你的深度学习水平 进入正文之前，我认为有必要详细解释一下什么是「传统计算机视觉」，什么是深度学习，及其革命性。 背景知识 在深度学习出现以前，如果你有一项诸如图像分类的工作，你会进行一步叫做「特征提取」的处理。所谓「特征」就是图像中「有趣的」、描述性的、或是提供信息的小部分。你会应用我在本文中称之为的「传统计算机视觉技术」的组合来寻找这些特征，包括边缘检测、角点检测、对象检测等等。 在使用这些与特征提取和图像分类相关的技术时，会从一类对象（例如：椅子、马等等）的图像中提取出尽可能多的特征，并将其视为这类对象的「定义」（称作「词袋」）。接下来你要在其它图像中搜索这些「定义」。如果在另一个图像中存在着词袋中相当一部分的特征，那么这个图像就被归为包含那个特定对象（如椅子、马等等）的分类。 这种图像分类的特征提取方法的难点在于你必须在每张图像中选择寻找哪些特征。随着你试图区分的类别数目开始增长，比如说超过 10 或 20，这就会变得非常麻烦甚至难以实现。你要寻找角点？边缘？还是纹理信息？不同类别的对象最好要用不同种类型的特征来描述。如果你选择使用很多的特征，你就不得不处理海量的参数，而且还需要自己来微调。 深度学习引入了「端到端学习」这一概念，（简而言之）让机器在每个特定类别的对象中学习寻找特征，即最具描述性、最突出的特征。换句话说，让神经网络去发现各种类型图像中的潜在模式。 因此，借助端到端学习，你不再需要手动决定采用哪种传统机器视觉技术来描述特征。机器为你做好了这一切。《连线》杂志如此写道： 举例来说，如果你想教会一个 [深度] 神经网络识别一只猫，你不必告诉它去寻找胡须、耳朵、毛或是眼睛。你只需展示给它成千上万的猫的图像，它自然会解决这一问题。如果它总是会将狐狸误认为是猫，你也不用重写代码。你只需对它继续进行训练。 下图描述了特征提取（使用传统计算机视觉）和端到端学习之间的这种区别： 以上就是背景介绍。现在接着讨论为什么传统计算机视觉仍然必不可少，而且学习它仍大有裨益。 深度学习需要大量数据 首先，深度学习需要数据，许许多多的数据。前文提到过的著名图像分类模型的训练都基于庞大的数据集。排名前三的训练数据集分别是： ImageNet——150 万图像，1000 个对象分类/类别； COCO——250 万图像，91 个对象分类； PASCAL VOC——50 万图像，20 个对象分类。 但是一个训练不良的模型在你的训练数据之外很可能表现糟糕，因为机器并没有对于问题的洞察力，也就不能在没看到数据的情况下进行概括归纳。而且对你来说查看训练模型内部并进行手动调整又太过困难，因为一个深度学习模型内部拥有数以百万计的参数——每个参数在训练期间都会被调整。某种程度上说，一个深度学习模型就是一个黑箱。 传统的计算机视觉完全透明，允许你更好地评估判断你的解决方案是否在训练环境之外依然有效。你对问题的深入见解可以放进你的算法之中。并且如果任何地方出现故障，你也可以更轻易地弄清楚什么需要调整，在哪里调整。 深度学习有时做过了头 这大概是我最喜欢的支持研究传统计算机视觉技术的理由。 训练一个深度神经网络需要很长的时间。你需要专门的硬件（例如高性能 GPU）训练最新、最先进的图像分类模型。你想在自己还不错的笔记本上训练？去度个一周的假吧，等你回来的时候训练很可能仍未完成。 此外，如果你的训练模型表现不佳呢？你不得不返回原点，用不同的训练参数重做全部工作。这一过程可能会重复数百次。 但有时候所有这些完全没必要。因为传统计算机视觉技术可以比深度学习更有效率地解决问题，而且使用的代码更少。例如，我曾经参与的一个项目是检查每个通过传送带的罐子里是否有一个红勺子。现在你可以通过前文叙述的旷日持久的过程来训练一个深度神经网络去检测勺子，或者你也可以写一个简单的以红色为阈值的算法（将任何带有一定范围红色的像素都标记为白色，所有其它的像素标记为黑色），然后计算有多少白色的像素。简简单单，一个小时就可以搞定！ 掌握传统的计算机视觉技术可能会为你节省大量的时间并减少不必要的烦恼。 传统计算机视觉会提升你的深度学习技巧 理解传统的计算机视觉实际上能帮你在深度学习上做得更好。 举例来说，计算机视觉领域最为普遍使用的神经网络是卷积神经网络。但什么是卷积？卷积事实上是一种被广泛使用的图像处理技术（比如，索贝尔边缘检测）。了解这一点可以帮助你理解神经网络内部究竟发生了什么，从而进行设计和微调以更好地解决你的问题。 还有一件事叫做预处理。你输入给模型的数据往往要经过这种处理，以便为接下来的训练做准备。这些预处理步骤主要是通过传统的计算机视觉技术完成的。例如，如果你没有足够的训练数据，你可以进行一个叫做数据增强的处理。数据增强是指对你训练数据集中的图像进行随机的旋转、移动、裁剪等，从而创造出「新」图像。通过执行这些计算机视觉操作，可以极大地增加你的训练数据量。 结论 本文阐述了为什么深度学习还没有取代传统计算机视觉技术，以及后者仍值得学习和传授。首先，本文将目光放在了深度学习往往需要大量数据才能表现良好这一问题上。有时并不具备大量数据，而传统计算机视觉在这种情况下可作为一种替代方案。第二，深度学习针对特定的任务偶尔会做过头。在这些任务中，标准的计算机视觉比起深度学习可以更为高效地解决问题，并且使用更少的代码。第三，掌握传统计算机视觉确实可以让你在深度学习方面做得更好。这是因为你可以更好地理解深度学习的内部状况，并可执行预处理步骤改善深度学习结果。 总而言之，深度学习只是一种计算机视觉的工具，而不是包治百病的良药。不要因为流行就一味地使用它。传统的计算机视觉技术仍然可以大显身手，了解它们可以为你省去很多的时间和烦恼。 
99,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739594&idx=1&sn=65298815d4159aa3b524b772ed65737b&chksm=871ad7f4b06d5ee2300e6b51aa0e26b47282178b31e72b0a7f914a6ba5fd7bfd7ee1939a7e39&scene=27,重磅 | 刚刚，Uber发生全球首例自动驾驶致死事件,  刚刚，据国外众多媒体报道，当地时间星期日晚上十点，一辆 Uber 的自动驾驶汽车在亚利桑那州坦佩市的公共道路上与一名行人相撞，该行人在送往医院后不治身亡。这是世界上首起自动驾驶车辆在公共道路上撞击行人并致死的事件。 当地警方在一份邮件声明中表示，意外发生时，汽车处于自动驾驶模式，驾驶座上有一名人类安全驾驶员。 图注：据 abc 报道，事故发生时死者正推着自行车在非人行横道线区域横穿马路 警方同时确认，死者为 49 岁的女性 Elaine Herzberg，事件发生时该女性正在人行横道外的地方横穿马路。 事件发生后，Uber 已经暂停了在坦佩、匹兹堡、旧金山和多伦多等城市进行的自动驾驶汽车测试。CEO Dara Khosrowshahi 发推称，「亚利桑那州传来了一些令人难以置信的坏消息。在与当地执法部门合作以了解发生了什么的同时，我们的心与受害者的家人在一起。」 美国国家运输安全委员会表示，将派遣一个小组调查车祸事件。 事件发生后，在国外引起了轩然大波。媒体、社交网站纷纷报道、讨论。因为这是全球首起自动驾驶致死事故，人们对自动驾驶（甚至人工智能）技术的信任程度可能会产生更大的波动。 演员 James Woods 表示，「自动驾驶汽车只是政府追踪我们生活的另一种方式。我们需要自动驾驶汽车以便有更多的时间在方向盘后面发短信吗？我希望这个家庭起诉那些导致这起意外的人。」 一名大学生则发推称，「这很不幸，但自动驾驶汽车仍然是未来。即使有一个人死了（希望她安息），但是与每年死于人类驾驶的成千上万人相比，这只是一个人而已。如果我们真的那么害怕自动驾驶，那么我们也需要更加害怕人类驾驶员。」 此外，这并不是 Uber 自动驾驶首次事故。 去年，Uber 在自动驾驶汽车卷入一起事故中后，短暂地暂停了在 Arizona 的自动驾驶汽车测试。当时，Uber 的车辆在与另一车辆相撞后发生了侧翻。之后警方判定，Uber 的自动驾驶汽车没有任何故障，责任在与其相撞的车辆。 图来自坦佩市当地警方推特 在此之前，虽然特斯拉带有 Autopilot 功能的汽车及 Waymo 的自动驾驶汽车都曾卷入过不同程度的事故中，但美国各州对自动驾驶的态度都倾向于包容和有监管的开放。今年，有不少州开始允许公司在驾驶员不在场的情况下测试自动驾驶汽车。 就在本月，加州政府表示将在 4 月开始允许公司在没有人驾驶的情况下测试自动驾驶汽车。 而亚利桑那州一直对这一情况持「放任自流」乃至「开门欢迎」的状态，科技公司也因此纷纷前往亚利桑那州测试其自动驾驶汽车。自去年年底以来，Waymo 的无驾驶员自动驾驶汽车就一直在亚利桑州的公开道路上进行测试。 
100,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739530&idx=2&sn=38dc50f78015e488db58ed390a07b31f&chksm=871ad734b06d5e2229b5e16ac2c4d1178450736bc23cd09a8bf83d570c204616661d494057b2&scene=27,业界 | 一文概览2017年Facebook AI Research的计算机视觉研究进展,"skrish13    本文概述了 2017年Facebook AI 研究院（FAIR）在计算机视觉上的研究进展，包括基础构建模块创新、CNN、one shot 目标检测模块等，然后介绍实例分割方面的创新，最后介绍用弱半监督方式来扩展实例分割。 首先我们先来介绍著名的  [1]（发表在 CVPR 2017 上）。FPN 论文非常棒，要知道，构建一个可在多项任务、子主题和应用领域中使用的基线模型并不简单。FPN 是通用特征提取网络（如 ResNet、DenseNet）的扩展。你可以从喜欢的 DL 库中选择预训练的 FPN 模型，然后像使用其他模型一样使用它！ 图像目标通常有多个尺度和大小。一般的数据集无法捕捉所有这些属性，因此人们使用图像金字塔（图像的多种分辨率降级），以方便 CNN 处理。但是这样很慢。因此人们使用单个尺度预测，一些人可能从中间层获取预测结果，它是在特征空间中进行的。这很好理解，在几个 ResNet 模块后放置一个反卷积层，获取分割输出（就分类而言，可能是 1x1 卷积和 GlobalPool）。现在大量此类架构在有辅助信息和辅助损失的情况下使用。 回到主题，FPN 的作者发现一种有效改善上述方法的方式。他们不只使用侧向连接，还使用自上而下的路径。这非常有效！他们使用一个简单的 MergeLayer（mode=『addition』）将二者结合起来。该想法的关键是底层特征图（初始卷积层）的语义不够强，无法用于分类。而深层特征图有更强的语义。这里还有一个优势，即自上而下的路径 FMaps（特征图），可以达到最深层网络同样的效果。这是因为结合了侧向连接和自上而下的连接。 细节 金字塔：同样大小的所有特征图属于同一个阶段。最后一层的输出是金字塔的 reference FMaps。如：ResNet，第 2、3、4、5 个模块的输出。你可以根据内存和特定使用情况改变金字塔。 侧向连接：1x1 卷积和自上而下的路径都经过两倍上采样。上层特征自上而下生成粗糙的特征，而侧向连接从自下而上的路径添加更细粒度的细节。 那篇论文中介绍了一个简单的 demo，展示了这个想法结合简单的设计思路的效果。 如前所述，这是一个可以在多项任务中使用的基线模型，如目标检测、分割、姿态估计、人脸检测，以及所有应用领域。这篇论文问世不过寥寥数月，但引用量已经超过 100！这篇论文的题目是《FPNs for Object Detection》，因此作者继续在 RPN（区域建议网络）和 Faster-RCNN 网络中使用 FPN 作为基线模型。所有关键细节这篇论文中都有讲述，这里我列出了一些要点。 实验要点 在 RPN 中使用 FPN：将单个尺度 FMap 替换成 FPN。他们在每一级都有单尺度 anchor（由于使用了 FPN，所以没必要使用多尺度 anchor）。作者还展示了金字塔所有层级都共享类似的语义水平。 Faster RCNN：他们用类似图像金字塔输出的方式处理这个金字塔。因此 RoI 使用以下公式被分配至特定的级别： 其中 w、h 代表宽度、高度。k 代表 RoI 被分配的级别。k_0 是 w,h=224,224 映射至的级别。 在 COCO 数据集上获取当前最优结果，没有任何不必要的功能。 他们对每个模块的功能进行了 ablation 研究，因此他们能够从一开始就证明了自己的说法。 他们还基于 DeepMask 和 SharpMask 论文展示了如何使用 FPN 进行分割建议生成（segmentation proposal generation）。 对实现细节、实验设置等感兴趣的同学应该认真阅读这篇论文。 代码  Official Caffe2 - https://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselines Caffe - https://github.com/unsky/FPN PyTorch - https://github.com/kuangliu/pytorch-fpn (just the network) MXNet - https://github.com/unsky/FPN-mxnet Tensorflow - https://github.com/yangxue0827/FPN_Tensorflow 该架构由同一个团队所开发，也是同一个一作。这篇论文 [2] 在 ICCV 2017 上发表。该论文中有两个关键点：通用损失函数 Focal Loss（FL）和单阶段的目标检测器 RetinaNet。两者的组合使其在 COCO 目标检测任务中表现得非常好，并打败了上述的 FPN 基准结果。 Focal Loss 该论文中的方法相当聪明和简单。如果你熟悉加权损失的话，那么该方法其实就是使用了巧妙的加权，让训练更加聚焦于分类难度高的样本。公式如下所示，其含义是很明显的。 γ是一个可改变的超参数。p_t 是分类器输出的样本概率。将 γ 设为大于 0 将减小分类结果较好的样本的权重。α_t 是通常的加权损失函数中的类别权重。在论文中它被称为 α-balanced 损失。需要注意，这个是分类损失，它将和 smooth L1 损失结合，用于 RetinaNet 的目标检测任务。 RetinaNet FAIR 竟然会发布单阶段检测器，这令人难以置信。直到目前，YOLOv2 和 SSD 仍在单阶段场景中占据主导地位。但作者指出，它们都没有接近当前最佳的结果。而 RetinaNet 可以轻松地做到，仅使用了单阶段，并且速度很快。他们称其顶尖结果源于新型损失函数的应用，而不是简单的网络（其后端为 FPN）。其中利用的思想是单阶段检测器将面临很多背景和正类别不平衡的情况（不是正类别之间的不平衡）。作者称加权损失函数仅仅是为了平衡，而 FL 是针对难度小/大的样本，而两者是可以结合的。 注意： 两阶段检测器无需担心不平衡的情况，因为第一阶段就移除了几乎所有不平衡。 两部分：主干网络（卷积特征提取器，如 FPN）和两个特定任务的子网络（分类器和边界框回归器）。 在选择不同的组件设计时，（性能）没有太多变化。 Anchor 或 AnchorBoxes 是 RPN 的相同 Anchor [5]。其中心围绕滑动窗口，且与长宽比有关。大小和长宽比分别对应 32^2 到 51^2 和 {1:2, 1:1, 2:1}。 在 FPN 的每一阶段，我们都有 cls+bbox 子网络，给出 anchor 中所有位置的对应输出。 代码  Official Caffe2 - https://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselines PyTorch - https://github.com/kuangliu/pytorch-retinanet Keras - https://github.com/fizyr/keras-retinanet MXNet - https://github.com/unsky/RetinaNet Mask R-CNN  [3] 几乎是同一个团队开发的，发表在 ICCV 2017 上，该模型用于实例分割。对于外行来说，这不过是不使用边界框的目标检测，任务是给出目标的准确分割掩码。你可以说它只是一个简单的想法，但是使之运行并成为当前最佳，为预训练模型提供最快的实现，这可是惊人的工作！ TL;DR：如果你了解 Faster-RCNN，那么 Mask R-CNN 很简单，只需要添加一个用于分割的 head（分支）。因此基本上是 3 个分支，分别用于分类、边界框回归和分割。 再次强调，其重点在于使用简单、基础的网络设计来展示该方法的效率。他们不需要什么花哨的功能就实现了当前最佳。 我很喜欢这篇论文，它很简单，但是看似简单的东西有大量解释。例如，多项式掩码 vs 独立掩码的使用（softmax vs sigmoid）。此外，它并未假设大量先验知识，也没有要解释一切。 如果你仔细查看这篇论文，你可能会找到他们（基于现有设置）的新想法无法有效运行的原因。以下解释基于你对 Faster RCNN 已经有了基础了解： Mask R-CNN 与 FasterRCNN 类似，两阶段，第一阶段是 RPN。 添加一个并行分支用于预测分割掩码——FCN。 损失是 L_cls、L_box、L_maskLcls、L_box、L_mask 的总和。 用 ROIAlign 层替换 ROIPool。这不像 ROIPool 那样将你的（x/spatial_scale）fraction 四舍五入成整数，而是通过双线性内插法找出特定浮点值的像素。 例如：ROI 高度和宽度分别是 54、167。空间尺度基本上是图像大小／FMap 大小（H/h），在此语境中也叫作 stride。通常是 224/14 = 16 (H=224,h=14)。 ROIPool: 54/16, 167/16 = 3,10 ROIAlign: 54/16, 167/16 = 3.375, 10.4375 现在，我们使用双线性内插法对其进行上采样。 类似的逻辑适用于根据 ROIAlign 输出形状（如 7x7）将对应的区域分割成合适的子区域。 使用 Chainer folks 检查 ROIPooling 的 Python 实现，并尝试自己实现 ROIAlign。 ROIAlign 代码可在不同库中获取，可查看下面给出的代码 repo。 主干网络是 ResNet-FPN。 我曾专门写文章介绍过 Mask-RCNN，文章地址：https://coming.soon/。 代码  Official Caffe2 - https://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselines Keras - https://github.com/matterport/Mask_RCNN/ PyTorch - https://github.com/soeaver/Pytorch_Mask_RCNN/ MXNet - https://github.com/TuSimple/mx-maskrcnn 如题目《 》所示，这篇论文是关于分割的，具体来说，是实例分割问题。计算机视觉中的标准分割数据集对于现实应用而言规模太小。即使是 2018 年最流行、最丰富的 COCO 数据集 [7] 也仅有 80 个目标类别。这根本无法达到实用的要求。相比之下，目标识别和目标检测数据集如 OpenImages [8] 就有将近 6000 个分类类别和 545 个检测类别。除此之外，来自斯坦福的另一个数据集 Visual Genome 拥有 3000 个目标类别。那为什么人们不选用这些数据集呢？因为每个类别中包含的目标数量太少了，从而 DNN 无法在这个数据集上取得足够好的性能，即使这些数据集在实际应用上更加丰富、有用。需要注意的是该数据集并没有任何分割标注，仅有 3000 个目标检测（边界框）标签类别。 接下来介绍论文 [4]。实际上，就领域而言，边界框和分割标注之间并不存在太大的区别，区别仅在于后者比前者更加精确。因此既然 Visual Genome [9] 数据集中有 3000 个类别，那么为什么不用来训练分割任务呢？这正是作者所做的，这种方法可称为弱监督学习（或弱半监督）学习，其中不需要相关任务的完整监督信息。如果他们使用的是 COCO+Visual Genome 数据集（即分割标签 + 边界框标签），这同样可称为半监督学习。 这篇论文简直不能再 cool，其网络架构有如下特点： 它建立在 Mask-RCNN 之上。 同时使用有掩码和无掩码的输入进行训练。 在掩码和边界框掩码之间添加了一个权重迁移函数。 当传递了一个没有掩码的输入时，将 ω_seg 函数预测的权重和掩码特征相乘。当传递了一个有掩码的输入时，则不使用该函数，而使用一个简单的 MLP。 如下图所示：A 是 COCO 数据集，B 是 VG 数据集。注意其中对不同输入的不同训练路径。 将两个损失同时进行反向传播将导致 ω_seg 的权重不一致，因为对于 COCO 和 VG 之间的共有类别，有两个损失（掩码和边界框），而对于非共有类别，则仅有一个损失（边界框）。作者使用的修改方法是： Fix：当反向传播掩码损失时，计算预测掩码权重 (τ) 关于权重迁移函数参数θ的梯度，而对边界框权重ω^c_det 不进行该计算。 w^c_seg=τ(stop_grad(w^c_seg); θ)，其中 τ 预测掩码权重。 由于 VG 数据集没有分割标注，从而无法给出在该数据集上的分割准确率。因此他们在可验证的数据集上展示结果。PASCAL-VOC 数据集有 20 个类别，全部包含于 COCO 数据集。因此，对于这 20 个类别，他们使用 VOC 的分割标注和 COCO 中这 20 个类别的边界框标签进行训练。论文展示了在 COCO 数据集中这 20 个类别上的实例分割任务结果。此外由于两个数据集包含两种真实标注，他们还对相反的情况进行了训练。结果如下图所示。 [1] Lin, Tsung-Yi, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan and Serge J. Belongie.「Feature Pyramid Networks for Object Detection.」*2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)* (2017): 936-944. [2] Lin, Tsung-Yi, Priya Goyal, Ross B. Girshick, Kaiming He and Piotr Dollár.「Focal Loss for Dense Object Detection.」*2017 IEEE International Conference on Computer Vision (ICCV)* (2017): 2999-3007. [3] He, Kaiming, Georgia Gkioxari, Piotr Dollár and Ross B. Girshick.「Mask R-CNN.」*2017 IEEE International Conference on Computer Vision (ICCV)* (2017): 2980-2988. [4] Hu, Ronghang, Piotr Dollár, Kaiming He, Trevor Darrell and Ross B. Girshick.「Learning to Segment Every Thing.」*CoRR*abs/1711.10370 (2017): n. pag. [5] Ren, Shaoqing, Kaiming He, Ross B. Girshick and Jian Sun.「Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.」*IEEE Transactions on Pattern Analysis and Machine Intelligence* 39 (2015): 1137-1149. [6] Chollet, François.「Xception: Deep Learning with Depthwise Separable Convolutions.」2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 1800-1807. [7] Lin, Tsung-Yi, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár and C. Lawrence Zitnick.「Microsoft COCO: Common Objects in Context.」ECCV (2014). [8] Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio et al. OpenImages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages [9] Krishna, Ranjay, Congcong Li, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, David A. Shamma, Michael S. Bernstein and Li Fei-Fei.「Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.」International Journal of Computer Vision 123 (2016): 32-73. "
101,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739530&idx=4&sn=4b08a6f9253473da9ae2396ca78fae05&chksm=871ad734b06d5e22256df7c21d56d3ecace4170866f7868fcd252daf2848b7f56a890bc04c78&scene=27,学界 | 斯坦福大学&DeepMind联合提出机器人控制新方法，RL+IL端到端地学习视觉运动策略,"arXiv   近日，来自斯坦福大学&DeepMind 的研究者提出一种学习机器人深度视觉运动策略的新方法，它结合强化学习和模仿学习来实现高效的强化学习智能体，该方法可解决大量视觉运动任务。实验证明该智能体性能显著优于仅使用强化学习或模仿学习训练出的智能体。 近期深度强化学习在多个领域取得了很好的表现，如视频游戏 [29] 和围棋 [46]。对于机器人，RL 结合强大的函数逼近器（如神经网络）可提供设计复杂控制器的通用框架，而这种控制器很难靠人力搭建。基于强化学习的方法在机器人控制领域历史很久，但通常与低维动作表示结合使用 [4, 20]。近年来，使用 model-based 和 model-free 技术的深度强化学习在机器人控制方面取得了大量成功案例，包括模拟和在硬件上运行两方面。然而，使用 model-free 技术端到端地学习视觉运动控制器来执行长跨度、多阶段控制任务仍然存在很大难度。 开发 RL 机器人智能体需要克服多项挑战。机器人策略必须将从带噪传感器中得到的多模态、部分观测数据改变为具备一定自由度的协作活动。同时，现实任务通常具备富接触动态，并且随着多个维度发生变化（视觉外观、位置、形状等），给泛化带来了很大挑战。 本论文中，研究者提出一种 model-free 的深度 RL 方法，直接从像素输入入手解决大量机器人控制任务。本论文关键洞察有：1）利用少量人类演示数据减少在连续域执行探索（exploration）的难度；2）在训练过程中使用多种新技术（这些技术利用（exploit）了私有和任务特定的信息），以加速和稳定视觉运动策略在多阶段任务中的学习；3）通过增加训练条件的多样性来改善泛化性能。因此，这些策略在系统动态、目标外观、任务长度等发生显著变化的情况下仍然运行良好。 此外，研究者还展示了该方法在两项任务上的初步结果，仿真训练出的策略达到了向真实机器人的 zero-shot 迁移。 为了解决这些挑战，本论文研究者提出的方法将模仿学习和强化学习结合起来，构建一个统一训练框架。该方法以两种方式利用演示数据：使用混合奖励，基于生成对抗模仿学习将任务奖励与模仿奖励结合起来；使用演示轨迹构建状态的课程（curriculum），以在训练中初始化 episode。因此，该方法解决了全部六个任务，这些任务单凭强化学习或模仿学习都无法解决。 3. 模型 本研究旨在为机器人控制任务学习深度视觉运动策略。该策略使用 RGB 摄像头观测结果和本体特征（proprioceptive feature）向量描述关节位置和角速度。这两种感官模态在真实机器人上同样可用，因此研究者进行仿真训练，并将习得的策略在不修改的情况下迁移至机器人。图 2 是模型概览。深度视觉运动策略使用卷积神经网络编码观测像素，使用多层感知机编码本体特征。这两种模块中的特征被级联并传输至循环 LSTM 层，然后输出关节速度（控制）。整个网络以端到端的方式进行训练。研究者首先简要介绍了生成对抗模仿学习（GAIL）和近端策略优化（PPO）的基础知识。该模型基于这两种方法扩展而来，提升视觉运动技能。 4. 实验 本节展示了该方法可向视觉运动策略学习提供灵活的框架。研究者在六个控制任务中对该方法的性能进行了评估（详见图 3）。视频包含了定性结果。 研究者将 episode 平均返回值表示为训练迭代次数的函数，如图 4 所示。完整模型在六项任务中达到了最高的返回值。 在图 5a 中，研究者用多种配置训练智能体，从单个修改到修改整个模型。研究者发现这些缺失可分为两类：学习堆叠的智能体（平均返回值大于 400）和仅学会提升的智能体（平均返回值在 200 和 300 之间）。结果表明从状态中学习价值函数的混合 RL/IL 奖励和以目标为中心的鉴别器特征在学习优良策略的过程中发挥了重要作用。 图 5b 展示了该模型在 λ 值在 0.3 到 0.7 之间的运行效果，λ 值的大幅变化提供了 RL 和 GAIL 奖励的平衡混合。 论文：Reinforcement and Imitation Learning for Diverse Visuomotor Skills 论文地址：https://arxiv.org/pdf/1802.09564.pdf 摘要：我们提出了一种 model-free 的深度强化学习方法，该方法利用少量人类演示数据帮助实现强化学习智能体。我们将该方法应用于机器人控制任务中，并训练端到端的视觉运动策略，使 RGB 摄像头输入可直接映射至关节速度。我们展示了该方法可解决大量视觉运动任务，而工程实现一个脚本控制器非常耗时费力。我们的实验表明强化学习和模仿学习智能体性能显著优于仅使用强化学习或模仿学习训练出的智能体。我们还介绍了这些策略，它们在视觉和动态条件变动较大的情况下进行训练，在 sim2real 迁移中取得了初步成功。本研究相关视频介绍：https://www.youtube.com/watch?v=EDl8SQUNjj0&feature=youtu.be。 "
102,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739530&idx=3&sn=32bdfdd26a53210ea78097359f67fb65&chksm=871ad734b06d5e22029292b0a38f3a2c5e46b8c9f55d79865782daf7441eace6ea336831a7a8&scene=27,专栏 | 百度深度学习平台PaddlePaddle框架解析,"PaddlePaddle 是 2016 年 8 月底百度开源的深度学习平台，并且在短时间内迅速引发全球开发热度，成为 Github Pull Request 数量增速极高的开源深度学习平台之一。如今，机器之心联合百度推出 PaddlePaddle 专栏，为想要学习这一平台的技术人员推荐相关教程与资源。 PaddlePaddle 的迭代速度非常快，同时也广受社区的关注。刚开源的时候，PaddlePaddle 的设计思想是基于 Layer 的设计。后来推出了「v2」和「Fluid」两次迭代：其中 v2 增加了 operators 的概念，把 layers「打碎」成更细粒度的 operators，同时支持更复杂的网络拓扑「图」；Fluid 类似 PyTorch，但是不依赖 Python 的控制流（if-else、for 等），而是提供自己的解释器甚至编译器，因此不受限于 Python 的执行速度。 我们今天就从 PaddlePaddleFluid 讲起，随后讲述为了能更方便利用集群而提供的在浏览器端训练的 PaddlePaddleCloud，也着重讲解集群训练的原理、配置、和实验结果，也就是 PaddlePaddleEDL 部分。最后，讲解 PaddlePaddleVisualDL，这个非常强大的训练日志解析和可视化工具。 PaddlePaddleFluid 提供类似于高级语言中的控制流结构（例如 while 、 if 、if-else、 for 等），不仅能利用编译优化技术保证计算性能，提升使用者的开发效率。 PaddlePaddleFluid 的设计思路非常领先，不再沿用层（layer）结构和操作（operator）结构的模式。也就是说不再有「模型」的概念，也就不再有「图」（graph of operators）或者「串」（sequence of layers）。而只有「程序」的概念。同时，程序是可以导出一个图的，从程序中可以导出成 ONNX 文件格式的模型。 深度学习基础架构是最快速发展的技术之一，在四年之内，已经发明了三代技术。从下表可以看出，深度学习技术架构设计的方向正是逐渐摆脱模型的。 基于 Python 语言强大的生态，PyTorch 和 Eager Execution 中的控制流都是用的 Python，但面临的一个瓶颈是 Python 执行速度慢且难以提速。解决 PyTorch 和 Eager Execution 程序的执行速度受限于 Python 的执行速度的问题，Fluid 有一个比 PyTorch 和 Eager Execution 更激进的技术思路。在 Fluid 的设计上，执行会把编写的 Python 程序输出成一个 protobuf message，随后调用 Fluid 解释器（而不是 Python 解释器）来解释执行这个 protobuf message。Fluid 解释器极大地加快了执行图的速度。同时，在编译执行的方式上，通过写一个 transpiler 把 protobuf message 翻译成 C++ 程序，然后用 nvcc、icc、gcc 编译成二进制代码，可以直接运行在服务器和手机上。 PaddlePaddle 有一个 Web-based IDE，支持使用者在浏览器用 JupyterNotebook 编程来开发 AI 应用，随后可以把程序发送到云端（Kubernetes 集群上）调试或者运行，程序运行时的输出会实时地显示在浏览器里。这样使用者就不需要在个人电脑和集群等多个编程环境之间切换并且维护多个环境的版本和配置的一致性，极大地提升了工作效率。 PaddlePaddle EDL 对标的是 Google KubeFlow。PaddlePaddle EDL 通过与 Kubernetes 合作来实现弹性作业调度，是全球首个支持弹性作业调度的开源 AI 云解决方案。 尽管现在很多深度学习应用用一个几台机器的小集群就可以解决，但是 随着数据量的增加和 AI 应用场景的不断扩大，例如 Web scaled 应用（广告、搜索、推荐等），以及通过传感器采集海量数据的无人车，都需要大规模深度学习计算能力的。 这里主要为了解决深度学习面临的两大挑战。其一是需要大量的计算能力。研究室和公司经常构建由 SLURM，MPI 或 SGE 管理的 GPU 集群。这些集群要么运行一个提交的作业（假定它需要的比闲置的资源要少）或者将作业挂起一段难以预估的时间。但是这种方法有个缺点：在有 99 个可用节点和一个需要 100 个提交作业的任务时，作业必须等待而不能运行。 PaddlePaddle EDL 弹性调度体现在可以空闲的时候一个训练作业多用一些资源，忙碌的时候少用一些，但是资源的变化并不会导致作业失败；这是优于 KubeFlow 的特点之一。同时，EDL 也弹性调度其他作业（比如 Nginx、MySQL 等），从而极大地提升集群总体利用率。[2] 这样在公有云和私有云上的推广和部署时，就很容易节省几倍的机器，为公司一年节省的计算成本可以高达百万、甚至数百万美元。 另一个挑战是，工业用户倾向于将深度学习作业作为完整数据管道的子集阶段，例如日志采集器等。这种通用集群需要基于优先级的弹性调度。比如网络开销较高的时间段内深度学习任务少运行，在网络流量较低时优先进行深度学习任务。这就需要了解全局的情况，并协调与各种工作有关的进程的数量。 PaddlePaddleEDL 的测试实验 面对上述这两种挑战，PaddlePaddle 作业都可以轻松应对进程数量忽高忽低的变化。这里有 Fluid EDL 的两种测试用例： Kubernetes 集群只运行 PaddlePaddle 作业; 集群运行 PaddlePaddle 和 Nginx 作业。 在第一个测试中，我们开始了 20 个 PaddlePaddle 作业，间隔 10 秒。每个作业有 60 个 trainers 和 10 个参数服务进程，并将持续数小时。我们重复实验 20 次：关闭 Fluid EDL 10 次，打开 Fluid EDL 10 次。在下图中，实线对应于前 10 个实验，其余的是虚线。在图的上半部分，我们看到未处理作业的数量在没有 EDL 的情况下单调递增。但是，当 EDL 打开时，资源将平均分配给所有作业。Fluid EDL 杀死了一些现有的进程，为新的其他任务腾出空间，并在晚些时候任务开始运行。在这两种情况下，集群都被平等利用（见图的下半部分）。 在第二个测试中，每个实验都运行了 400 个 Nginx Pods，其优先级高于 6 个 PaddlePaddle 作业。最初，每个 PaddlePaddle 工作有 15 个 trainers 和 10 个参数服务。我们每 90 秒杀死 100 个 Nginx Pods，直到剩下 100 个，然后我们开始将 Nginx 工作的数量每 90 秒增加 100 个。下图的上半部分显示了这个过程。图中的中间显示，Fluid EDL 通过减少 Nginx Pods 来自动启动一些 PaddlePaddle 进程，并在稍后增加 Nginx Pods 来杀死 PaddlePaddle 进程。结果，该集群维持在 90％左右的利用率，如图所示。当 Fluid EDL 被关闭时，没有 PaddlePaddle 进程自动增加，并且利用率随着 Nginx Pods 数量的变化而波动。 那上面这种深度学习服务和其它云端服务共享计算资源的过程，以及各个任务的优先级动态地调整和伸缩的过程，从而充分地利用集群 CPU/GPU 是如何实现的呢？ EDL 和 HPA Horizontal Pod Autoscaling（HPA）是 Kubernetes 提供的一种弹性调度机制。它的设计出发点是通过公平分配计算资源给某一个单一的计算任务中的各个 Pod 来实现分布式系统资源针对单一任务的最优化利用。但我们的训练任务可能多种多样（语音、图像等）、部署时间有先有后，对资源的需求也不通，因此我们希望这种弹性调度机制能对每一种训练任务所需的系统资源有个全局的了解，然后按需分配。但目前 HPA controller 还没有实现。 同时，HPA 的弹性调度是针对同种类型的计算任务（homogenous computing task）下的 Pods。但深度学习系统里的计算节点和参数服务器往往是在不同类型的 Pods 里 的。 上述特有的需求导致使用 Kubernetes 的时候需要有特定的弹性调度解决方案，而不能直接采用 HPA。因此更好的解决方案是 PaddlePaddle EDL。 PaddlePaddleEDL 的具体设计和实现 1. 让 Kubernetes 支持定制的弹性调度机制 Kubernetes 本身就支持定制的资源管理机制。用户可以通过提交定制的 resource declaration file 和 controller file 来实现对某种 Pods 的弹性调度。以下图为例，这个 training_job.yaml 保证了 controller 会自动监管 pservers，并且保证它们的数量在 min-instance 和 max-instance 之间。 在 Kubernetes 集群上，这个定制的资源可以通过 kubectl create -f training_job.yaml 命令获得。接下来，我们需要有个定制的 training job controller 来调度这个资源。 定制的 training job controller 跑在一个 Pod 里，对集群资源有一个统一的了解，它通过 Kubernetes API 对集群资源起到监控和调度的作用。下图是 training job controller 配置文件的一个例子。 在 Kubernetes 集群上，这个定制的资源管理 Pod 可以通过 kubectl create -f training_job_controller.yaml 命令启动。 2. 控制程序的实现 上面提到的定制化资源在 Kubernetes 里面目前有两种实现方式。一种是 Custom Resource Definition (CRD)，由 Kubernetes 1.7 版本引入;另一种是 Third Party Resource (TRP)。PaddlePaddle 项目现在用的是 Kubernetes 1.6 版本，所以实现的是 TRP 模式，今后将整合 CRD 模式。 当前 PaddlePaddle 假设只有一个单独的 training job controller 在运行。当前的 training job controller 依照下面的逻辑管理资源: 3. 弹性调度算法 PaddlePaddle 根据定制资源的配置文件（training_job.yaml）来判断某个 job 需不需要弹性调度，而判断的标准是 trainer 和 pserver 的 min-instance =/ max-instance。 集群中 GPU 的调度 controller 知道集群中全部 GPU 个数，以及当前闲置的 GPU 的个数，并试图把闲置的 GPU 全部分配给当前训练任务。PaddlePaddle 给需求 GPU 的训练任务定义一个「满足程度」的评分（fulfillment score），此评分的范围是 [0，1]。PaddlePaddle 会优先分配 GPU 资源给满足程度评分最低的训练任务。如果有分数相同的情况，则分别优先考虑 GPU 需求数，CPU 需求数，内存需求数。如果有某个训练任务的 GPU min-instance 没有满足（除非 cur-instance=min-instance），那么 PaddlePaddle 会把一个满足程度最高分的训练任务里的 GPU 资源拿出来分给它。如果满足程度分数最高的训练任务 cur-instance=min-instance，则整个集群不再执行新的训练任务，新来的任务需等待。 集群中 CPU 的调度 CPU 资源的分配和 GPU 思路相同。controller 知道集群中一共有多少个 CPU，内存，它们的负载情况；同时也知道训练任务对 CPU 的需求。同样的，CPU 资源根据满足程度评分被按需分配。 PaddlePaddle 容错机制 这里讨论 PaddlePaddle 的容错机制。在一个分布式训练任务里，如果 master 进程或者所有的参数服务进程都死掉了，那么整个训练任务会被停掉，过一段时间被 Kubernetes 整个重启。如果具体训练进程没有都死掉，则整个训练任务继续。 PaddlePaddle 用 etcd 来记录训练进程的状态。etcd 是高可靠性的分布式 key-value 存储，训练进程会定时把自身状态写进 etcd，而这些信息将会在必要的时候用来恢复训练进程。具体过程如下图： 当 master 进程被 Kubernetes 启动时，它进行如下操作: 1. 从 etcd 中取一个唯一的 master lock，以此避免多个 master 实例存在 2. 查看 etcd 中是否存在任务队列。如果不存在，则新建一个任务队列;否则得到这个任务队列中的信息 3. 把自身的 ip 地址写进 etcd 中/master/addr 这个 key 中，便于后来的训练进程和自己通信 4. 开端口监听训练进程的任务需求，如果收到来自训练进程的任务请求，从任务队列中取任务分配之，并且更新任务队列。 如果 master 进程因为任何原因死掉了，Kubernetes 会将它重启，从被重启到获取 etcd 的信息，获取训练进程的任务，这个过程一般是几分钟。 训练进程 当训练进程被 Kubernetes 启动时，它进行如下操作: 1. 查看 etcd 中包含参数服务前缀 /ps/ 获取当前参数服务进程的数量并等待，直到该数量达到配置文件中的要求 2. 从 etcd 的/master/addr key 中获取 master 进程地址 3. 向 master 发起任务请求，根据任务开始训练程序 当训练进程死掉之后，Kubernetes 会将它重启，新起来的进程会重复上述工作直到开始新的训练工作。 参数服务进程 当参数服务进程被 Kubernetes 启动时，它进行如下操作: 1. 从 etcd /ps_desired 中读取训练任务所需求的参数服务进程个数 2. 在 etcd /ps/<index>（/ps/0, /ps/1, ...）里找一个小于所需进程数里最大的还不存在的 id，并在 etcd 里创建这个 entry，以此作为自身的 id。（如下图） 当第三个参数服务器加入时： 3. 参数服务进程会从自身对应的 etcd path 中找到已有的训练结果参数并且将它读入 4. 参数服务进程开始接收来自训练进程的请求。 PaddlePaddleVisualDL 是 PaddlePaddle 自带的一个强大的可视化工具，也是一个 Web 应用程序套件。PaddlePaddleVisualDL 目前支持 4 种可视化，即 SCALARS、IMAGES、GRAPHS、HISTOGRAMS。这 4 种可视化的主要功能如下。 ● SCALARS：展示训练过程中的准确率、损失值、权重/偏置的变化情况。 ● IMAGES：展示训练过程中记录的图像。 ● GRAPHS：展示模型的数据流图，以及训练在各个设备上消耗的内存和时间。 ● HISTOGRAMS：展示训练过程中记录的数据的柱状图。 PaddlePaddleVisualDL 通过运行一个本地服务器，来监听 8080 端口。在浏览器发出请求时，分析训练时记录的数据，绘制训练过程中的图像。而且 VisualDL 兼容 ONNX, 通过与 python SDK 的结合，VisualDL 可以兼容包括 PaddlePaddle, pytorch, mxnet, Caffe2 在内的大部分主流 DNN 平台。而 Tensorboard 目前仅适用于 Tensorflow、Pytorch、MXNet 等。 PaddlePaddleVisualDL 的可视化界面如下图所示。 VisualDL 的使用 VisualDL 的使用方式非常简单，只需要下面三个过程： VisualDL 的特性 VisualDL 的特性主要有下面 4 点： 支持 Scalar 打点折线图展示，方便观察训练整体趋势 支持 Image 查看数据样本的质量和训练中间结果 支持 Histogram 查看参数分布展示和变化趋势 支持 Graph 查看深度神经网络的模型结构 随着 PaddlePaddle 新特性的不断增加，相信在科研和使用中能给广大使用者带来很多帮助。这里从 PaddlePaddleFluid 讲起，随后讲述为了能更方便利用集群而提供的在浏览器端训练的 PaddlePaddleCloud，也着重讲解集群训练的原理、配置、和实验结果，也就是 PaddlePaddleEDL 部分。最后，讲解 PaddlePaddleVisualDL，这个非常强大的训练日志解析和可视化工具。 "
103,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739530&idx=5&sn=d91c8914a83fc05d41008352f002cb22&chksm=871ad734b06d5e2295562da5abe00a3fd9752cfb7a16291b2d9cc5d8f15ab19890038a926bab&scene=27,征集令 | 2018 年，你想从机器之心INTERFACE听哪些技术话题？,机器之心 Interface「智能机器系列活动」是面向智能机器领域高质量的知识和技能传输平台，也是促进从业人员进行交流与合作的小范围聚会活动。每一期，我们都会根据特定活动主题邀请该领域的权威专家进行讲解。 过去一年以来，上海交大教授、思必驰联合创始人兼首席科学家俞凯为我们讲解过「  」，第四范式联合创始人、首席研究科学家陈雨强分享过「  」，地平线机器人技术联合创始人杨铭系统地讲解过「 」。 作为专业的技术传播平台，分享者们的 PPT 内容是这样的： 现场的学习氛围是这样的： 在新的一年里，机器之心更是准备了神经机器翻译、深度学习框架、芯片、神经网络压缩、概率图模型、强化学习等话题。但是，你们想学的，才是我们最想提供的。在这里，我们希望读者们能够提出你想了解的话题，机器之心将诚心邀请对应领域的专家为大家做分享。 话题召集令 读者们可以在本文下方 ，提出自己想要参与的 Interface 主题，机器之心会选择 开展未来一期的活动。 
104,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739530&idx=1&sn=d6841e9eaa2882da07f8989dbbd58b4d&chksm=871ad734b06d5e22e103e20622a5d2792ecd79f109045e4f2f2403d13ba83c2e312299a7eb33&scene=27,从背景介绍到未来挑战，一文综述移动和无线网络深度学习研究,"arXiv   近来移动通信和 5G 网络等快速发展，它们的调控与配置因为充满了多样性和动态变化而面临非常多的挑战。因此近来很多研究科学家开始利用机器学习及深度学习加强移动和无线网络的配置，并帮助应对数据量和算法驱动的应用程序的增长。本论文基本是首篇综述深度学习及无线网络交叉学科研究面貌的调研，读者可以阅读原论文全面了解该新兴交叉学科。 互联网连接的移动设备正在渗透生活、工作和娱乐的各个方面。智能手机数量不断增加以及不断增多的应用程序引发了移动数据流量的激增。事实上，最新行业预测显示，到 2021 年，全球 IP 年流量将达到 3.3 泽字节 ( 1015 兆字节)，同年，智能手机流量将超过 PC 流量 [1]。由于用户偏好转向无线连接，当前移动基础设施面临着巨大的容量需求。针对这一日益增长的需求，有人建议采用灵活的资源供给方式 [ 2 ]，分布式解决移动管理问题 [ 3 ]。然而，长远来看，互联网服务提供商 ( ISP ) 必须开发智能异构架构和工具，以催生第五代移动系统 ( 5G )，并逐步满足终端用户的迫切需求 [4], [5]。 移动网络架构日益多样化且复杂性不断提高，监控和管理众多网络元素的问题因而变得棘手。因此，广大研究人员对多功能机器智能嵌入未来移动网络这一课题的兴趣空前高涨 [6]，[7]。这种趋势反映在机器学习（ML）解决方案中，从无线接入技术（RAT）选择 [8] 到恶意软件检测 [9]，以及支持机器学习实践的网络系统的开发（例如 [10 ]，[11]）。机器学习能够从流量数据中系统地挖掘有价值的信息，并自动发现其相关性，这类问题对于人类专家来说太过复杂 [12]。作为机器学习的重要部分，深度学习在计算机视觉 [13] 和自然语言处理（NLP）[14] 等领域取得了卓越的进展。网络研究人员也开始认识到深度学习的重要性，并探索如何将深度学习应用到移动网络领域 [15], [16]。 我们有充分理由在在 5G 移动和无线网络中嵌入深度学习，尤其在处理移动环境产生的异构数据。因为这些数据通常来源广泛，格式各异，并且表现出复杂的相关性 [17]。传统的机器学习工具需要繁琐的特征工程才能根据这些数据做出准确的推论和决策。深度学习消除了领域专业知识的门槛，因为它采用分层特征提取，该技术可以有效地提取信息并从数据中获取越来越抽象的相关性，同时最大限度地减少数据预处理工作量。基于图形处理单元（GPU）的并行计算进一步使深度学习能够在毫秒内进行推理。这有利于分析网络，提高管理准确度并克服传统数学技术（例如凸优化、博弈论、元启发式）的运行时间限制。 尽管移动网络领域的深度学习炙手可热，但现有的成果分散在不同的研究领域，缺乏全面而简明的研究。本文通过介绍这两个领域交叉研究的最新调查，填补了深度学习与移动、无线网络之间的鸿沟。除了回顾相关度最高的文献之外，本文还讨论了各种深度学习架构的优缺点，并提出深度学习模型的选择策略，以解决移动网络问题。此外，本文还进一步研究了针对个人移动网络任务量身定制深度学习的方法，以在复杂环境下实现最佳性能的方法。最后，本文指出未来值得深入研究的方向和尚未解决的重要问题，而最终目标是为用深度学习来解决各领域问题的网络研究人员和从业人员提供明确的指导。 本文结构：如图 1 所示，本文采用自上而下的方式组织文章。首先，本文将讨论围绕深度学习、未来移动网络和使用深度学习构建的网络应用程序进行的高层次概述工作，这些工作有助于定义本文的范围和贡献 (第 2 节)。鉴于深度学习技术是移动网络社区中的新课题，第 3 节给出了深度学习的基本背景，突出了解决移动网络问题的直接优势。许多因素能够促进移动网络应用领域的深度学习实现 (包括专用的深度学习库、优化算法等)。第 4 节讨论了这些促进因素，帮助移动网络研究人员和工程师选择合适的深入学习软硬件平台。 第 5 节介绍和比较了最先进的深度学习模型，并提供了网络问题解决方案。第 6 节回顾了近期深度学习领域中移动和无线网络应用，本文将这些应用分为不同的场景，从移动流量分析到安全以及新兴应用。第 7 节讨论了如何针对移动网络问题定制深度学习模型，并强调网络研究中，深度学习应用相关的开放性问题（第 8 节）。本文结尾部分简要讨论移动网络和深度神经网络之间的相互作用（第 9 节）。 论文： Deep Learning in Mobile and Wireless Networking: A Survey 论文地址：https://arxiv.org/abs/1803.04311 摘要：移动设备以及移动应用和服务的日益普及对移动和无线网络基础设施的需求达到前所未有的高度。即将推出的 5G 系统正在发展，以应对移动通信量的爆炸式增长，并灵活管理网络资源，从而最大限度地提高用户体验及细粒度抽取实时分析的能力。该任务颇具挑战性，因为移动环境日趋复杂、多样化并不断发展变化。一个潜在的解决方案是采用先进的机器学习技术来帮助应对数据量和算法驱动的应用程序的增长。深度学习的最新成果是有效解决这一问题的基础。 本文通过对深度学习与移动、无线网络研究的交叉领域开展综合性研究，填补了二者之间的鸿沟。首先，本文简要介绍了深度学习技术的基本背景和最新进展，以及在网络方面的潜在应用。然后讨论了几种有助于在移动系统上高效部署深度学习的技术和平台。随后，本文对基于深度学习的移动和无线网络研究进行了百科全书式的回顾，并按不同领域进行了分类。此外，本文还基于自己的经验讨论了如何根据移动环境定制深度学习。最后明确了当前的挑战和未来的研究方向。 近来深度学习有很多先进的综述性调查或研究，它们都令深度学习尤其是移动端网络架构在终端设备上有很大的提升。一般来说，深度学习的这些进步主要体现在先进的并行计算、分布式机器学习系统、便捷的深度学习框架和高效的最优化方法等。我们在下表 3 展示了这些进步，并在论文中详细讨论它们。 3. 深度学习基础 我们首先将简述深度学习，并重点介绍该领域的核心原则以及成功的关键优势。深度学习实际上是机器学习的一个分支，它通过多层非线性处理单元从原始数据中分层抽取特征与知识，因而能针对一些目标执行高效的预测或决策。相比传统机器学习，深度学习的主要优势是在于自动抽取特征，因而避免了昂贵的人工特征预处理过程。此外，我们在下图 2 中展示了深度学习、机器学习以及人工智能之间的关系。 对于一般的深度学习来说，它们可以抽象为一系列相互嵌套的复合函数，且这种复合函数在直观上可以表示为一系列叠加的层级。如下展示了一般深度神经网络架构的训练和推断过程，其中正向传播即神经网络的推断过程。因为它们可以抽象为一个复合函数，那么根据复合函数的链式求导法则，我们可以轻松地使用反向传播和最优化方法训练神经网络。 4. 深度学习在移动网络中的应用 5G 系统是试图提升信息吞吐量并获得低延迟通信服务的协议，它们能很大程度上提升用户的 QoE [4]。但在 5G 系统上构建深度学习智能系统是非常复杂和昂贵的。幸运的是，目前一些进步令深度学习在移动端的应用变成了可能：（i）高级并行计算，（ii）分布式机器学习系统，（iii）优秀的深度学习框架，（iv）快速优化算法和（v）雾计算（fog computing）。我们在下表 3 中总结了这些优势。 因为并行计算和分布式系统的进步，目前深度学习出现了很多适用于移动端的框架和平台，它们都寻求在移动端上精简和优化深度模型。我们在表 4 中对比了这些平台。 5. 深度学习：顶尖性能 若我们重新回顾图 2，机器学习方法可以很自然地分为监督学习、无监督学习和强化学习，而深度学习在这些领域中都实现了当前最顶尖的性能。在这一章节中，我们将介绍深度学习的关键原则，并讨论它们在解决移动网络问题上的潜力。下图 4 和表 5 都展示了当前基本的深度神经网络架构。 如上所示为基本的深度网络架构，当然最基础的是有感知机发展而来的多层感知机或全连接网络，该网络前后两层的所有神经元都相互有连接。而后卷积神经网络和循环神经网络都基于一些先验特征而只有局部连接，这样不仅减少了权重数量，同时还加强了模型的性能。 以下展示了各神经网络架构的属性、优势和应用于移动网络的潜力等： 6. 深度学习驱动的移动和无线网络 深度学习在移动网络领域应用广泛。本文在不同的网络管区中组织和分类深度学习应用，并描述其贡献。接下来，本文将介绍所有领域的重要出版物，并对比其设计和原则。 无线传感器网络（WSN）由一组分布在不同地理区域的独特或异构传感器组成。它们通过无线通道协同监测物理或环境状态（如温度、压力、运动和污染），并将已收集数据传输到中心服务器。 不同无线网络中收集的数据是不一样的，如下展示了由不同基础设施收集的不同数据及及其类型。 后面表 7、图 6 和表 8 展示了上述不同数据类型的应用、部署与研究工作等。 无线网络其实还能使用强化学习和模仿学习等先进的技术控制移动网络： 7. 把深度学习适用于移动网络 尽管深度学习在诸多移动网络领域表现不凡，但免费午餐定理（NFL）表明，一个模型不可能一劳永逸地解决所有问题 [356]。这意味着对于任何特定的移动和无线网络问题，我们可能需要采用不同的深度学习架构以取得更好表现。本节将重点讨论如何从三个角度为移动网络应用定制深度学习，即移动设备和系统、分布式数据中心以及不断变化的移动网络环境。 8. 未来研究展望 尽管深度学习在移动网络领域取得的成果日益显著，但仍有若干个关键的开放性研究问题存在，值得去关注。接下来，本文将讨论这些挑战并界定这些可通过深度学习解决的重要移动网络问题，进而为未来的移动网络研究提供见解。 9. 结论 深度学习在移动和无线网络领域扮演着日益重要的角色。本文给出了一份有关这两个不同领域之间交叉点的最新的全面工作调查，并总结了各种深度学习模型的基本概念和高级原理，然后通过回顾不同应用场景下的工作来关联深度学习和移动网络学科。本文还讨论了如何针对一般移动网络应用定制深度学习模型，这是以前调查完全忽视的一个方面。最后，本文得出了可能会带来有价值的未来研究结果的若干个开放的研究问题和有希望的方向，并希望这篇文章能成为研究人员和从业人员将机器智能应用于移动网络环境中复杂问题的有趣而明确的指南。 "
105,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739470&idx=2&sn=7a3d8f31cda123638f7131b21a1c1e4a&chksm=871ad770b06d5e66aae17b2a264f8cde3136da8b9d50c3e33ca927322de3b72f06a25f9e3f88&scene=27,入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同,"Medium 尽管近年来神经网络复兴并大为流行，但是 boosting 算法在训练样本量有限、所需训练时间较短、缺乏调参知识等场景依然有其不可或缺的优势。本文从算法结构差异、每个算法的分类变量时的处理、算法在数据集上的实现等多个方面对 3 种代表性的 boosting 算法 CatBoost、Light GBM 和 XGBoost 进行了对比；虽然本文结论依据于特定的数据集，但通常情况下，XGBoost 都比另外两个算法慢。 最近，我参加了 kaggle 竞赛 WIDS Datathon，并通过使用多种 boosting 算法，最终排名前十。从那时开始，我就对这些算法的内在工作原理非常好奇，包括调参及其优劣势，所以有了这篇文章。尽管最近几年神经网络复兴，并变得流行起来，但我还是更加关注 boosting 算法，因为在训练样本量有限、所需训练时间较短、缺乏调参知识的场景中，它们依然拥有绝对优势。 2014 年 3 月，XGBOOST 最早作为研究项目，由陈天奇提出 2017 年 1 月，微软发布首个稳定版 LightGBM 2017 年 4 月，俄罗斯顶尖技术公司 Yandex 开源 CatBoost 由于 XGBoost（通常被称为 GBM 杀手）已经在机器学习领域出现了很久，如今有非常多详细论述它的文章，所以本文将重点讨论 CatBoost 和 LGBM，在下文我们将谈到： 算法结构差异 每个算法的分类变量时的处理 如何理解参数 算法在数据集上的实现 每个算法的表现 在过滤数据样例寻找分割值时，LightGBM 使用的是全新的技术：基于梯度的单边采样（GOSS）；而 XGBoost 则通过预分类算法和直方图算法来确定最优分割。这里的样例（instance）表示观测值/样本。 首先让我们理解预分类算法如何工作： 对于每个节点，遍历所有特征 对于每个特征，根据特征值分类样例 进行线性扫描，根据当前特征的基本信息增益，确定最优分割 选取所有特征分割结果中最好的一个 简单说，直方图算法在某个特征上将所有数据点划分到离散区域，并通过使用这些离散区域来确定直方图的分割值。虽然在计算速度上，和需要在预分类特征值上遍历所有可能的分割点的预分类算法相比，直方图算法的效率更高，但和 GOSS 算法相比，其速度仍然更慢。 为什么 GOSS 方法如此高效？ 在 Adaboost 中，样本权重是展示样本重要性的很好的指标。但在梯度提升决策树（GBDT）中，并没有天然的样本权重，因此 Adaboost 所使用的采样方法在这里就不能直接使用了，这时我们就需要基于梯度的采样方法。 梯度表征损失函数切线的倾斜程度，所以自然推理到，如果在某些意义上数据点的梯度非常大，那么这些样本对于求解最优分割点而言就非常重要，因为算其损失更高。 GOSS 保留所有的大梯度样例，并在小梯度样例上采取随机抽样。比如，假如有 50 万行数据，其中 1 万行数据的梯度较大，那么我的算法就会选择（这 1 万行梯度很大的数据+x% 从剩余 49 万行中随机抽取的结果）。如果 x 取 10%，那么最后选取的结果就是通过确定分割值得到的，从 50 万行中抽取的 5.9 万行。 在这里有一个基本假设：如果训练集中的训练样例梯度很小，那么算法在这个训练集上的训练误差就会很小，因为训练已经完成了。 为了使用相同的数据分布，在计算信息增益时，GOSS 在小梯度数据样例上引入一个常数因子。因此，GOSS 在减少数据样例数量与保持已学习决策树的准确度之间取得了很好的平衡。 每个模型是如何处理属性分类变量的？ CatBoost CatBoost 可赋予分类变量指标，进而通过独热最大量得到独热编码形式的结果（独热最大量：在所有特征上，对小于等于某个给定参数值的不同的数使用独热编码）。 如果在 CatBoost 语句中没有设置「跳过」，CatBoost 就会将所有列当作数值变量处理。 注意，如果某一列数据中包含字符串值，CatBoost 算法就会抛出错误。另外，带有默认值的 int 型变量也会默认被当成数值数据处理。在 CatBoost 中，必须对变量进行声明，才可以让算法将其作为分类变量处理。 对于可取值的数量比独热最大量还要大的分类变量，CatBoost 使用了一个非常有效的编码方法，这种方法和均值编码类似，但可以降低过拟合情况。它的具体实现方法如下： 1. 将输入样本集随机排序，并生成多组随机排列的情况。 2. 将浮点型或属性值标记转化为整数。 3. 将所有的分类特征值结果都根据以下公式，转化为数值结果。 其中 CountInClass 表示在当前分类特征值中，有多少样本的标记值是「1」；Prior 是分子的初始值，根据初始参数确定。TotalCount 是在所有样本中（包含当前样本），和当前样本具有相同的分类特征值的样本数量。 可以用下面的数学公式表示： LightGBM 和 CatBoost 类似，LighGBM 也可以通过使用特征名称的输入来处理属性数据；它没有对数据进行独热编码，因此速度比独热编码快得多。LGBM 使用了一个特殊的算法来确定属性特征的分割值。 注意，在建立适用于 LGBM 的数据集之前，需要将分类变量转化为整型变量；此算法不允许将字符串数据传给分类变量参数。 XGBoost 和 CatBoost 以及 LGBM 算法不同，XGBoost 本身无法处理分类变量，而是像随机森林一样，只接受数值数据。因此在将分类数据传入 XGBoost 之前，必须通过各种编码方式：例如标记编码、均值编码或独热编码对数据进行处理。 所有的这些模型都需要调节大量参数，但我们只谈论其中重要的。以下是将不同算法中的重要参数按照功能进行整理的表格。 在这里，我使用了 2015 年航班延误的 Kaggle 数据集，其中同时包含分类变量和数值变量。这个数据集中一共有约 500 万条记录，因此很适合用来同时评估比较三种 boosting 算法的训练速度和准确度。我使用了 10% 的数据：50 万行记录。 以下是建模使用的特征： 月、日、星期：整型数据 航线或航班号：整型数据 出发、到达机场：数值数据 出发时间：浮点数据 到达延误情况：这个特征作为预测目标，并转为二值变量：航班是否延误超过 10 分钟 距离和飞行时间：浮点数据 import as as from import ""flights.csv"" 0.1 10 ""MONTH"" ""DAY"" ""DAY_OF_WEEK"" ""AIRLINE"" ""FLIGHT_NUMBER"" ""DESTINATION_AIRPORT"" ""ORIGIN_AIRPORT"" ""AIR_TIME"" ""DEPARTURE_TIME"" ""DISTANCE"" ""ARRIVAL_DELAY"" True ""ARRIVAL_DELAY"" ""ARRIVAL_DELAY"" 10 1 ""AIRLINE"" ""FLIGHT_NUMBER"" ""DESTINATION_AIRPORT"" ""ORIGIN_AIRPORT"" for in ""category"" 1 ""ARRIVAL_DELAY"" 1 ""ARRIVAL_DELAY"" 10 0.25 XGBoost import as from import   : return 1 1 # Parameter Tuning ""max_depth"" 10 30 50 ""min_child_weight"" 1 3 6 ""n_estimators"" 200 ""learning_rate"" 0.05 0.1 0.16 3 10 -1 50 1 200 -1 1 0.16 Light GBM import as from import   : return False ""max_depth"" 25 50 75 ""learning_rate"" 0.01 0.05 0.1 ""num_leaves"" 300 900 1200 ""n_estimators"" 200 -1 3 ""roc_auc"" 5 ""max_depth"" 50 ""learning_rate"" 0.1 ""num_leaves"" 900 ""n_estimators"" 300 # Without Categorical Features #With Catgeorical Features ""MONTH"" ""DAY"" ""DAY_OF_WEEK"" ""AIRLINE"" ""DESTINATION_AIRPORT"" ""ORIGIN_AIRPORT"" CatBoost 在对 CatBoost 调参时，很难对分类特征赋予指标。因此，我同时给出了不传递分类特征时的调参结果，并评估了两个模型：一个包含分类特征，另一个不包含。我单独调整了独热最大量，因为它并不会影响其他参数。 import as 0 1 2 3 4 5 6   : return 1 1 'depth' 4 7 10 'learning_rate' 0.03 0.1 0.15 'l2_leaf_reg' 1 4 9 'iterations' 300 ""roc_auc"" 3 ""AUC"" 10 500 9 0.15 ""AUC"" 31 10 500 9 0.15 为了评估模型，我们应该同时考虑模型的速度和准确度表现。 请记住，CatBoost 在测试集上表现得最好，测试集的准确度最高（0.816）、过拟合程度最小（在训练集和测试集上的准确度很接近）以及最小的预测和调试时间。但这个表现仅仅在有分类特征，而且调节了独热最大量时才会出现。如果不利用 CatBoost 算法在这些特征上的优势，它的表现效果就会变成最差的：仅有 0.752 的准确度。因此我们认为，只有在数据中包含分类变量，同时我们适当地调节了这些变量时，CatBoost 才会表现很好。 第二个使用的是 XGBoost，它的表现也相当不错。即使不考虑数据集包含有转换成数值变量之后能使用的分类变量，它的准确率也和 CatBoost 非常接近了。但是，XGBoost 唯一的问题是：它太慢了。尤其是对它进行调参，非常令人崩溃（我用了 6 个小时来运行 GridSearchCV——太糟糕了）。更好的选择是分别调参，而不是使用 GridSearchCV。 最后一个模型是 LightGBM，这里需要注意的一点是，在使用 CatBoost 特征时，LightGBM 在训练速度和准确度上的表现都非常差。我认为这是因为它在分类数据中使用了一些修正的均值编码方法，进而导致了过拟合（训练集准确率非常高：0.999，尤其是和测试集准确率相比之下）。但如果我们像使用 XGBoost 一样正常使用 LightGBM，它会比 XGBoost 更快地获得相似的准确度，如果不是更高的话（LGBM—0.785, XGBoost—0.789）。 最后必须指出，这些结论在这个特定的数据集下成立，在其他数据集中，它们可能正确，也可能并不正确。但在大多数情况下，XGBoost 都比另外两个算法慢。 所以，你更喜欢哪个算法呢？ 原文地址：https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db "
106,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739594&idx=3&sn=91c8462a57965bddeab01e3299ef7669&chksm=871ad7f4b06d5ee2aa1fb37d410d530d80d33c82f4f155782b30c83b51c82f2ba6d55fe208b1&scene=27,学界 | 新型循环神经网络IndRNN：可构建更长更深的RNN（附GitHub实现）,"arXiv 近日，澳大利亚伍伦贡大学联合电子科技大学提出一种新型的循环神经网络 IndRNN，不仅可以解决传统 RNN 所存在的梯度消失和梯度爆炸问题，还学习长期依赖关系；此外，借助 relu 等非饱和激活函数，训练之后 IndRNN 会变得非常鲁棒，并且通过堆叠多层 IndRNN 还可以构建比现有 RNN 更深的网络。实验结果表明，与传统的 RNN 和 LSTM 相比，使用 IndRNN 可以在各种任务中取得更好的结果。同时本文还给出了 IndRNN 的 TensorFlow 实现，详见文中 GitHub 链接。 循环神经网络 (RNN) [16] 已在动作识别 [8]、场景标注 [4] 、语言处理 [5] 等序列学习问题中获得广泛应用，并且成果显著。与卷积神经网络 ( CNN ) 等前馈网络相比，RNN 具有循环连接，其中最后的隐藏状态是到下一状态的输入。状态更新可描述如下： 其中  和 分别为时间步 t 的输入和隐藏状态。 、 和 分别为当前输入的权重、循环输入以及神经元偏差，σ 是神经元的逐元素激活函数，N 是该 RNN 层中神经元的数目。 由于循环权重矩阵不断相乘，RNN 的训练面临着梯度消失和梯度爆炸的问题。长短期记忆 ( LSTM ) [ 10，17 ] 和门控循环单元 ( GRU ) [5] 等若干 RNN 模型可用来解决这些梯度问题。然而，在这些变体中使用双曲正切和 Sigmoid 函数作为激活函数会导致网络层的梯度衰减。因此，构建和训练基于 RNN 的深度 LSTM 或 GRU 其实存在困难。 相比之下，使用 relu 等非饱和激活函数的现有 CNN 可以堆栈到非常深的网络中 (例如，使用基本卷积层可以堆叠到 20 层以上；使用残差连接可以到 100 层以上 [12])，并且仍然在接受高效的训练。虽然在若干研究 [44, 36] 中已经尝试把残差连接用于 LSTM 模型，但情况并没有明显改善 (上述使用双曲正切和 sigmoid 函数的 LSTM 的梯度衰减是主要原因)。 此外，现有的 RNN 模型在 ( 1 ) 中使用相同的  ，其中的循环连接连通所有神经元。这使得解释和理解已训练的神经元 (如每个神经元响应哪种模式) 的作用变得困难，因为单个神经元 [18] 的输出的简单可视化很难在不考虑其它神经元的情况下确定一个神经元的功能。 本文提出了一种新型循环神经网络——独立循环神经网络（IndRNN）。在 IndRNN 中，循环输入用 Hadamard 乘积处理为 。与传统 RNN 相比，它有许多优点，其中包括： 通过调节基于时间的梯度反向传播，可以有效地解决梯度消失和爆炸问题。 利用 IndRNN 可以保留长期记忆，处理长序列。实验表明，IndRNN 可以很好地处理 5000 步以上的序列，而 LSTM 能够处理的序列还不到 1000 步。 IndRNN 可以很好地利用 relu 等非饱和函数作为激活函数，并且训练之后非常鲁棒。 IndRNN 可以实现高效的多层堆叠以增加网络的深度，尤其是在层上具有残差连接的情况下。语言建模实验给出了一个 21 层 IndRNN 的实例。 由于各层神经元相互独立，就很容易解释每层 IndRNN 神经元的行为。 实验表明，IndRNN 在加法问题、序贯 MNIST 分类、语言建模和动作识别等方面的性能明显优于传统的 RNN 和 LSTM 模型。 3. 独立循环神经网络（IndRNN） 本文提出了一种独立循环神经网络 ( IndRNN )，具体描述如下： 其中循环权重 u 是向量， 表示 Hadamard 乘积。每一层的每个神经元各自独立，神经元之间的连接可以通过堆叠两层或更多层的 IndRNNs 来实现（见下文）。对于第 n 个神经元，隐藏状态 h_n,t 可以通过下式得出： 其中 w_n 和 u_n 分别是输入权重和循环权重的第 n 行。每个神经元仅在前一时间步从输入和它自己的隐藏状态中接收信息。也就是说，IndRNN 中的每个神经元独立地处理一种类型的时空模型。传统上，RNN 被视为时间上的、共享参数的多层感知器。与传统的 RNN 不同的是，本文提出的 IndRNN 神经网络为循环神经网络提供了一个新视角，即随着时间的推移 (即通过 u ) 独立地聚集空间模式 (即通过 w )。不同神经元之间的相关性可以通过两层或多层的堆叠来加以利用。在这种情况下，下一层的每个神经元处理上一层所有神经元的输出。 4. 多层 IndRNN 如上所述，同一 IndRNN 层中的神经元彼此独立，时间上的跨通道信息通过多层 IndRNN 进行探索。 IndRNN 基本架构如图 1(a) 所示，其中「weight」和「Recurrent+ ReLU」表示以 relu 作为激活函数的每个步骤的输入处理和循环处理。通过堆叠此基本架构，可以构建深度 IndRNN 网络。 基于 [13] 中残差层的「预激活」类型的残差 IndRNN 实例见图 1(b)。在每个时间步，梯度都可以从恒等映射直接传播到其他层。由于 IndRNN 解决了随时间累积的梯度爆炸和消失的问题，所以梯度可以在不同的时间步上有效地传播。因此，网络可以更深更长。 5. 实验 表 1：序贯 MNIST 和置换 MNIST(误差率 ( % ) ) 结果。 表 2：IndRNN 模型的 PTB-c 结果与文献记录结果的对比（基于 BPC）。 表 3：所有基于骨架的方法在 NTU RGB+D 数据集上的结果。 论文：Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN 论文链接：https://arxiv.org/abs/1803.04831 摘要： 循环神经网络 ( RNN ) 已广泛应用于序列数据的处理。然而，由于众所周知的梯度消失和爆炸问题以及难以保持长期学习的模式，RNN 通常难以训练。长短期记忆 ( LSTM ) 和门控循环单元 ( GRU ) 被用来解决这些问题，但是双曲正切函数和 sigmoid 函数的使用会导致层上梯度衰减。因此，构建可有效训练的深度网络颇具挑战性。此外，每层 RNN 中的所有神经元都连接在一起，它们的运行状况很难解释。针对这些问题，本文提出了一种新的循环神经网络——独立循环神经网络 ( IndRNN )，即同一层的神经元相互独立，跨层连接。我们指出，IndRNN 可以通过简单的调节避免梯度爆炸和消失问题，同时允许网络学习长期依赖关系。此外，IndRNN 可以使用 relu 等非饱和激活函数，训练之后可变得非常鲁棒。通过堆叠多层 IndRNN 可以构建比现有 RNN 更深的网络。实验结果表明，本文中的 IndRNN 能够处理很长的序列 (超过 5000 个时间步)，可以用来构建很深的网络 (实验中使用了 21 层)，并且经过训练还可以更加鲁棒。与传统的 RNN 和 LSTM 相比，使用 IndRNN 可以在各种任务中取得更好的结果。 上文是 IndRNN 具体的论文简介，论文作者其实还提供了该循环架构的 TensorFlow 实现代码和试验结果。我们发现架构代码中有非常详尽的参数解释，因此各位读者可参考 ind_rnn_cell.py 文件详细了解 IndRNN 的基本架构。此外，作者表示该实现使用 Python 3.4 和 TensorFlow 1.5 完成，所以我们可以在该环境或更新的版本测试。 项目地址：https://github.com/batzner/indrnn 1. 用法 将 ind_rnn_cell.py 文件复制到你的项目目录中，如下展示了 IndRNN 单元的简单调用过程： 2. 原论文中提到的实验 有关本文中重构「相加问题」的相关脚本，请参见示例 examples/addition_rnn.py。更多实验（如 Sequential MNIST）将在今后几天进行更新与展示。 "
107,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739431&idx=2&sn=a9be2567074498f31b6b340233174da8&chksm=871ad699b06d5f8f93af1c972143fbff2f3f7faea68c7dca77b76ee4969c5dcb1353f137088d&scene=27,业界 | 特斯拉正式发布Autopilot 2.0自动驾驶系统：今天你升级了吗？,Electrek 自动驾驶对于我们来说还是一个全新的，有待验证的事物。不过，目前已经在市面上售卖的量产汽车里已经或多或少包含了自动驾驶辅助的元素。去年 7 月份发布的新奥迪 A8 就宣称 。而作为最早提出引入自动驾驶辅助功能的车厂，特斯拉的 Autopilot 技术一直吸引着所有人的关注。最近，该公司正式向用户推送了最新软件升级包，大幅提升了汽车自动驾驶的能力。 特斯拉于本周开始推出了新的软件版本，抢先升级的车主纷纷报告新的 Autopilot 系统获得了重大改进。正如特斯拉向消费者承诺的，这次更新为 Autopilot 系统带来了更加先进的神经网络系统。消息人士称，尽管目前的系统仍未达到「Enhanced Autopilot」的长期目标，但其内含的新技术已经为未来的部署奠定了基础。 目前，特斯拉对于本次升级（2018.10.4）的官方说明仍只是「一些小的修复与提升」，以及可以允许用户使用手机 APP 打开汽车尾箱等新功能。但正如我们所看到的，新系统在自动驾驶等方面有了很大提升。虽然新的自动驾驶系统在启用时不会像此前的 Beta 演示那样在仪表板上显示附近所有其他车辆，但自动控制车辆的能力却完整地保留到了正式版上。已经升级新版本系统的用户纷纷表示，新的系统在车道检测方面有了很大的改善，在车道内反复「碰撞」两侧标线的情况已经得到了解决。 部分用户还报告了新系统带来了更加智能化的交通感知巡航控制系统，可以锁定前车进行跟随，并在前车停止时自动停止，如上图所示。 目前看来，特斯拉的自动驾驶系统的研发的进展比较顺利。上个月，伊隆·马斯克曾表示特斯拉的自动驾驶系统更新正处于最后的测试阶段。不过他也表示：「（新系统的）早期版本，特别是开发版本虽然拥有众多功能，但其稳定性仍不尽如人意。」所以目前的正式版绝对不会带来「功能的丰富」，而仅限于当前已有功能的大幅改进。 这一升级适用于搭载 Autopilot 2.0/2.5 硬件套装的汽车，除了上述改进之外，部分用户也报告说新的版本可以大大加速 Autosteer 系统的启动速度——这意味着汽车在自动检测路线与周围环境时的反应变得更加灵敏。 目前看来，大多数已经升级的用户都认为新的特斯拉 Autopilot 2.0 大大超过了基于 Mobileye 的 Autopilot 1.0 系统——不过具体效果如何还有待于大规模使用的反馈。 虽然目前特斯拉官方仍然仅建议用户在高速公路上使用自动驾驶辅助系统，但根据用户的使用结果看来，新的系统也在未划线的复杂道路上运行良好。 下面的视频展示了 Autopilot 2.0 正式版的自动驾驶能力： 该用户指出自动驾驶辅助系统已经有了一些重要的改进，但仍不完美，这意味着驾驶者仍需要手握方向盘随时准备控制汽车。 另一名特斯拉 Model S 90D 的车主 John Chimento 将汽车开上了布鲁克林的街道上： 他显然非常满意于新的升级：「车道变化非常平滑，汽车不再会在车道内来回接触两侧标线——车道线的检测有了很大提升。它看起来聪明多了，我感觉它比之前的版本安全了五倍。」 下面则是新系统在 Santa Cruz 一条蜿蜒小路上的测试： 原文链接：https://electrek.co/2018/03/16/tesla-autopilot-update-first-drive-videos/ 
108,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739470&idx=4&sn=2b36c03eda0304565bca310d74da2e17&chksm=871ad770b06d5e66f05d18a946c24b7af3a701b49ff262b6ded75199d9fe6a7a8da7ee366999&scene=27,CVPR 2018 | 使用CNN生成图像先验，实现更广泛场景的盲图像去模糊,"arXiv 现有的最优方法在文本、人脸以及低光照图像上的盲图像去模糊效果并不佳，主要受限于图像先验的手工设计属性。本文研究者将图像先验表示为二值分类器，训练 CNN 来分类模糊和清晰图像。实验表明，该图像先验比目前最先进的人工设计先验更具区分性，可实现更广泛场景的盲图像去模糊。 盲图像去模糊（blind image deblurring）是图像处理和计算机视觉领域中的一个经典问题，它的目标是将模糊输入中隐藏的图像进行恢复。当模糊形状满足空间不变性的时候，模糊过程可以用以下的方式进行建模： 其中⊗代表的是卷积算子，B、I、k 和 n 分别代表模糊图像、隐藏的清晰图像、模糊核以及噪声。式（1）中的问题是不适定性，因为 I 和 k 都是未知的，存在无穷多个解。为了解决这个问题，关于模糊核和图像的额外约束和先验知识都是必需的。 最近的去模糊方法的成功主要来自于有效图像先验和边缘检测策略方面的研究进展。然而，基于边缘预测的方法常常会涉及到启发式的边缘选择步骤，当边缘不可预测的时候，这种方法表现不佳。为了避免启发式的边缘选择步骤，人们提出了很多基于自然图像先验的算法，包括稀疏性归一化（normalized sparsity）[16]、L0 梯度 [38] 和暗通道先验（dark channel prior）[27]。这些算法在一般的自然图像上表现良好，但是并不适用于特殊的场景，例如文本 [26]、人脸 [25] 以及低光照图像 [11]。大多数上述的图像先验都有相似的效果，它们更加适用于清晰的图像，而不是模糊的图像，这种属性有助于基于 MAP（最大后验）的盲图像去模糊方法的成功。然而，大多数先验都是手工设计的，它们主要是基于对特定图像统计的有限观察。这些算法不能很好地泛化以处理自然环境中的多种场景。所以，开发能够使用 MAP 框架来处理不同场景的图像先验是很有意义的。 为达到这个目的，研究者将图像先验表示为能够区分清晰图像和模糊图像的二值分类器。具体来说，他们训练深度卷积神经网络来分类模糊图像 (标记为 1 ) 和清晰图像 (标记为 0 )。由于基于 MAP（最大后验）的去模糊方法通常使用 coarse-to-fine（由粗到精）策略，因此在 MAP 框架中插入具有全连接层的 CNN 无法处理不同大小的输入图像。为了解决这个问题，他们在 CNN 中采用了全局平均池化层 [ 21 ]，以允许学习的分类器处理不同大小的输入。此外，为了使分类器对不同输入图像尺寸具有更强的鲁棒性，他们还采用多尺度训练策略。然后将学习到的 CNN 分类器作为 MAP（最大后验）框架中潜在图像对应的正则项。如图 1 所示，本文提出的图像先验比目前最先进的人工设计的先验 [ 27 ] 更具区分性。 然而，使用学习到的图像先验去优化这个去模糊方法是很困难的，因为这里涉及到了一个非线性 CNN。因此，本文提出了一种基于半二次方分裂法（half-quadratic splitting method）和梯度下降算法的高效数值算法。这个算法在实际使用中可以快速地收敛，并且可以应用在不同的场景中。此外，它还可以直接应用在非均匀去模糊任务中。 本文的主要贡献如下： 提出了一种高效判别图像先验，它可以通过深度卷积神经网络学习到，用于盲图像去模糊。为了保证这个先验（也就是分类器）能够处理具有不同大小的输入图像，研究者利用全局平均池化和多尺度训练策略来训练这个卷积神经网络。 将学习到的分类器作为 MAP（最大后验）框架中潜在图像对应的正则化项，并且提出了一种能够求解去模糊模型的高效优化算法。 研究者证明，与当前最佳算法相比，这个算法在广泛使用的自然图像去模糊基准测试和特定领域的去模糊任务中都具备有竞争力的性能。 研究者展示了这个方法可以直接泛化到非均匀去模糊任务中。 我们的目标是通过卷积神经网络来训练一个二分类器。这个网络以图像作为输入，并输出一个标量数值，这个数值代表的是输入图像是模糊图像的概率。因为我们的目标是将这个网络作为一种先验嵌入到由粗到精的 MAP（最大后验）框架中，所以这个网络应该具备处理不同大小输入图像的能力。所以，我们将分类其中常用的全连接层用全局平均池化层代替 [21]。全局平均池化层在 sigmoid 层之前将不同大小的特征图转换成一个固定的大小。此外，全局平均池化层中没有额外的参数，这样就消除了过拟合问题。图 2 展示了整个网络架构和二分类网络的细节参数。 图 4. 数据集 [15] 中的一个很具挑战性的例子。本文提出的方法以更少的边缘振荡效应和更好的视觉愉悦度恢复了模糊图像。 图 5. 在实际的模糊图像中的去模糊结果。本文的结果更加清晰，失真较少。 图 6. 文本图像上的去模糊结果。与目前最先进的去模糊算法 [26] 相比，本文的方法生成了更加尖锐的去模糊图像，其中的字符更加清晰。 图 12. 去模糊结果和中间结果。作者在图 (a)-(d) 中与目前最先进的方法 [40, 27] 比较了去模糊结果，并在 (e)-(h) 中展示了迭代中的（从左至右）中间隐藏图像。本文的判别先验恢复了用于核估计的具有更强边缘的中间结果。 论文：Learning a Discriminative Prior for Blind Image Deblurring（学习用于盲图像去模糊的判别先验） 论文链接：https://arxiv.org/abs/1803.03363 我们提出了一种基于数据驱动的判别先验的盲图像去模糊方法。我们的工作是基于这样一个事实:一个好的图像先验应该有利于清晰的图像而不是模糊的图像。在本文中，我们将图像先验表示为一个二值分类器，它可以通过一个深度卷积神经网络 ( CNN ) 来实现。学习到的先验能够区分输入图像是否清晰。嵌入到最大后验 ( MAP ) 框架中之后，它有助于在各种场景 (包括自然图像、人脸图像、文本图像和低照明图像) 中进行盲去模糊。然而，由于去模糊方法涉及非线性 CNN，因此很难优化具有学习已图像先验的去模糊方法。为此，本文提出了一种基于半二次分裂法和梯度下降法的数值求解方法。此外，该模型易于推广到非均匀去模糊任务中。定性和定量的实验结果表明，与当前最优的图像去模糊算法以及特定领域的图像去模糊方法相比，该方法具备有竞争力的性能。 "
109,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739431&idx=4&sn=c0d0e702010a508d61e18b065c378e0c&chksm=871ad699b06d5f8f7389e1a06bcdfb8db8ceae9fe23b4e5e0067b880faa49c964625821e6cbe&scene=27,入门 | 机器学习模型的衡量不止准确率：还有精度和召回率,我们倾向于使用准确率，是因为熟悉它的定义，而不是因为它是评估模型的最佳工具！精度（查准率）和召回率（查全率）等指标对衡量机器学习的模型性能是非常基本的，特别是在不平衡分布数据集的案例中，在周志华教授的「西瓜书」中就特别详细地介绍了这些概念。 GitHub 地址：https://github.com/WillKoehrsen/Data-Analysis/blob/master/recall_precision/recall_precision_example.ipynb 为分类任务选择正确的衡量指标 倘若某人声称创建了一个能够识别登上飞机的恐怖分子的模型，并且准确率（accuracy）高达 99%。你相信吗？好了，有这么一个模型：将从美国机场起飞的所有乘客简单地标注为非恐怖分子。已知美国全年平均有 8 亿人次的乘客，并且在 2000-2017 年间共发现了 19 名恐怖分子，这个模型达到了接近完美的准确率——99.9999999%。这听起来确实令人印象深刻，但是我怀疑美国国土安全局不会在近期购买这个模型。尽管这个模型拥有接近完美的准确率，但是在这个问题中准确率显然不是一个合适的度量指标。 恐怖分子检测是一个不平衡的分类问题：我们需要鉴别的类别有两个——恐怖分子和非恐怖分子，其中一个类别代表了极大多数的数据点。另一个不平衡分类问题出现在当疾病在公众中的发病率很低时的疾病监测。在这两种情况下，正例类别——疾病或恐怖分子，远远少于负例类别的数量。这种问题是数据科学中比较常见的例子，其中准确率并不是评估模型性能的很好的衡量标准。 直观地说，我们知道在恐怖分子检测的问题中宣布所有的数据点为负例（非恐怖分子）是毫无裨益的，相反，我们应该聚焦于正例（恐怖分子）的识别。直觉告诉我们，我们应该最大化的是统计学上称为召回率或查全率（recall）的衡量指标，或者是最大化模型找到数据集中所有相关案例的能力。召回率的准确定义是：真正例除以（真正例+假反例）的和，如下图所示。真正例（true positives）是被真确分类的正例数据点，假反例（false negatives）是被错误分类的负例数据点。在恐怖分子检测的例子中，TP 是被正确识别的恐怖分子，FN 是模型误分类为非恐怖分子的恐怖分子的数据点。召回率可以被理解为模型找到数据集中所有感兴趣的数据点的能力。 你可能注意到了这个等式中的一些细节：如果我们将所有的个体都预测为恐怖分子，那么模型的召回率就是 1.0！这样我们就得到了一个完美的模型吗？当然，不是！与数据科学中的绝大多数概念一样，在我们想要最大化的指标之间存在一个权衡。在召回率的例子中，当召回率增大的时候，精度就会减小。同样，直觉告诉我们，一个将 100% 的乘客标记为恐怖分子的模型可能是不可用的，因为我们必须禁止每一个乘客的飞行。统计学为我们提供了表达直觉的词汇：这个新的模型是低精度（precision）的，或者说较低的仅识别相关数据点的能力。 精度被定义为真正例除以（真正例+假正例）的和，如下图所示。假正例（FP）指的是模型将实际上是反例的样本误判为正例的情况，或者说，在我们的例子中指的是那些被模型判断为恐怖分子，而实际上不是恐怖分子的个体。召回率（查全率）表达的是模型找到数据集中相关实例的能力，而精度（查准率）表达模型找到的数据点中实际相关的比例。 现在我们可以看到，第一个模型给所有的个体标上了非恐怖分子的标签，这个模型是不能使用的。尽管它有着近乎完美的准确率，但是它的精度和召回率都是零，因为没有 TP（真正例）！假设我们轻微地修改一下模型，然后将一个个体正确地识别为恐怖分子。现在，精度是 1（没有假正例，FP），但是召回率很低，因为实际上会有很多假反例（FN）。假设我们走到了另一个极端，将所有的乘客标记为恐怖分子，召回率就会是 1——我们将抓住每一个恐怖分子，但是精度会特别低，我们最终会拘留很多无辜的人。换言之，随着精度的增加，召回率会降低，反之亦然。 结合精度和召回率 在某些情况中，我们也许需要以牺牲另一个指标为代价来最大化精度或者召回率。例如，在之前的例子中，在对患者进行随访检查的初步疾病筛查中，我们可能希望得到接近于 1 的召回率—我们想找到所有实际患病的患者。如果随访检查的代价不是很高，我们可以接受较低的精度。然而，如果我们想要找到精度和召回率的最佳组合，我们可以使用 F1 score 来对两者进行结合。 F1 score 是对精度和召回率的调和平均： 我们使用调和平均而不是简单的算术平均的原因是：调和平均可以惩罚极端情况。一个具有 1.0 的精度，而召回率为 0 的分类器，这两个指标的算术平均是 0.5，但是 F1 score 会是 0。F1 score 给了精度和召回率相同的权重，它是通用 Fβ指标的一个特殊情况，在 Fβ中，β 可以用来给召回率和精度更多或者更少的权重。(还有其他方式可以结合精度和召回率，例如二者的几何平均，但是 F1 score 是最常用的。) 如果我们想创建一个具有最佳的精度—召回率平衡的模型，那么就要尝试将 F1 score 最大化。 可视化精度和召回率 我已经向你抛出了几个新术语，接下来我将通过一个例子向你展示它们在实际中是如何使用的。在使用之前，我们要简单地谈一谈精度和召回率的概念。 首先要介绍一下混淆矩阵（confusion matrix），给定一个模型的预测标签时，它可以被用来快速计算精度和召回率。二分类的混淆矩阵总共包含四个不同的结果：真正例（TP）、假正例（FP）、真反例（TN），以及假反例（FN）。列表示真实值，行表示预测值。行和列的交叉点指的就是这四种结果。例如，如果我们将一个数据点预测为正例，但是它实际上是反例，那么这就是一个假正例。 用混淆矩阵计算精度和召回率需要找到矩阵中对应的值，并应用以下的等式进行计算。 其他的用来展示分类模型性能的可视化技术是受试者特征曲线（ROC 曲线，Receiver Operating Characteristic curve）。别被这个复杂的名词吓到！这个思想是相当简单的：ROC 曲线展示了当改变在模型中识别为正例的阈值时，召回率和精度的关系会如何变化。如果我们有一个用来识别疾病的模型，我们的模型可能会为每一种疾病输出介于 0 到 1 之间的一个分数，为了将某个病人标记为患有某种疾病（一个正例标签），我们为每种疾病在这个范围内设置一个阈值，通过改变这个阈值，我们可以尝试实现合适的精度和召回率之间的平衡。 ROC 曲线在 Y 轴上画出了真正例率（TPR），在 X 轴上画出了假正例率 (FPR)。TPR 是召回率，FPR 是反例被报告为正例的概率。这两者都可以通过混淆矩阵计算得到。 下图是一个典型的 ROC 曲线： 黑色对角线表示随机分类器，红色和蓝色曲线表示两种不同的分类模型。对于给定的模型，只能对应一条曲线。但是我们可以通过调整对正例进行分类的阈值来沿着曲线移动。通常，当降低阈值时，会沿着曲线向右和向上移动。 在阈值为 1.0 的情况下，我们将位于图的左下方，因为没有将任何数据点识别为正例，这导致没有真正例，也没有假正例（TPR = FPR = 0）。当降低阈值时，我们将更多的数据点识别为正例，导致更多的真正例，但也有更多的假正例 ( TPR 和 FPR 增加)。最终，在阈值 0.0 处，我们将所有数据点识别为正，并发现位于 ROC 曲线的右上角 ( TPR = FPR = 1.0 )。 最后，我们可以通过计算曲线下面积 ( AUC ) 来量化模型的 ROC 曲线，这是一个介于 0 和 1 之间的度量，数值越大，表示分类性能越好。在上图中，蓝色曲线的 AUC 将大于红色曲线的 AUC，这意味着蓝色模型在实现准确度和召回率的权衡方面更好。随机分类器 (黑线) 实现 0.5 的 AUC。 回顾 我们已经介绍了几个判断模型性能的属性，每个属性的定义都不是很复杂，但是结合在一起就有点复杂了。让我们通过一个例子做一个快速的回顾来巩固一下这些思想。 对于二分类问题： 真正例（TP）：实际上是正例的数据点被标记为正例 假正例（FP）：实际上是反例的数据点被标记为正例 真反例（TN）：实际上是反例的数据点被标记为反例 假反例（FN）：实际上是正例的数据点被标记为反例 召回率和精度衡量指标： 召回率（R）：分类模型识别所有相关实例的能力 精度（P）：分类模型仅仅返回相关实例的能力 F1 score：使用调和平均结合召回率和精度的指标 召回率和精度的可视化： 混淆矩阵：展示分类模型的真实和预测标签的对应结果。 受试者特征曲线（ROC 曲线）：画出真正例率（TPR）和假正例率（FPR），并将此作为模型归类正例阈值的函数。 曲线下面积（AUC）：基于 ROC 曲线下方的面积，计算分类模型总体性能的指标。 实例应用 我们的任务是为 100 名病人诊断一种在普通人群中患病率是 50% 的疾病。我们将假设一个黑盒模型，我们输入关于患者的信息，并得到 0 到 1 之间的分数。我们可以改变将患者标记为正例 (有疾病) 的阈值，以最大化分类器性能。我们将以 0.1 为增量从 0.0 到 1.0 评估阈值，在每个步骤中计算 ROC 曲线上的精度、召回率、F1 score 以及在 ROC 曲线上的位置。以下是每个阈值的分类结果： 我们将以阈值为 0.5 为例计算对应的召回率、精度、真正例率、假正例率。首先我们得到混淆矩阵： 阈值为 0.5 时的混淆矩阵 我们可以利用混淆矩阵中的数值来计算召回率、精度和 F1 score： 然后计算真正例率和假正例率来确定阈值为 0.5 时，模型在 ROC 曲线上对应的点。 为了得到整个 ROC 曲线，我们在每个阈值下都进行这个过程。你可能会想，这是相当乏味的，所以，我们用 python 语言来代替手工计算。用来做这些计算的 Juoyter Notebook 放在了 github 上，每个人都可以看到。最终的 ROC 曲线如下所示，点上面的数字是阈值。 在这里我们可以看到，所有的概念都汇集到一起了！在阈值等于 1.0 的点，我们没有将任何病人归类为患病，因此模型的召回率和精度都是 0。随着阈值的减小，召回率增加了，因为我们发现更多的患者患有该疾病。然而，随着召回率的增加，精度会降低，因为除了增加真正例之外，还会增加假正例。在阈值为 0.0 的时候，我们的召回率是完美的——我们发现所有的患者都患有这种疾病——但是精度很低，因为有很多假正例。通过更改阈值并选择最大化 F1 score 的阈值，我们可以沿着给定模型的曲线移动。要改变整个曲线，我们需要建立一个不同的模型。 在每个阈值下最终模型的统计量如下表： 基于 F1 score，整体最佳的模型出现在阈值为 0.5 的地方。如果我们想要在更大程度上强调精度或者召回率，我们可以选择这些指标上最佳时对应的模型。 结论 我们倾向于使用准确率，因为每个人都知道它意味着什么，而不是因为它是完成任务的最佳工具！虽然更适合的度量指标 (如召回率和精度) 看起来可能很陌生，但我们已经直观地了解了为什么它们在某些问题 (如不平衡的分类任务) 中有着更好的表现。统计学为我们提供了计算这些指标的形式化定义和方程。数据科学是关于寻找解决问题的正确工具的学科，而且在开发分类模型时，我们常常需要超越准确率（accuracy）的单一指标。了解召回率、精度、F1 score 和 ROC 曲线使我们能够评估分类模型，并应使我们怀疑是否有人仅仅在吹捧模型的准确率，尤其是对于不平衡的问题。正如我们所看到的，准确率（accuracy）并不能对几个关键问题提供有用的评估，但现在我们知道如何使用更聪明的衡量指标！ 
110,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739470&idx=1&sn=8556f0261844848c62cae514cfa89103&chksm=871ad770b06d5e66d2b52cf985e70ad4995a1cec5fea7390335356eaa20cd94accad5cab8c4c&scene=27,变革尚未成功：深度强化学习研究的短期悲观与长期乐观,"alexirpan 深度强化学习是最接近于通用人工智能（AGI）的范式之一。不幸的是，迄今为止这种方法还不能真正地奏效。在本文中，作者将为我们解释深度强化学习没有成功的原因，介绍成功的典型案例，并指出让深度强化学习奏效的方法和研究方向。 本文所引文献大多数来自于 Berkeley、Google Brain、DeepMind 以及 OpenAI 过去几年的工作，因为它们更容易获得。我难免遗漏了一些比较古老的文献和其他研究机构的工作，我表示很抱歉——毕竟一个人的时间精力有限。 简介 我曾经在 Facebook 说过： 当别人问我强化学习能否解决他们的问题时，至少有 70% 的时候我的回答是：不能。 深度强化学习被成堆的炒作包围着，并且都有足够好的理由！强化学习是一种难以置信的通用范式，原则上，一个鲁棒而高性能的强化学习系统可以处理任何任务，而且将这种范式和深度学习的经验学习能力相结合是很自然的。深度强化学习是最接近于通用人工智能（AGI）的范式之一。 不幸的是，它目前还不能真正地奏效。 但现在，我相信它会取得成功的。如果我不相信强化学习，我是不会从事相关工作的。但是在通往成功的路上存在很多问题，而且很多问题从根本上来说都是很困难的。智能体的漂亮 demo 背后隐藏着创造它们的过程中所付出的所有心血、汗水和泪水。 我多次看到人们被最新的研究所吸引，他们初次尝试使用深度强化学习，而且没有失败，于是低估了深度强化学习所面临的困难。毫无疑问，「玩具问题」并不像看起来那么简单。无一例外，这个领域会数次「摧残」他们，直至他们学会设定更现实的研究期望。 实际上这并不是任何人的错，它更像是一个系统问题。讲述积极结果的故事是很容易的，但承认消极的结果是很困难的。问题在于消极的结果是研究者最常遇到的。某种程度上，消极的结果实际上比积极的结果更加重要。 在这篇文章的其余部分，我会解释一下深度强化学习没有成功的原因，它成功的典型案例，以及将来让深度强化学习更加可靠地工作的方式。我相信如果在这些问题上可以达成一致，并实实在在地讨论相关的问题，而不是独立地重复地去一次又一次地重新发现相同的问题。 我希望看到更多的关于深度强化学习的研究。我希望有新人加入这个研究领域，我也希望知道新人们能够了解他们正在做什么。 在开始文章的剩余部分之前，有几点提示： 我在这篇文章中引用了一些论文。通常，我会因其令人信服的负面例子而引用一篇论文，而不引用正面例子。这并不意味着我不喜欢那些论文。我喜欢这些论文，如果有时间的话，它们是值得一读的。 我在这篇文章中可互换地使用「reinforcement learning，强化学习」和「deep reinforcement learning，深度强化学习」，因为在我的日常工作中，强化学习一直蕴含着深度强化学习的意思。我所批判的是深度强化学习的经验行为，而不是一般的强化学习范式。我所引用的论文中通常使用了深度神经网络的智能体。尽管这种经验批判可能同样适用于线性强化学习或者列表格式强化学习，但是我并不认为这也适用于到更小的问题。强化学习有望被应用于大型、复杂、高维的环境中，在这些环境中良好的函数逼近是必要的。受此驱动，人们才炒作强化学习，这些炒作正是需要重点解决的问题。 这篇文章的基调是由悲观向乐观转换的。我知道文章有些长，但是我更希望你花点时间读完全文再做回复。 下面是深度强化学习的一些失败案例。 深度强化学习可能是非常采样低效的（sample inefficient） 用于深度强化学习的最著名的基准测试就是 Atari 游戏。正如目前最出名的深度 Q 网络论文中所展示的一样，如果你将 Q-Learning 与合理规模的神经网络和一些优化技巧相结合，你可以在几款 Atari 游戏中实现和人类相当甚至超越人类的性能。 Atari 游戏以每秒 60 帧的速度运行，那么目前最先进的 DQN 需要多块的速度才能达到人类的性能呢？ 这个问题的答案取决于游戏，那么我们一起来看一下最近 deepmind 发表的一篇论文 Rainbow DQN。这篇论文对原始 DQN 的几个渐变版本的体系结构进行了 ablation study（类似于控制变量法，对模型进行简化测试），结果证明组合所有的改进可以得到最佳的性能。在试验的 57 场 Atari 游戏中，有超过 40 场的表现超越了人类。结果如下图所示： y 轴是「人类性能标准化的中值得分」。通过为 57 个 Atari 游戏中的每一个训练一个 DQN 模型，然后将每个智能体的得分进行标准化，使得人类的性能是 100%，然后将模型在 57 款游戏上的中值得分画出来。RainbowDQN 模型的训练数据达到约 1800 万帧的时候突破了 100% 的阈值。这相当于玩了 83 小时的游戏，再加上训练模型所花的时间。而人类通常能够在几分钟之内学会一款 Atari 游戏。 请注意，1800 万帧实际上已经是相当好的结果了，如果考虑到之前的记录，分布式 DQN（Distributed DQN）需要 7000 万帧才能达到 100% 的中值性能，大约是 4 倍于 RainbowDQN 的训练时间。至于发表在 nature 上的关于 DQN 的论文，即便经历了 2 亿帧的游戏体验，也从未达到 100 的中值性能。 规划谬误理论认为，完成一件事情所花的时间通常要比你想象的更多。强化学习也有其规划谬误，学习一个策略通常需要比想象更多的样本。 这并不是 Atari 游戏特有的问题。另一个非常流行的测试基准是 MuJoCo 基准，这是在 MuJoCo 物理模拟器中设置的一个任务集合。在这些任务中，输入状态通常是模拟机器人各关节的位置和速度。即使不必解决视觉问题，根据任务的不同，这些基准的学习仍然需要 105105 到 107107 的学习时间步。对于控制一个如此简单的环境而言，这是一个惊人的实验量。 在 DeepMind 的跑酷论文（Emergence of Locomotion Behaviours in Rich Environments）的 demo 中，使用了 64 个 worker 在超过一百小时的时间里训练策略。这篇论文并没有阐明 worker 的含义，但是我认为它的意思是一个 worker 意味着 1 个 CPU。 这些结果超级酷。当它刚出现的时候，我很惊讶，强化学习竟然可以学习这些奔跑的步态。 同时，需要 6400 个 CPU 小时训练的事实多少有些令人沮丧。并不是说我期望用更少的时间，更让人沮丧的是深度强化学习仍然比实践水平的采样效率水平要低好几个数量级。 有一个显而易见的对比：倘若我们忽略了采样效率呢？某些环境设置比较易于生成经验，游戏就是一个很好的例子。但是，对于任何不正确的设置，强化学习将面临一场艰苦的战斗，不幸的是，大多数现实世界的设置都属于这一类。 如果你仅仅关心最终的性能，那么很多问题都能够通过其他方法更好地解决 在寻求任何研究问题的解决方案时，通常会在不同的目标之间进行权衡。你可以通过优化以获得针对该研究问题的真正好的解决方案，也可以优化以做出良好的研究贡献。最好的问题就是需要作出很好的研究贡献以得到解决方案的问题，但是满足这种标准是很困难的。 对于单纯地得到良好的性能，深度强化学习方法得到的记录并不是那么好，因为它们总是被其他方法击败。MuJoCo 机器人由在线的路径优化控制。正确的动作几乎是实时、在线计算得到的，而且没有经过任何离线训练。等等，它是运行在 2012 个硬件上面的（Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization）。 我觉得这些行为可以和那篇跑酷论文相提并论。那么这两篇论文的差别是什么呢？ 差别在于 MuJoCo 机器人中使用的是模型预测控制，这种控制方法可以根据真实世界的模型 (物理模拟器) 执行规划。而无模型的强化学习不做这种规划，因此它的训练过程更困难。另一方面，如果根据一个模型来规划会有如此大的帮助，那么为何还要训练一个强化学习策略来自寻烦恼呢？ 类似的情况，在 Atari 游戏中，使用现成的蒙特卡洛树搜索也能够很容易地得到强于 DQN 的性能。这是 Guo 等人在 NIPS 2014 上发表的基准数据（Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning）。他们比较了训练好的 DQN 和 UCT 智能体的得分。其中 UCT 是目前使用的蒙特卡洛树搜索的一个标准版本。 同样，这是一个不公平的比较，因为 DQN 没有执行任何搜索，蒙特卡洛树搜索会执行基于真实世界模型（Atari 模拟器）的搜索。然而，有时候我们并不需要关心比较是否公平，有时候我们只是想让它起作用。如果你对 UCT 的全面评估感兴趣，你可以参考原始论文「The Arcade Learning Environment: An Evaluation Platform for General Agents」。 理论上强化学习可以解决任何问题，包括在世界模型未知的环境中执行任务。然而，这种泛化是需要代价的：很难利用任何特定问题的信息来帮助学习，这就迫使我们使用大量的样本来学习那些可能已经被硬编码的东西。 然而经验法则是，除了极少数情况，特定领域的算法都会比强化学习表现得更快更好。如果仅仅是为了强化学习而做强化学习，那这不是问题，但是，我个人觉得，将强化学习的性能与其他任何方法进行比较的时候都会令人沮丧。我非常喜欢 AlphaGo 的一个原因是，它是强化学习的明确胜利，而且这不会经常发生。 这使得我更难向外行人士解释为什么我的问题很酷、很难、很有趣，因为他们通常缺乏相应的经验，来理解为什么这些问题很困难。在人们认为强化学习能做什么和强化学习实际上能做什么之间存在一个理解鸿沟。我现在从事于机器人学相关的研究。当提到机器人的时候，很多人都会想到一家公司：波士顿动力。 这个并没有使用强化学习。我经历过几次谈话，人们认为波士顿动力的机器人使用了强化学习，但是实际上并没有。如果你查阅这个研究团队的论文，你会发现有一篇提到时变 LQR、QP 求解器和凸优化的论文（https://dspace.mit.edu/openaccess-disseminate/1721.1/110533）。换言之，他们绝大多数情况使用的是经典的机器人技术。事实证明，当你正确地使用这些经典技术的时候，它们能够工作得相当好。 强化学习通常需要一个奖励函数 强化学习假定存在一个奖励函数。通常，奖励函数要么是给定的，要么是离线手动调整的并在学习过程中保持固定。这里说「通常」，是因为存在例外情况，例如模仿学习或者逆强化学习，但是绝大多数强化学习方法都将奖励函数视为必要的。 重要的是，为了让强化学习做正确的事，你的奖励函数必须精确地捕捉到你希望得到的东西，我的意思是准确无误地捕捉。强化学习有个恼人的倾向，它会把奖励函数过拟合，从而导致不希望出现的结果。这正是 Atari 为什么是出色基准的原因，在 Atari 游戏中，不仅能够轻易地得到大量的样本，而且每款游戏的目标都是将得分最大化，所以根本不必担心奖励函数的定义，每款游戏都有一样的奖励函数。 这同样也是 MuJoCo 如此受欢迎的原因。因为它们运行在模拟环境中，你拥有关于所有对象状态的完美知识，这一切都使得奖励函数的定义变得更加容易。 在 Reacher 任务中，你控制着一个两段机器臂，它被连接在一个中心点上，这个任务的目标就是将机器臂的端点移动到目标位置。 由于所有的位置都是已知的，奖励函数可以定义为机械臂端点到目标点之间的距离，再加上一个小的控制代价。原则上，如果你拥有足够多的传感器来获取环境中足够精确的位置，在现实世界也可以做到这点。但是，根据系统任务的不同定义合理的奖励函数可能会很困难。 就其本身而言，需要一个奖励函数并不是什么大事，除非...... 奖励函数的设计是困难的 创建一个奖励函数并不是很困难。困难在于设计可学习的奖励函数去激励智能体得到期望的行动。 在 HalfCheetah 环境中，你拥有一个两条腿的机器人，它被限制在一个竖直平面中，这意味着它只能向前或者向后移动。 这里的目标是学习一个奔跑的步态。奖励函数是 HalfCheetah 的速度。 这是一种定型的奖励，也就是说它越接近最终目标，给出的奖励就越高。与之对应的是稀疏奖励函数，它仅仅在目标状态给出奖励，在其他任何地方都没有奖励。定型奖励函数通常更加容易学习，因为即便所学的策略没有给出问题的完全解决方案，它也能给出积极的反馈。 不幸的是，定型的奖励函数会给学习带来偏差。正如之前所说的，这会导致并不期望得到的行为。一个典型的例子就是 OpenAI 博客中的赛船游戏（https://blog.openai.com/faulty-reward-functions/）。其目标是完成比赛。你可以想象，稀疏奖励函数会在某个时间内完成游戏时给出+1，否则就是 0。 该提供的奖励函数会在你命中关卡的时候给出分数，也会因为收集能更快完成比赛的能量而给出奖励。结果收集能量能够比完成比赛获得更多的分数。 老实说，当我看到 OpenAI 的这篇博客的时候是有些恼火的。很明显，在奖励函数不匹配的情况下，强化学习会得到很诡异的结果！我觉得他们那篇博客在给定的例子上做了大量的不必要的工作。 然后我就开始写这篇博客，而且意识到最引人注目的误匹配奖励函数的视频就是那个赛船游戏的视频。并且从那时开始，这个视频就被用在了好几个阐明这个问题的报告中。所以，好吧，我很不情愿地承认这是一篇很好的博客。 强化学习算法沿着一个连续体发展，在这个连续体中算法需要或多或少地假设一些关于它们所处环境的知识。最广泛应用的是无模型强化学习，它几乎和黑箱优化是一样的。这些方法仅仅被允许假设它们处于一个马尔科夫决策过程（MDP）中。否则，它们将没有任何意义。智能体只是被简单地告知这个会给它+1 的奖励，这个不会给它奖励，它必须自行学会其余的东西。就像黑箱优化一样，问题在于任何一个能够给出+1 奖励的都是好的，即使有时候+1 的奖励并不是源于正确的原因。 一个经典的非强化学习的例子就是使用遗传算法来设计电路，最终得到的电路中，一个未连接的逻辑门竟然是最终设计中所必须的。 最近的一个例子，可以参考 Salesforce 公司的这篇博客（https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/）。他们的目标是文本摘要。他们的基线模型是用监督学习训练得到的，然后使用叫做 ROUGE 的自动指标去评估模型。ROUGE 是不可微的，但是由于强化学习可以处理不可微的奖励函数，所以他们尝试直接使用强化学习的方法来优化 ROUGE。这个方法得到了很高的 ROUGE，但是它却无法给出一个很好的摘要。下面是一个例子： Button 在迈凯轮车队的第 100 场比赛被拒绝，因为 ERS 阻止了他进入起跑线。这对英国人来说是一个悲惨的周末。Button 已经合格了。在巴林比的尼科罗斯伯格之前完成。Lewis Hamilton，在 11 场比赛中领先 2000 圈…… 所以，尽管强化学习模型得到了最高的 ROUGE 得分，他们最终还是选择了另外一个模型。 这里还有另外一个例子，是 Popov 等人的论文（Data-efficient Deep Reinforcement Learning for Dexterous Manipulation），有时候被称为「乐高堆叠」。作者使用了 DDPG 的一个分布式版本来学习抓取策略。它的目标就是抓到红色的方块，并将其堆叠在蓝色的方块上面。 他们使系统跑通了，但是遇到了非常严重的失败案例。在原始的举起动作中，奖励函数是基于红色方块被举的高度，通过底平面的 Z 坐标来定义。其中一种失败的模式就是学到的策略将红色的方块翻了过来，并没有将它捡起来。 显然这并不是期望的解决方案。但是强化学习并不关心这个。从强化学习的角度来看，翻转红色的方块会得到奖励，从而它会继续翻转。 解决这个问题的一种方式是，仅仅在机器人完成方块堆叠之后才给出正奖励使奖励函数稀疏化。有时候这种方式会奏效，因为稀疏奖励函数是可学习的，然而通常都不会起作用，因为缺乏正增强会使得整个过程都变得很困难。 解决这个问题的另一个方式就是仔细地进行奖励函数调整，添加新的奖励项并调整已有的系数，直至得到期望学到的行为。在这条战线上「打赢强化学习之战」是可能的，但是这是一场不怎么令人称心的战斗。有时候这样做是必要的，但是我并不觉得我从中学到了什么。 作为参考，以下列出了「乐高堆叠」那篇论文中的奖励函数之一。 我不清楚设计这么一个奖励函数花费了多少时间，但是由于这里存在这么多的项和不同的系数，我猜应该是花费了「大量的」时间。 在与其他强化学习研究者交谈的过程中，我听到了一些因为没有设计合适的奖励函数，而导致奇怪结果的轶事。 我的一位合作者在教智能体进行室内导航。如果智能体走出了边界，事件就会终止。然而当事件以这种方式终止的时候，他并没有增加任何惩罚项。最终智能体学到的策略表现出了自杀的行为，因为负奖励太多了，而正奖励很难实现。 一个朋友训练机械臂来到达桌子上的一个点。而事实是这个目标点是相对于桌子定义的，而桌子并没有固定到任何东西上。结果策略学会了猛烈地撞击桌子，最终将桌子掀翻，目标点也被移动了。目标点凑巧落在了机械臂末端的旁边。 一位研究者谈论了使用强化学习来训练模拟机器手，使它拿起锤子钉钉子。最开始，奖励是由钉子插入孔中的距离决定的。机器人并没有拿起锤子，而是使用自己的肢体将钉子砸入了孔中。因此，他们增加了一个奖励项来鼓励机器人拿起锤子，并重新训练策略。最后他们得到了拿起锤子的策略....... 但是后来它将锤子扔在了钉子上，而不是使用锤子去砸钉子。 诚然，这些都是我道听途说的，我并没有见过任何一个相关的视频。然而，没有一个听起来是难以置信的。我已经被强化学习雷了好多次了，所以我选择相信。 即使给定了较好的奖励函数，也很难跳出局部最优 之前关于强化学习的例子有时候被称作「reward hacking」。对我而言，这意味着存在一个更加聪明的现成的解决方案，它能够给出比奖励函数的设计者预期更多的奖励。 Reward hacking 是一个例外。更普遍的情况则是比较糟糕的局部最优解，这个局部最优解源于「探索—利用」权衡过程中的错误。 这里有一段我喜欢的视频。它是对「Normalized Advantage Function」的实现（Continuous Deep Q-Learning with Model-based Acceleration），是在 HalfCheetah 环境中学习得到的。 从外部角度来看，这实在是很愚蠢。但是只有当我们以第三人的角度来看，并且大量的预置知识告诉我们靠双足奔跑才是更加好的，才会觉得它很蠢。然而强化学习并不知道这个！它只看到了一个状态向量，并且输出动作向量，并且它知道自己能够获得某些正奖励。这是它知道的所有。 我猜，在它学习的过程中发生了以下这些现象： 在随机探索中，策略发现向前扑倒比原地不动要更好一些。 它做了大量努力来「铭记」那种行为，所以它现在会连续地向前扑倒。 在向前扑倒之后，策略学习到，如果它一次使很大的劲，它会做一次后翻，这个后翻能够得到稍微多一点的奖励。 它做了足够多的探索之后，相信后翻是一个不错的注意，然后就将后翻铭刻在了策略之中。 一旦后翻策略持续进行，这对已学到的策略而言更加容易：是纠正自己按照「标准姿势」奔跑，还是学会如何躺着仰面前行？我想应该是后者。 这是很有趣的，但是这绝对不是我们想让机器人所做的。 下面另一次失败的运行，这次是发生在 Reacher 环境中的。 在这次运行中，初始随机权重倾向于输出特别大的正值或特别小的负值的动作输出。这使得绝大多数的动作输出可能是最大加速度或者最小加速度。这实际上很容易发生超快的旋转：在每个关节处输出高强度的力。一旦机器人开始运行，它很难以一种有意义的方式偏离这种策略—如果要偏离，必须采取一些探索步骤来阻止这种疯狂的旋转。这当然是可能的，但是在这次运行中，并没有发生。 这些都是一直以来困扰着强化学习的经典「探索—利用」问题的两种情况。你的数据来自于目前的策略。如果你当前的策略探索太多，那将会得到大量的垃圾数据，从而学习不到任何东西。利用太多，则记忆的行为经常是非最优的。 这里有几个解决这个问题的直观想法—内在动机、好奇心驱动的探索、基于重要性的探索，等等。其中很多方法都是在上世纪 80 年代或者更早的时候提出来的，而且一些方法已经使用深度学习的模型进行了重新研究。然而，据我所知，这些方法都不能在所有的环境中一致奏效。有时候它们会有帮助，有时候则不会。如果已经存在一种探索技巧能够在所有环境中起作用的话，那将会是很好的，但是我很怀疑这种万精油会不会在近期被发现。不是因为人们没有尝试解决这个问题，而是因为探索—利用问题实在是太难了。可以参考维基百科中的多臂老虎机问题： 这一问题最初由二战中的盟军科学家考虑到的，但事实证明非常棘手，据 Peter Whittle 说，这个问题被提议搁置在德国，以便德国科学家也能在这个问题上浪费时间。（参考： Q-Learning for Bandit Problems, Duff 1995） 我开始想象深度强化学习是一个恶魔，故意曲解你的奖励，并积极寻找最懒的局部最优解。这有点荒谬，但我发现这实际上是一种富有成效的思维方式。 即使当深度强化学习成功的时候，它也有可能仅仅是过拟合了环境中的某些奇怪的模式 深度强化学习之所以流行是因为它是唯一一个被认可在测试集上做训练的机器学习方法。 强化学习的好处在于如果你想在一个环境中做得很好，你可以肆意地进行过拟合。但是不好的地方在于如果你想把模型泛化到另一个环境中，你可能会做得很糟糕。 DQN 可以解决很多 Atari 游戏，但是它是通过将所有的学习聚焦在一个单独的目标上实现的—仅在一款游戏中表现得极其好。最终的模型不会泛化到其他的游戏中，因为它没有以其他游戏中的方式训练过。你也可以将一个学到的 DQN 模型精调到一款新的 Atari 游戏中（Progressive Neural Networks (Rusu et al, 2016)），但是不能保证它能够完成迁移，而且人们通常不期望它能够完成这种迁移。这并不是我们已经在 ImageNet 中所见证的巨大成功。 原则上，在一个广泛的环境分布中训练应该会使这些问题消失。在某些情况下，你可以免费地使用这种分布。导航就是其中的一个例子，你可以随机地采样目标位置，然后使用统一的价值函数去泛化（Universal Value Function Approximators, Schaul et al, ICML 2015）。我发现这项工作是很有希望的，我后续会给出关于这项工作更多的一些例子。然而，我认为深度强化学习的泛化能力还不足以处理很多样的任务集合。认知传感已经变得更好了，但是深度强化学习还没有达到 ImageNet 的成就。OpenAI 尝试挑战这个问题，但是据我所知，这个问题太难解决了，所以也没有多少进展。 在拥有这种泛化能力之前，我们还是会受到策略被局限在极小范围内的困扰。有这么一个例子（这也是 daq 我打趣自己所做工作的机会），考虑一下这个问题：Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games？(Raghu et al, 2017) 我们研究了一个轻量级的两玩家组合游戏，存在一个寻求最优玩法的闭环形式分析解决方案。在我们初次进行的一个实验中，我们固定了玩家 1 的行为，然后用强化学习的方法去训练玩家 2。这样，你可以将玩家 1 的动作视为环境的一部分。通过用玩家 1 的最优解来训练玩家 2，最终证明强化学习可以达到很高的性能。但是当我们将策略部署到一个非最优的玩家 1 上时，它的性能下降了。 Lanctot 等人在 NIPS 2017 的论文中展示了类似的结果（A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning）。其中有两个智能体在玩激光标记。它们都是以多智能体强化学习的方式训练得到的。为了测试其泛化能力，他们用 5 个随机种子来运行训练过程。下图的两个智能体都是在对方的基础上训练得到的。 它们学会了朝着彼此射击。然后他们从一个实验中选择玩家 1，与另一个实验中的玩家 2 进行对战。如果学习到的策略可以泛化，那么我们应该能够看到类似的行为。 剧透：它们并没有！ 这是多智能体强化学习中的一个常见问题。当智能体进行对抗训练时，就发生了某种协同进化。智能体非常擅长和对手对抗，但是当它们被部署在一个没有见过的智能体上时，性能就会下降。我还想指出的是，这些视频中唯一不同的地方就是随机种子。完全相同的学习算法，完全相同的超级参数。它们的不同表现完全来自于初始条件的随机性。 话虽如此，但是竞争性自我对抗博弈中的一些简单例子貌似是与此矛盾的。OpenAI 的博客介绍了他们在这个领域中的一些工作（ ）。自我对抗也是 AlphaGo 和 AlphaZero 的重要部分。我的直觉是：智能体在相同的环境中学习，它们可以不断地挑战彼此，并加速彼此的学习，但是如果其中的一个学习得比较快，它就会过度地利用较弱的那个智能体，并导致过拟合。当从对称自我博弈松弛化到多智能体环境中时，保证以同样的速度学习就会变得更加困难。 即使忽略了泛化问题，最终的结果也可能是不稳定的和难以复现的 几乎每个机器学习算法都有能够影响模型行为和学习系统的超参数。通常，这些参数都是通过手动挑选得到的，或者是通过随机搜索得到的。 监督学习是稳定的。固定的数据集，固定的真实目标。如果对超参数做了很小的改动，最终的性能并不会改变很多。并不是所有的超参数都具有很好的性能，但是，基于多年来发现的经验技巧，很多参数都会在学习得过程中表现出一种生命迹象。这种生命迹象是很重要的，因为它们会告诉你，你走上了正确的道路，值得投入更多的时间。 目前，深度强化学习一点都不稳定，这对于研究来说是非常恼人的。 当我开始在 Google Brain 工作的时候，我所做的第一批工作之一就是实现那篇「Normalized Advantage Function」的论文。我本以为花费大约两到三个星期就可以完成了。对我而言需要做以下几件事情：对 Theano 要有一定的熟悉（当然现在转移到了 TensorFlow），一些深度强化学习的经验，由于 NAF 论文的一作也在 Google Brain 实习，所以我可以向他请教。 由于几个软件 bug，最终我花了 6 个星期复现了结果。而问题在于，为什么花了这么久才找到 bug？ 为了回答这个问题，我们先考虑一下 OpenAI Gym 中最简单的连续控制问题：摆任务。在这个任务中，有一个摆，它被固定在一个点上，受到重力作用。输入状态是 3 维的。动作空间是 1 维的，也就是施加的力矩。目标是使这个摆完全直立。 这是一个比较简单的问题，通过一个较好的定型奖励函数可以很容易做到。奖励函数被定义为摆的角度。将摆靠近垂直方向的动作会给出奖励，而且会给出递增的奖励。奖励函数基本上是凸的。 下面展示了一个基本成功的策略（视频见原文链接）。尽管它并没有使摆保持直立平衡，但是它输出了能够抵消重力的准确力矩。 下面是修复了所有 bug 之后的性能图。每一条曲线都是 10 次独立运行中的奖励函数。它们拥有相同的超参数，唯一的区别是随机种子。 其中有 7 次运行是成功的，3 次没有成功，成功率是 70%。这里还有一些已经发表过的研究中的图，「Variational Maximizing Exploration, Houthooft et al，NIPS 2016」。所用环境是 HalfCheetah 环境。虽然奖励函数被修改得更加稀疏，但是这些细节并不是十分重要。Y 轴是事件奖励，X 轴是时间步数，所用算法是信赖域策略优化（TRPO）。 深色曲线是 10 个随机种子的中值性能，浅色区域是第 25 至第 75 百分位数。别误会，这幅图是支持 VIME 的很好的证明。但是另一方面，第 25 个百分位确实很接近 0 奖励。这意味着 25% 的运行失败了，仅由于随机种子的不同而导致。 注意，这种方差在监督学习中也是存在的，但是它很少会这么糟糕。如果我的监督学习代码在 30% 的时间不能成功运行，那么我会高度质疑数据加载或者训练过程的可信度。但是，如果我的强化学习代码没有随机性能好，那我并不清楚到底是代码有 bug，还是超参数不太好，或者我只是运气不好。 这幅图来自于这篇博客：「Why is Machine Learning 『Hard』?」。它的核心主题就是机器学习给失败的案例空间中增加了维度，这些维度导致出错形式的指数增加。深度强化学习增加了一个新的维度：随机几率（random chance）。解决随机几率的唯一方法就是在问题上投入足够多的实验来降低噪声。 当你训练一个样本低效并且不稳定的算法时，它会严重降低生产性研究的速度。或许它只需要一百万步。但是如果你使用了 5 个随机数种子，那就是将调节的超参数变成了原来的 5 倍，为了有效地测试你的假设，你需要极其多的计算量。 如果这样能让你感觉好一些的话，我愿意分享一下我的一些经历，我做这项工作已经有一段时间了，从零开始实现到能在很多强化学习问题上的 50% 时间中取得成功的策略梯度，花费了我大约六个星期的时间。我还有一个便于使用的计算机集群，以及一群自我来到这个地区之后就可以每天共进午餐的朋友。 此外，我们从监督学习中了解到的良好的 CNN 设计似乎并不适用于强化学习，因为经常受到信任分配/监督比特率的限制，而并不是因为缺乏强大的表征。ResNets、batchnorms 或者非常深的网络在这里没有任何作用。 [监督学习] 希望它起作用。即使把有些东西搞砸了，你也会得到一个非随机的结果。但是强化学习是被强迫起作用的。如果你把某些事情搞砸了或者你没有将一些东西调节到足够好，你很有可能得到一个比随机结果更糟糕的情况。即使一切都正常，你也有可能在 30% 的时间里得到一些糟糕的策略。长话短说，仅仅是因为深度强化学习的难度，更少是因为「设计神经网络」的难度。 随机种子的不稳定性就像飞翔在矿井中的金丝雀。如果单纯的随机就足以在运行中导致如此大的差异，那么想象一下代码中实际的差异会有多大。 幸运的是，我们不必进行这样的想象。因为这已经被这篇论文检验过了——「Deep Reinforcement Learning That Matters」(Henderson et al, AAAI 2018)。论文结论如下： 给奖励函数乘以一个常量会导致显著的性能差别 5 个随机种子（常用值）不足以证明这种显著的结果，因为通过仔细挑选可以得到一些不重叠的置信区间。 同一算法的在同一个任务上的不同实现会有不同的性能，甚至是当使用相同的超参数的时候。 我在这里持有的观点是：强化学习对初始化和训练过程的动态变化都很敏感，因为数据总是在线采集到的，你可以执行的唯一监督只有关于奖励的单个标量。在较好的训练样例上随机碰到的策略会比其他策略更快地引导学习。没有及时地遇到好的训练样本的策略会崩溃而学不到任何东西，因为它越来越坚信：它所尝试的任何偏离都会导致失败。 但是，我们又该如何看待深度强化学习的成功案例呢？ 深度强化学习确实做了许多很酷的事情。虽然 DQN 现在已经是老生常谈了，但是在那个时候确实是比较疯狂的。单个模型就可以直接从原始像素开始学习，而不需要为每个游戏进行单独地调节。后来 AlphaGo 和 AlphaZero 又继续获得了引人注目的成就。 然而，除了这些成功之外，很难在现实世界中发现深度强化学习产生实际价值的案例。 我曾经费尽心力去思考深度强化学习在现实世界中的生产应用，发现这是惊人的困难。我曾经期望在推荐系统中寻找有用的案例，但是我认为这些系统仍旧被协同过滤（collaborative filtering）和上下文老虎机 (contextual bandits) 主导着。 最终，我能找到的最好案例是 Google 的两个项目：降低数据中心能耗（https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/）和最近发布的 Auto ML Vision。OpenAI 的 Jack Clark 在推特上的发问也得到了类似的结论。 我知道奥迪也在使用深度强化学习研发技术，因为他们在 NIPS 上展示了一辆自动驾驶汽车的 RC 版本，据说这款汽车使用了深度强化学习。我知道有一些优化大规模 TensorFlow 图中设备部署的优秀工作（Device Placement Optimization with Reinforcement Learning）。Salesforce 公司有自己的文本摘要模型，基本也可以工作。金融公司肯定正在尝试使用强化学习，但是目前还没有确凿的证据。Facebook 一直在用深度强化学习做一些聊天机器人和广告方面的优秀工作。每家互联网公司可能都考虑过将深度强化学习添加到它们的广告服务模型中，但是即使这么做了，他们也会对此守口如瓶。 我认为，要么深度强化学习仍然是一个研究课题，它不够鲁棒，所以没有广泛的应用，要么，深度强化学习已经可用了，并且使用深度强化学习的人没有公之于众。我认为前者更有可能。 如果是图像分类的问题，我会推荐预训练的 ImageNet 模型，它们很可能表现得更好。我们现在处于这么一个世界，硅谷的人们可以开发出一款 Not Hotdog 应用来开玩笑。然而我很难看到深度强化学习也有如此的盛况。 在目前的局限下，深度强化学习何时才能真正地工作呢？ 很难说。尝试用强化学习解决一切的问题其实就是用同一个方法解几个特别不同的环境中的问题。不会总是成功的，这很自然。 尽管如此，我们还是可以从目前深度强化学习的成功案例得出一些结论。在这些项目中，深度强化学习要么学会了十分令人印象深刻的东西，要么它学会了相比以前的工作更好的东西。（诚然，这是非常主观的判断标准） 下面是我列出的清单： 之前所提及的：DQN、AlphaGo、AlphaZero、跑酷机器人、降低数据中心能耗的应用，以及使用神经架构搜索的 Auto ML。 OpenAI 的 Dota2 1v1 暗影恶魔机器人，它在简化版本的决斗环境中击败了顶级人类职业玩家（https://blog.openai.com/dota-2/）。 超级粉碎机器混战机器人（https://arxiv.org/abs/1702.06230），它在 1v1 的猎鹰 dittos 游戏中可以击败职业玩家。 顺便说一下：机器学习最近在无限注德州扑克中击败了专业玩家。Libratus（Brown et al, IJCAI 2017，https://www.ijcai.org/proceedings/2017/0772.pdf) 和 Deepstack（Moravčík，https://arxiv.org/abs/1701.01724）都完成了这样的壮举。我和一部分人交谈过，它们认为这项工作使用了深度强化学习技术。它们都很酷，但是它们并没有使用深度强化学习。而是使用了反事实后悔值最小化（CFR minimization）和巧妙的子游戏迭代求解。 我们可以从上面这个列表中发现一些让学习变得更加容易的共同属性。以下列出的属性都不是学习所必需的，但是更多地满足这些属性学习效果会更好。 生成近乎无限多的经验是很容易的。我们应该清楚为什么这是有帮助的。拥有的数据越多，学习问题就会变得越容易。这适用于 Atari 游戏、围棋游戏、象棋游戏、日本将棋游戏以及跑酷机器人的迷你环境。它也可能适用于电力中心的项目，因为之前 Gao 的工作已经显示：神经网络能以很高的准确率预测能源效率。这正是你想要为一个强化学习系统训练的模拟模型。它也可能适用于 Dota2 和 SSBN 的工作，但是它取决于游戏运行速度的吞吐量，以及有多少机器可以用来运行游戏。 问题被简化成了一个更简单的形式。我在深度强化学习中看到的一个常见的错误就是梦想过于庞大。总以为强化学习无所不能！但是这并不意味着可以立即达成一切。OpenAI 的 Dota2 机器人只玩早期的游戏，只在使用硬编码组件构建的 1v1 laning 环境中玩暗影恶魔和暗影恶魔的对抗，想必是为了避免感知问题而称其为 Dota2 API。虽然 SSBM 机器人实现了超越人类的性能，但是那仅仅是 1v1 的游戏，在无限时间的比赛中只有猎鹰队长在战场上。这不是在嘲讽任何一个机器人。你为何要在连一个简单问题都无法解决的时候就去解决一个艰难的问题呢？所有研究的广泛趋势都是先去证明最小的问题上的概念，然后再去泛化。OpenAI 在扩展他们在 Dota2 上的工作，当然也存在将 SSBN 上的工作扩展到其他角色上的工作。 也有可以将自我对抗引入到学习中的方式。这是 AlphaGo、AlphaZero、DOTA2 暗影狂魔机器人以及 SSBN 猎鹰机器人的组成部分。应当注意的是，我在这里所说的自我对抗指的是这样的游戏环境：游戏是竞争性的，两个玩家都可以被同一个智能体控制。目前，这样的环境似乎拥有最好的稳定性和最好的性能。 有一个简洁的方式来定义一个可学习的、不冒险的奖励函数。两个玩家的游戏有这样的特点：赢了得到+1 的奖励，输了得到-1 的奖励。Zoph 等人最早的文章有这样的奖励函数：验证已训练模型的准确率（neutral architecture search for reinforcement learning，ICLR,2017）。当你引入奖励函数重塑时，就可能学习了一个优化错误目标的非最优策略。如果你对如何拥有一个更好的奖励函数比较感兴趣，「proper scoring rule」是一个不错的检索词。至于可学习性，除了做实际尝试之外，我没有更好的建议。 如果必须对奖励函数进行重塑，那么它至少是结构较丰富的。在 Dota2 中，奖励可以来自于上次的命中（每个玩家杀死一只怪兽之后就会触发）和生命值（在每一次攻击或者技能命中目标之后就会触发）。这些奖励信号出现的很快而且很频繁。对 SSBN 机器人而言，可以对所造成的伤害给予奖励，这将为每次的攻击提供信号。行动和结果之间的延迟越短，反馈回路能够越快地闭合，强化学习就越容易找到回报更好的路径。 案例研究：神经架构搜索 我们可以结合一些原则来分析神经架构搜索的成功。从 ICLR 2017 中的原始版本论文中可以得知，在 12800 个样本之后，深度强化学习能够设计出最先进的神经网络架构。诚然，每个例子都需要训练一个神经网络以得到收敛，但是这仍然是非常有效的。 正如上面所提到的，其奖励函数是验证准确率。这是结构非常丰富的奖励信号——如果一个神经网络的设计决策将准确率仅仅从 70% 提高到了 71%，强化学习仍然可以从这一点上获得进展，此外，深度学习中的超参数接近线性无关。Hazan 等人在「Hyperparameter Optimization: A Spectral Approach」中进行了经验证明。神经架构搜索（NAS）并不是在调参数，但是我认为，神经网络设计决策是以类似的方式工作。这对学习来说是一个好消息，因为决策和性能之间的关联是很强的。 与其他环境中需要数百万个样本相比，以上这些要点的结合可以帮助我们理解为什么「仅仅」需要 12800 个训练好的神经网络就可以学习到一个更加好的网络架构。这个问题中的几个部分都在朝着强化学习倾斜。 总之，如此强大的成功案例仍然是一种例外，而不是必然的规则。深度强化学习现在还是不是拿来即用的技术。 展望未来 有这么一句话—每个研究人员都知道如何厌恶自己的研究领域。然而关键在于，尽管如此，大家还是会坚持下去，因为他们实在是太喜欢这些问题了。 这也是我对深度强化学习的大致感受。尽管我有所保留，但是我认为人们绝对应该将强化学习投入到不同的问题中去，包括那些可能不会成功的地方。那么我们还能怎样使强化学习变得更好呢？ 给再多的时间，我也找不到深度强化学习无法工作的理由。当强化学习变得足够鲁棒和得到广泛应用的时候，一些十分有趣的事情就会发生。问题在于如何到达那一步。 我在下面列举出了一些比较合理的未来情况。对于基于进一步研究的未来，我已经引用了这些领域的相关研究论文。 局部最优已经足够好了：如果说人类在任何事情上都是最棒的，那可能有些傲慢。与其他物种相比，我想我们只是足够好的到达了文明阶段。同样的，强化学习解决方案也不必获得全局最优解，只要它的局部最优解能够比人类的基线好就行了。 硬件解决一切：我知道一些人觉得可以为人工智能做的最重要的事情就是简单地扩展硬件。我自己则对硬件解决一切持怀疑态度，但是硬件确实是很重要的。你的运行速度越快，就可以越少地关心采样低效，就可以更容易地暴力解决探索问题。 增加更多的学习信号：稀疏奖励函难以学习，因为你获得的帮助信息很少。我们要么误以为获得了正奖励（Hindsight Experience Replay），要么通过自我监督学习建立更好的世界模型（ ）。可以说是在蛋糕上增加了更多的樱桃。 基于模型的学习可以提高采样效率：这是我对基于模型的强化学习的描述，「大家都想做，但是没几个人知道如何做」。原则上，一个好的模型可以解决一系列的问题。就像我们在 AlphaGo 中看到的一样，拥有一个比较全面的模型使得学习解决方案变得更加容易。好的世界模型可以很好地迁移到新的任务上面，而世界模型的展开可以让你想象新的体验。据我所见，基于模型的方法使用的样本也比较少。 问题在于学习一个好模型是很困难的。在我的印象中，低维度的状态模型有时候会起作用，而图像模型则通常是比较困难的。但是，如果这个也变得很容易的话，一些有趣的事情就会发生。 Dyna 和 Dyna2 是这个领域中的经典论文。对于使用深度网络结合基于模型学习的论文而言，我想推荐一下最近伯克利机器人学实验室的一些论文： Neutral Network Dynamics for Model-Based Deep RL with Model-Free Fine-Tuning Supervised Visual Planning with Temporal Skip Connections Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning Deep Spatial Autoencoders for Visuomotor Learning End-to-End Training of Deep Visuomotor Policies 仅将强化学习用作微调步骤：第一篇 AlphaGo 论文从监督学习开始，然后在此基础上进行强化学习的微调。这是一个很好的方法，因为它可以让你使用一个更快但功能不太强大的方法来加速初始学习。还存在其他环境下的工作——参考《Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control》。可以将此视为以合理的先验（而不是随机先验）开始强化学习过程，在这种情况下，学习先验的问题可以用其他方法求解。 奖励函数是可以学习的：机器学习的承诺是我们可以使用数据去学习到比人类设计更好的东西。如果奖励函数设计这般难，为什么不用它来学习到更好的奖励函数呢？模仿学习和逆强化学习都是成果很丰富的领域，它们已经展示了奖励函数可以通过人类的演绎或者评估来隐式地定义。 逆强化学习和模仿学习方面的著名论文有： Algorithms for Inverse Reinforcement Learning Apprenticeship Learning via Inverse Reinforcement Learning A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning 关于最近将这些思想扩展到深度学习领域的工作，可以参考以下论文： Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization Time-Contrastive Networks: Self-Supervised Learning from Video Learning from Human Preferences 其中，《Learning from Human Preferences》这篇论文表明，从人类评级中获得的奖励实际上比原来的硬编码奖励更适合于学习。 对于无需深度学习的长期研究，我更喜欢： Inverse Reward Design Learning Robot Objectives from Physical Human Interaction  迁移学习可以节省时间：迁移学习承诺可以利用之前任务中学习到的知识来加快新任务的学习。我认为这绝对是未来的希望，那时候任务学习已经足够鲁棒来解决多个不同的任务。如果根本学习不到任何东西，就很难进行迁移学习，给定了任务 A 和任务 B，就很难预测 A 是否迁移到了 B。根据我的经验，这要么是超级明显的，要么是很不清楚的，即便是超级明显的情况，也不会是很普通的工作。 这个方向上有三篇最近的论文： Universal Value Function Approximators Distral: Robust Multitask Reinforcement Learning Enabling Continual Learning in Neural Networks 而比较早期的工作可以考虑一下 Horde： Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction 机器人学特别是在从仿真向真实的转移方面的进展（仿真版本的任务和真实版本任务之间的迁移），参见： Spam Detection in the Physical World Sim-to-Real Robot Learning from Pixels with Progressive Nets Closing the Simulation-to-Reality Gap for Deep Robotic Learning 好的先验知识会大大缩短学习时间：这与前面的几点息息相关。有一种观点：迁移学习是使用过去的经验来构建较好的学习其他任务所需的先验知识。强化学习算法被设计适用于任何马尔科夫决策过程，这才是泛化性的痛点来源。如果承认我们的解决方案只能在一小部分环境中运行良好，我们就能够以一种有效的方式利用共享结构来解决这些问题。 Pieter Abbeel 在他的演讲中喜欢提到的一点是：深度强化学习只需要解决我们在现实生活中需要解决的任务。我赞同这个观点。应该有一个现实世界的先验知识，让我们能够快速学习新的现实世界的任务，代价是在非现实的任务上学习更慢，但这是一个完全可以接受的权衡。 困难在于设计这样的一个现实世界先验知识是很难的。然而，我认为不是不可能的。就我个人而言，我对最近在元学习方面的工作感到兴奋，因为它提供了一种数据驱动的方法来生成合理的先验知识。例如，如果我想使用强化学习进行仓库导航，我会非常好奇如何使用元学习事先学习一个好的导航，然后针对机器人将要部署到的特定仓库对这个先验知识进行微调。这看起来很有希望，问题是元学习能否实现。 BAIR 在这篇文章中总结了最近的「学习到学习」方面的工作：http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/（参阅： 学界 | 与模型无关的元学习，UC Berkeley提出一种可推广到各类任务的元学习方法 ）。 更困难的环境可能会使问题变得更容易：DeepMind 的跑酷论文给我们的最大的教训就是：如果可以通过增加一些任务变体使得任务变得非常困难，实际上你可以使学习变得更加容易，因为策略不能在不损失所有其他环境中的性能的情况下过拟合任何一个环境。在域随机化的论文中，甚至在 ImageNet 中，我们也看到了类似的情况：在 ImageNet 上训练的模型比在 CIFAR - 100 上训练的模型更具泛化性。正如我前面提到的，也许我们只是一个「受控的 ImageNet」，而远没有使强化学习更通用。 环境方面倒是有很多选择。OpenAI Gym 非常具有吸引力，当然也有街机学习环境（Arcade Learning Environment）、机器人学习环境（Roboschool）、DeepMind Lab、DeepMind 控制组件（DeepMind Control Suite），以及 ELF。 最后，尽管从研究的角度来看还是不尽如人意，但是深度强化学习的经验问题或许对实际目的来说不是很重要的。作为一个假设的例子，假设一家金融公司正在使用深度强化学习。基于美国股市过去的数据，他们使用三个随机数种子训练了一个智能体。在实时的 A/B 测试中，一个给出了 2% 的收益减少，第二个和第一个表现相同，第三个带来了 2% 的收入增益。在这种假设下，再现性并不重要，你大可以部署那个带来 2% 收入增益的模型，并准备庆祝。同样地，交易智能体可能只在美国表现良好，这也没关系——如果它在全球市场泛化得不好，就不要在那里部署它。在做一些非凡的事情和让非凡的成功重现之间有很大的差距，也许值得把注意力集中在前者。 我们现在的处境 从很多方面来说，我发现自己对深度强化学习的现状感到恼火。然而，它吸引了一些我所见过的最强烈的研究兴趣。我的感受最能概括为吴恩达在他的演讲《Nuts and Bolts of Applying Deep Learning》中所说的：短期悲观，长期乐观。深度强化学习目前还有些混乱，但是我仍然相信它可以成功。 话虽如此，下次有人问我强化学习能否解决他们的问题，我还是要告诉他们，不行，不行。但我也会告诉他们几年后再问我。到那时，也许可以。 （原文视频较多，微信文章无法一一展现，全部视频请参见原文链接） 原文链接：https://www.alexirpan.com/2018/02/14/rl-hard.html "
111,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739431&idx=3&sn=329da59b4a2f11a464fd03121a02c40e&chksm=871ad699b06d5f8ff202026cbf4d6fe9fee7c6c8c5a0d1214c2bc7cde71704af2be484a587da&scene=27,观点 | 增加深度，加速神经网络优化？这是一份反直觉的实验结果,"深度学习的根本理论问题之一是「深度有何作用」？虽然增加神经网络的层数可以提高其性能，但是训练和优化的难度也随之增加。本文却给出了一个相反观点，有时增加深度反而可以加速网络优化；同时提出端到端更新规则，证明深度网络过度的参数化（overparameterization）也可能是件好事。 深度学习理论中有一个根本的问题：即「网络的深度有何作用？」传统观点（如：Eldan & Shamir 2016; Raghu et al. 2017; Lee et al. 2017; Cohen et al. 2016; Daniely 2017）认为添加隐藏层可以提升网络表现力。但是往往这种能力的提升需要付出一定的代价——网络越深，优化越难（此即梯度消失或梯度爆炸问题）。最近有关「landscape characterization」的工作其实就隐含地采用了这种世界观，虽然看起来不明显（Kawaguchi 2016; Hardt & Ma 2017; Choromanska et al. 2015; Haeffele & Vidal 2017; Soudry & Carmon 2016; Safran & Shamir 2017）。他们证明了在深度网络目标中有关局部最小值与鞍点的理论。本文作者与 Sanjeev Arora 以及 Elad Hazan 合作的新论文《On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization》给出了一个反直观的观点，文章认为，有时增加深度反而可以加速网络优化。 有关深度网络过度参数化（overparameterization）可能是件好事是近来刚兴起的一种观点，作者的工作可说是支撑这一观点的又一个证据。相反，经典统计学并不推荐给模型设置更多的参数，没有必要，因为这会导致过拟合现象。 ℓ_p 回归 我们从一个非常简单的学习问题出发——ℓ_p 损失的标量线性回归（我们的理论和实验应用在当 p>2 的情况下）： 这里 S 代表一个训练集，它由一对（x,y）组成，其中 x 是一个表征事件的向量，y 是一个表征其标签的（数值）标量；w 是我们想要学习的参数向量。现在，让我们用向量 w_1 乘上标量 ω_2 替换向量 w，把线性模型转换成一个非常简单的深度为 2 的网络。显然，这种过参数化操作并没有改变网络的表现力，但却生成了非凸目标函数（non-convex）： 如论文所示，如果在 w_1 和ω_2 上应用梯度下降，学习速率很低，初始化接近零（深度学习的常见操作），那么在整个（端到端）模型 w=w_1·ω_2 上产生的动态影响可以写作： 其中ρ^(t) 和μ^(t,τ) 可被恰当地定义为（依赖时间的）系数。因此，看似一个正常的乘法标量的加法将普通的梯度下降转换成了一个以某种方式记录了过往的梯度（momentum 方法的关键特征）以及随时间变化学习速率有所差异的组合形式。尽管对 momentum 方法之精确好处的理论分析绝不容易，但是一项基于 UCI 机器学习库的「Gas Sensor Array Drift at Different Concentrations」数据集的简单实验（p=4）为我们展示了如下的效果： 这里我们注意到，过度参数化不仅仅加速了梯度下降，而且其加速效果还优于两个著名的梯度下降方案——AdaGrad 和 AdaDelta，在本实验中，前者并没有真正起到加速作用。在其它设置中，我们也观察到了类似的加速。 所以在这里发生了什么？与深度网络相对应的非凸目标函数比起凸目标函数来说是否更容易优化？这是一个普遍现象，还是只限于上述这种简单问题？我们首先来解决这些问题。 过参数化：表现性的解耦优化 一项有关深度对优化影响的综合研究表明了一个固有的问题存在——由于其优秀的表现力，更深的网络似乎收敛的更快。换句之，如果深度网络的优化过程进展得比浅层网络快，那么或许并不能显而易见地认为，这是一种真正加速现象的结果，或者说它仅仅是以下事实的副产品，即浅层模型不能达到和深层模型一样的损失。通过研究那些表征能力与网络深度无关的模型——线性神经网络（这是最近很多研究的主题），我们解决了这个难题。对于线性网络来说，增加层数并不会改变网络的表现力。只有当矩阵参数被矩阵乘积所替代——即过参数化，这才会发生。因此，如果这导致了收敛过程的加速，那么我们可以确定这并不是任何现象的结果，而是用于优化的深度的良好属性。 深度所带来的隐性动态过程 假设我们有兴趣来学习一个线性模型，其参数化由矩阵 W 完成，通过一些训练损失函数 L(W) 极小化来得到。我们不直接用使用矩阵 W，而是将其替代为一个深度为 N 的线性神经网络，例如我们将其过参数化成 W=W_NW_N−1⋯W_1，其中 W_j 是特定层的权重矩阵。在文中，我们指出如果人们在 W_1…W_N 上以低学习率η应用梯度下降，且有： 满足系统的优化启动（注意，这大致适用于标准的近零初始化），那么在全局端到端映射 W 引发的动力机制可以写作如下形式： 我们在经验上进行了验证，发现这个由分析得出的更新规则（对经典线性模型）实际上可以用于深度网络的优化，在理论上我们可以通过一系列步骤来解释它。我们发现，应用在 ∇L(W) 梯度上的变换（从左边乘上  从右边乘上  ，然后再是 j 的求和）是一种特殊的预处理方案，可促进沿优化方向的运动。更具体地说，预处理可以被认为是下述两种元素的结合： 一个适应性学习速率，它可以让步长增大，远离初始值； 一种类似于 momentum 的操作，沿着目前采用的方位角来拉伸梯度。 重要的一点是，上述更新规则（以下称为端到端更新规则）不是依靠线性神经网络中隐藏层的的宽度，而是深度（N）。这意味着，从优化的角度看，使用宽或窄的网络的过参数化具有同样的效果。重要的仅仅是网络层的数量，因此用深度来加速没有计算上的需求——我们在实验中清楚地观察到了这一事实（例如，前面的图像展示了通过牺牲单一的额外标量参数来实现数量级加速）。 超越正则化 端到端更新规则定义了一种新优化方法，其过程由梯度∇L(W) 函数与参数 W 构成。与很多加速方法（比如 momentum 或者 Adam）那种明显要保持辅助变量相反，这种方法是 memoryless 的，并且根据定义它由对过参数化目标进行梯度下降法得来。所以自然地可以提出问题：如果我们可以将端到端更新规则表征成对一些损失 L(W) 正则化进行的梯度下降（即一些由 W 确定的函数），那么我们可以令人吃惊地证明，答案总是否定的——只要损失 L(W) 不在 W=0 位置有临界点，那么端到端更新规则（即过参数化的效力）就不能通过任何正则化来实现。 加速 到目前为止，通过给出一种等价的预处理方法并讨论了其一些特性，我们分析了深度（以过参数化的方式）在优化性能上的效用。然而，我们并没有给出任何理论证据来支持加速（更快地收敛速度）过程来自于这种方法。对这种加速过程场景的全面分析超越了我们文章的讨论范围。然而，我们在文中也的确分析了简单的ℓ_p 回归问题，并且发现，无论我们是否增加网络的深度加速效果，一切都决定于对 p 的选择：对于 p=2（平方误差）增加网络的层数并不会导致加速（根据 Saxe et al. 2014 的发现）；然而对于 p>2 来说，这是可以的，这或许是因为预处理方法可以在目标 landscape 上处理大型 plateaus。大量 p 的值为 2 和 4，深度在 1 到 8（经典线性模型）的实验，都支持了这个结论。 非线性实验 作为最后一项实验，我们评估了在一个简单深度学习设置（基于 MNIST 数据集的 TensorFlow 卷积网络教程）上过参数化对优化的效果。通过简单地连续放置两个矩阵，而不是每一密集层的矩阵，我们引入了过度参数化。通过添加大约 15% 的参数数量，优化的提升呈数量级增长。 我们发现，其它卷积网络上的相似实验同样可以获得提速，但是不如上述方法显著。过度参数化加速非线性优化的条件下的实证表征将是未来研究的一个有趣方向。 结论 本文从优化的角度借助过度参数化展示了增加深度的好处，但依然存在很多问题。例如，严格地分析端到端更新规则的加速效果是否可能（比如，可与 Nesterov 1983 或 Duchi et al. 2011 相类比）？当然，对非线性深度网络以及更多其它实证评估的探讨也是有益处的。 "
112,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739470&idx=3&sn=154a81fcf71f7197c8a5eafac51f23da&chksm=871ad770b06d5e66a61442b547fe93008c97e9ee76ad40e5ba39576f94d9942c094e35b0ab6b&scene=27,业界 | Uber开源神经进化算法开发的交互式可视化工具VINE,Uber 近日，Uber 开源了神经进化算法开发的交互式可视化工具 VINE，该工具可以轻松实现神经网络群体的各种特定指标以及适应度分数的可视化和随时间的变化，用户可对其进行实时评估。此外，VINE 还支持默认功能之外的高级选项和自定义可视化。 VINE 项目地址：https://github.com/uber-common/deep-neuroevolution/tree/master/visual_inspector 对于 Uber 的产业规模来说，机器学习的发展可以大大提高交通运输技术的安全性和可靠性，Uber AI Labs 最近宣布的 就是其中一例。深度神经进化的进化算法，如进化策略（ES）和遗传算法（GA），可以帮助训练深度神经网络，以解决棘手的强化学习 ( RL ) 问题。最近，深度神经进化热度不断增加， I 、 、  和 Sentient 也多有贡献，该领域研究人员对相应工具的需求也越来越大。 特别是，在神经进化和神经网络的优化中，学习过程的潜在动态通常难以观察。为解决上述两个问题，Uber 开发了神经进化视觉检查器 ( Visual Inspector for Neuroevolution，VINE )，这是一个开源交互式数据可视化工具，旨在帮助神经进化研究者更好地理解和探索这一系列算法。该工具非常轻便，使用 Python 语言编写。Uber 希望该技术可以促进神经进化的创新和应用。 VINE 可以实现 ES 和 GA 类方法的可视化。本文给出的例子着重探讨应用 ES 的 Mujoco 拟人运动任务结果的可视化。 在 ES 的常规应用 (如 OpenAI 所推广的应用) 中，一组名为伪子代云（pseudo-offspring cloud）的神经网络针对一个目标进行了几代优化。云中每个单独神经网络的参数通过随机扰动单个亲代神经网络的参数来生成。然后对照目标对每个伪子代神经网络进行评估：在拟人运动任务中，每个伪子代神经网络控制机器人的运动，并根据机器人行走的情况得出一个分数，称为适应度（fitness）。ES 通过聚集基于这些适应度得分的伪子代的参数来构造下一个亲代神经网络 (类似于复杂的多亲代交叉，还可以联想到随机有限差分)，然后重复该循环。 图 1 :经过遗传算法 (左) 和进化策略 (右) 训练的模拟机器人的行走情况。 VINE 的应用 为了利用 VINE，在评估期间记录了每个亲代和伪子代的行为特征 ( BC)。这里，BC 可以是智能体与其环境交互时的任何行为指标。例如，在 Mujoco 中，我们仅使用智能体的最终 { x，y } 位置作为 BC，因为它指示了智能体从源位置移动了多远以及最终到了什么位置。 接下来，这个可视化工具根据亲代和伪子代的 BC 将它们映射到 2D 平面上。为此，它会调用图形用户界面 ( GUI )，GUI 的主要组成部分是两类相互关联的图：一个或多个伪子代云图 (在单独的 2D 平面上) 和一个适应度图。如图 2 所示，伪子代云图显示云中亲代和伪子代的每一代的 BC，而适应度图则显示作为各代运行情况关键指标的亲代适应度得分曲线。 图 2：伪子代云图与适应度图实例 用户随后与这些图交互以探索伪子代云的总体趋势及任何亲代或伪子代在进化过程中的个体行为：( 1 ) 用户可以实现任何给定代的亲代、最佳表现者和/或整个伪子代云的可视化，并探索具有不同适应度得分的伪子代在 2D BC 平面上的数量和空间分布；( 2 ) 用户可以实现代之间的对比，并在各代之间穿行，以可视化亲代云和/或伪子代云如何在 2D BC 平面上移动，并探索这些移动与适应度得分曲线有何关联 (如图 3 所示，移动云的完整影像片段可以自动生成)；( 3 ) 点击云图上的任意点可以显示相应伪子代的行为信息和适应度得分。 图 3 :可视化代际行为进化。每一代的颜色都会发生变化。在同一代中，每个伪子代的颜色强度基于该代适应度得分的百分位数 (聚合到五个仓中)。 其他应用案例 该工具还支持默认功能之外的高级选项和自定义可视化。例如，BC 不仅可以表示单个最终 { x，y } 点，还可以表示每个智能体的完整轨迹 (例如 1000 个时间步的串联 { x，y } )。在这种情况下，如果 BC 的维数大于 2，则需要使用降维技术 (例如 PCA 或 t - SNE ) 将 BC 数据的维数降到 2D。该工具可以自动执行这些步骤。 如图 4 所示，GUI 能够加载多组 2D BC (可能通过不同的降维技术生成)，并显示在实时连接的云图上。该功能为用户探索不同的 BC 选项和降维方法提供了方便的途径。 此外，用户还可以使用自定义功能扩展基本可视化。图 4 展示了一个自定义云图，它可以显示某些类型的特定域高维 BC(本例是智能体的完整轨迹) 以及相应的简化 2D BC。图 5 是自定义云图的另一个应用实例，它允许用户回放智能体与环境交互时产生的确定性行为和随机行为。 图 4 :多个 2D BC、一个高维 BC 及一个适应度图的可视化。 图 5 : VINE 允许用户查看任何智能体确定性行为和随机行为。 该工具还可用于运动任务以外的领域。图 6 的云图将 ES 训练智能体玩寒霜引擎（Frostbite）游戏（Atari 2600 游戏之一）进行了可视化，其中我们使用最终仿真器 RAM 状态 (长度为 128 的整数值向量，用于捕获游戏中的所有状态变量) 作为 BC，并应用 PCA 将 BC 映射到 2D 平面上。 图 6 :将智能体学习寒霜引擎游戏玩法可视化。 从图中我们可以观察到，随着进化的进行，伪子代云向左移动并在那里聚集。这个工具让我们看到每个智能体相应的玩游戏视频，由此可以推断出每个集群对应的语义意义上的不同结束状态。 VINE 还可以实现与其他神经进化算法的完美配合，例如 GA，该算法可以在若干代内保持子代的数量。事实上，该工具的运行独立于任何特定的神经进化算法。用户只需稍微修改他们的神经进化代码，就可以保留他们为解决特定问题选择的 BC。在发布的代码中，我们将对 ES 和 GA 实现的修改作为示例。 下一步工作 因为进化方法对一组点进行操作，所以它们为新类型的可视化提供了可能。Uber 希望与机器学习团体分享有用的可视化工具，以便所有人都能从中受益。随着神经进化扩展到具有数百万或更多连接的神经网络，通过 VINE 等工具加深理解对于进一步的研究越来越重要，也越来越有价值。 
113,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=2&sn=592ef223e200fc91ecdc787b6f4bc0b8&chksm=871ad6c7b06d5fd17434a0ae6cd801431e75b3af91b9fdfc50ae12b93ddb86cb99552b9df76f&scene=27,业界 | 进化算法 + AutoML，谷歌提出新型神经网络架构搜索方法,Google Research Blog 通过在 AutoML 中结合进化算法执行架构搜索，谷歌开发出了当前最佳的图像分类模型 AmoebaNet。本文是谷歌对该神经网络架构搜索算法的技术解读，其中涉及两篇论文，分别是《 》和《Regularized Evolution for Image Classifier Architecture Search》。 从 5 亿年前的超简单的蠕虫脑到今天的各种各样的现代结构，大脑经历了漫长的进化过程。例如，人类大 脑可以指导完成非常广泛的活动，大部分活动都能轻而易举地完成，例如辨别一个视觉场景中是否包含动物或建筑对我们来说是很简单的事。而要执行类似的活动，人工神经网络需要专家数年的艰苦研究、精心设计，且通常只能执行单个具体的任务，例如识别照片中的目标、调用遗传变异，或者帮助诊断疾病等。人们希望拥有自动化的方法，为任意给定的任务生成合适的网络架构。 使用进化算法生成这些架构是其中一种方法。传统的拓扑神经进化研究（如《Evolving Neural Networks through Augmenting Topologies》Stanley and Miikkulainen，2002）为大规模应用进化算法奠定了基础，很多团队都在研究这个主题，例如  I、 、Sentient Labs 和  。当然，Google Brain 团队也在尝试用   执行架构搜索。除了基于学习的方法（如强化学习），谷歌想了解使用谷歌的计算资源以前所未有的规模来程序化地演化图像分类器，会得到什么样的结果。可以用最少的专家参与获得足够好的解决方案吗？目前的人工进化神经网络能达到什么样的程度？谷歌通过两篇论文来解决这个问题。 在 ICML 2017 大会中展示的论文《Large-Scale Evolution of Image Classifiers》中，谷歌用简单的构建模块和常用的初始条件设置了一个进化过程。其主要思想是让人「袖手旁观」，让进化算法大规模构建网络架构。当时，从非常简单的网络开始，该过程可以找到与手动设计模型性能相当的分类器。这个结果振奋人心，因为很多应用可能需要较少的用户参与。例如，一些用户可能需要更好的模型，但没有足够的时间成为机器学习专家。接下来要考虑的问题自然就是手动设计和进化的组合能不能获得比单独使用一个方法更好的结果。因此，在近期论文《Regularized Evolution for Image Classifier Architecture Search》（2018）中，谷歌通过提供复杂的构建模块和较好的初始条件（参见下文）来参与进化过程。此外，谷歌还使用其新型 TPUv2 芯片来扩大计算规模。通过现代硬件、专家知识和进化过程的组合，谷歌获得了在两个流行的图像分类基准 CIFAR-10 和 ImageNet 上的当前最优模型。 简单方法 接下来我们介绍第一篇论文中的一个例子。下图中，每个点都是一个在 CIFAR-10 数据集（通常用于训练图像分类器）上训练的神经网络。在初始阶段，该群体中有 1000 个相同的简单种子模型（没有隐藏层）。从简单的种子模型开始非常重要：假如从初始条件包含专家知识的高质量模型开始，则系统将更容易最终获得高质量模型。而一旦从简单的模型开始，进化过程就可以随时间步逐渐提高模型质量。在每个时间步，进化算法会随机选择一对神经网络，具备更高准确率的网络被选为亲代网络，并通过复制和变异获得子代网络，然后该子代网络被加入原来的群体中，而另一个准确率较低的网络则被移除。在这个时间步内，所有其它的网络都保持不变。通过持续应用多个此类时间步，该群体得以不断进化。 谷歌第一篇论文中的变异设置得很简单：随机删除卷积层，在任意层之间添加 skip connection，或者改变学习率等等。通过这种方式，研究结果证实了进化算法的潜力，与搜索空间的质量成反比。例如，如果我们使用单个变异，在某一步将一个种子网络变换成 Inception-ResNet 分类器，那么我们会错误地认为该算法找到了优秀的答案。但是，在那种情况中，我们只能将最终答案硬编码为复杂的变异，控制输出。而如果我们坚持使用简单的变异，则这种情况不会发生，进化算法能够真正完成任务。在图中的实验中，简单的变异和选择过程导致网络随着时间不断改进，并达到了很高的测试准确率，且测试集在训练过程中不曾出现。在这篇论文中，网络可以继承其亲代网络的权重。因此，除了促进架构进化以外，群体可以训练其网络，同时探索初始条件和学习率调度（learning-rate schedule）的搜索空间。因此，该过程获得了完全训练的模型，且该模型具备优化过的超参数。实验开始后不需要专家输入。 上述情况中，即使我们通过掌握简单的初始架构和直观变异最小化研究人员的参与，这些架构的构建模块中也存在大量专家知识，包括卷积、ReLU 和批归一化层等重要创新。谷歌对这些组件构成的架构进行了进化操作。「架构」一词并非偶然：它就像用高质量砖石建筑房屋。 结合进化和手动设计 在第一篇论文发布后，谷歌想通过给予算法更少探索选择来减少搜索空间，提高可控性。就像刚才那个关于架构的类比一样，谷歌去除了搜索空间中所有可能导致大型误差的方式，比如建筑房屋时把墙建在屋顶上。类似地，在神经网络架构搜索方面，固定网络的大尺度结构可以为算法解决问题提供一定的帮助。例如，Zoph et al. (《Learning Transferable Architectures for Scalable Image Recognition》) 论文中提出的用于架构搜索的类 inception 模块非常强大。他们想构建一个重复模块 cell 的深度堆叠结构。该堆叠是固定的，但是单个模块的架构可以改变。 在第二篇论文《Regularized Evolution for Image Classifier Architecture Search》中，谷歌展示了将进化算法应用到上述搜索空间的结果。通过随机重连输入（上图右侧的箭头）或随机替换操作（如将图中的最大池化操作「max 3x3」替换成任意其他可替换操作）等变异来修改 cell。这些变异仍然比较简单，但是初始条件并不简单：群体中初始化的模型必须遵从 cell 外部堆叠（由专家设计）。尽管这些种子模型中的 cell 是随机的，但是我们不再从简单模型开始，这样更易获得高质量模型。如果进化算法发挥出很大作用，则最终网络应该显著优于我们目前已知的可在该搜索空间内构建的网络。这篇论文展示了进化算法确实能够找到当前最优的模型，可匹配甚至优于手动设计的搜索方式。 受控比较 尽管变异／选择进化过程并不复杂，但可能存在更直接的方法（如随机搜索）可以达到同样的效果。其他方法尽管并不比进化算法简单，但仍然存在（如强化学习）。因此，谷歌第二篇论文的主要目的是提供不同技术之间的受控比较。 上图对比了进化算法、强化学习与随机搜索。左图，每个曲线表示实验的进程，结果表明进化算法在搜索的早期阶段要比强化学习快。这非常重要，因为在计算资源有限的情况下，实验可能不得不早早结束。此外，进化算法对数据集或者搜索空间的变化具备很强的稳健性。总之，这一受控对比旨在向研究社区提供该计算成本高昂的实验的结果。谷歌希望通过提供不同搜索算法之间关系的案例分析，为社区做架构搜索提供帮助。这里面有些需要注意的东西，例如，上图中表明，尽管使用更少的浮点运算，进化算法获得的最终模型也能达到很高的准确率。 在第二篇论文中，谷歌所用进化算法的一大重要特征是采用了一种正则化形式：相比于移除最差的神经网络，他们移除了最老的神经网络（无论它有多好）。这提升了对任务优化时所发生变化的稳健性，并最终更可能得到更加准确的网络。其中一个原因可能是由于不允许权重继承，所有的网络必须都从头开始训练。因此，这种形式的正则化选择重新训练后依旧较好的网络。也就是说，得到更加准确的模型只是偶然的，训练过程中存在的噪声意味着即使完全相同的架构准确率也可能不同。更多细节可参看论文《Regularized Evolution for Image Classifier Architecture Search》。 谷歌开发出的当前最优模型叫作 AmoebaNet，这也是从 AutoML 中发展出的最新成果。所有实验都需要大量算力，谷歌使用数百 GPU/TPU 运行了数天。 
114,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739431&idx=1&sn=9caca86e6cd0812403e8c0309829b100&chksm=871ad699b06d5f8f2cdf0057aedd1807d07b36a194fd1a8a03aaf65673801608fdf3654d76b4&scene=27,打开黑箱重要一步，MIT提出TbD-net，弥合视觉推理模型的性能与可解释性鸿沟,"arXiv 近日，MIT 林肯实验室和 Planck Aerosystems 联合发布论文，提出一组可组合的视觉推理原语，并构建了 Transparency by Design network（TbD-net），通过整合注意力机制推进了模型透明度，同时又保证了高性能。TbD 在 CLEVR 数据集上达到了当前最优的准确率 99.1%；在 CoGenT 泛化任务上，TbD 比当前最优的模型提升了超过 20 个百分点。该论文被贴到 reddit 上后立刻引起大量关注。机器之心对该研究进行了介绍。 GitHub 地址：https://github.com/davidmascharka/tbd-nets 视觉问答（VQA）模型必须能够对图像进行复杂的空间推理。例如，为了回答问题：「大金属球右侧的立方体是什么颜色？」这个问题，机器学习模型必须确定哪个球体是大个、金属材质的，必须理解右侧是什么样的位置概念，并将这些概念应用于视野内所有物体。在新的探索区域内，模型必须找到立方体，并识别它的颜色。该行为应该是组合的，并可以允许任意长度的推理链。 尽管最近研究者针对 VQA 任务提出了大量不同的模型 [8, 12, 23, 26, 35, 37]，但神经模块网络 [2, 3, 12, 18] 是其中最直观的。神经模块网络由 Andreas et al. [2] 提出，由各自执行独立操作的一系列模块组成，以解决特定问题。它很好地建模了视觉推理任务的组合属性。早期研究中运用注意力机制设计模块，这种设计允许观察模型操作。但是，这一方法在复杂的视觉推理任务比如 CLEVR [17] 上表现并不好。Johnson et al. [18] 以损失模型透明度为代价解决了这一性能问题。但问题依然存在，因为要想确保适当的模型行为、取得用户信任、诊断推理误差，检查推理过程每一步的能力在实际应用中十分关键。 通过根据视觉注意力机制明确地设计一个模块网络，该论文的研究弥合了模型性能与可解释性之间的鸿沟。本论文作者把这一方法称为 Transparency by Design（TbD），如图 1 所示。Lipton [20] 指出，透明度和可解释性经常被提及，却从未被定义。本文将透明度定义为检查每个模块的中间输出、理解其高级行为的能力。也就是说，如果模块从视觉上强调了输入图像的正确区域，则模块输出是可解释的。这确保了推理过程的可解释性。章节 4.1 中具体定义了这一概念，并提供了量化分析。在本文中，研究者： 提出一组可组合的视觉推理原语，其整合了注意力机制，推进了模型透明度； 在 CLEVR [17] 数据集上展示了当前最优的性能； 表明组合性视觉注意力可以清晰洞察模型行为； 提出一种可以量化评估视觉注意力机制可解释性的方法； 在 CoGenT 泛化任务中 [17]，把当前最优性能提升了 20 个百分点。 Transparency by Design 将复杂的推理链分解成一系列较小的子问题，每个可以单独解决和组合，这是一种强大、直观的推理方式。这种模块结构还允许检查推理过程中每一步的网络输出，因为模块设计可以输出可解释的输出。受此启发，本论文作者引入一种神经模块网络，可以对图像空间中的注意力机制进行明确建模，该网络叫做 Transparency by Design 网络（TbD-net），设计时遵循透明度是激发因子这一原则。研究者希望该模型达到 Johnson et al. [18] 模型的性能水平，同时保持类似 Andreas et al. [2]、Hu et al. [12] 模型的透明度，因此该模型整合了这三种架构中的设计决策。Johnson et al. [18] 架构中的程序生成器具备极高的灵活性，性能优异，因此研究者在 TbD-net 网络中使用这这一组件。他们使用表 1 所示原语操作，但根据预期功能重新设计每个模块。生成的模块类似 Andreas et al. [2] and Hu et al. [12] 所用的方法。 为了执行这一设计决策，考虑到一些模块只需要关注图像的局部特征，如注意力模块关注明确的物体或属性。其他模块需要全局语境以执行操作，如 Relate 模块必须具备在整个图像中转换注意力的能力。研究者结合每个模块执行任务的实验数据，构建了一组新型模块架构，并针对每个操作进行了优化。 在视觉问答任务中，推理链中的大多数步骤需要定位具备显著可视化属性（如颜色、材料等）的物体。研究者确定每个执行此类过滤的 TbD 模块可输出一维注意力掩码，明确划分相关空间区域。因此，TbD-net 没有精细定义高维特征图，而是在其模块之间传输注意力掩码。通过特意执行该行为，研究者输出了一个可解释性和透明度极高的模型。这是远离把复杂神经网络视作黑箱的重要一步。图 3 展示了在解决复杂 VQA 问题时，TbDnet 在推理链中如何恰当地变换注意力，通过对其生成的注意力掩码的直观可视化，该过程很容易就可以解释。注意该模型对注意力的使用借鉴了 Hu et al. [12] 的研究。这些模块必须利用通过它们的注意力，必须输出准确的注意力地图。研究者展示的所有注意力掩码都是使用视觉均匀的颜色图生成 [14]。 图 3：阅读顺序是自上而下，TbD-net 使视觉注意力掩码回答关于场景中物体的问题。左侧的树状图表示 TbD-net 使用的模块，右侧表示模块对应的注意力掩码。 图 4：输入图像（左）和（大型）注意力模块在输入图像上生成的注意力掩码。未对注意力掩码输出进行惩罚时，注意力掩码带有噪声，在背景区域输出响应（中）。对注意力掩码输出进行惩罚给出了减少无关注意力的信号（右）。 表 2：顶尖模型在 CLEVR 数据集上的性能对比。该论文提出的模型性能良好，且保持模型的透明度。该模型在 Query 问题上实现了当前最优性能，在其他领域也具备很强的竞争力。TbD 模型训练时未对输出注意力掩码执行正则化，+ reg 表示使用了正则化。+ hres 表示模型训练时使用了更高分辨率 28 × 28 的特征地图，而不是 14 × 14。 论文：Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning 论文链接：https://arxiv.org/abs/1803.05268v1 摘要： 视觉问答需要对图像进行高阶推理，这是机器系统遵循复杂指令所需的基本能力。最近，模块网络（modular network）展现出其执行视觉推理任务的高效性。尽管模块网络最初设计时具备一定程度的模型透明度，但其在复杂视觉推理基准任务上的表现并不好。当前最优方法不提供理解推理过程的有效机制。本论文中，我们缩小了可解释性模型和当前最优视觉推理方法的差距，并提出了一组视觉推理原语，把它们组合为模型，可以明确可解释的方式执行复杂的推理任务。原语输出的准确度和可解释性使其具备诊断所得模型孰优孰劣的无与伦比能力。我们同样关键地展示了这些原语的高性能，它们在 CLEVR 数据集上达到了当前最优的准确率 99.1%。我们还展示了该模型在提供少量包含新型目标属性的数据时能够高效学习泛化表征。在 CoGenT 泛化任务上，我们的模型比当前最优的模型提升了超过 20 个百分点。 "
115,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=4&sn=c58835a2b9d92ac4426703ba7aea54e3&chksm=871ad6c7b06d5fd1c52c4f9fef4f15f7d8274050479d1b633a3fcc04c0b2c8890e5b47340a43&scene=27,ICASSP 2018 | 阿里巴巴语音交互智能团队：基于线性网络的语音合成说话人自适应,"语音领域的顶会 ICASSP 2018 将于 4 月 15-20 日在加拿大阿尔伯塔卡尔加里市举行。据机器之心了解，国内科技巨头阿里巴巴语音交互智能团队有 5 篇论文被此大会接收。本文对论文《Linear networks based speaker adaptation for speech synthesis》做了编译介绍。 欢迎大家向机器之心推荐优秀的 ICASSP 2018 相关论文。 论文：Linear networks based speaker adaptation for speech synthesis 原文链接：https://arxiv.org/abs/1803.02445 摘要： 说话人自适应算法利用说话人少量语料来建立说话人自适应语音合成系统，该系统能够合成令人满意的语音。在本文中，我们提出了基于线性网络的语音合成说话人自适应算法。该算法对每个说话人学习特定的线性网络，从而获得属于目标说话人的声学模型。通过该算法，使用 200 句目标说话人的自适应语料训练的说话人自适应系统能够获得和使用 1000 句训练的说话人相关系统相近的合成效果。 对于一个目标说话人，如果他（她）拥有充足的训练数据，那么我们便可以建立一个说话人相关的声学模型，基于该声学模型的系统称之为说话人相关的语音合成系统。利用该系统，我们能够合成和目标说话人声音很像的语音。但是，大多数时候，目标说话人没有充足的数据，这使得合成出来的语音效果不太理想。利用说话人自适应算法，能够基于比较有限的数据来获得较好的语音合成系统，该类算法节省了大量的录音、转录和检查工作，使得建立新的声音的代价变得很小。 本文中，我们提出了基于线性网络（Linear Network, LN）的语音合成说话人自适应算法。该算法通过在源说话人声学模型的层间插入线性网络，然后利用目标说话人的数据来更新该线性网络和神经网络的输出层，从而能够获得属于目标说话人的声学模型。另外，一种基于低秩分解（low-rank plus diagonal，LRPD）的模型压缩算法被应用于线性网络。实验发现，当数据量较少的时候，通过 LRPD 来移除一些冗余的参数，从而能够使得系统合成的声音更加稳定。 本文中，源说话人声学模型是一个基于多任务（multi-task）DNN-BLSTM 的声学模型，见 Fig. 1 左侧。声学模型的输入为语音学特征，输出为声学特征。声学特征包括梅尔倒谱系数等。实验证明，在声学模型的底层使用深层神经网络（Deep Neural Network，DNN）可以获得更好的底层特征，并且收敛速度上相比于不使用 DNN 更快。在输出层上，不同的声学特征使用各自的输出层，它们仅共享声学模型的隐层。 基于线性网络的自适应算法首先被提出于语音识别领域，它的系统结构见 Fig. 1 右侧。根据线性网络插入的位置不同，它可以被分为线性输入网络（Linear Input Network，LIN）、线性隐层网络（Linear Hidden Network，LHN）和线性输出网络（Linear Output Network，LON）。 当线性网络被插入到声学模型的第 和 层之间时，线性网络的输出 为： 其中， 表示第 层的输出， 表示说话人相关的线性变换矩阵， 表示说话人相关的偏置矢量。模型训练流程如下： 1）	将线性网络插入至源说话人声学模型特定位置。此时， 被初始化为单位矩阵， 的所有元素都初始化为 0。 2）	利用目标说话人的数据来更新线性网络中的参数 ，直到收敛。此时，保持声学模型中的其它层参数固定不变。最后，获得目标说话人的声学模型。 LRPD 算法主要被应用于线性网络的模型压缩。在语音识别中，基于 LRPD 的线性网络（LRPD-LN）能够减少普通线性网络（Full-LN）82% 的模型参数量，并且性能几乎不出现下降。LRPD 算法利用对角矩阵和低秩矩阵来表达 Full-LN 中的 ： 其中， 分别表示 和 的矩阵， 为对角矩阵。可以看到，Full-LN 中的模型参数量为 ，LRPD-LN 的模型参数量为 。通过实验证明，由于 LRPD-LN 所需要更新的参数量特别少，因此在目标说话人数据量有限的情况下能够获得较 Full-LN 更加稳定的合成声音。 本文提出的算法，在中文数据集上进行实验，该数据集包含 3 个说话人，每个说话人有 5000 句话，时长约 5h。数据集中语音的采样率为 16k，特征提取中的窗长和窗移分别为 25ms 和 5ms。分别用 A-male、B- female 和 C-female 来命名这三个说话人。本实验中，源说话人声学模型训练过程所使用的句子数为 5000。为了对比不同句子数目下的合成效果，目标说话人的自适应数据集对应的句子数从 50 到 1000 不等。在自适应数据集之外，我们取 200 句话作为开发集，取 20 句话作为测试集（用于主观打分）。为了分析性别对自适应效果的影响，进行了三对源说话人-目标说话人之间的实验：女生-女生、男生-女生和女生-男生。另外，使用客观度量和主观测听两种方式来衡量模型的性能。客观度量主要包括：Mel-Cepstral Distortion (MCD)、root mean squared error (RMSE) of F0、unvoiced/voiced (U/V) prediction errors 和开发集的 MSE。主观测听主要是对系统合成的声音样本进行自然度和相似度上的打分——mean opinion score (MOS)。 以女生-女生（C-female – B-female）为例，Fig. 3 显示了不同自适应句子数目和客观度量之间的关系曲线图。其中，SD 表示说话人相关系统，OL 表示只更新源说话人声学模型输出层的说话人自适应系统，OL+Full-LN 和 OL+LRPD-LN 分别表示基于 Full-LN 和 LRPD-LN 的说话人自适应系统。根据 Fig. 3，随着训练/自适应句子数的增加，所有系统间的客观度量趋于相近。对比 SD 和另外三个自适应系统，自适应系统的性能在相同句子数目下要更优。另外，OL+LRPD-LN 和 OL+Full-LN 相比于 OL 均出现性能上的跳变（提升），说明只更新输出层而不对其他层进行更新不能够得到较好的自适应效果。同时，当自适应句子数较少的时候，OL+Full-LN 在客观性能上要差于 OL+LRPD-LN，这是因为 OL+Full-LN 引入太多的参数量，出现过拟合问题。反之，在句子数多的时候 OL+Full-LN 在客观性能上要优于 OL+LRPD-LN，此时 OL+LRPD-LN 由于参数量少，出现欠拟合问题。 Fig. 4 上对比了不同系统间的自然度和相似度。随着句子数的减少，SD 系统的性能出现急剧下降，OL+LRPD-LN 相比于 SD 和 OL+Full-LN 要更加稳定。与客观度量一致，在相同句子数下，OL+Full-LN 和 OL+LRPD-LN 在性能上要优于 SD。并且，OL+Full-LN 和 OL+LRPD-LN 在 200 句话的性能和 SD 在 1000 句话时的性能相近。与客观度量不同，OL+LRPD-LN 在 500 句以下的时候性能上就优于 OL+Full-LN。这是因为过拟合导致合成出来的声音不稳定（虽然客观度量更优）声音的可懂度下降导致的。由此，我们依然可以得到相同的结论：当自适应句子数较少的时候，过拟合使得 OL+Full-LN 的性能变差。 本文中，基于线性网络的说话人自适应算法被应用于语音合成领域，基于 LRPD 的模型压缩算法能够提高声音的稳定性。通过三对不同的源说话人-目标说话人的实验，我们发现，当自适应句子数目非常少的时候，LRPD 能够提升声音的稳定性。另外，通过提出的算法，使用 200 句目标说话人的训练语料训练的说话人自适应系统能够获得和使用 1000 句训练的说话人相关系统相近的效果。 "
116,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=3&sn=741fed377750e34a71540855a0a1bb4c&chksm=871ad6c7b06d5fd1789afe2919ebde802a5a27e000601d3567271fd4532eb467065f097bca8a&scene=27,观点 | 争议、流派，有关GAN的一切：Ian Goodfellow Q&A,自 2014 年提出以来，生成对抗网络（GAN）已经成为深度学习领域里最为重要的方向之一。其无监督学习的特性有助于解决按文本生成图像、提高图片分辨率、药物匹配、检索特定模式的图片等多种任务。近日，GAN 的提出者，谷歌大脑研究科学家 Ian Goodfellow 在问答平台上面向所有人进行了 Q&A 活动，向我们解答了有关 GAN 的背景、技术、流派，以及一些有趣的问题，我们对本次活动的内容进行了整理。 Gfred：有传言说你是在一家酒吧里想出生成对抗网络（GAN）的，真的假的？ Ian Goodfellow ：这是真的，并不是谣言而已。我曾在一些采访中介绍过当时的经历： Wired：https://www.wired.com/2017/04/googles-dueling-neural-networks-spar-get-smarter-no-humans-required/ NVIDIA Blog：https://blogs.nvidia.com/blog/2017/06/08/ai-podcast-an-argument-in-a-bar-led-to-the-generative-adversarial-networks-revolutionizing-deep-learning/ Gautam Ramachandra：能否介绍一下为实现目标函数所采用的函数导数所涉及的步骤，以及，GAN 中是否采用了贝叶斯理论？ 上述目标函数导数如下： 这些方程来自 NIPS 2016 上的 Tutorial《Generative Adversarial Networks》（https://arxiv.org/pdf/1701.00160.pdf）在文章的第 46 和 47 页，能不能帮我完成目标函数的函数导数步骤？ Ian Goodfellow： 在 AMA 中我可能没有足够时间来完成所有 LaTex 格式函数推导的过程，不过你可以在深度学习书中找到一些有用的信息：http://www.deeplearningbook.org/contents/inference.html 大部分你需要知道的内容在方程 19.46 里。 一旦将该方程应用于 J（D），你就有了一个表达式，而且不需要知道任何有关函数导数的东西。 Avhirup Chakraborty：目前是否存在研究生成模型可迁移性的论文和工作？例如：我有一个用于生成猫和狗图片的模型，我可以利用前面的一些层级来生成狼和老虎的图片。 Ian Goodfellow ：我还不知道目前有这样的研究，但我认为这种思路是可行的。对于生成器来说，后面的一些层（接近输出端的）更可能用在其他任务上。 Anshuman Suri：这可能有一点偏离主题了——你认为重新思考深度学习中「神经元」的工作方式是目前唯一一种解决对抗样本问题的可行思路吗？在你的一些视频中，你提到了一些这种通用样本的存在，因为模型的「线性」（与许多人所说的「非线性」相反）。我们目前的神经网络研究路线是错误的吗？还是说一些结合了完美的激活函数、数据增强/防御的模型就可以解决所有问题？ Ian Goodfellow ：这个问题没有偏离我们的主题，因为 GAN 的鉴别器需要对于生成器制造的对抗性输入具有鲁棒性。 我确实认为目前我们使用的神经元在面临对抗样本的时候难以保持稳健，但我不认为这是唯一的问题。最近的一些论文，如《Adversarial Spheres》（https://arxiv.org/abs/1801.02774）认为，为了确保分类器的安全性，我们确实需要从基础上重新思考自己的策略，而不是仅仅重新训练其他类型的模型。Geoffrey Hinton 等人提出的 Capsule 确实在防御对抗样本的时候强于普通模型（https://openreview.net/forum?id=HJWLfGWRb）。我们尚不清楚这是防御的最佳方式，目前还没有一种防御对抗样本和业内最强攻击方式的对抗性评测。 Foivos Diakogiannis：GAN 和强化学习（RL）是否有什么相似之处？据我浅显的理解，我认为 GAN 上的生成器-鉴别器和强化学习中的代理-环境相互作用的方式非常相近，我的思考是对的吗？ Ian Goodfellow ：我或许没有资格评价强化学习，但是我认为 GAN 是使用强化学习来解决生成建模的一种思路。GAN 案例的一些奇怪的地方在于奖励函数在行动中是完全已知和可微分的，奖励是非稳态的，奖励也是代理策略的一种函数。但我确实认为它基于强化学习。 Jakub Langr：你最近使用 Jacobian claping 用于训练稳定性的论文令人着迷。你是否可以提供： 另一种解释 Jacobian clamping 的方式？ 稍稍解释一下该工作的重要性。GAN 的训练有好有坏，但有时我们难以理解其中的全部，因为几乎所有人都已认识到 WGAN 的贡献，但至今却没有多少超过它的研究。 我发现 GAN 训练方式的阵营有两到三个：你和 OpenAI、谷歌的同事；Mescheder、Sebastian Nowozin 与其他微软研究院等机构的研究人员。你也是这样认为的吗？ Ian Goodfellow ：第三个问题，不，我认为有更多的阵营。FAIR/NYU 也有一些重要研究结果，事实上，确实是 FAIR/NYU 首先提出了 LAPGAN，真正生成了引人注目的高分辨率图像——这是 GAN 第一次得到媒体的大规模报道。另外一个非常重要的阵营是 Berkeley 和 NVIDIA 阵营，他们专注于高分辨率图像、视频、无监督翻译等方向。我不确定你所说的「阵营」是什么意思。如果你的意思是使用 GAN 的不同方式，那么是的，这些团队都是以不同方向为目标的。很多时候「阵营」意味着团体之间或多或少有冲突，但我认为在这里并没有这种情况。 第一个问题，这是一个高难度的问题，你需要了解 Jacobian clamping 的哪个方面？ 至于第二个问题，在 Augustus 未来的几篇论文公布之前，想要回答有关重要性的问题还有点难。 Vineeth Bhaskara：你的论文与 Jurgen Schmidhuber 的 GAN vs PM 有多大区别？你们之间仍有分歧吗？ Ian Goodfellow ：那篇论文已经被撤销提交了，我们之间仍有分歧。 Adam Ferguson：我是一名来自波士顿大学的学生， 1）在 GAN 和其他生成模型如 VAE 和 FVBN（以及 NADE、MADE、PixelCNN）之间该如何进行选择？ 2）像 Librarian/Fermat Library 这样的工具对于 ML/DL 这样的领域很重要，其中的论文在 arXiv 上发表的速度远远快于期刊，这方面的未来会是什么样的？ Ian Goodfellow ：这个 AMA 就是在帮助 Librarian/Fermat Library 提升效果，因为我认为这些工具非常重要。arXiv 目前基本上是一个绕开同行评议过程的方式，它使得深度学习论文的信噪比猛降。其中虽然有很多有关深度学习的好论文，但也有很多低质量的工作。今天，即使是最好的工作也会夹杂私货——论文里写了一个好的主意，却又包含不公平的推销、与其他研究不准确的比较等等。因为在其中没有任何同行评议，所以作者变得有点肆无忌惮了。 如果你想生成连续变量的真实样本，如果你需要做无监督翻译（如 CycleGAN），或者说你想做半监督学习，那么通常你需要用 GAN。如果你想生成离散的负号，那么你应该还不能用 GAN，但我们仍在做这方面的努力。如果你想要最大似然，那么你可能不需要 GAN。 Kushajveer Singh：最近你在 Twitter 中提到了你心中的 10 篇最佳 GAN 论文，你能写一篇文章列举对于初学者和专业人数来说重要的论文和其他资源吗？ Ian Goodfellow ：我可能没有时间做这样的工作，而且如果我做了，它也很快就会过时。我在 2016 年 12 月写过一篇 GAN 的大型教程：https://arxiv.org/abs/1701.00160。 其中的很多理念在今天仍然适用，不过其中不包含最近发展出来的最先进的模型，如 Progressive GAN、谱归一化 GAN、带投影鉴别器的 GAN 等。 Rafa Ronaldo：定量评估 GAN 的方法是什么？ Ian Goodfellow ：它取决于你希望拿 GAN 来做什么。如果你希望将它用于半监督学习，那么就使用测试集的准确率度量；如果你希望用于生成人类比较欣赏的图像（如超分辨率等），那么就需要使用人类的评分。如果你仅希望使用一般的自动化质量评分，那么我认为 Frechet Inception Distance ( https://arxiv.org/abs/1706.08500 ) 可能是最好的。当然度量方法本身仍然是研究领域中非常重要的一部分。 Andres Diaz-Pinto：现在有方法将隐变量映射到生成图像的一部分吗？换句话说，能否使用几个变量改变背景颜色，然后然后另外几个个变量修正形状或其它的属性？ Ian Goodfellow： 通常情况下是可以实现的，但我们需要以特定的方式训练模型，详情请查看 InfoGAN：https://arxiv.org/abs/1606.03657。 Rafa Ronaldo：你怎样来提升编程技能以便能快速实现如 GAN 等各种有意思的想法？能不能推荐几本提升编程技能的书，或具体学习 TensorFlow 与其它深度学习框架的书？ Ian Goodfellow ：我学习现代深度学习编程的路径非常间接，因为在学习使用 Python 之前，我学过各种 C、汇编、网页等编程语言。因此我也不确定怎样才能加速编程的学习过程。在编程能力方面，对我来说非常重要的一个转折点就是 2006 年参加 Jerry Cain 在斯坦福开设的 CS107 课程。在那之前，我基本只是一个编程爱好者，但上过课后，基本上我在软件开发方面就不会再困惑了。现在你们也可以在 YouTube 或 iTunes U 等站点找到该课程。 此外，当我开始第一次尝试构建 GAN 时，那时有非常多的优秀工具，如 Theano、LISA lab 计算机集群等。所以编写 GAN 不那么困难的原因有一部分就是因为这些优秀的深度学习框架，同时我在整个博士期间都在学习深度学习，因此有非常多的代码块可随时嵌入到新模型中。我第一个 GAN 的实现主要是从 MNIST 分类器代码中复制粘贴。 Jason Rotella：GAN 能用于主题建模吗？现在除了生成模型，GAN 框架还能扩展应用到其它领域吗？ Ian Goodfellow ：我猜测是可以用于主题建模的，但现在并不知道任何在该领域的具体研究。一个主要的挑战即文本是由离散的字符、标记或单词组成的，但是 GAN 需要通过生成器的输出计算梯度，因此它只能用于连续型的输出。当然还是有可能使用对抗自编码器或 AVB 等模型，因此生成器实际上或是一个编码器，并能输出连续的编码。这对于文本建模可能是非常有用的属性，因为它给出了表征主题的分布。 Greg McInnes：在基因组学中，有 GAN 的用武之处吗？ Ian Goodfellow ：我对基因组学没有太多的了解，但我认为用于半监督学习的 GAN 模型可能会对该领域有非常重要的作用。因为未标注的基因远要比标注的基因多，所以半监督学习有助于利用大量未标记数据从少数标注样本中学习。 Tim Salimans 为此也开发了一些方法，这些方法在 MNIST 和 SVHN 等基准测试中非常高效（https://arxiv.org/abs/1606.03498）。但到目前为止，我发现其它半监督学习方法并不是非常有效，不过半监督 GAN 目前还处于研究中。 Nicholas Teague：我比较好奇现在是否有其它一些案例成功地将 GAN 用于图像或视频之外的数据。 Ian Goodfellow ：以下是我们近来将 GAN 用于文本数据的论文： https://arxiv.org/abs/1801.07736。 Gonçalo Abreu：以下陈述正确吗：「可能存在两个不同的 GAN，其中一个有较好的评分，但是在作为外部分类器时，从鉴别器中抽取特征要更差或更低效。」此外，关于结论的第四点：您为什么建议我们使用 GAN 作为特征提取器？也就是说，我们怎样才能确保由 GAN 架构产生的表征是有用的或表示了输入数据的潜在结构？最后，如果网络有足够的容量，那么生成器会不可阻挡地学习如何模拟输入数据？ Ian Goodfellow ：你说的那个陈述是对的，假设你有一个完美的生成器，那么对于所有的 x，P_generator(x)=P_data(x)。那么最优判别器 D*（x）= P_data（x）/（P_generator(x)+P_data(x)）=1/2，这个判别器的特征可能完全没用，因为我们可以将判别器的权重设为 0 而达到该效果。 对于特征提取，一种方法是使用判别器隐藏层的特征，另一种方法就如同自编码器一样单独学习一个独立的编码器，以下是一些关于 GAN 编码器的论文： 对抗性学习推断：https://arxiv.org/abs/1606.00704 BiGAN：https://arxiv.org/abs/1605.09782 对抗性自编码器：https://arxiv.org/abs/1511.05644 对抗性变分贝叶斯：https://arxiv.org/abs/1701.04722 alpha-GAN：https://arxiv.org/abs/1706.04987 对于生成器模拟输入数据而言，这与几乎所有生成模型的点估计都有关系。如果你在无限模型容量的情况下最大化密度函数的似然度，那么模型仅会记住训练数据。在实践中，GAN 似乎出现更多的问题是欠拟合而不是过拟合，这可能是因为我们不知道如何平衡博弈过程。下面是相关的两篇论文： On the Quantitative Analysis of Decoder-Based Generative Models：https://arxiv.org/abs/1611.04273。 Do GANs actually learn the distribution? An empirical study：https://arxiv.org/pdf/1706.08224.pdf。 原文链接：https://fermatslibrary.com/arxiv_comments?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1406.2661.pdf 
117,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739431&idx=5&sn=2f1b5b4061dccda9239b86b7c1f78f79&chksm=871ad699b06d5f8f2d4469e2687b881096b917b3393347970d95938186934c52c8506f8c39a4&scene=27,ICLR 2018 | 斯坦福大学教授Christopher Manning提出全可微神经网络架构MAC：可用于机器推理,"现今，神经网络已在图像识别、语音识别等感知层面取得巨大成功，但是在更进一步的推理层面仍有欠缺。为解决这一问题，本文提出了一种新的全可微神经网络架构 MAC，可使网络具有结构化推理和迭代思考的能力, 提升其推理的明确性和表现力；在通过 CLEVR 数据集解决视觉推理的任务中，MAC 实现了 98.9% 的当前最优准确率，同时所需数据量减少 5 倍。 推理，即通过先前已有知识，形成新推断或者解决新问题的能力，是智能体必不可少的基础模块之一。如今神经网络在感知层面已取得巨大成功，我们希望在此基础上更进一步，胜任一些需要更高级和更成熟思考的任务，因此让神经网络拥有可以从事实得出结论的能力显得非常重要。为了达到这一目的，我们思考如何最优地设计一个神经网络，使得它可以拥有结构化推理和迭代思考的能力，而这些能力，对于解决复杂问题必不可少。 具体而言，我们开发了一个全新的模型，并运用在视觉问答 (VQA) 的 CLEVR 项目中 (Johnson et al., 2017a)。VQA (Antol et al., 2015; Gupta, 2017) 是一个富有挑战性的多模式任务，要求回答关于图像的自然语言问题。但是，Agrawal et al. (2016) 表明，无论在图像还是问题上，第一代成功的 VQA 模型都仅仅倾向于挖掘数据集的偏差，获取浅层理解，而不是构建一个合理的感知和推导流程来得到正确答案 (Sturm, 2014)。CLEVR 的诞生就是为了解决这个问题。如图 1 所示，数据集的特征是无偏差、高度结构化的问题，解决这些问题需要一系列富有挑战性的推理能力，如传递关系、逻辑关系、计数和比较，而不允许在此类推理中采取捷径。 图 1：一个 CLEVR 实例。为了便于描述，加入了颜色。 但是，深度学习方法往往难以在具有组合性和结构性特点的任务中表现优秀 (Garnelo et al.，2016; Lake et al.，2017)。绝大多数的神经网络本质上都是巨大的关联引擎，为了提升在观测样本中的准确率，神经网络会拟合出任何的统计模式，即使它们可能是错误的。网络深度、规模和统计特性可以使其应对各种充满噪声的数据，往往也限制了模型的可解释性，并阻碍给出明确合理的推理过程，而这些推理过程在以解决问题为目的的任务中是必不可少的。为了缓解这个问题，最近一些方法采用类似编程语言中表达式树的符号结构，从一堆预定义的确定集合中组成神经网络的模块。但因此，它们需要依赖外部预先设定好的结构化表达、功能性程序、不可靠的人工分析或者专家说明，同时需要相当复杂的多阶段强化学习训练框架。这些模型结构上的严格要求，以及使用的一系列专门的指定操作模式，最终降低了模型的鲁棒性和泛化能力。 平衡端到端神经网络方法的泛化性和鲁棒性的同时，也要满足更明确的结构化推理的要求，为此我们提出 MAC 网络：一种新的全可微神经网络结构，来完成推理任务。通过排序新的循环 MAC 神经元（记忆、注意力、组合神经元），我们的模型实现结构化的明确推理。MAC 神经元是受到计算机架构的设计原则启发而有的神经元，我们希望它可以捕获基本但通用的推理步骤中的内在原理。MAC 神经元可以明确的将记忆从控制中分离出来，这两种结构都是循环表征的，MAC 神经元由三个运算元串联运行组成，以展现推理步骤：控制元更新控制状态，以便在每次迭代中参与待解答问题的一些部分; 读取元在控制状态和记忆状态的引导下，从知识库中提取信息; 写入元将这些检索得到的信息整合进记忆状态，迭代计算答案。MAC 神经元的这个通用设计将作为结构先验，引导 MAC 网络将问题分解为一系列基于注意力的推理运算，并解决它们。在这个过程中，分解是直接基于数据的，而没有使用任何的强监督手段。通过神经元之间的自我注意力的联系，MAC 网络可以通过一种柔和的方法，表征任意复杂程度的无环推理图，同时依然突出物理结构顺序和端到端的可微性，以适应简单地通过反向传播算法进行模型训练。 我们展示模型在 CLEVR 任务及相关数据集上的定性和定量表现。在大量的推理任务和设定中——无论是原始数据集还是更复杂的人为问题——模型都达到了当前最佳的准确率。值得注意的是，在涉及到计数和加总能力的问题中，MAC 网络的表现格外优秀，而这些问题往往是其他 VQA 模型（Santoro et al.，2017; Hu et al.，2017; Johnson et al.，2017b）非常难以完成的挑战。同时，我们也表明 MAC 网络的学习速度非常快，另外，和其他方法相比，它有效泛化所需的数据量级也更小。最后，大量的简化测试和误差分析印证了 MAC 网络的鲁棒性、多样性和泛化能力。这些结果突出说明了在推动神经网络解决组合推理论证时，加入强结构先验的重要性和价值。根据 Bottou（2014）提出的设想实现，以及在模型中加入新结构，使它明确执行一系列互相实现的运算操作，让 MAC 网络可以从零开始，一步一步发展出推理能力。虽然每个神经元的功能都被限制在一个很小的可能的连续行为范围之内，也仅仅是为了实现一个简单的推理运算，当它们被连接在一起，组成 MAC 网络时，整个系统就变得富于表现力且强大。 MAC 网络 MAC 网络是一个端到端可微架构，旨在实现明确的多阶段推理论证过程。它连接了 p 个循环 MAC 神经元，其中每个负责一步推理步骤。给定知识库 K（在 VQA 场景中是一个图像）和任务描述 q（在 VQA 场景中是一个问题），模型得出一系列的 p 个和知识库相互作用的推理运算，并通过迭代整合，控制信息，来完成手中的任务。它有三个组成部分:（1）一个输入神经元，（2）核心的循环网络，由 p 个 MAC 神经元组成，以及（3）一个输出神经元。 论文：Compositional Attention Networks for Machine Reasoning 论文链接：https://arxiv.org/abs/1803.03067 摘要： 我们提出了 MAC 网络：一种新的全可微神经网络架构，旨在提升推理的明确性和表现力。受到计算机架构第一原则的启发，MAC 不再使用统一的神经网络黑箱架构，转而采用了提倡透明性、多用途的设计。模型将问题分解为一系列基于注意力的推理步骤，然后处理它们，其中每一个步骤都由全新的记忆单元、注意力单元和结构性单元（合称 MAC 神经元）通过将控制和记忆进行分离来实现。通过将神经元连接到一起，并引入结构性约束来规范其互动，MAC 非常有效地学习并实现迭代推理过程，这种学习是通过端到端方法从数据中直接获取得到的。在模型通过 CLEVR 数据集解决视觉推理问题时，我们通过比较它和先前最优的模型的误差率，论述了 MAC 所表现出的优点、鲁棒性和可解释性——MAC 实现了当前最优的 98.9% 的准确率。更重要的是，我们说明了模型的计算和数据效率都非常高，尤其是，为了取得很好的结果，它所需要的数据量比其他现有模型所需要的数据量少 5 倍。 "
118,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=5&sn=d7ddbd5e93330d12ed2ea63e7b3c6607&chksm=871ad6c7b06d5fd1749f49fd4dbe17eb430025cd51e7ae78be0bba2de6596a1e4c3df4170092&scene=27,活动 | 看完今天的头条，是否想线下体验神经机器翻译产品？,3 月 12 日，搜狗旅行翻译宝在京东平台首发，售价 1498 元。据了解，搜狗旅行翻译宝结合了搜狗神经网络机器翻译、语音识别、图像识别等多项技术，不仅支持语音、图像翻译等多种翻译模式，还提供中英日韩俄德等 18 种语言互译。 对搜狗的这款旅行翻译宝，你是不是充满好奇？想要了解其背后的语音翻译技术，体验这款产品？ 在 3 月 17 日的 INTERFACE 线下分享中，搜狗语音交互技术中心研发总监陈伟、搜狗 IOT 事业部产品负责人李健涛，将从技术到产品为我们做解读。 分享主题一：搜狗语音翻译技术是怎样炼成的？ 陈伟：搜狗语音交互技术中心研发总监，语音技术负责人，负责搜狗语音识别、语音合成、机器翻译、声纹识别、手写识别等多项技术的研发工作，同时负责搜狗知音引擎语音技术的研发，致力于通过技术和产品的创新提升语音交互品质，为用户提供优质的语音使用体验。 分享主题二：搜狗旅行翻译宝产品研发理念   李健涛：搜狗 IOT 事业部产品负责人，负责搜狗旅行翻译宝、速记翻译笔、糖猫儿童智能手表等智能硬件产品研发工作。他在 2005 年加入搜狐，负责博客开发，经历了社交网络大爆炸、移动互联网浪潮，又投身智能硬件领域，致力于通过产品创新提升智能硬件产品使用体验，为用户提供优质的服务。 日期 ：2018 年 03 月 17 日 13:00-14:00 签到、产品体验 14:00-14：30 陈伟分享 14：30-15:00 李健涛分享  15:00-15:30 现场提问 15:30-16:00 现场交流、产品体验 地址 ：北京市海淀区中关村东路搜狐网络大厦 8 层四海会议室 参与方式 
119,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739320&idx=3&sn=03ca88024cfecd70128ac7afb08ad499&chksm=871ad606b06d5f109f10727a34afe8a80df6f492cc07d062d219edf647ebe86ca86c470cb921&scene=27,业界 | 现代「罗塞塔石碑」：微软提出深度学习框架的通用语言,"深度学习框架就像语言一样：很多人会说英语，但每种语言都有自己的特殊性。作者为几种不同的网络结构创建了通用代码，并可在多个不同的框架中使用。 repo 1.0 完整版 GitHub 地址：https://github.com/ilkarman/DeepLearningFrameworks 我们的想法是创建一个深度学习框架的罗塞塔石碑（Rosetta Stone）：假设你很了解某个深度学习框架，你就可以帮助别人使用任何框架。你可能会遇到论文中代码是另一个框架或整个流程都使用另一种语言的情况。相比在自己喜欢的框架中从头开始编写模型，使用「外来」语言会更容易。 感谢 CNTK、Pytorch、Chainer、Caffe2 和 Knet 团队，以及来自开源社区的所有人在过去几个月为该 repo 所做的贡献。 我们的目标是： 1. 创建深度学习框架的罗塞塔石碑，使数据科学家能够在不同框架之间轻松运用专业知识。 2. 使用最新的高级 API 优化 GPU 代码。 3. 创建一个 GPU 对比的常用设置（可能是 CUDA 版本和精度）。 4. 创建一个跨语言对比的常用设置（Python、Julia、R）。 5. 验证自己搭建框架的预期性能。 6. 实现不同开源社区之间的合作。 基准深度学习框架的结果 下面我们来看一种 CNN 模型的训练时间和结果（预训练的 ResNet50 模型执行特征提取），以及一种 RNN 模型的训练时间。 训练时间（s）：CNN（VGG-style，32bit）在 CIFAR-10 上执行图像识别任务 该模型的输入是标准 CIFAR-10 数据集（包含 5 万张训练图像和 1 万张测试图像），均匀地分成 10 个类别。将每张 32×32 图像处理为形状 (3, 32, 32) 的张量，像素强度从 0-255 重新调整至 0-1。 处理 1000 张图像的平均时间（s）：ResNet-50——特征提取 加载预训练 ResNet-50 模型在末端 (7, 7) 平均池化之后裁断，输出 2048D 向量。其可插入 softmax 层或另一个分类器（如 boosted tree）来执行迁移学习。考虑到热启动，这种仅前向传播至 avg_pool 层的操作有时间限制。注意：批量大小保持常量，但是增加 GPU 内存可带来更好的性能提升（GPU 内存越多越好）。 训练时间（s）：RNN (GRU) 在 IMDB 数据集上执行情感分析任务 模型输入为标准 IMDB 电影评论数据集（包含 25k 训练评论和 25k 测试评论），均匀地分为两类（积极／消极）。使用 https://github.com/keras-team/keras/blob/master/keras/datasets/imdb.py 中的方法进行处理，起始字符设置为 1，集外词（OOV，本次训练使用的词汇表包括 3 万单词）设置为 2，这样单词索引从 3. Zero 开始，通过填充或截断使每条评论固定为 150 词。 经验教训 1. 使用自动调参模式：大部分框架使用 cuDNN 的 cudnnFindConvolutionForwardAlgorithm() 来运行穷举搜索，优化在固定大小图像上前向卷积所使用的算法。这通常是默认的设置，但是一些框架可能需要一个 flag，例如 torch.backends.cudnn.benchmark=True。 2. 尽可能多地使用 cuDNN：常用的 RNN（如基础 GRU/LSTM）通常可以调用 cuDNN 封装器来加速，即用 cudnn_rnn.CudnnGRU() 代替 rnn.GRUCell()。缺点是稍后在 CPU 上运行推断时难度可能会增加。 3. 匹配形状：在 cuDNN 上运行时，为 CNN 匹配 NCHW 的原始 channel-ordering、为 RNN 匹配 TNC 可以削减浪费在重塑（reshape）操作上的时间，直接进行矩阵乘法。 4. 原始生成器：使用框架的原始生成器，增强和预处理（例如 shuffling）通过多线程进行异步处理，实现加速。 5. 对于推断，确保指定的 flag 可以保存被计算的非必要梯度，以及 batch-norm 和 drop-out 等层得到合理使用。 当我们从头开始创建该 repo 的时候，为了确保在不同框架之间使用的是相同的模型，并以最优化的方式运行，我们使用了很多技巧。过去几个月里，这些框架的改版之快令人惊讶，框架的更新导致很多在 2017 年末学会的优化方法如今已然过时。 例如，以 TF 为后端的 Keras 拥有 channel-ordering 硬编码作为 channels-last（对于 cuDNN 不是最优的），因此指定 channels-first 意味着它将在每个批次（batch）之后重塑到硬编码值，从而极大降低训练速度。现在以 TF 为后端的 keras 支持原始 channels-first ordering。之前，TensorFlow 可以通过指定一个 flag 来使用 Winograd 算法用于卷积运算，然而现在这种方法不再有用。你可以在该 repo 的早期版本（https://github.com/ilkarman/DeepLearningFrameworks/tree/cb6792043a330a16f36a5310d3856f23f7a45662）中查看其中的最初学习阶段部分。 通过在不同的框架中完成端到端解决方案，我们可以用多种方式对比框架。由于相同的模型架构和数据被用于每一个框架，因此得到的模型准确率在各个框架之间是非常相似的（实际上，这正是我们测试代码以确保相同的模型在不同框架上运行的一种方法）。此外，该 notebook 的开发目的是为了使框架之间的对比更加容易，而模型加速则不是必要的。 当然，该项目的目的是使用速度和推断时间等指标来对比不同的框架，而不是为了评估某个框架的整体性能，因为它忽略了一些重要的对比，例如：帮助和支持、提供预训练模型、自定义层和架构、数据加载器、调试、支持的不同平台、分布式训练等。该 repo 只是为了展示如何在不同的框架上构建相同的网络，并对这些特定的网络评估性能。 深度学习框架的「旅行伴侣」 深度学习社区流行着很多种深度学习框架，该项目可以帮助 AI 开发者和数据科学家应用不同的深度学习框架。一个相关的工作是 Open Neural Network Exchange（ONNX），这是一个在框架间迁移深度学习模型的开源互通标准。当在一个框架中进行开发工作，但希望转换到另一个框架中评估模型的时候，ONNX 很有用。类似地，MMdnn 是一组帮助用户直接在不同框架之间转换的工具（以及对模型架构进行可视化）。 深度学习框架的「旅行伴侣」工具如 ONNX 和 MMdnn 就像是自动化的机器翻译系统。相比之下，我们今天发布的 repo 1.0 完整版更像是深度学习框架的罗塞塔石碑，在不同的框架上端到端地展示模型构建过程。 参考阅读： 业界 | Facebook 联合微软推出神经网络交换格式 ONNX：实现不同框架间模型迁移 "
120,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739320&idx=5&sn=0f26d88df43299230eab2706d6b38aca&chksm=871ad606b06d5f10de5675a5523df18fdac496c40b9b9aae3d06e0ccd76f0a99a3db5b8b6658&scene=27,ICLR 2018 | 清华&斯坦福提出深度梯度压缩DGC，大幅降低分布式训练网络带宽需求,"林宇鋆、韩松等 来自清华大学和斯坦福大学的研究者们发现，分布式随机梯度下降训练中 99.9% 的梯度交换都是冗余的——通过他们提出的深度梯度压缩（DGC）方法，神经网络训练可以大幅降低通信带宽需求。在多个基准模型上的对比实验表明，该方法可以在不降低准确率的情况下达到 270 倍到 600 倍的梯度压缩率，使得小带宽甚至移动设备上的大规模分布式训练变为可能。 作者简介 林宇鋆是清华大学电子工程系 NICS 实验室 2014 级本科生，于 2017 年暑假在斯坦福参加暑研期间同韩松博士一起出色完成了 DGC 的工作，收到 MIT, Stanford, CMU, UMich 等美国名校的博士项目录取，并将于 2018 年秋加入 MIT HAN Lab 攻读博士学位。 韩松博士于 2017 年毕业于斯坦福大学，师从 GPU 之父 Bill Dally 教授。他的研究涉足深度学习和计算机体系结构，他提出的 Deep Compression 模型压缩技术曾获得 ICLR 2016 最佳论文，ESE 稀疏神经网络推理引擎获得 FPGA 2017 最佳论文，引领了世界深度学习加速研究，对业界影响深远，于博士期间联合创立了深鉴科技。基于对一系列重要科研成果的继续深入探索，韩松博士将于 2018 年任职 MIT 助理教授，创立 HAN Lab。 大规模分布式训练可提升训练大型深度模型的训练速度。同步随机梯度下降（SGD）已被普遍用于分布式训练。通过增加训练节点数量，利用数据并行化的优势，我们能够极大地减少在相同规模的训练数据上做前向-后向传播的计算时间。然而，分布式训练中梯度交换的成本过高，尤其是在计算-通信比率较低的循环神经网络（RNN）等情况下；由并行训练带来的计算时间上的节省，可能将不足以补偿通信时间上代价的增长。因此，网络带宽成为了分布式训练规模化的最大瓶颈。特别是在移动设备上做分布式训练时，带宽问题变得更加显著，例如联合学习（federated learning）。因为具备更好的隐私性、个性化等特点，在移动设备上训练神经网络模型变得更加诱人，但其面临的重大挑战包括移动设备网络中的更低的带宽、不连贯的网络连接、价格昂贵移动数据流量等问题。 图 1：DGC 可以减少梯度交换时间，提高多节点训练的可扩展性，并对分布式训练进行加速。 深度梯度压缩（Deep Gradient Compression，DGC）通过压缩梯度解决了通信带宽问题，如图 1 所示。为了确保没有损失准确率，DGC 在梯度稀疏化之上应用了动量修正（momentum correction）和局域梯度修剪（local gradient clipping）方法。DGC 还应用了动量因子掩蔽（momentum factor masking）和预热训练（warm-up training）以克服通信量减少带来的陈化问题。 研究者通过实验在多种任务、模型和数据集上验证了 DGC 的有效性： CNN 网络：图像分类任务，CIFAR-10 和 ImageNet 数据集； RNN 网络：语言建模任务，Penn Treebank 数据集； 语音识别任务，Librispeech Corpus。 这些实验证明了我们可以对梯度进行 600 倍的压缩而不损失准确率，相比于之前的研究（Aji & Heafield, 2017），DGC 的性能提升了一个量级。 图 2：动量修正图示：（a）没有使用动量修正的局域梯度累积（Local Gradient Accumulation）；（b）使用动量修正的局域梯度修正。 表 1：深度梯度压缩（DGC）中应用到的技术。 表 2：在 CIFAR-10 数据集上训练的 ResNet-110 的准确率结果，图中展示了基线模型、Gradient Dropping 方法和 DGC 方法优化后的结果对比。 图 3：ResNet 在图像分类任务上的学习曲线（梯度稀疏度为 99.9%）。（a）ResNet-110 在 CIFAR-10 数据集上的 top-1 准确率；（b）ResNet-110 在 CIFAR-10 数据集上的训练损失；（c）ResNet-50 在 ImageNet 数据集上的 top-1 准确率；（d）ResNet-50 在 ImageNet 数据集上的训练损失。 表 3：AlexNet 和 ResNet-50 模型的多种优化方法在 ImageNet 数据集上得到的梯度压缩率对比。 图 6：DGC 提升了分布式训练的加速性能和可扩展性。每一个训练节点有 4 个英伟达 Titan XP GPU 以及一个 PCI 交换机。 5. 系统分析和表现部署  DGC 需要对梯度进行 top-k 选择。给定目标稀疏系数为 99.9%，我们要从百万权重中选择最大的 0.1%。其复杂度通常为 O（n），n 是梯度单元（gradient element）的数量。我们提出使用采样法来减少 top-k 的选择时耗。我们只采样梯度的 0.1% 到 1%，然后在采样上的进行 top-k 选择来估算整个梯度矩阵的阈值。 如果被选出梯度的数量（远超期望地）超出了阈值，我们就可以从已选择的梯度中重新使用 top-k 算法计算精确的阈值。分层地计算阈值可以显著地减少 top-k 选择时间。在实践中，相比网络的通信时间，总的额外计算时间是可忽略的，通常为几百毫秒到数秒（取决于网络带宽）。 我们使用了 Wen 等人（2017）提出的性能模型来执行可扩展性分析，将单个训练节点上的轻量剖析和分析通信建模技术结合了起来。通过全归约（all-reduce）通信模型（Rabenseifner, 2004; Bruck et al., 1997），在最坏的情况下，稀疏数据的密度在每个聚合步骤翻一番。然而，即使考虑了这种效应，DGC 仍然能显著地减少网络通信时间，如图 6 中所示。图 6 展示了多节点训练和单节点训练的加速性能对比。传统的训练方法在 1Gbps（图 6（a））的以太网上的加速性能相比在 10Gbps（图 6（b））上的加速性能要差很多。尽管如此，DGC 可以将 1Gbps 以太网上的训练过程进行优化，并得到和 10Gbps 以太网训练相当的结果。例如，当将 AlexNet 在 64 个节点上训练时，传统的训练方法在 10Gbps 以太网上仅能达到约 30 倍的加速（Apache, 2016），而应用 DGC 时，仅仅在 1Gbps 以太网上训练就能获得 40 倍的加速。对比图 6 中的（a）和（b），当通信-计算比率更高，以及网络带宽更低时，DGC 的优势更加明显。 论文：Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training 论文地址：https://arxiv.org/abs/1712.01887 大规模的分布式训练需要为梯度交换提供很高的通信带宽，并依赖昂贵的高带宽网络基础设施，因而限制了多节点训练的可扩展性。当在移动设备上进行分布式训练时（联合式学习），情况将变得更加糟糕，导致更高的延迟、更低的吞吐量以及断续的连接状态。在本文中，我们发现在分布式 SGD 中 99.9% 的梯度交换都是冗余的，因此我们提出了深度梯度压缩方法（Deep Gradient Compression，DGC）以大幅压缩通信带宽。为了在压缩过程中保持准确率，DGC 使用了四个方法：动量修正（momentum correction）、局域梯度修剪（local gradient clipping）、动量因子掩蔽（momentum factor masking）以及预热训练（warm-up training）。我们将 DGC 应用到图像分类、语音识别和语言建模中，评估数据集包括 Cifar10、ImageNet、Penn Treebank 和 Librispeech Corpus。在这些场景中，DGC 在不降低准确率的情况下达到了 270 倍到 600 倍的梯度压缩率，将 ResNet-50 的梯度规模从 97MB 压缩到了 0.35MB，将 DeepSpeech 的梯度规模从 488MB 压缩到了 0.74MB。DGC 可以帮助我们在通用的 1Gbps 以太网上执行大规模的分布式训练，并能促进移动设备上的分布式训练开发。 "
121,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739385&idx=1&sn=141f853343fa288b2a7d8a9fb1589fe1&chksm=871ad6c7b06d5fd11dab87562bb244bdedce82aeae1ad22b3ec9681ff60e90d876222c15a01c&scene=27,从冷战到深度学习：一篇图文并茂的机器翻译史,作者：Ilya Pestov 英语版译者：Vasily Zubarev 中文版译者：Panda 实现高质量机器翻译的梦想已经存在了很多年，很多科学家都为这一梦想贡献了自己的时间和心力。从早期的基于规则的机器翻译到如今广泛应用的神经机器翻译，机器翻译的水平不断提升，已经能满足很多场景的基本应用需求了。近日，Ilya Pestov 用俄语写的机器翻译介绍文章经 Vasily Zubarev 翻译后发表到了 Vas3k.com 上。机器之心又经授权将其转译成了汉语。希望有一天，机器自己就能帮助我们完成这样的任务。 俄语版：http://vas3k.ru/blog/machine_translation/  英语版：http://vas3k.com/blog/machine_translation/  我打开谷歌翻译的频率是打开 Facebook 的两倍，价格标签的即时翻译对我而言再也不是赛博朋克了。这已经成为了现实。很难想象这是机器翻译算法百年研发之战的结果，而且在那段时间的一半时间里其实都没什么明显的成功。 我在本文中讨论的确切发展将立足于所有的现代语言处理系统——从搜索引擎到声控微波。我将探讨的是当今的在线翻译技术的演化和结构。 P. P. Troyanskii 的翻译机器（根据描述绘制的图片。很遗憾没有照片留下。） 故事开始于 1933 年。苏联科学家 Peter Troyanskii 向苏联科学院提交了《用于在将一种语言翻译成另一种语言时选择和打印词的机器》。这项发明非常简单——它有四种语言的卡片、一台打字机和一台旧式胶片相机。 操作员先取文本的第一个词，然后找到对应的卡片，拍一张照片，再在打字机上键入其形态特征（名词、复数、性别等）。这台打字机的按键编码了其中一项特征。打字带和相机胶片是同时使用的，从而得到一组带有词及它们的形态的帧。 尽管看起来很不错，但和苏联的很多事情都一样，人们认为这项发明是「没用的」。Troyanskii 用了 20 年时间试图完成他的发明，之后因心绞痛逝世。在 1956 年两位苏联科学家找到他的父母之前，这世上没人知道这种机器。 那是冷战的铁幕刚刚降下的时候。在 1954 年 1 月 7 日，IBM 在纽约的总部启动了 Georgetown-IBM 实验。IBM 701 计算机有史以来第一次自动将 60 个俄语句子翻译成了英语。 「一位不认识任何一个苏联语言词汇的女孩在 IBM 卡片上敲出了这些俄语消息。这个「大脑」以每秒两行半的惊人速度在一台自动打印机上赶制出了它的英语翻译。」——IBM 的新闻稿 IBM 701 但是，宣告胜利的头条新闻里却隐藏了一个小小的细节。没人提到这些翻译得到的样本是经过精心挑选和测试过的，从而排除了歧义性。对于日常使用而言，该系统并不比口袋里的常用语手册更好。尽管如此，军备竞赛还是开始了：加拿大、德国、法国以及（特别是）日本全都加入到了机器翻译竞赛中。 改进机器翻译的徒劳工作持续了四十年之久。1966 年，US ALPAC 在其著名的报告中称机器翻译是昂贵的、不准确的和毫无希望的。他们转而建议将重点放在词典开发上，这将美国研究者排除在了竞赛之外近十年时间。 即便如此，仅凭科学家和他们的尝试、研究和开发，现代自然语言处理的基础还是建立了起来。多亏了这些彼此监视的国家，当今所有的搜索引擎、垃圾信息过滤器和个人助理都出现了。 基于规则的机器翻译（RBMT） 最早的基于规则的机器翻译思想出现于 70 年代。科学家研究了翻译员的工作，试图让当时还极其缓慢的计算机也能重复这些行为。这些系统包含： 双语词典（比如，俄语->英语） 每种语言一套语言学规则（比如，以 -heit、-keit、-ung 等特定后缀结尾的名词都是阴性词） 这就是这种系统的全部。如有需要，该系统还能得到一些补充，比如增加姓名列表、拼写纠错器和音译功能。 PROMPT 和 Systran 是 RBMT 系统中最有名的案例。如果你想感受下那个黄金时代的柔和气息，去试试 Aliexpress 吧。 但即使它们也有一些细微差别和亚种。 直接机器翻译 这是机器翻译中最直接的类型。它会将文本分成词，然后翻译这些词，再稍微校正一下形态，最后协调句法得到结果；或多或少听起来还行。当太阳落山后，训练有素的语言学家还在为每个词编写规则。 其输出会返回某种类型的翻译结果。通常情况下，结果很糟糕。就好像是这些语言学家白白浪费了自己的时间。 现代系统完全不会使用这种方法，现代语言学家对此感激不尽。 基于迁移的机器翻译 与直接翻译相比，我们翻译时要做准备——首先确定句子的语法结构，就像上学时老师教的那样。然后我们再操作整个结构，而不是一个个的词。这有助于在翻译中得到相当好的词序转换。理论上是这样。 但在实践中，这仍然会得到逐词翻译的结果并会让语言学家身疲力竭。一方面，它带来的是简化过的一般性语法规则。但另一方面，由于词结构的数量比单个的词要多得多，所以这又会变得更加复杂。 语际机器翻译 在这种方法中，源文本会被转换成中间表征，并且会被统一用于全世界的所有语言（中间语言）。这正是笛卡尔所梦想的那种中间语言：一种元语言，遵循普适的规则并且可以将翻译变成一种简单的「来回切换」任务。接下来，中间语言可以转换成任何目标语言，而这就是奇点！ 正是由于存在这种转换，所以语际机器翻译常常会和基于迁移的系统混淆。语际机器翻译的不同之处是语言学规则是针对每种单独的语言和中间语言的，而不是针对语言对。这意味着我们可以向语际系统加入第三种语言并且在它们三者之间彼此翻译。而我们无法在基于迁移的系统中做到这一点。 看起来很完美，但实际并不。创建这样一种通用的中间语言极其困难——很多科学家都在这上面投入了一生。他们还没有取得成功，但多亏了他们，我们现在有了形态层面、句法层面、甚至语义层面的表征。但只有语义-文本理论（Meaning-text theory）耗费了巨资！ 中间语言的思想还会再回来的。让我们再等等看。 如你所见，所有的 RBMT 都很蠢笨和可怕，所以它们很少得到使用，除了一些特定的案例（比如天气报告翻译等）。RBMT 最常被提及的优点有形态准确性（不会混淆词）、结果的可再现性（所有翻译器的结果都一样）和调节到特定学科领域的能力（比如为了教授经济学家或特定于程序员的术语）。 就算有人真的成功创造出了一个完美的 RBMT，语言学家也用所有的拼写规则强化了它，但还是会存在某些例外情况：英语中的不规则动词、德语中的可分前缀、俄语中的后缀以及人们的表达方式存在差异的情况。任何试图涵盖所有细微差别的行为都会耗费数以百万小时计的工作时间。 还不要忘记多义词。同一个词在不同的语境中可能会具有不同的含义，这会得到不同的翻译结果。你试试能从这句话中理解到几种含义：I saw a man on a hill with a telescope? 语言不会按照什么固定的规则而发展——语言学家倒是喜欢这个事实。过去三百年中的侵略活动对语言的影响非常大。你怎么能向机器解释这一点？ 四十年的冷战没能帮助找到任何明确的解决方案。RBMT 已死。 日本对机器翻译竞赛尤其感兴趣。原因不是冷战，而另有其它：这个国家理解英语的人非常少。这在即将到来的全球化方面是一个很严重的问题。所以日本人非常积极地想要找到一种可行的机器翻译方法。 基于规则的英日翻译极其复杂。这两种语言的语言结构完全不一样，几乎所有词都需要重新排列，而且还需要添加新词。1984 年，京都大学的長尾真提出了一个思想：使用现成的短语而不是重复进行翻译。 假设我们想翻译一个简单的句子——「I'm going to the cinema.」而且我们之前已经翻译了一个类似的句子——「I'm going to the theater.」而且我们也能在词典中找到「cinema」这个词。 那么我们只需找到这两个句子的不同之处、翻译缺失的词、不要搞错了即可。我们拥有的实例越多，翻译结果就会越好。 我正是采用这种方式构建了下面的我不熟悉的外语短语！ EBMT 让全世界的科学家看到了方向：事实证明，你可以直接向机器输入已有的翻译，而不必花费多年时间构建规则和例外。革命还没有发生，但显然已经迈出了第一步。革命性的统计机器翻译发明将在那之后短短五年内诞生。 1990 年初，IBM 研究中心首次展示了一个对规则和语言学一无所知的机器翻译系统。它分析了两种语言的相似文本并且试图理解其中的模式。 这是一个简洁而又优美的思想。两种语言中的同一句子被分成单词，然后再进行匹配。这种操作重复了近 5 亿次，记录下了很多模式，比如「Das Haus」被翻译成「house」或「building」或「construction」等词的次数。 如果大多数时候源词都被翻译成「house」，那么机器就会使用这一结果。注意我们没有使用任何规则，也没有使用任何词典——所有的结论都是由机器完成的，其指导方针是统计结果和这样的逻辑——「如果人们这样翻译，我也这样翻译」。统计翻译由此诞生。 这个方法比之前的所有方法都更加有效和准确。而且无需语言学家。我们使用的文本越多，我们得到的翻译结果就越好。 谷歌的统计翻译内部情况示例。它不仅给出了概率，而且还显示了反向翻译结果统计 仍然还有一个遗留问题：机器该怎样将「Das Haus」与「building」对应起来呢——我们又怎么知道翻译结果是正确的？ 答案是我们没法知道。一开始，机器会假设「Das Haus」一词与来自翻译句子的任意词都有同等的关联。接下来，当「Das Haus」出现在其它句子中时，与「house」关联的数量会增多。这就是词对齐算法，这是大学级机器翻译的典型任务之一。 机器需要成百万上千万的双语句子才能收集到每个词的相关统计结果。我们如何得到这些数据？好吧，我们决定取用欧洲议会和联合国安理会会议的摘录，这些都是以所有成员国的语言提供的，而且可供下载： UN Corpora：https://catalog.ldc.upenn.edu/LDC2013T06  Europarl Corpora：http://www.statmt.org/europarl 基于词的 SMT 一开始的时候，最早期的统计翻译系统的工作方式是将句子分成词，因为这种方法很直观而且符合逻辑。IBM 的第一个统计翻译模型被称为 Model 1。名字也相当优雅，对吧？猜猜他们的第二个模型叫什么？ Model 1：逐词对应 Model 1 使用了一种经典方法来将句子分成词和记录统计信息。这个过程不考虑词序。唯一要用的技巧是将一个词翻译成多个词。比如「Der Staubsauger」可能会变成「Vacuum Cleaner」，但并不意味着反过来也可以。 这里有一些基于 Python 的简单实现：https://github.com/shawa/IBM-Model-1 Model 2：考虑句子中的词序 缺乏语言词序知识是 Model 1 的一个问题，而且这个问题在某些情况下很重要。 Model 2 解决了这个问题：它记忆了输出句子中词通常出现的位置，并且会通过一个中间步骤将词排列成更自然的形式。结果变得更好了，但仍然不尽人意。 Model 3：额外增添 翻译结果中常常会出现新词，比如德语的冠词或英语否定句中的「do」。比如「Ich will keine Persimonen」→「I do not want Persimmons.」为了解决这个问题，Model 3 又增加了两个步骤： 如果机器认为有加入新词的必要性，则插入 NULL 标记 为每个标记词的对齐选择合适的小品词或词 Model 4：词对齐 Model 2 考虑了词对齐，但对词序重排一无所知。比如，形容词常会与名词交换位置，所以不管词序记忆得多好，都不会让输出结果更好。因此，Model 4 考虑了所谓的「相对顺序」——如果两个词总是交换位置，模型就能学到。 Model 5：修正错误 这里没什么新鲜的。Model 5 所要学习的参数更多了，而且修正了词位置冲突的问题。 尽管基于词的系统本身是革命性的，但它们仍然无法处理格、性和同义词。每一个词都只有单一一种翻译方式。现在我们已经不再使用这种系统了，因为它们已经被更为先进的基于短语的方法替代。 基于短语的 SMT 这种方法基于所有基于词的翻译原则：统计、重新排序和词法分析。但是，在学习时，它不仅会将文本分成词，还会分成短语。确切地说，这些是 n-gram，即 n 个词连在一起构成的连续序列。 因此，这个机器能学习翻译稳定的词组合，这能显著提升准确度。 其中的诀窍在于，这里的短语并不总是简单的句法结构，而且如果有人明白语言学并干预了其中的句子结构，那么翻译的质量就会大幅下降。计算语言学先驱 Frederick Jelinek 曾经开玩笑地说：「每次我炒掉一个语言学家，语音识别器的表现就会上升一点。」 除了提升准确度，基于短语的翻译在选择所要学习的双语文本上提供了更多选择。对于基于词的翻译，源文本之间的准确匹配是至关重要的，这就排除了让任何文学翻译或自由翻译。基于短语的翻译则可以从中学习。为了提升翻译质量，研究者甚至开始解析不同语言的新闻网站。 自 2006 年以来，每个人都开始使用这种方法。谷歌翻译、Yandex、必应等一些著名的在线翻译工具将基于短语的方法用到了 2016 年。你们可能都还记得谷歌要么得到毫无差错的翻译句子，要么得到毫无意义的结果的时候吧？这种毫无意义就来自基于短语的功能。 老一辈基于规则的方法总是会得到可预测的但也很糟糕的结果。统计方法则总是会得到出人意料和让人困惑的结果。谷歌翻译会毫不犹豫地将「three hundred」变成「300」。这就是所谓的统计异常（statistical anomaly）。 基于短语的翻译已经变得非常流行，当你听到人们说「统计机器翻译」时，多半就是指它。在 2016 年之前，所有的研究都称赞基于短语的翻译是表现最好的。那时候，甚至没人认为谷歌已经在燃起战火，准备改变整个机器翻译图景了。 基于句法的 SMT 这种方法应当被简要提及一下。在神经网络出现的很多年前，基于句法的翻译被认为是「翻译的未来」，但这一思想并未迎来腾飞。 基于句法的翻译的支持者相信它有可能与基于规则的方法融合。它需要对句子进行相当准确的句法分析——以确定主语、谓语和句子的其它部分，然后再构建一个句子树。机器可以使用它来学习转换语言之间的句法单元并根据词或短语来翻译其余部分。那应该可以一劳永逸地解决词对齐问题。 来自 Yamada and Knight [2001] 的示例（http://www.aclweb.org/anthology/P01-1067）以及这个很棒的幻灯片（http://homepages.inf.ed.ac.uk/pkoehn/publications/esslli-slides-day5.pdf） 问题是句法分析的效果很差，尽管事实上我们认为这在之前已经得到了解决（因为我们有很多语言的现成可用的库）。我曾经试过使用句法树来解决比单纯地解析主语和谓语更复杂的任务。但我每次都放弃了，然后使用了另一种方法。 如果你成功过至少一次，请让我知道。 2014 年，一篇关于将神经网络用于机器翻译的出色论文发布：https://arxiv.org/abs/1406.1078。互联网并没关注这项研究，但谷歌除外——他们挽起袖子就干了起来。两年之后的 2016 年 9 月，谷歌发布了改变机器翻译领域的公告，参阅《 》。 这一思想接近照片之间的风格迁移。知道 Prisma 这样的应用吗？它能用某幅著名艺术作品的风格来渲染图片。但这不是魔法。是神经网络学会了识别艺术家的画作。接下来，包含网络决策的最后一层被移除了。所得到的风格化图像只是网络所得到的中间图像。这是网络自己的幻想，而我们觉得这很美。 如果我们可以迁移照片的风格，那我们能不能将另一种语言施加到源文本上呢？我们可以将文本看作是带有某种「艺术家风格」，我们希望在迁移这个风格的同时又保证这些文本的本质不变。   想象一下，假如我要描述我的狗——平均个头、尖鼻子、短尾巴、老是叫唤。如果我把这些狗的特征给你并且描述是准确的，你就可以画出它，即使你从没见过它。 现在，再想象源文本是特定特征的集合。基本上而言，这意味着你可以编码它，然后再让其它神经网络将其解码回文本——但是另一种语言的文本。解码器只知道自己的语言。它对这些特征的来源一无所知，但它可以用西班牙语等语言将其表达出来。再继续前面的比喻，不管你是怎么画这条狗的（用蜡笔、水彩或你的手指），你都可以把它画出来。 再说明一次：一个神经网络只能将句子编码成特定的特征集合，另一个神经网络只能将其解码成文本。这两者彼此都不知情，而且都只各自了解自己的语言。想起什么没有？「中间语言」回来了！ 问题是，我们如何找到这些特征？对于狗来说，特征当然很明显，但文本的特征是怎样的？三十年前科学家就已经在尝试创建通用语言代码了，但最终以失败告终。 尽管如此，我们现在有深度学习了。寻找特征是它的基本任务！深度学习和经典神经网络之间的主要区别是搜索这些特定特征的能力，而无需对这些特征的本质有任何了解。如果神经网络足够大，而且有数千块显卡可用，那就能很好地找到文本中的这些特征。 理论上讲，我们可以将这些神经网络得到的特征交给语言学家，这样他们就可以为自己打开一片新视野了。 但问题是编码和解码应该使用哪种类型的神经网络呢？卷积神经网络（CNN）完美适用于图像，因为它们可以操作独立的像素块。 但文本中没有独立的块——每个词都取决于自己的语境。文本、语音、音乐都是连续的。所以循环神经网络（RNN）是处理它们的最佳选择，因为它们能记住之前的结果——在这里即是之前的词。 现在很多应用都已经使用了 RNN，包括 Siri 的语音识别（解析声音序列，其中后一个声音取决于前一个声音）、键盘提示（记住之前的经历，猜测下一个词）、音乐生成和聊天机器人。 致像我一样的技术宅：事实上，神经翻译器的架构非常多样。一开始是用的常规 RNN，后来升级成了双向 RNN，其中翻译器不仅要考虑源词之前的词，还有考虑其后的词。这要高效得多。然后它又使用了带有 LSTM 单元的多层 RNN，可以实现翻译语境的长期存储。 短短两年时间，神经网络在翻译上的表现就超越了过去 20 年来的一切。神经翻译的词序错误少了 50%、词汇错误减少了 17%、语法错误减少了 19%。神经网络甚至学会了协调不同语言的性和格。而且并没有人教它们这样做。 这一领域最值得提及的进展是从没使用过直接翻译。统计机器翻译方法总是可以使用英语作为关键源。因此，如果你要将俄语翻译成德语，机器会首先将俄语翻译成英语，然后再将英语翻译成德语，这会造成双倍损失。 神经翻译无需这样做——只需要一个解码器就行了。没有共同词典的语言之间也能实现直接翻译，这是有史以来的第一次。 谷歌翻译（自 2016 年以来） 2016 年，谷歌为 9 种语言启用了神经翻译。他们开发出了名为谷歌神经机器翻译（GNMT）的系统。它由 8 个编码器和 8 个解码器 RNN 层构成，另外还有来自解码器网络的注意连接。 他们不仅会切分句子，而且还会切分词。这正是他们解决 NMT 一大主要难题的方法——即罕见词问题。但出现了它们词汇库中没有的词时，NMT 是无能为力的。比如说「Vas3k」。我估计没人让神经网络学习翻译我的昵称。在遇到罕见词时，GNMT 会尝试将词分解成词片段，然后根据这些片段得到翻译结果。很聪明的做法。 提示：浏览器中用于网站翻译的谷歌翻译仍然用的是老旧的基于短语的算法。不知为何谷歌没有升级，而且其翻译结果和在线版本谷歌翻译相比差距其实相当大。 在线版本的谷歌翻译使用了众包机制。人们可以选择他们认为最正确的版本，而且如果很多用户都认同，那么谷歌就会一直按这种方式翻译这个短语并将其标注为一个特例。对于「Let』s go to the cinema」或「I』m waiting for you」等日常使用的短句而言，这种做法效果很好。谷歌的英语会话水平比我还好，不开森～ 微软必应的工作方式和谷歌翻译差不多。但 Yandex 不一样。 Yandex Translate（自 2017 年以来） Yandex 于 2017 年推出了自己的神经翻译系统。该公司宣称其主要特色是混合性（hybridity）。Yandex 将神经方法和统计方法组合到了一起来执行翻译，然后再使用其最喜欢的 CatBoost 算法从中选出最好的一个。 问题是神经翻译在翻译短句时常常出错，因为它需要使用上下文来选择正确的词。如果一个词在训练数据中出现的次数非常少，那就很难得到正确的结果。在这种情况下，简单的统计翻译能轻松快捷地找到正确的词。 在句子末尾加上句号后，Yandex 的翻译结果更好了，因为这时候它启用了神经网络机器翻译。 Yandex 没有分享具体的技术细节。它用营销新闻稿搪塞了我们。好吧。 看起来谷歌使用了 SMT 来执行词和短句的翻译。他们没有在任何文章中提及这一点，但如果你查看短表达和长表达之间的差别，你就能相当明显地注意到。此外，SMT 也被用来展示词的统计情况。 每个人都仍然为「巴别鱼」（即时语音翻译）的构想感到兴奋。谷歌已经带着 Pixel Buds 耳机向这个方向迈出了一步，但事实上这仍然达不到我们梦想的效果。即时语音翻译与通常的翻译不同。系统需要知道何时开始翻译以及何时闭嘴聆听。我还没见到过任何能够解决这一问题的方法。也许，Skype 还行吧…… 而且有待推进的领域不止这一个：所有的学习都受限于并列文本块的集合。最深度的神经网络仍然是在并列文本中学习。如果不向神经网络提供资源，它就无法学习。而人类可以通过阅读书籍和文章来扩增自己的词汇库，即使不会将其翻译成自己的母语。 如果人类能做到，神经网络就也能做到。理论上讲是这样。我只发现了一个原型设计试图开发出能让知晓一种语言的网络能通过阅读另一种语言的文本来获取经验：https://arxiv.org/abs/1710.04087。我倒是想自己试试看，但我很笨。好了，就这样吧。 有用的链接 Philipp Koehn：统计机器翻译：https://www.amazon.com/dp/0521874157/。我发现方法的最完整的集合。 Moses：http://www.statmt.org/moses/。一个很受欢迎的库，可用于创建自己的统计翻译系统。 OpenNMT：http://opennmt.net/。另一个库，但是用来创建神经翻译器的 我最喜欢的博主的文章，解释了 RNN 和 LSTM：https://colah.github.io/posts/2015-08-Understanding-LSTMs/ 视频「如何制作一个语言翻译器」：https://youtu.be/nRBnh4qbPHI。这家伙很有趣，解释得很不错，但不够充分。 来自 TensorFlow 的文本教程，教你如何创建神经翻译器：https://www.tensorflow.org/tutorials/seq2seq。想查看更多案例和尝试代码的人可以参考。 
122,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739320&idx=2&sn=53e40c26d0b42eed03599a0678f06491&chksm=871ad606b06d5f10f715eafc61ad1e65b34055508f126f71d7ad1b93e3867eab4242ca796417&scene=27,业界 | 腾讯成立机器人实验室Robotics X，与Nature宣布长期合作,3 月 15 日，腾讯 AI Lab 第二届学术论坛在深圳举行，在上午的论坛开幕活动中，腾讯揭晓了两个重磅消息：成立机器人实验室 Robotic X；腾讯 AI Lab 与自然科研达成战略合作。 「过去几年间，人工智能是处在风口浪尖的领域，有很多 AI 技术已经进入了我们的生活，如人脸识别，智能翻译等技术，」腾讯副总裁姚星表示。「腾讯 AI Lab 在近两年的发展中基本实现了学术有影响、工业有产出的初衷。」本次两大消息的发布，也宣示着腾讯正在将人工智能方面的研究影响力进一步扩展到学界，以及机器人等「真实世界」领域。 论坛上，姚星宣布腾讯将成立机器人实验室「Robotics X」，与 AI Lab 一起成为腾讯 AI 产业的双基础支撑部门。据了解，Robotics X 是在腾讯的 TEG 下面，与腾讯 AI Lab 平级。 「腾讯 Robotics X 将和 AI Lab 一起成为腾讯公司最重要的两个基础研究实验室，去支撑 AI + 行业，成为虚拟世界连接真实世界的最重要部分，」姚星介绍道，「我相信随着 Robotics X 的成立和未来的成长，我们一定能开拓出与早期 PC 时代的互联网产业、智能手机时代的移动互联网产业类似的市场规模。」 目前，腾讯还没有透露有关 Robotics X 的更多信息，机器之心将持续跟进这一消息的进展。 为了推动「AI+医疗」领域的跨学科研究，促进人工智能产学研一体化，在本次论坛上，腾讯 AI Lab 与国际领先的学术与教育出版集团施普林格•自然集团（Springer Nature）旗下的自然科研（Nature Research）正式达成战略合作。双方将着眼于医疗领域的实际痛点，探讨通过学术奖金、产学研交流等多种形式，整合全球科研资源，支持医疗行业与人工智能研究的跨学科合作，推动更多「AI+医疗」产品的研发与落地。 作为拥有《Nature》等重要科学期刊的发行集团，施普林格•自然集团近年来也正随着人工智能技术的兴起不断改进自身的发展思路。此前，集团旗下拥有施普林格、Nature、BMC、帕尔格雷夫•麦克米伦和《科学美国人》，及众多备受关注的专业附属品牌。 2016 年底，与 Nature 相对应的 Science 创办了机器人专属子刊《Science Robotics》，致力于推动机器人领域的发展，以及不同研究应用领域的交叉技术。 在人工智能领域，自然科研在旗舰刊 Nature 上发表过诸多突破性的科研进展，2019 年还将推出聚焦于人工智能的新子刊《自然-机器智能》（Nature Machine Intelligence）；集团旗下的施普林格（Springer）则出版了数千种 AI 和机器人学方面的图书和期刊论文，其中包括备受欢迎的获奖图书《机器人手册》（Springer Handbook of Robotics）。此外，在医疗领域，自然科研旗下领先的综合学术期刊，每年都会发表医学领域诸多突破性的研究成果。 腾讯副总裁、AI Lab 负责人姚星在签约仪式上表示，腾讯 AI Lab 一直以来积极推动与各界共同打造产学研生态。AI Lab 与自然科研在促进人工智能发展，尤其是在解决医疗等人类重大难题这方面有着共同目标。双方强强携手，将进一步加快实现「让 AI 无处不在（Make AI Everywhere）」的愿景，并期望在医疗等具有重大普惠意义的民生领域实现突破。 1、举办 AI 学术会议 双方将于明年在深圳联合举办人工智能学术会议（Nature AI Conference），这是自然科研首度与科技企业合作推出的人工智能国际学术会议，旨在推动中外科研界与产业界的交流互动，促进该领域学术成果的转化落地。 2、共同探索「AI+医疗」，解决重大医疗难题 在这次合作中，自然科研将充分发挥其在医疗领域的资源网络，通过与腾讯建立「AI+医疗」Fellowship 等方式，来推动医疗行业与人工智能的跨学科交流，促进「AI+医疗」的应用发展。 腾讯在 AI LAb 论坛上也展示了自己在医疗领域的计划。「AI+医疗」是人工智能可见的重要落地场景，也将是腾讯与自然的重点合作的领域之一。2017 年，腾讯曾发布了首款「AI+医疗」产品「腾讯觅影」，筛查一个内镜检查用时不到 4 秒，对早期食管癌的检出率高达 90%。目前，该产品从最初的食道癌早期筛查已经拓展到肺结节、糖尿病、眼底病变等多病种筛查，并在全国多省市的 100 多家三甲医院落地。 2017 年 11 月 15 日，科技部召开新一代人工智能发展规划暨重大科技项目启动会，宣布首批国家新一代人工智能开放创新平台名单，中国国家医疗影像新一代人工智能开放创新平台依托腾讯公司技术能力组建，「腾讯觅影」也成为「AI+医疗」的国家级标杆。 3、促进跨学科合作，打造「通用人工智能」 通用人工智能不仅仅是对机器的研究，更是对人类的研究，涉及脑神经科学、心理学等众多交叉领域。在应用领域上，人工智能技术和互联网技术一样，在未来可能会成为一种新技术基础设施，赋能各行各业。腾讯 AI Lab 将与自然科研合作开展更多跨学科合作和研究，打造 AI 生态体系。 自然科研董事总经理 Dean Sanderson 表示：「我们集团以『促进探索发现』为使命，并深知科研成果如在科研界之外也能获得广泛使用，那将产生许多积极的社会和经济影响力。因此，我们一直致力于通过自然学术会议等平台，汇聚包括科学家在内的全球各界人士，与我们一道推进这一使命的实现。我们很高兴能与腾讯 AI Lab 结成战略合作伙伴，促进各国科研人员之间，以及与产业界的互动合作，推动全球人工智能领域的研究和应用，造福社会。」 自 2016 年 4 月建立以来，腾讯 AI Lab 已建成了位于深圳和西雅图的两个实验室，分别由张潼（AI Lab 主任）和俞栋（AI Lab 副主任）带领。 据介绍，该机构目前已拥有 70 多位研究科学家和 300 多位应用工程师。其仅在 2017 年一年中就在 CVPR、ACL、ICML、NIPS 和 Nature 子刊等衡量研究能力的顶级 AI 会议和期刊中，被收录了超过 100 篇论文。与此同时，AI Lab 也发布了著名围棋程序「绝艺」，并在与人类顶尖棋手的较量中取得了优异的成绩。 腾讯正在努力打造应用于现实生活中的 AI 技术，随着 AI Lab 研究的不断推进，我们会在未来看到越来越多的人工智能技术进入到腾讯产品当中。 
123,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739320&idx=4&sn=0a0fbfa4a913ac961967d5880679e912&chksm=871ad606b06d5f10061b279cefa8d261da3d7b883f5e567e04e618bd2f8e1658ec6411e8075e&scene=27,资源 | Texture：一个优雅的开源学术论文书写工具,近日，Substance 在 GitHub 上开源了一个用于结构文本的文字处理工具 Texture，他们表示该工具像 LaTeX 一样可以开放使用，且如经典的文字处理工具那样简单。机器之心使用后发现，该工具可以非常流畅与简单地编写学术论文，我们可以添加作者、正文或参考文献等结构化的模块而快速完成文章。 项目地址：https://github.com/substance/texture 用于处理结构化文本的文字处理工具 Texture： Texture 是一个用于生产科学论文或内容的工具，它使用 Dar 格式，因此也严格定义了 JATS Archiving 和 Interchange Tag Set（「green」v.1.1）XML 格式。读者可以查看以下在线演示或直接下载桌面版 APP。 在线演示：http://cdn.substance.io/texture-preview-1/ 桌面版 APP：https://github.com/substance/texture/releases/tag/v1.0.0-preview.1 Texture 的开发项目早在 2015 年就已经启动，2016 年发布了两个预览版，并在 2017 年持续完善。今年三月，Substance 发布了 Texture 的最终测试版 Texture 1.0.0 Preview 1。该项目首先会满足研究社区的论文书写需求，因此也会有明确的讨论和补充渠道。 其实 Texture 就是为了解决撰写学术论文成本过高的问题。因为不论我们选择 Word 那样的传统文字处理工具还是使用 LaTeX 那样的标记语言，完成学术论文都有很大的局限性。Word 非常容易使用，但只提供非结构化的内容，而 LaTeX 能提供很多结构，却需要我们学习一门新的标记语言。这两种方式最后都需要转化为结构化的 JATS-XML 格式，才能继续提交到出版物。因此 Texture 希望结合 Word 易于使用的特点和 LaTeX 结构化内容的属性，而大大减少完成学术论文所需要的成本。 在机器之心的试用中，论文的不同结构模块可以使用对应的对话框添加，如下是添加预印本论文参考文献的界面： 因此，Texture 允许将原始内容以交互的方式转化为结构化内容，并添加语义信息以满足出版物的要求。最后，Texture 目前还有一些功能模块没有完成，我们也没有找到 Mathjax 等数学模块。在 GitHub 中，Substance 展示了 Texture 所有的特征与功能，我们也非常期望该工具完成后能给学术写作带来真正意义上的飞跃。 
124,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739320&idx=1&sn=864e1c70475788ba9454a2366bda413b&chksm=871ad606b06d5f10c155e18c5d07de9bc137df8aeef979209f6d102439722dcb95fc2111e75b&scene=27,你的加密货币有价值吗？这里有一个深度学习ICO诈骗鉴别系统,"随着区块链的热潮，加密数字货币正吸引着越来越多人的注意。然而，面对层出不穷的新「币种」，我们很难判断其投资价值，甚至会面临很大的欺诈风险。近日， 创始人、斯坦福大学博士李纪为联合美国加州大学圣塔芭芭拉分校、斯坦福大学共同提出了一种基于深度学习的加密数字货币 ICO 诈骗鉴别系统 IcoRating，试图帮助解决这一问题。这种新方法评估了市面上绝大部分加密货币的生命周期、价值变化与其 ICO 信息（包括白皮书、github 库、创始人信息）之间的关联，去预测并实现了很高的准确率。 加密货币（如 BTC、ETH、NEO 等）正吸引着越来越多人的关注。与中心化电子货币和中心化银行系统相反，大多数数字代币不需要任何授权。这些去中心化系统通过区块链运转，区块链是一个开放、持续增长的分布式账本。加密货币的市场资本化在过去的三年中取得了显著增长，如图 1 所示。根据 CoinMarketCap.com 提供的数据，加密货币日最高交易量接近 2017 年纽约证券交易所的日均交易量。 因其去中心化的本质，加密货币众筹不需要满足风险投资的必要条件，而是经过 ICO。在 ICO 中，投资者用法定货币（如美元、人民币）或其他加密货币（BTC、ETH）来获得众筹的加密货币。ICO 后，这些众筹的加密货币成为具备货币功能的货币单元。新型加密货币发行前一般会先准备一个白皮书，详细介绍这个币的商业、技术和财政细节。如图 2 所示，ICO 项目的数量从 2013 年 7 月到 2017 年 1 月稳定增长，而 2017 年突飞猛进。 尽管 ICO 项目可以提供公平的投资机会，众筹的便捷性给肆无忌惮的企业创造了使用 ICO 进行「拉高出货」的机会。也就是说 ICO 的发起人抬高了众筹加密货币的价格（「俗称拉盘」），然后迅速出售加密货币来获利（「俗称砸盘」）。此外，加密货币的去中心化本质对政府监管带来了极大的挑战。根据 Engadget 的数据，2017 年，902 个基于众筹的数字货币中 46% 都失败了。图 3 和图 4 展示了一个更严重的问题。图 3 和图 4 中 x 轴上每一个间隔（bucket）表示价格改变范围，y 轴上对应的值表示 ICO 项目的百分比。可以看到的，4.56% 现有 ICO 项目在发行半年后都遭受了价格下跌，下跌程度超过令人发指的 99.9%，一年后该比例甚至上升到了 6.89%。大约 29% 的项目在半年后价格下降超过 80%，一年后这一比例上升到了 39.6%。虽然说每一个价格骤跌的 ICO 项目都是诈骗是不确切的，但构建一个可靠的 ICO 信用评级系统在 ICO 之前对数字货币进行评估是必要且紧迫的。 本论文提出 IcoRating，一种基于机器学习的 ICO 评级系统。通过分析 2251 个 ICO 项目，研究者关联了数字货币的生命周期和价格变化与不同级别的 ICO 信息，包括白皮书、创始团队、GitHub 库、网站等。通过整合不同种类的信息，该模型能够以 0.83 的准确率和 0.80 的 F1 分数预测一个 ICO 项目能够在 ICO 半年以后依然存活。 IcoRating 是一个基于机器学习的系统。与人类评级系统相比，IcoRating 有两大强项：（1）客观性：机器学习模型包含更少的先验知识，可以从数据中学习因果关系，这与需要大量人类专家的人类评级系统相反，且人类专家不可避免会引入错误。（2）不会被肆无忌惮的人随意篡改：该信用评级结果是机器学习模型通过黑箱训练得到的输出。该过程涉及极少的人为操作。 希望这个工作可以鼓励大家更多地分析和评估 ICO 项目的质量，并且可以潜在地帮助投资者识别哪些是 ICO 项目诈骗。 本论文其余部分的结构如下：第二部分简要概述加密货币、区块链和 ICO。第三部分介绍了 ICO 项目数据集的构建过程，并且提供了一些基本的数据分析。第四章介绍了论文提出的机器学习模型，第五部分是简短总结。 加密货币、区块链和 ICO 加密货币 加密货币是「一种数字资产，旨在作为交换媒介，使用加密技术来保护其交易。」大多数加密货币使用去中心化控制。第一个去中心化的加密货币是比特币（简称 BTC）（Nakamoto, 2008），由一个或一群不明身份的人用 Satoshi Nakamoto（中本聪）的名字于 2009 年创建。自 BTC 出现后，多种加密货币被创造出来，最著名的包括 Ethereum、Ripple、EOS 和 NEO。 区块链 加密货币的交易由区块链进行验证。人们可以将区块链视为分布式账本，它不断增长并永久记录两方之间的所有交易。每条记录叫作一个块，包含链接到前一个块、时间戳和交易数据的加密哈希指针。账本以分布形式被所有参与者拥有，且记录只能在改变所有后续网络块的情况下得到更改。交易被广播给网络中的所有节点。区块链使用多种时间戳方案，例如工作量证明或股权证明。区块链概念消除了数据集中储存的风险：它没有中心故障点，数据对每个参与者都是透明的。 ICO ICO 是以加密货币为中心的众筹融资手段。在 ICO 中，众筹加密货币（主要以代币形式）被转移给投资者，以换取法定货币或其他加密货币。当 ICO 的融资目标达到时，这些代币就成为具备货币功能的货币单元，可用于交易货物或者其他加密货币。 ICO 为早期项目提供众筹机会，以逃避风险投资家、银行和证券交易所的规定。它们还提供了超越风险投资或私募股权投资的投资机会，二者是早期投资机会的主导。另一方面，由于缺乏监管，ICO 给投资者带来了重大风险。不同的国家对 ICO 和加密货币有不同的规定。例如，中华人民共和国政府禁止所有 ICO，而美国证监会（SEC）则表示它有权对 ICO 应用联邦证券法，而委内瑞拉政府则推出了自己的加密货币 petromoneda。 研究者收集了 2251 个 ICO 项目的信息，包括白皮书、网站信息、ICO 时的 GitHub 库，以及创始团队。我们从多个提供商处获取数据，提供商包括 CryptoCompare、CoinMarketCap 和 CoinCheckup。 LDA 研究者在收集到的白皮书上运行了一个潜在狄利克雷分布（LDA）模型（Blei et al., 2003）。LDA 是一个生成统计模型，解释了具备基于词共现的词丛（即「topic」）的文本文档。每个文档以潜在主题的概率分布形式呈现，每个潜在主题都是词的概率分布。LDA 的生成过程如图 5 所示，过程图示见图 6。 ICO 评级模型 IcoRating 是一个基于机器学习的评级模型。这里使用的模型是监督学习模型。在标准监督学习设置中，研究者希望找到模型 F，可使输入 x 映射至输出 y： 输入 输入 x 是 ICO 项目，包括其公开可获取的各方面信息。 输出 输出 y ∈ {0, 1} 是一个二分变量，表示输入 ICO x 是否为诈骗项目。这里的关键问题是如何确定 ICO 项目是诈骗项目。本文认为如果数字货币在其 ICO 一年后下跌 80%，则该 ICO 项目是诈骗项目。换言之，通过检查数字货币的历史价格，如果数字货币一年后的价格不到初始价格的 20%，则我们认为该 ICO 项目是诈骗。在收集到的 2251 个项目中，研究者收集了 1482 个项目的历史价格，且这些 ICO 项目都实施至少一年（截止到本研究进行时）。 实验结果展示了使用不同的特征组合识别 ICO 诈骗项目的结果。随着 m 值从 0.01 逐渐提升至 1，诈骗项目的比例也逐渐增加，查准率不断提升，召回率逐渐降低。实验显示，白皮书和 GitHub 库是最重要的两类特征，在 m 值分别为 0.1 和 0.5 时 F1 分数都达到了 0.7。研究者考虑了更多特征，能够逐渐获得更好的查准率和召回率。研究者使用所有类型的特征，在 m=1 时预测 ICO 诈骗项目达到了 0.83 的查准率、0.77 的召回率和 0.80 的 F1 分数。 论文：IcoRating: A Deep-Learning System for Scam ICO Identification 论文链接：https://arxiv.org/pdf/1803.03670.pdf 摘要：加密货币（或数字代币、数字货币，如比特币、以太币、 XRP 和 NEO）近年来在公众使用、看重程度和理解方面增长迅速，为投资者带来了惊人的利润。与其他货币或银行系统不同，大多数数字代币无需中央授权。这种去中心化的方式对信用评级构成了重大挑战，大多数 ICO 项目目前不受政府监管，这意味着我们急需一种值得信赖的 ICO 评级系统。 本论文介绍了 IcoRating，第一个基于机器学习的加密货币评级系统：我们分析了目前 2251 种数字货币，并评估了货币的生命周期、价格变化与其 ICO 信息之间的关联，ICO 信息包括白皮书内容、创始团队、GitHub 库和网站。该系统预测 ICO 诈骗的准确率达到了 0.85。 我们希望该研究可以帮助投资者识别 ICO 诈骗，同时引出更多对 ICO 项目的分析与评估研究。 "
125,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739284&idx=2&sn=445f149b37be7ca04d801f7efcba309d&chksm=871ad62ab06d5f3cb2d4569baf6c01688aa2bcec8f8b90fbc44c619ac6044f86cfd906b47054&scene=27,业界 | Stack Overflow 2018开发者调研结果公布：DevOps和机器学习行业收入最高,选自Stack Overflow 近日，Stack Overflow 公布了其第八次年度开发者调查结果，这次参与者超过 10 万人，创下历史新高。本次的调研内容涵盖开发者基本情况、技术、工作、社区、方法论等多个方面。机器之心重点编译了有关开发技术的内容，同时也涉及开发者的薪资待遇，更多详细内容请参见文末链接。 调查地址：https://insights.stackoverflow.com/survey/2018 Stack Overflow 2018 开发者调查有超过 10 万开发者参与，告诉 Stack Overflow 他／她们如何学习、如何规划职业、使用什么工具以及在工作中的追求。 每一年，Stack Overflow 都会向开发者社区进行调查，从开发者最喜欢的技术到工作偏好等等。今年是 Stack Overflow 第八次发布年度开发者调查结果，此次调查受访者数量是历年最高。今年 1 月份有超过 10 万名开发者参加了 30 分钟的调查。 DevOps 和机器学习在今天的软件行业中是很重要的趋势。这些工作中涉及的语言和框架的数量正日渐增长，在这些领域工作的开发者拥有最高的薪水。 只有很少一部分开发者称他们会编写不道德的代码，或者说他们没有义务考虑代码的伦理性，但是受访者也能看到大量道德灰色地带。开发者并不确定他们会如何报告道德问题，且在对不道德代码的责任问题上存在分歧。 整体上，开发者对人工智能技术带来的益处是乐观的，但对于 AI 的危险性存在分歧。 调查中，Python 在编程语言的排行有所上升，在今年的流行度排行中超过了 C#，正如它去年超过了 PHP。 在评估一项工作时，不同类型的开发者考虑不同的优先事项。女性开发者称她们的最优先事项是公司文化和职业发展机会，而男性开发者称他们的最优先事项是薪酬和具体的技术。 Stack Overflow 将把调查的匿名结果放在 Open Database License (ODbL）上提供下载。 下载地址：https://opendatacommons.org/licenses/odbl/1.0/ 将近 60% 的受访者是后端开发者，大约 20% 是移动端开发者。每个开发者的涉及类型的中位数是 2，最常见的组合是前端、后端和全栈开发者。高度关联的组合是数据库管理员和系统管理员、开发运营专家和系统管理员，以及设计师和前端开发者。 性别 Stack Overflow 调查了受访者的性别，发现超过 90% 的受访者是男性。根据 Quantcast，女性占 Stack Overflow 美国流量的 10%；今年，9% 的美国调查受访者是女性。在包含美国、印度和英国的区域中，女性学生开发者占学生开发者的比例略高于女性专业开发者占专业开发者的比例。 编程、脚本和标记语言 在过去六年来，JavaScript 一直是最常用的编程语言。Python 的排名有所上升，在今年超过了 C#，正如它去年超过了 PHP。Python 坚定主张要成为「最快速成长的主流编程语言」。 专业开发者和所有开发者在技术选择上的偏好相近。 框架、库和工具 Node.js 和 AngularJS 仍然是这一类别中最常用的技术，React 和 .Net Core 的排名也很靠前。 数据库 和去年一样，MySQL 和 SQL Server 仍然是最常用的数据库。 平台 Linux 和 Windows Desktop 或 Server 是最常用的开发平台。 最喜爱、最讨厌和最关注的语言 Rust 连续第三年成为受访者最喜爱的编程语言，第二名是 Kotlin，它是今年第一次加入调查的语言。该结果表明更多的开发者希望继续使用这些语言，而不是其它语言。 同样是连续三年，Visual Basic 被列为最讨厌的编程语言。这意味着目前从事技术的开发者有很大一部分对学习和使用它们不感兴趣。 Python 连续两年被列为最受开发者关注的语言，这意味着那些还没用过 Python 的开发者最希望学的就是 Python。 最喜爱、最害怕和最关注的框架、库和工具 TensorFlow 在 Stack Overflow 社区是增长最快的技术之一，它深受数据科学家、算法工程师和机器学习工程师的喜爱。相比之下，Cordova 是最让人讨厌的框架。此外，React 是开发者最想要使用的 UI 框架。 最喜爱、最害怕和最关注的数据库 Redis 连续第二年成为最受喜爱的数据库，这意味着更多的开发者想要继续使用 Redis，而不是其他数据库。IBM's Db2 是开发者最讨厌的数据库，MongoDB 第二年成为开发者最关注的数据库。 最喜爱、最害怕和最关注的平台 Linux 再次成为开发者最喜爱的开发平台，今年 Serverless 也很受欢迎。Sharepoint 连续第二年成为开发者最讨厌的平台，很多开发者最想在 Android 平台和树莓派上进行开发工作。 最常用的开发环境 Visual Studio Code 略微领先 Visual Studio，成为最流行的开发环境工具，但不同类型的开发者有不同的选择。移动 app 开发者更倾向于选择 Android Studio 和 Xcode，DevOps 和系统管理者的最常选择则是 Vim，数据科学家更可能选择 IPython/Jupyter、PyCharm 和 RStudio。 全球最高薪酬开发者职位涉及的语言 在全世界，使用 F#、Ocalm、Clojure 和 Groovy 语言的开发者拥有最高的薪酬，薪酬中位数是 7 万美元年薪。不同区域的最高薪酬职业涉及语言是有区别的。美国的 Erlang 和 Scala 开发者享有最高的薪酬，而印度的 Clojure、Erlang 和 Haskell 开发者享有最高的薪酬。 技术的关联性 技术聚合在一起形成相关联的生态系统，可以被开发者使用。从上图中我们可以看到，中间的较大簇是 web 开发技术（使用 JavaScript、HTML 和 CSS），该簇以通过 SQL 与微软技术联系起来（微软技术包括 C#、Visual Studio、.NET Core）。左边是一个簇，将 Java、Android、iOS 和 Linux、bash/shell、Python 联系起来。其他较小的簇包括 Scala/Spark、C/C++和其他小型技术（如特定语言的 IDE）。 开发者认为 AI 哪些方面最危险？哪些方面最让人激动？ 现在一些开发者在参与机器学习和人工智能研究，因为我们询问开发者他们对这些技术的看法。关于 AI 最危险的方面，众口不一，各选项占比大致相同。而认为 AI 发展最令人振奋的一点最高选项是工作自动化。 工作 行业 软件开发者分布在不同的行业之中，而不仅限于技术行业。上图主要聚焦于今年的技术行业，并且在这些选择中，更专业的开发者在网页开发、IT 和 SaaS 公司工作，其他行业的开发者，比如咨询和医疗，具有更多年的专业开发经验。相比于网页开发/设计或电子商务开发者，这些行业的开发者从业超过 20 年的可能性是前者的 2 倍。 不同类型开发者的薪水 工程经理、DevOps 专家和数据科学家有着最高的薪资。平均的高收入者因地理位置而变化。比如在印度，数据科学家是高收入人群，而在欧洲国家，后端开发者和嵌入式设备开发者是高收入人群。 薪水与不同类型开发者的经验 通常，拥有更多年经验的开发者所获薪资更高。但是，我们也看到在经验相同的情况下，一些类型的代码工作获得了更高的薪资，比如数据科学家和 DevOps 专家。 薪水与语言开发经验 即使在经验不多情况下，使用上图中黄线以上语言（比如 Go、Clojure 和 F#）的开发者的薪资也更高；而使用黄线以下语言（比如 PHP 和 Visual Basic 6）的开发者的薪资更少，即使有数年的开发经验。图中圆点的大小表示相较于其他语言有多少开发者正在使用这一语言。 
126,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739284&idx=5&sn=1d976d1aead3141eebe3e951a285606a&chksm=871ad62ab06d5f3cd6ad21d78814e02e8f8d0952d9a65b18a8a1389e27702ca83bd86c4eaae2&scene=27,学界 | 谷歌论文新突破：通过辅助损失提升RNN学习长期依赖关系的能力,"本文提出了一种简单的方法，通过在原始函数中加入辅助损失改善 RNN 捕捉长期依赖关系的能力，并在各种设置下评估了该方法，包括用长达 16，000 的序列对一张图的逐个像素进行分类，以及对一个真实的基准文件进行分类；和其他常用模型和大小相当的转换器相比，该方法在性能和资源使用效率方面的表现都非常突出。 介绍 大量人工智能应用的前提是首先理解序列中事件间的长期依赖关系。例如，在自然语言处理中，有时就必须要对书中描述的远距离事件之间的关系有所了解，这样才能回答问题。一般而言，现在是通过梯度下降和带有循环网络的 BPTT（Rumelhart et al., 1986）解决这一问题的。然而，通过梯度下降方法学习长期依赖性很难，因为借助 BPTT 计算的梯度在训练过程中有消失或爆炸的倾向。除此以外，如果想要使 BPTT 起作用，人们需要存储中间过程的隐藏状态。内存需求与序列长度成正比，使得这种方法难以处理大问题。 也有人提出过若干个有望解决这些问题的方法。首先，可以使用 LSTM（Hochreiter & Schmidhuber, 1997）代替常用的循环神经网络，这可以改善循环网络中的梯度流的问题。此外，还可以使用梯度裁减（Pascanu et al., 2013）提高 LSTM 训练过程的稳定性。最后，为了减少内存方面的需求，可以使用截断的 BPTT 或合成梯度（Jaderberg et al., 2017）定期存储隐藏层的状态（Gruslys et al., 2016; Chen et al., 2016）。 卷积神经网络也可以消除长期的依赖关系问题，因为内核较大，而且像 ResNets（He et al., 2016）这样的深度网络允许跨越图像中相距较远的两个部分学习长期依赖关系。但这样就会用到完全不同的架构，我们可以对此进行权衡。例如，在训练过程中，模型的输入（一张图像或者一个序列）以及中间的激活都要存储在内存中。在推断时，典型的 CNN 需 O(n) 的存储空间（n 代表输入的大小）。尽管由于训练和推断的计算需要随机存取到内存 O(n)，但变换器（Vaswani et al., 2017）也有相似的问题，并且严重一些。 RNN 的优势在于，假设 BPTT 的长度为 l，训练就需要 O(l) 的内存。这是一个用 PTB 数据集（Marcus et al., 1994）训练语言模型的典型实例，这样做 100 万个符号序列的状态就永远不会重置。因此，从理论上讲 RNN 可以从极远的距离学到这种关系。此外，RNN 的推断也需要 O(l) 的内存，因为 RNN 不需要「回头」。 在这篇论文中，我们提出一种正交技术以进一步解决循环网络单纯依赖 BPTT 的缺陷。该方法介绍了一种无监督辅助损失，可以重建/预测锚点前后的一部分随机序列。实现这个方法，只需要几步有监督损失的 BPTT。 论文结果表明无监督辅助损失显著改善了 LSTM 的优化和泛化能力。此外，如果使用这一方法，无需在训练过程中执行冗长的 BPTT 以获得良好的结果。因此，该方法适用于长序列，在此之前，这些长序列中出现的梯度消失/爆炸问题以及冗长的 BPTT 消耗问题都是模型训练中的重要瓶颈。 实验采用的序列长达 16，000 个元素，带有辅助损失的 LSTM 训练得更快并使用了更少的内存，而采用完整的反向传播训练 LSTM 则非常困难。 方法 假设目标是使用循环网络阅读序列并分类。我们随机采样一个或多个锚点，并在每个锚点插入无监督辅助损失。 3.1. 重建辅助损失 在重建过去事件时，我们取样了锚点之前的子序列，并将第一段标记序列插入解码器网络；然后我们要求解码器网络预测出剩下的子序列。整个过程如图 2 左图所示。 我们推断，如果拟预测序列离定位点足够近，解码重建过去事件所需的 BPTT 的步骤就会非常少。另外，随着训练的进一步加强，定位点会在循环网络中充当临时存储的角色来记录序列中过去的事件。如果我们选择了足够多的定位点，就会在整段序列上建立足够多的存储，当我们到序列末端时，分类器会记住序列从而更好地进行分类。因此，分类器仅需几步反向传播步骤对 LSTM 的权重进行微调，因为网络已经通过优化的辅助损失很好地对输入序列的嵌入进行了学习。 3.2. 预测辅助损失 本文考虑的另一种辅助损失类似于语言模型损失，如图 2 右图所示。这种情况要求解码器网络在子序列中从锚点出发预测出所给序列的下一段标记序列。这类无监督辅助损失第一次是 Dai & Le (2015) 提出的，他们将其应用于整个输入序列。但我们将其应用在长期依赖关系学习的扩展方案中，因此我们仅将这种损失应用在随机锚点之后的子序列中。 3.3. 训练 我们将前一种方法称为 r-LSTM , 将后一种方法称为 p-LSTM（r 和 p 分别代表重建和预测），在两个阶段对这两个模型进行训练。第一阶段是单纯的无监督预训练，在这一方法中辅助损失取最小值。而在第二阶段中，执行的是半监督学习，在这一阶段中我们取主要目标损失 L_supervised 和 L_auxiliary 的总和最小值。用定期采样（Bengio et al., 2015a）的方法训练执行重建操作的辅助 LSTM。 论文：Learning Longer-term Dependencies in RNNs with Auxiliary Losses 论文链接：https://arxiv.org/abs/1803.00144 尽管训练循环神经网络（RNNs）最近仍有进展，但在长序列中捕捉长期依赖关系仍旧是根本的挑战。现在一般会用通过时间的反向传播（BPTT）解决这一问题，但这很难应用于极长的序列。本文提出了一种简单的方法，可以通过在原始函数中加入辅助损失改善 RNN 捕捉长期依赖关系的能力。辅助损失强制 RNN 在序列中重建之前的事件或是预测接下来的事件，这样的操作可以截断长序列中的反馈，还可以提高 BPTT 整体的能力。我们在各种设置下评估了所述方法，包括用长达 16，000 的序列对一张图的逐个像素进行分类，以及对一个真实的基准文件进行分类。和其他常用模型和大小相当的转换器相比，我们的方法在性能和资源使用效率方面的表现都非常突出。我们进一步分析了辅助损失在优化和正则化方面的积极影响，和没有反向传播相比，几乎不会出现极端情况。 "
127,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739284&idx=4&sn=20bb5862c73c3c52a17e2909667dca5a&chksm=871ad62ab06d5f3c1721825b4e9fe1a36d5cdad82ec93ca256a5f72397be3c6a1d7df9b76b46&scene=27,业界 | 第四范式业界首推免费智能客服服务,2018 年 3 月 13 日，人工智能公司第四范式正式宣布，即日起将免费对外开放旗下「第四范式智能客服」平台（以下简称「智能客服」）。据了解，这是智能客服市场的首款免费产品。第四范式方面表示，定位于免费的智能客服开放平台，目前「智能客服」可以提供任务对话、业务咨询、知识图谱、智能聊天等多项较为常见的功能，仅需配置 2 分半钟，即可接入 APP、微信公众号、Web 端、微博等渠道。此外，企业级开发者还可根据自身业务需求，开发定制化服务。 近年来，中国消费的升级，极大地拉动了客服类需求的快速增长，但传统客服低效、高成本的特性却制约着这一行业的发展，这便推动了人工智能在客服领域的迅速兴起与落地。事实上，开发一套智能客服系统，需拥有自然语言处理、意图识别、知识图谱、单轮/多轮对话等引擎作为支撑，其中涉及自然语言处理、语义相似度、状态机、高维机器学习、深度学习等诸多底层算法。专业人才匮乏、成本制约等原因使众多企业难以低成本、低代价的获得智能客服的能力，第四范式则希望改变这种行业困境。 为了让企业用户、开发者更易使用智能客服，第四范式加大了在技术方面的投入。在训练智能客服的智能化程度上，第四范式基于大规模机器学习和迁移学习技术，研发了自学习技术。一方面，在用户配置后，知识库作为训练数据，自动进行模型更新迭代，优化效果；另一方面，在用户与机器人对话的过程中，机器人会根据用户的行为、情绪反馈自主学习，调整对话策略和对话结果。随着反馈的增多，智能客服理解用户意图会越来越精准。具体来说，在对话过程中，智能客服会从用户的问题中，提取数百种特征，包括自然语言理解、意图识别、用户画像、渠道特征、环境特征、机器人特征、行业特征、会话特征等八种以上的特征集合，随着用户、行业、对话的不断增加，模型的特征维度也会水涨船高，高维度的对话模型可实现不同领域、不同用户需求的个性化问答。 据介绍，除了提供通用的服务，在各垂直领域的应用方面，第四范式综合使用了包括深度学习技术在内的多种技术，用海量语料训练不同领域的特定模型，保证在垂直领域的模型效果。此前已经在金融、文化、法律等领域有所积累，例如第四范式的智能客服加入了神经网络翻译模型与统计机器翻译模型，实现了自动对春联的能力。两种模型的深度融合，不仅充分发挥了神经网络强大拟合能力，又弥补了生成式模型效果不稳定的缺陷。 此外，在面对智能客服领域棘手的「冷启动」（没有语料或标注语料少时如何训练模型）问题时，第四范式研发了可定制模拟器，针对不同厂商和领域定制，生成常规语料，随后训练模型。模型在使用过程中会根据用户数据的增多自主更新，直到模型成熟可用。除此之外，第四范式还引入了迁移学习来降低人工标注数据的成本，以解决标注语料少的问题。 智能客服平台在接入方式上，可通过 API 的形式快速接入 APP、Web 端、公众号、微博等多个渠道。以微信公众号的接入为例，用户首先登录「智能客服」进行注册（平台链接：https://bot.4paradigm.com），随后导入功能性的知识规则，再通过公众号管理员扫码授权，以上三步即可完成智能客服平台的接入，使用平台上的各项服务。 第四范式方面表示，这套智能客服产品是基于「第四范式先知」打造，通过免费的技术平台的形式，最大限度地降低企业应用智能客服的成本与门槛，以快速推广、形成规模效应；同时，希望吸引更多不同领域的开发者共同参与，在本行业利用 AI 提供更为专业的智能客服服务。目前，已经有超过二十家企业正在试用智能客服平台，涵盖汽车、保险、银行、医疗、电商、教育、法律、O2O 等领域。 
128,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739284&idx=1&sn=7a8c18a9113eaa492b311886872ec801&chksm=871ad62ab06d5f3c40919e8b2f78d34c574e442b760b7795eef0638a419a4b5be66991676401&scene=27,中到英新闻翻译媲美人类，微软机器翻译新突破,今日，微软研究团队表示，微软和微软亚研创造了首个在质量与准确率上匹配人类水平的中英新闻机器翻译系统。黄学东告诉机器之心，他们采用专业人类标注与盲测评分代替 BLEU 分值而具有更高的准确性，且新系统相比于现存的机器翻译系统有非常大的提升。因此，根据人类盲测评分，微软机器翻译取得了至少和专业翻译人员相媲美的效果。 微软亚洲与美国实验室的研究者称，其中英新闻机器翻译系统在常用的新闻报道测试集 newstest 2017 上达到了人类水平。该测试集由来自业界和学界的团队共同开发，去年秋季在 WMT17 会议上发布。为了保证结果既准确又能达到人类水平，该团队聘请了外部双语评估员，他们对比了微软的结果与两组独立翻译的人类译文。 微软语音、自然语言与机器翻译的技术负责人黄学东称之为自然语言处理最具挑战性任务中的重要里程碑。他对机器之心说：「我们的新系统相比之前的翻译系统有非常大的提升，因此它确实是一个重大突破，是一个历史性的里程碑。」 「机器翻译达到人类水平是我们所有人的梦想，」黄说道，「我们只是没想到这么快就实现了。」 黄学东也领导了最近在对话语音识别任务中达到人类水平的研究组，他认为取得机器翻译任务的这一里程碑尤其令人高兴，因为它可以帮助人们更好地理解彼此。 黄学东说：「消除语言障碍，帮助人们更流畅地交流，这真的非常了不起，非常非常有价值。」 机器翻译是研究者研究了数十年的问题，专家称，很长时间以来人们都认为机器翻译无法达到人类水平。研究者现在仍应该注意该里程碑时间并不意味着机器翻译问题已经被解决。 微软亚洲研究院副院长、自然语言处理组主任周明参与了该项目，称研究团队非常激动能够在该数据集上达到与人类匹配的机器翻译水平。但是他提到，目前仍然存在很多挑战，如还需要在实时新闻报道上对该系统进行测试。 微软机器翻译团队研究经理 Arul Menezes 称，他们团队计划在测试集上证明该系统在中英语言对上可以达到与人类匹配的水平（中英语言对数据较多），此外，测试集还包括大众新闻报道中更常见的词汇。 「考虑到目前数据和可用资源所能达到的最佳案例，我们想找出该系统是否能够实际匹配人类专业译者的水平。」Menezes 称，他也主导了该项目。 Menezes 称研究团队可以将这一技术突破应用于微软的多语商用翻译产品。这将为更准确、自然流畅的跨语言翻译和更复杂或罕见的词汇翻译铺平道路。 对偶学习、推敲、联合训练和一致性正则化 尽管学术和业界的研究者多年来一直在研究机器翻译，但近期使用深度神经网络训练 AI 系统的方法取得了实质性的突破。这些机器翻译系统能够输出更流畅、自然的译文，且比以前的统计机器翻译方法有更广阔的适用范围。 为了在该数据集上训练出能达到人类水平的翻译系统，位于北京、雷德蒙德的三个微软研究团队通力合作，增加了许多其它训练方法帮助系统更加流畅和准确。在许多情况下，这些新方法模拟人类改进翻译工作的过程，一遍遍地迭代直到实现正确结果。 微软亚研首席研究员刘铁岩领导了该项目的机器学习团队，他表示：「我们大部分研究都受到人类工作方式的启发。」 他们使用的一种方法是对偶学习（dual learning）。我们可以把它看作一种核查系统工作的方法：每次他们向系统发送一个中译英的语句，然后再将英译文翻译成中文。这就好像人们想要确保自动翻译结果是准确的，这一方法允许系统从自身的错误中学习。微软研究团队研发的对偶学习也可用于提升其他 AI 任务的结果。 另一种方法称为推敲网络（deliberation network），它与人类经常通读全文来编辑和修改译文的过程非常相似。研究人员会教系统重复翻译相同语句的过程，并逐步润色和提升译文效果。 研究者同样开发了两种新技术以提升其翻译准确率，周明说。一项叫作联合训练（joint training）的技术可用于迭代地提升英中、中英翻译系统。通过这一方法，英中翻译系统把新的英语语句翻译成中文，从而获得新的句对，用于增强中英翻译训练集。相同操作接着再用于中英翻译系统。随着不断收敛，两个系统的表现都获得了提升。 另一项技术是一致性正则化（agreement regularization）。有了它，系统通过从左到右或从右到左的读取即可生成翻译。如果这两个翻译技术生成了相同的翻译，则结果相比没有获得相同翻译更加值得信任。该方法用于鼓励系统生成一致的翻译结果。 周明称他希望这些方法和技术也对其他语言的机器翻译提升有所帮助，并带来翻译领域之外的 AI 突破。 「这些有助于机器翻译的方法和技术也可应用于整个 AI 研究领域」他说。 没有「正确」答案 该研究团队使用的测试集包含了一个在线新闻样本的 2000 个语句，同时该测试集也被专业译者翻译过。 微软在该测试集上进行了多轮评估，每次随机选取数百个译文。为了验证微软的机器翻译水平和人类相当，该公司在该测试集的评估规范之外，还聘请了外部双语语言顾问来对比微软和人类译者的翻译结果。 验证结果的方法也表明训练准确的机器翻译系统的复杂性。在其他任务中，例如语音识别，判断系统的表现是否和人类一样好是很直接的，因为理想结果对于人类和机器来说都是一样的。研究者称其为模式识别任务。 而在翻译任务中存在很多微妙差别。即使两个文笔流畅的人类译者对同一句话的译文也可能略有不同，并且二者都是正确的。这是因为一句话的正确译文并不是唯一的。 「机器翻译相比模式识别任务要复杂得多，」周说。「人们可以使用不同的词来描述同样的东西，你未必能够指出哪一个更好。」 研究者称正是这种复杂性使得机器翻译尤其困难，也正是这一点让它变得如此有趣。 刘说没有人知道机器翻译是否有朝一日能将任何语言文本翻译得足够好，在准确性和抒情性方面都能和人类译者相当。但是，他说，近期的这些研究突破将使他们迈向下一个长期计划，向这个目标和其它伟大的 AI 成就前进，例如在语音转语音翻译中达到人类水平。 「我们可以预测，我们一定能做得越来越好。」刘说。 摘要：机器翻译近年来发展迅速，现在数百万人使用在线翻译系统和移动 app 进行跨语言沟通。那么我们自然会想到这个问题：机器翻译系统能否接近或达到人类翻译水平。本论文中，我们首次解决了如何定义和准确评估机器翻译是否与人类翻译水平相当的问题。我们介绍了微软的机器翻译系统，并在广泛使用的 WMT 2017 中英新闻翻译任务上对该系统的译文质量进行了评估。评估结果表明我们最新的神经机器翻译系统实现了新的当前最优结果，译文质量与人类专业译者水平相当。我们还发现它显著优于众包业余译者的译文质量。 直观来看，我们将与人类翻译水平相当定义为： 1. 如果一个具备双语能力的人判断人类输出的译文质量与机器输出的译文质量相当，则机器达到人类水平。 2. 如果机器翻译系统在测试集上的译文质量评分（人工评分）与人类译文得分没有显著差别，则机器达到人类水平。 微软选择了第二个定义来鉴定机器翻译是否达到了人类翻译水平，这相对而言比较公平且比较有实际意义。给定可靠的翻译质量评分指标，基于人类直接测评的方式，我们可以使用成对统计显著性检验来决定机器翻译系统在测试集上是否达到了人类翻译水平。 现有的多种机器翻译评测方法通常基于参考译文，可能会出现偏差，因此微软采用了 WMT17 [6] 使用的直接评估方法作为人工评分方法。为了避免人工评分过程中出现偏差，微软和 IWSLT17 [7] 一样使用了基于来源（source-based）的评价方法。 表 1 第一部分展示了基线模型的结果。首先，我们对比了 WMT 2017 最佳结果搜狗系统 [42]。尽管搜狗系统是多个系统的集成，我们这里仍把它作为对照。该表中的其他系统都是单个系统。我们的基线系统 Base 在 1800 万句子上训练。BT 在基线模型的基础上添加了回译数据。 选择数据的实验结果 Base8K 使用基线数据和回译数据，但是它使用的模型架构较大，处理大型数据集的效果更好。 组合系统的实验结果 如表 3 所示，结合一组异构系统可以互补，实现更好的结果。我们对许多组合系统的配置与特征进行了实验，发现最有帮助的评分特征为 SY SScore、LMScore、R2Lscore、R2LSV 和 E2ZSV。这是非常令人惊奇的，因为组合系统关注于建模相似的特征。这可能是由于这些模型学习互补特征，它们有额外的能力相互补充。 5 人类评估结果 表 4 展示了我们的大规模人类评估结果。基于这些结果，我们认为，根据定义 2，我们在新闻领域中英翻译方面已经达到了人类水平，因为我们的系统结果和人类译文无显著差别。 上表中，根据 p-level（p ≤ 0.05）的 Wilcoxon 秩和检测（和 WMT17 一样），更高层集群的系统显著优于更低集群的系统。相同集群中的系统通过 z 分数进行排序（z 分数即围绕平均值的标准差），z 分数在标注者级别上进行计算，以避免不同标注行为的影响，同时保证质量。 6 人类分析 表 7 展示了标注出的错误的分布，即包含特定错误类别的句子片段所占比例。 
129,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739176&idx=5&sn=5baa57546ed326269a577b74a97f8750&chksm=871ad596b06d5c807d78571197da6c2c889297ea3f59784d3a472f6aa09c575f68998103ae8c&scene=27,CVPR 2018 | 新研究提出深度残差等价映射：由正脸加强侧脸识别效果,Kaidi Cao等 由于类别样本不均衡，人脸检测只在正脸识别上有优秀的表现，它们很难识别侧脸样本。近日，香港中文大学和商汤科技等研究者提出了一种在深度表示空间中通过等变映射在正脸和侧脸间建立联系的方法，该方法的计算开销较少，但可以大大提升侧脸识别效果。 引言 深度学习的出现大大推动了人脸识别的发展。而人脸识别的焦点倾向于以正脸附近为中心，然而在不受限的环境中进行人脸识别，并不能保证其结果。尽管人类从正面识别侧面的表现只比从正面识别正面的表现差一点，可现存的算法在处理类似问题时准确率会下降 10% 以上。因此，姿势的变化仍旧是人脸识别应用在现实世界的重大挑战。 我们在图 1 中展示了最先进的人脸识别模型的错误模式。我们训练了与 [34] 中提到的一样的模型——ResNet-18 模型。这个模型在 LFW 基准数据集中的准确率高达 99.3%。尽管该模型很强大，但它还是会误匹配不同人的正脸和侧脸从而得到一些假正类结果。此外，该模型还会错配相同个体的正脸和侧脸，从而导致假负类结果。 为什么人脸识别技术在侧脸上表现欠佳？深度学习系统很大程度上依靠数据驱动。一般而言，模型的泛化能力和数据量成正比。模型训练使用的数据集正脸和侧脸数据量不均衡，因此训练出的模型更擅长分辨正面。目前我们还没有涵盖人类所有姿势且分布均匀的数据集，因此研究人员要用其他方法解决侧脸识别问题。很多方法在识别前尽可能细致地描述面部 3D 结构，使其归一化为只含正面的图像；或采用另一个深度模型（或生成对抗网络）将人脸转正。这些方法会给整个系统增加负担。此外，面部图像，尤其是极侧面的图像很难转化为自然状态。一般而言，合成的正面图片有人工造成的遮挡或非严格表情，它们都会影响模型的性能。我们还可以采用分而治之的方法，也就是说，用独立的模型来学习特定姿态的一致性特征 [19]。但这些策略因为使用多个模型而会增加计算成本。 本研究假设在深度特征空间中，侧脸区域和正脸区域有关联。图 2 表明同一主体不同姿势的面部的深度表示。输入任意姿势的图像，我们可以将其特征通过添加的残差映射函数映射到正脸的特征空间上。这一理论和特征等变性的概念很接近，通过特征等变性发现，可以通过转换输入图得到深度学习层的表示。有趣的是，这样的转换可以通过基于数据的映射函数学习到，而映射函数之后还可以应用于控制输入图的表示，以达到想要的转换。 我们由此受到启发，开发了深度残差等变映射（DREAM）模块。该模块可以在高层深度特征空间中将正脸和侧脸进行转换。该模块自适应地将残差添加到输入表示中，将侧脸转换为标准姿势使识别变得更为简单。为了适应任意姿势的面部输入，我们引入软门控机制（soft gate）自适应地控制残差量，这样在输入正面姿势的情况下，为保结果不变，会给极侧的姿势加入更多残差。 从概念上讲，我们的这项工作与脸部正向化（frontalization）有关，因此我们的方法也可应用于除图像空间的正向化的其他问题。我们从实验中观察到：从侧脸特征向正脸特征转化比图像级的正向化效果更好，也就是说，在图像合成问题上该方法对负影响更为敏感。据我们所知，我们所做的这项研究，是第一次尝试在深度特征空间进行侧面到正面的转换。 DREAM 模块的吸引力在以下方面： 该模块实施简单。具体来说，DREAM 模块是一个简单有效的门控残差分支。它可以通过将这个模块拼接到基础网络，集成到现有的卷积神经网络架构中，无需改变面部表示的原始维度，还可以用标准反向传播进行端到端的训练。 该模块权重较轻，它在基础的模型上添加的参数很少，因此无须太多计算资源。以 ResNet-18 为例，该模块只增加了 0.3% 的参数和 1.6% 的时间成本。 基础网络识别近正面图片效果很好，且该模块能帮助基础网络在识别极端姿势的面部时取得更好的表现。该方法并不需要更详细的数据，面部数据的标准化也是以大多数现有的针对面部识别的研究为基础实现的。 深度残差等变映射 我们描述了三种使用 DREAM 的方法。三种实验的方法比较将在后面的实验部分提出。 拼接。部署 DREAM 模块最方便的办法是直接将模块「拼接」到训练好的 CNN 中。特别是在给定基本网络架构的时候，我们可以在不改变任何原始模型学习参数的情况下，将 DREAM 模块拼在基础网络最后的特征层中。   端到端。我们提出的这个轻权重的模块也可以端到端的方式和主干 CNN 一起训练。给定一个简单的基础网络，我们将 DREAM 模块插入该网络，并直接用随机初始参数训练新网络。如果这个 CNN 不够简单或是之前训练过，我们可以使用现有的面部识别损失（例如，验证损失（verification）、识别损失（identification）等）用端到端的方法训练 DREAM 模块时对 CNN 进行微调。我们将这种策略命名为「end2end」。使用这种策略，模型在侧脸识别方面的表现无法保证，因为 DREAM 模块可能无法分辨正面和侧面，原因是在模块训练过程中没有具体地将一张脸的正面和侧面配对。 端到端+重新训练。我们先将 CNN 和 DREAM 同时训练，再用成对的正面侧面的数据对 DREAM 模块进行有针对性地训练。 论文：Pose-Robust Face Recognition via Deep Residual Equivariant Mapping 论文链接：https://arxiv.org/abs/1803.00839   摘要： 深度学习的发展使人脸识别取得了非凡的成就。然而，现在的许多人脸识别模型识别侧面的性能，尤其相较于识别正面，表现仍有不足。主要原因之一在于训练数据中正面数据和侧面数据分布不均匀——正脸数据比侧脸数据要多得多。此外，由于存在姿态的大范围变化，几何学意义上的不可变也是模型学习深度特征表示的难点之一。在本研究中，我们假设在正脸和侧脸间存在固有的映射关系，因此，在深度表示空间中可以通过等变映射在正脸和侧脸间建立联系。在构建映射的过程中，我们建立了新的深度残差等变映射（DREAM）模块，该模块可以自适应地在输入深度特征表示中添加残差连接，使侧脸表示转换为标准姿势，以简化识别。对许多强大的深度网络而言，包括 ResNet 模型，DREAM 模块在无需增强数据中侧脸部分的情况下，大大增强了模型在侧脸识别方面的表现。该模块易于使用，而且运行中计算开销较少。 
130,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739176&idx=2&sn=0f7964fbf321bf6e1774f4d0e4e3c784&chksm=871ad596b06d5c80827e8f1093301c374c5133e7aef51c4be55188614fa8929e66dc713e5b68&scene=27,业界 | 谷歌最新语义图像分割模型DeepLab-v3+今日开源,"Liang-Chieh Chen、Yukun Zhu 刚刚，谷歌开源了语义图像分割模型 DeepLab-v3+，DeepLab-v3+结合了空间金字塔池化模块和编码器-解码器结构的优势，是自三年前的 DeepLab 以来的最新、性能最优的版本。 GitHub 地址：https://github.com/tensorflow/models/tree/master/research/deeplab 语义图像分割任务是指将语义标签（例如「道路」、「天空」、「人」、「狗」）分配给图像中的每一个像素，这种技术有很多新应用，例如，Pixel 2 和 Pixel 2 XL 智能手机中肖像模式的合成浅景深效应，以及移动设备的实时语义分割等。分配这些语义标签的时候需要精准定位目标的轮廓，因此相比其他的视觉实体识别任务（如图像级分类或边界框级检测等），该任务需要更高的定位准确率。 今天，谷歌开源了其最新、性能最优的语义图像分割模型 DeepLab-v3+ [1]，该模型使用 TensorFlow 实现。DeepLab-v3+ 模型建立在一种强大的卷积神经网络主干架构上 [2,3]，以得到最准确的结果，该模型适用于服务器端的部署。此外，谷歌还分享了他们的 TensorFlow 模型训练和评估代码，以及在 Pascal VOC 2012 和 Cityscapes 基准语义分割任务上预训练的模型。 自三年前谷歌发布第一个版本的 DeepLab 模型 [4] 以来，CNN 特征提取器、目标尺度建模技术、语境信息处理、模型训练流程、深度学习硬件和软件的不断改进和优化，促使该模型升级到了 DeepLab-v2 [5] 和 DeepLab-v3 [6]。谷歌通过添加一个简单而有效的解码器模块以精炼分割结果（尤其是在目标边界处），将 DeepLab-v3 扩展为 DeepLab-v3+。他们还进一步将深度可分卷积（depthwise separable convolution）应用到金字塔型的空洞池化（Atrous Spatial Pyramid Pooling，ASPP）[5, 6] 和解码器模块上，以得到更快更强大的语义分割编码器-解码器网络。 现代语义图像分割系统都是建立在卷积神经网络之上，并达到了五年前无法想象的准确率，这得归功于方法、硬件和数据集的优化。谷歌希望通过和社区共享该系统，学界和业界能更容易地复现和提升当前最优系统，在新的数据集上训练模型，以及为该技术开发新的应用。 论文：Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 论文链接：https://arxiv.org/abs/1802.02611 摘要： 深度神经网络使用空间金字塔池化模块或编码器-解码器结构执行语义分割任务。前者通过在多个 rate、多个有效视野上用滤波器探测输入特征或执行池化操作，来编码多尺度的上下文信息；后者通过逐渐恢复空间信息来捕捉更加精细的目标边界。在这项研究中，我们将二者的优势结合起来。具体来说，我们通过添加一个简单有效的解码器模块以精炼分割结果（尤其是目标边界），将 DeepLab-v3 扩展为本文提出的新模型 DeepLab-v3+。我们进一步探索了 Xception 模型，并将深度可分卷积应用到金字塔型的空洞池化（ASPP）和解码器模块上，以得到更快更强大的编码器-解码器网络。我们在 PASCAL VOC 2012 语义图像分割数据集上证明了该模型的有效性，在没有任何后处理的情况下该模型达到了 89% 的准确率。 参考阅读： 原文链接：https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html "
131,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739176&idx=1&sn=46c4593f55cf294a5ff8cbf0b37011b3&chksm=871ad596b06d5c8070d4101159d6f7a7e89abe02845dc247f76b7c45564d3fc215c00d72f681&scene=27,PyTorch为何如此高效好用？来探寻深度学习框架的内部架构,Christian S. Perone 作为 Facebook 人工智能团队（FAIR）提供支持的深度学习框架，PyTorch 自 2017 年 1 月推出以来立即成为了一种流行开发工具。其在调试、编译等方面的优势使其受到了学界研究者们的普遍欢迎。本文中，来自蒙特利尔综合理工学院的研究员 Christian S. Perone 将为我们介绍这种神经网络框架的内部架构，揭开 PyTorch 方便好用的真正原因。 前言 本文主要介绍了 PyTorch 代码库，旨在为 PyTorch 及其内部架构设计提供指导，核心目标是为那些想了解 API 知识之外的人提供有益的帮助，并给出之前教程所没有的新内容。注意：PyTorch 构建系统需要大量使用代码设置，因此其他人的描述我将不再重复。如果你感兴趣，请参考原文提供的扩展资料。 C/C++中 Python 扩展对象的简介 你可能知道可以借助 C/C++扩展 Python，并开发所谓的「扩展」。PyTorch 的所有繁重工作由 C/C++实现，而不是纯 Python。为了定义 C/C++中一个新的 Python 对象类型，你需要定义如下实例的一个类似结构： 如上，在定义的开始有一个称之为 PyObject_HEAD 的宏，其目标是标准化 Python 对象，并扩展至另一个结构，该结构包含一个指向类型对象的指针，以及一个带有引用计数的字段。 Python API 中有两个额外的宏，分别称为 Py_INCREF() 和 Py_DECREF()，可用于增加和减少 Python 对象的引用计数。多实体可以借用或拥有其他对象的引用（因此引用计数被增加），而只有当引用计数达到零，Python 才会自动删除那个对象的内存。想了解更多有关 Python C/++扩展的知识，请参见：https://docs.python.org/3/extending/newtypes.html。 有趣的事实：使用小的整数作为索引、计数等在很多应用中非常见。为了提高效率，官方 CPython 解释器缓存从-5 到 256 的整数。正由于此，声明 a = 200; b = 200; a is b 为真，而声明 a = 300; b = 300; a is b 为假。 Zero-copy PyTorch 张量到 Numpy，反之亦然 PyTorch 有专属的张量表征，分离 PyTorch 的内部表征和外部表征。但是，由于 Numpy 数组的使用非常普遍，尤其是当数据加载源不同时，我们确实需要在 Numpy 和 PyTorch 张量之间做转换。正由于此，PyTorch 给出了两个方法（from_numpy() 和 numpy()），从而把 Numpy 数组转化为 PyTorch 数组，反之亦然。如果我们查看把 Numpy 数组转化为 PyTorch 张量的调用代码，就可以获得有关 PyTorch 内部表征的更多洞见： 正如你在这段代码中看到的，PyTorch 从 Numpy 表征中获取所有信息（数组元数据），并创建自己的张量。但是，正如你从被标注的第 18 行所看到的，PyTorch 保留一个指向内部 Numpy 数组原始数据的指针，而不是复制它。这意味着 PyTorch 将拥有这一数据，并与 Numpy 数组对象共享同一内存区域。 还有一点很重要：当 Numpy 数组对象越出范围并获得零引用（zero reference）计数，它将被当作垃圾回收并销毁，这就是为什么 Numpy 数组对象的引用计数在第 20 行有增加。该行之后，PyTorch 将从这一 Numpy 数据 blob 中创建一个新的张量对象，并且在创建这一新张量的过程中，PyTorch 将会传递内存数据指针，连同内存大小、步幅以及稍后张量存储将会使用的函数（我们将会在下节讨论），从而通过减少 Numpy 数组对象的引用计数并使 Python 关心这一对象内存管理而释放数据。 tensorFromBlob() 方法将创建一个新张量，但只有在为这一张量创建一个新「存储」之后。存储是指存储数据指针的地方，它并不在张量结构内部。张量存储正是我们下一节要讨论的内容。 张量存储 张量的实际原始数据并不是立即保存在张量结构中，而是保存在我们称之为「存储（Storage）」的地方，它是张量结构的一部分。 正如我们前面在 tensor_from_numpy() 中看到的代码，它调用了 tensorFromBlob() 函数以从原始数据 Blob 中创建一个张量。tensorFromBlob() 函数在内部会调用另一个名为 storageFromBlob() 函数，该函数主要根据类型为数据创建一个存储。例如在 CPU 浮点型的情况下，它会返回一个新的 CPUFloatStorage 实例。 CPUFloatStorage 基本上是包含 utility 函数的包装类（wrapper），且实际存储结构如下所示称为 THFloatStorage： 如上所示，THStorage 有一个指向原始数据、原始数据大小、flags 和 allocator 的指针，我们会在后面详细地讨论它们。值得注意的是，THStorage 不包含如何解释内部数据的元数据，这是因为存储对保存的内容「无处理信息的能力」，只有张量才知道如何「查看」数据。 因此，你可能已经意识到多个张量可以指向相同的存储，而仅仅对数据采用不同的解析。这也就是为什么我们以不同的形状或维度，查看相同元素数量的张量会有很高的效率。下面的 Python 代码表明，在改变张量的形状后，存储中的数据指针将得到共享。 如 THFloatStorage 结构中的第七行代码所示，它有一个指向 THAllocator 结构的指针。它因为给分配器（allocator）带来灵活性而显得十分重要，其中 allocator 可以用来分配存储数据。 如上所述，该结构有三个函数指针字段来定义分配器的意义：malloc、realloc 和 free。对于分配给 CPU 的内存，这些函数当然与传统的 malloc/realloc/free POSIX 函数相关。然而当我们希望分配存储给 GPU，我们最终会使用如 cudaMallocHost() 那样的 CUDA 分配器，我们可以在下面的 THCudaHostAllocator malloc 函数中看到这一点。 如上所示，分配器调用了一个 cudaMallocHost() 函数。你可能已经注意到版本库组织中有缩写的表示模式，在浏览版本库时记住这些约定非常重要，它们在 PyTorch README 文件中有所总结： TH = TorcH THC = TorcH Cuda THCS = TorcH Cuda Sparse THCUNN = TorcH CUda Neural Network THD = TorcH Distributed THNN = TorcH Neural Network THS = TorcH Sparse 该约定同样存在于函数/类别名和其它对象中，因此了解它们十分重要。你可以在 TH 代码中找到 CPU 分配器，在 THC 代码中找到 CUDA 分配器。最后，我们可以看到主张量 THTensor 结构的组成： ptrdiff_t 如上，THTensor 的主要结构为张量数据保留了 size/strides/dimensions/offsets/等，同时还有存储 THStorage。我们可以将所有这些结构总结为以下图表： 现在，如果我们有多重处理的需求，且希望在多个不同的进程中共享张量数据，那么我们需要一个共享内存的方法。否则每次另一个进程需要张量或我们希望实现 Hogwild 训练过程以将所有不同的进程写入相同的内存区域时，我们就需要在进程间创建副本，这是非常低效的。因此，我们将在下一节讨论共享内存的特定存储方法。 共享内存 共享内存可以用很多种不同的方法实现（依赖于支持的平台）。PyTorch 支持部分方法，但为了简单起见，我将讨论在 MacOS 上使用 CPU（而不是 GPU）的情况。由于 PyTorch 支持多种共享内存的方法，由于代码中包含很多级的间接性，这部分会有点困难。 PyTorch 为 Python multiprocessing 模块提供了一个封装器，可以从 torch.multiprocessing 导入。他们对该封装器中的实现做出了一些变动，以确保每当一个 Tensor 被放在队列上或和其它进程共享时，PyTorch 可以确保仅有一个句柄的共享内存会被共享，而不会共享 Tensor 的完整新副本。现在，很多人都不知道 PyTorch 中的 Tensor 方法是 share_memory_()，然而，该函数正好可以触发那个特定 Tensor 的保存内存的完整重建。该方法的执行过程是创建共享内存的一个区域，其可以在不同的进程中使用。最终，该函数可以调用以下的函数： ptrdiff_t 如上所示，该函数使用了一个特殊的分类器 THManagedSharedAllocator 来创建另一个存储。它首先定义了一些 flags，然后创建了一个格式为 /torch_ [process id] _ [random number] 的字符串句柄，最后在使用特殊的 THManagedSharedAllocator 创建新的存储。该分配器有一个指向 PyTorch 内部库 libshm 的函数指针，它将实现名为 Unix Domain Socket 的通信以共享特定 quyu 的内存句柄。这种分配器实际上是「smart allocator」的特例，因为它包含通信控制逻辑单元，并使用了另一个称之为 THRefcountedMapAllocator 的分配器，它将创建市级共享内存区域并调用 mmp() 以将该区域映射到进程虚拟地址空间。 现在我们可以通过手动交换共享内存句柄而将分配给另一个进程的张量分配给一个进程，如下为 Python 示例： 在这段代码中，执行进程 A，我们就创建了一个 5×5，被 1 所填充的张量。在此之后，我们将其共享，并打印 Unix Domain Socket 地址和句柄的元组。现在我们可以从另一个进程 B 中接入这一内存区域了： 进程 B 执行代码： 如你所见，使用 Unix Domain Socket 地址和句柄的元组信息，我们可以接入另一个进程的张量存储内容。如果你在进程 B 改变张量，你会看到改动也会反映在进程 A 中，因为张量之间共享着同样的存储区域。 DLPack：深度学习框架 Babel 的希望 现在让我们来看看 PyTorch 代码库最新的一些内容——DLPack（https://github.com/dmlc/dlpack）。DLPack 是一个内存张量结构的开放标准，允许张量数据在框架之间交换。非常有趣的是，这种内存表示是标准化的——与大多数框架已经在使用的内存表示方法非常类似，这就允许我们可以在框架之间共享，且完全无需复制数据。鉴于目前我们还没有内部通信的工具，DLPack 是一个非常了不起的创造。 它无疑会帮助我们解决今天存在于 MXNet、PyTorch 等框架上「孤岛」一样的张量表示，并允许开发者在多个深度学习框架之间自由操作，享受标准化为框架带来的优势。 DLPack 的核心结构 DLTensor 非常简单，如下所示： void 如你所见，这里有一个未加工数据的数据指针，以及形态/步幅/偏移/GPU 或 CPU，以及其他 DLTensor 指向的元信息。 这里还有一个被称为 DLManagedTensor 的受管理版本，其中框架可以提供一个环境，以及「删除」函数，后者可以从借用张量来通知其他框架不再需要资源。 在 PyTorch 中，如果你想要转换到 DLTensor 格式，或从 DLTensor 格式转换，你可以找到 C/C++的方法，甚至 Python 方法来做这件事： 这个 Python 函数会从 ATen 调用 toDLPack 函数，如下所示： const new 如上所示，这是一个非常简单的转换，它可以将元数据的 PyTorch 格式转换为 DLPack 格式，并将指针指向内部张量的数据表示。 我们都希望更多的深度学习框架可以学习这种标准，这会让整个生态系统受益。希望本文能够对你有所帮助。 原文链接：http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/ 
132,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739176&idx=4&sn=f87880320e6b7d92322cfc82a3ad750c&chksm=871ad596b06d5c807cc9cb65c64c7ee9537daff5b80b4de997f7a4aea03984eb74247805c850&scene=27,教程 | 从预处理到部署：如何使用Lore快速构建机器学习模型,"Montana Low 机器学习的构建和部署通常需要非常多的工作与努力，这对于软件开发者和入门者造成了很多困难。本文介绍了如何使用软件库 Lore 快速而高效地构建机器学习模型，并从数据预处理到模型部署等七个步骤介绍构建的经验。 一般问题 Python 或 SQL 等高级语言编写代码时，模型性能很容易出现瓶颈。 代码复杂性在增长，因为有价值的模型需要通过许多次迭代才能得到。当代码以非结构化的方式演化时，难以保证与传达最初的想法。 对数据和函数库的依赖不断变化，从而导致性能再现受到影响。 当人们试图理解最新的论文、软件包、特征和问题时，信息过载使得人们很容易错过唾手可得的成果，这一问题对于刚入行的人来说更为严重。 为了解决这些问题，我们标准化了 Lore 中的机器学习方法，并使用 Lore 开发新的机器学习模型。此外，我们 Instacart 也在产品中运行着十几个 Lore 模型。 如果您想在没有上下文的情况下看一下快速的演示，可以从 GitHub 上复制 my_app。如果您想看到完整工程介绍，请跳至大纲。 特征说明 了解该项目最好的方法是在十五分钟内开始我们自己的深度学习项目。如果你想在开始新项目前了解本文所述模型的特性，请参阅以下简要概述： 模型支持使用估计器搜索超参数，它们将采用几个不同的策略有效地利用多个 GPU（如果条件允许的话），因此可以分布式地搜索超参数分布。 支持使用多个软件库的估计器，包括 Keras、XGBoost 和 SciKit Learn 等。这些包都可以通过构建、拟合或预测进行分类，完整地覆盖了用户的算法和架构类型。 数据处理流程避免了信息在训练集和测试集间泄露的风险，且一条流程允许许多不同的估计器进行试验。如果您在实验过程中超出了可用 RAM，那么您可以使用基于磁盘处理流程。 转换标准化的高级特征工程。例如，使用美国人口普查数据可以将美国人的姓转换为年龄或性别的统计学特征；或是从任意格式的电话号字符串中提取地域编码。此外，pandas 包可以支持一般的数据、时间和字符串的转化操作。 编码器则为您的评估器提供鲁棒性的输入，并能避免常见的缺失和长尾问题。 对流行的（非）关系型数据库而言，IO 连接在应用程序中以一种标准的方式进行配置和汇集。这种方式对批量数据进行任务管理和读写优化，而非传统的 ORM 单行操作。IO 连接除了用加密的 S3 buckets 分配模型和数据集外，还共享了可配置的查询缓存。 对每个独立开发中的 APP 而言，依赖项管理都可以将对应包完整地复制到产品中去。因此我们无需了解 venv、pyenv、pyvenv、virtualenv、virtualenvwrapper、pipenv 和 conda 依赖项管理工具。 这样的工作流程可以让您选择是使用命令行、python 控制台、jupyter notebook 或者是其它 IDE。每个环境都可以在产品开发和配置过程中生成可读的日志记录。 15 分钟开启机器学习 您只需基础的 python 知识即可开始。如果您的模型没有开始进行学习，那您可以用省出来的时间继续探索机器学习的复杂性。 1. 创建一个新的 app（3 分钟） 2. 设计一个模型（1 分钟） 3. 生成架构（2 分钟） 4. 铺设流程（5 分钟） 5. 测试代码（1 分钟） 6. 训练模型（1 分钟） 7. 部署产品（2 分钟） 上述时间有些言过其实，只用作博客宣传。没有一个机器学习研究人员可以只用一分钟就设计出一个模型，但是一旦你开始跟着学，并且将过程中得到的一切都做上笔记，那么你也可以在 15 分钟内高效地构建一个自定义的 AI 项目，在你的朋友和同事中一鸣惊人。 1. 创建一个新的 APP Lore 独立管理每个项目，这是为了避免与系统中的 python 或其他项目发生冲突。我们可以将 Lore 作为一个标准的 pip 包安装： 当你无法复制他人环境时，是很难复现他人的工作。Lore 保护系统中 python 项目的方式可以避免依赖项错误和项目冲突。每一个 Lore 的应用程序都有自己的字典和安装目录，它特定需要的依赖库会锁定在文件 runtime.txt 和 requirements.txt 中。这使 Lore 的应用程序共享起来更加高效，也让我们离复现这个机器学习项目更近一步。 在安装 Lore 之后，我们可以在阅读本文后创建一个新的深度学习项目的 app。Lore 默认是模块化的，所以我们需要指定——keras 来为这个项目安装深度学习的依赖项。 2. 设计一个模型 为了演示，我们将建立一个预测模型，这个模型将基于产品名字及其所在部门，预测出 Instacart 网站的产品能有多流行。世界各地的制造商零售时会在不同的人群间测试产品名字，看什么样的名字能得到最多的关注，我们这个简单的 AI 项目也可以提供相同的服务。 机器学习中最难的一部分是获得良好的数据。幸运的是，Instacart 以匿名的方式公布了 300 万份杂货订单。基于此，我们可以将问题调整为建立一个有监督的回归模型，该模型可以基于两个特点预测年均销量：产品名称和产品类别。实际上，该模型的表现并不好，因此后文会继续讨论更加强大的模型。 3. 生成架构 每一个 Lore 模型都包含一条用于加载数据和编码数据的流程，还包含一个可以实现特定机器学习算法的估计器。模型最有趣的部分在于类别生成中的实现细节。 流程从左侧的原始数据开始，将原始数据编码为右侧所需格式。估计器可以用编码数据训练模型，并根据验证集的性能确定是否终止训练，最后再用测试集评估。所有内容都可以被序列化存在模型存储区，然后用一个单线程再次加载进行部署。 4. 铺设流程 得到很适合机器学习算法的原始数据是很难的。我们通常会从一个数据集中加载数据或是下载 CSV 文件，将其编码为适合算法的格式，然后再将其分割为训练集和测试集。lore.pipelines 将这一预处理逻辑封装起来，成为标准的工作流程。 lore.pipelines.holdout.Base 可以将数据分为训练集、验证集和测试集，并可以编码数据，使数据适用于我们的机器学习算法。我们的子类将定义三种方法：get_data、get_encoders 和 get_output_encoder。 Instacart 公布的数据分为了多个 csv 文件，如下表所示。 get_data 可以下载 Instacart 的原始数据，使用 pandas 可以将带有所需特征（product_name，department）的数据加入 DataFrame，还可将预测量（sales）合并在一起。如下所示： 下列代码展示了如何实现 get_data ： class Holdout (lore.pipelines.holdout.Base) def get_data (self) def read_csv (name) 接下来，我们需要为每一列指定一个编码器。计算机科学家可能认为编码器是使机器学习更高效的方法。一些产品的名字太长，所以我们将其名称限定为前 15 个单词。 def get_encoders (self) def get_output_encoder (self) 这就是预处理流程。我们起初用的评估器是 lore.estimators.keras.Regression 的一个简单子类，可以用默认值实现简单的深度学习架构。 class Keras (lore.estimators.keras.Regression) 最后，我们的模型可以通过将其委托给估计器来改变深度学习架构的高级属性，还可从我们构建的处理流程中提取数据。 class Keras (lore.models.keras.Base) def __init__ (self, pipeline=None, estimator=None) 5. 测试代码 当搭建架构时模型会自动运行通烟测试（smoke test），第一次运行时会花一些时间下载一个 200 MB 的数据集进行测试。您还可以对缓存在 ./tests/data 中的文件进行修改，将其移入回收站以加快测试速度。 6. 训练模型 训练模型需要 ./data 中的缓存数据，并将工作保存在 ./models 中。 查阅终端日志可以了解 Lore 的时间都花在了哪里。 尝试添加更多隐藏层以了解这一操作是否有助于提高您模型的性能。您可以编辑模型文件或者直接通过命令行调用合适的属性，如  --hidden_layers=5 。 检验模型特性 您可以在 lore 环境下运行 jupyter notebooks。Lore 将安装一个可自定义的 jupyter 内核，该内核将为 lore notebook 和 lore console 提供应用程序虚拟环境。 浏览 notebooks/product_popularity/features.ipynb 或「运行所有」可以了解您模型的可视化分析。 「生产」类被编码到「20」，这是很大的销售量了。 汇总特定特征时您就可以了解到模型的预测结果（蓝色）和测试结果（黄色）有多一致。在本例中，有 21 类重合程度相当高。「生产」类是例外，模型没有充分说明其逸出值。 您还可以通过运行 notebooks/product_popularity/architecture.ipynb 中的笔记了解生成的深层学习架构。 缩减为 15 个字符的名字通过左边的 LSTM 运行，类名输入到右边的嵌入中，然后一起通过隐藏层。 发布模型服务 Lore 的应用程序可以作为 HTTP API 在本地运行。默认情况下模型会通过 HTTP GET 端点公开其「预测」方法。 结果表明，在「香蕉」中加入「有机」，将会在「生产」类卖出超过两倍的水果。预测得出，「绿香蕉」比「棕香蕉」销量更差。如果您需要果汁的记录，则应该选择「有机黄香蕉」。 7. 部署产品 Lore 的应用程序可以通过任何支持 Heroku buildpack 的基础架构进行部署。Buildpacks 将 runtime.txt 和 requirements.txt 的依赖项在容器中安装以供模型部署。 您可以在 ./models/my_app.models.product_popularity/Keras/ 中了解每一次运行 lore fit 指令的结果。这个字典和 ./data/ 都默认在 .gitignore 中，您的代码随时可以重建路径。您可以在发布前检查要发布的模型版本，这是一个简单的部署策略： Heroku 使发布一个应用程序变得非常简单，您可以点击链接浏览其入门介绍：https://devcenter.heroku.com/articles/getting-started-with-python#introduction。 下面是「太长不看」版： 现在您可以用 heroku 应用的名字替代 http://localhost:5000/，并在任何地方获取预测结果。 "
133,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739143&idx=4&sn=5ef22bfb0cb077b092288e17a2d6b9ce&chksm=871ad5b9b06d5caf0bfe6dbfa21d5c0acd7bbd44668a84924f6a649b7a61e8d8055e8f637424&scene=27,CVPR 2018 | 优于Mask R-CNN，港中文&腾讯优图提出PANet实例分割框架,"近日，港中文联合腾讯优图提出了新型实例分割框架 PANet，该网络通过加速信息流和整合不同层级的特征，可以极大提高生成预测掩码的质量。PANet 在 COCO 2017 挑战赛的实例分割任务中取得了第一名，优于  COCO 2016 实例分割挑战赛冠军和  Mask R-CNN。 实例分割是最重要、最具挑战性的任务之一。该任务的目的是预测类别标签和像素级实例掩码以定位图像中不同数量的实例。实例分割对自驾汽车、机器人、视频监控等很有用。 由于深度卷积神经网络的助力，人们提出了多种实例分割的框架 [21,33,3,38]，其性能也提升得很快 [12]。Mask R-CNN [21] 是一个很简单有效的实例分割框架。基于 Fast/Faster R-CNN [16,51]，研究者还提出了全卷积网络（FCN），FCN 结合边框回归和分类网络用于预测掩码。为了获得足够高的分割性能，研究者利用特征金字塔网络（FPN）[35] 来提取网络内部的特征层级，其中增强了自上而下的路径（具备侧向连接）以传播语义较强的特征。 近期发布的新数据集 [37,7,45] 为算法提供了很大的提升空间。COCO [37] 由 20 万张图像构成，每张图像都包含了空间布局复杂的多个实例。与此不同，Cityscapes [7] 和 MVD [45] 的每张图像都是包含大量交通参与者的街景，因此这些数据集会出现很多模糊的、高度遮挡以及非常小的实例。 研究者提出了多个原则，用于设计既可以执行图像分类又可以执行目标识别的网络。例如，通过残差连接 [23,24] 和密集连接 [26] 缩短信息路径，使信息更有效地传播。此外，通过分离-转换-融合策略创建并行路径以增加信息路径的灵活性和多样性也大有裨益 [61, 6]。 研究发现 本文研究者指出当前最优的 Mask R-CNN 中的信息传播还可以进一步优化。具体来说，低层级的特征对于大型实例识别很有用。但最高层级特征和较低层级特征之间的路径很长，增加了访问准确定位信息的难度。此外，每个建议区域都是基于从一个特征层级池化得到的特征网格而预测的，此分配是启发式的。由于其它层级的丢弃信息可能对于最终的预测还有用，这个流程还有进一步优化的空间。最后，掩码预测仅在单个视野上执行，无法获得更加多样化的信息。 贡献 受到这些理论和观察的启发，本文作者提出了新的实例分割框架 PANet，如图 1 所示。 首先，为了缩短信息路径和用低层级的准确定位信息增强特征金字塔，作者创建了自下而上的路径增强。实际上，文献 [44,42,13,46,35,5,31,14] 中的系统就使用了低层级的特征。而传播低层级特征来增强整个特征分层，从而提升实例分割质量的方向，尚未有人进行探索。 第二，为了恢复每个建议区域和所有特征层级之间被破坏的信息，作者开发了适应性特征池化（adaptive feature pooling）技术。这是一个简单的组件，可以将所有特征层级中的特征整合到每个建议区域中，避免了任意分配的结果。通过该操作，研究者创建了更简洁的路径（与 [4,62] 相比）。 最后，为了捕捉每个建议区域的不同视野，研究者使用小型全连接层来增强掩码预测，作为对 Mask R-CNN 所用的 FCN 的补充。通过结合这两种视野的预测结果，网络输出的信息多样性有所改善，掩码质量有所提升。 目标检测和实例分割共享前两个组件，这使得二者性能均有明显提升。 实验结果  PANet 在多个数据集上达到了顶尖的性能。研究者使用 ResNet-50 [23] 作为 PANet 的初始网络，使用单个尺度进行测试，其性能优于 COCO 2016 挑战赛目标检测和实例分割任务冠军。注意：之前的结果均由具备多尺度和水平翻转测试的大型模型获取。 本研究提出的模型在未经大批量训练的情况下，取得了 COCO 2017 挑战赛实例分割任务第一名、目标检测任务的第二名。研究者还在 Cityscapes 和 MVD 上对该系统进行了基准测试，同样获得了顶尖结果，这表明 PANet 是一个非常实际、且性能优秀的框架。之后研究者将公开代码和模型。   论文：Path Aggregation Network for Instance Segmentation 论文链接：https://arxiv.org/abs/1803.01534 神经网络中的信息传播方式是非常重要的因素。本论文提出了 Path Aggregation Network（PANet），该网络可以对基于区域建议的实例分割框架中的信息流进行加速。具体来说，我们通过自下而上的路径增强，在较底层用准确的定位信号增强了整个特征分层，从而缩短了较底层和最高层特征之间的信息路径。我们展示了适应性特征池化（adaptive feature pooling）操作，将特征网格和所有特征层级连接起来，以使每个特征层级中的有用信息能直接传播到随后的建议子网络。我们还创建了一个互补的分支网络为每个建议捕捉不同的视野，从而进一步提升生成掩码预测的质量。这些提升都能够很简单地实现，只需要少量的额外计算量。PANet 在 COCO 2017 挑战赛的实例分割任务中取得了第一名，在目标检测任务中取得了第二名，且无需大批量训练。它在 MVD 和 Cityscapes 上也取得了当前最佳的结果。 "
134,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739143&idx=5&sn=12e04fd70efff11a3e683eac7ae67981&chksm=871ad5b9b06d5caf1fa500abc93753c1c1575c22414fccb549fc64027b0abb10bb7c4e1cb8cf&scene=27,活动 | 今日开售的搜狗旅行翻译宝，你想近距离体验吗？,3 月 12 日，搜狗旅行翻译宝在京东平台首发，售价 1498 元。据了解，搜狗旅行翻译宝结合了搜狗神经网络机器翻译、语音识别、图像识别等多项技术，不仅支持语音、图像翻译等多种翻译模式，还提供中英日韩俄德等 18 种语言互译。 对搜狗的这款旅行翻译宝，你是不是充满好奇？想要了解其背后的语音翻译技术，体验这款产品？ 在 3 月 17 日的 INTERFACE 线下分享中，搜狗语音交互技术中心研发总监陈伟、搜狗 IOT 事业部产品负责人李健涛，将从技术到产品为我们做解读。 分享主题一：搜狗语音翻译技术是怎样炼成的？ 陈伟：搜狗语音交互技术中心研发总监，语音技术负责人，负责搜狗语音识别、语音合成、机器翻译、声纹识别、手写识别等多项技术的研发工作，同时负责搜狗知音引擎语音技术的研发，致力于通过技术和产品的创新提升语音交互品质，为用户提供优质的语音使用体验。 分享主题二：搜狗旅行翻译宝产品研发理念   李健涛：搜狗 IOT 事业部产品负责人，负责搜狗旅行翻译宝、速记翻译笔、糖猫儿童智能手表等智能硬件产品研发工作。他在 2005 年加入搜狐，负责博客开发，经历了社交网络大爆炸、移动互联网浪潮，又投身智能硬件领域，致力于通过产品创新提升智能硬件产品使用体验，为用户提供优质的服务。 日期 ：2018 年 03 月 17 日 13:00-14:00 签到、产品体验 14:00-14：30 陈伟分享 14：30-15:00 李健涛分享  15:00-15:30 现场提问 15:30-16:00 现场交流、产品体验 地址 ：北京市海淀区中关村东路搜狐网络大厦 8 层四海会议室 参与方式 
135,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739143&idx=3&sn=43d14af8667d8bfa1f6827952e97b195&chksm=871ad5b9b06d5caf75a60876450677811e80c7f44345254de13f15ecf4baa45f2cea90e492fa&scene=27,教程 | 如何通过PyTorch上手Tensor Comprehensions？,"pytorch Tensor Comprehensions 是一个降低高性能代码编写门槛的工具，可以将高级语言代码直接生成 GPU 代码并针对特定输入自动调整代码。本文作者将展示 TC 如何在 PyTorch 中无缝使用，并与 PyTorch Tensors 和 nn Variables 进行互操作。 Tensor Comprehensions (TC) 是一个降低高性能代码编写门槛的工具，可以将高级语言代码直接生成 GPU 代码并针对特定输入自动调整代码。本文强烈推荐你首先阅读 Tensor Comprehensions 发布博文（详见  ）。 如果你面临如下任一场景，TC 将会帮到你。 你的 Pytorch 层又大又慢，你打算为此写一段 C++ 或者 CUDA 的优化代码，但是你又不擅长。 你写好了 CUDA 代码，但是你还要花费大量时间去优化。你希望可以在最短时间内搞定这些。 为了加速，你想在网络中融合多个层，例如 Conv-ReLU-BatchNorm 或者 Linear-ReLU-Linear-ReLU。但是这很难理解。 你的研究涉及 CuDNN 和 MKL 未能优化的不寻常的张量。例如，你要使用 13 x 24 的卷积核对 143 x 55 的输入图像进行卷积。你试着用 CUDNN 跑，并且发现它慢的超乎想象。 通过调整 Tensors 以适应特定的内存布局，你的代码会变慢。你希望编写在你的输入布局上高效运行的自定义代码很容易。 TC 可以在 PyTorch 中无缝使用，与 PyTorch Tensors 和 nn Variables 进行互操作。 教程现在开始！ 1. 安装包 此次我们只提供在 Ubuntu 16.04 和 CentOS7 上测试过的 Linux-64 二进制包。TC 依赖于大型 C++ 项目，如 Halide、Tapir-LLVM 及 ISL。因此，我们使用 Anaconda 来可靠地分配这些依赖。因此 TC 不提供 PyPI 安装包。 2. 导入 Python 包 3. 定义 TC 表达式并创建一个 Python 函数 fcrelu 以 PyTorch Tensors 为输入并返回一个 PyTorch Tensor。输入 inputI、权重 W1、偏置项 B1 并返回一个 O1。 4. 创建一些虚拟输入张量 5. 自动调节函数以适用你的输入大小 自动调节器是你最好的朋友。你通常在调用 tc 函数前都会先自动调节它。当自动调节开始运行，则会显示当前的最佳性能。如果你满足于当前结果或时间耗完了，请按 Ctrl + C 终止调节进程。 cache 保留自动自动调节的核搜索结果并存到 fcrelu_100_128_100.tc 文件中。下一次再调用这句代码时，它就会自动加载已调节的结果而无需再计算。 自动调节器有几个超参数 (就像你的卷积网络有学习率，层数等等）。我们预设了一些缺省项，但是你可以在这里了解如何使用高级选项。 6. 调用函数并传参，返回你的结果 现在，我们来看看怎么写 TC 表达式。 TC 符号关注层的数学本质，将性能优化交给使用 Halide 和 polyhedral 编译技术的后端代码，这些技术累积了数十年来最先进的 Loop Nest Optimization（LNO）研究成果。 TC 和 np.einsum 很像。我们用一个实例快速展开学习。 我们定义了这个实例中的一个函数，它接受两个输入 M x N 的 A 和 N x K 的Ｂ并返回一个结果。output 的尺寸由 TC 语言自动推导得出 (下面展开讨论）。看看这行代码： 意思是: output(i, j) 表示输出是 2 维的 对于每个坐标 output(i, j), 加上 (+=) A(i, kk) * B(kk, j) i 是 A 中 dim=0 的所有坐标, 即，i 在 range(0, M) 里 j 是 B dim = 1 中的所有坐标，即，j 在 range(0, K) 里 kk 是由 0 到 N 推断出的所有坐标 output 的尺寸由 i 和 j 可取的最大值推理而来，也就是 M 和 K，所以 output 的尺寸就是 M x K。 ! 符号用 0.0 初始化，相当于： 标量输入和范围约束：实现 AvgPool2d 这里 where 关键字可以选取取值范围来进行运算。0:{kH} 类似与 Python 中的 range(kH）。 注意：传入标量的语法在下一版本中可能会发生变化。 我们对 TC 的基本 PyTorch 集成进行了封装，以便通过定义正向和反向 TC 表达式并有 Variable 输入/输出来轻松地将 TC 集成到更大的 torch.nn 模块。这里有一个用 TC 定义卷积操作的实例。 自动调节变量长度序列 TC 自动调节器要求预先自动调节好所有的输入尺寸。例如，如果你有一批输入图像 I1，调节器想要知道 I1 的确切尺寸以便生成优化的卷积核。你不能指定：高度介于 200 到 300 之间的图像。这对于如 NLP 之类的序列数据更为重要，NLP 每个语句可以具有不同的长度。 自动调节器不参数化是因为自动调整参数约束越来越难，并且这是一项热门研究。由于这是首次发布，我们决定主动提供工具，并保证其良好工作。 作为解决方法，如果你事先知道有一些要用到的特定尺寸，则可以输入它们运行自动调节器。 当前的自动调节器自动调节三个特定的图片尺寸： 32x32、48x48 和 64x64。 缺少循环 如果你想写一个 RNN，很容易把它视作相对于时间的 for 循环。但是，TC 语言目前还没有循环结构。如果你真的想写 RNN，可以试着把它展开。 TC 后台暂不支持不连续的张量。如果你输入的张量不连续，TC 在传递到后台代码前会作连续化处理。 使用 TC 表达式进行张量变形 在 TC 中你不能这么写操作：torch.matmul(...).view(...).mean(...)。任何时候都需要一个 view 来改变输入张量的尺寸，你必须获取到输出张量，在 PyTorch 级别上 view 它。 《Walk through Tutorial》会帮助你理解并上手入门 TC PyTorch 包。 超过 20 个实例含有 TC 写的不同的 ML 层，包括 avgpool、maxpool、matmul、matmul - give output buffers、batch-matmul、convolution、strided-convolution、batchnorm、copy、cosine similarity、Linear、Linear + ReLU、group-convolutions、strided group-convolutions、indexing、Embedding (lookup table)、small-mobilenet、softmax、tensordot、transpose。 有关 TC 的详细文档以及与 PyTorch 的整合。 "
136,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739143&idx=2&sn=a8f9c0dd6e41756e724caf96c2243fb9&chksm=871ad5b9b06d5caf35ab9995ed1b67aba2a1d75454e74532276f836da8e7bdf2765be09f1089&scene=27,资源 | 吴恩达deeplearning.ai五项课程完整笔记了解一下？,"自吴恩达发布 deeplearning.ai 课程以来，很多学习者陆续完成了所有专项课程并精心制作了课程笔记，在此过程中机器之心也一直在为读者推荐优质的笔记。上个月，deep learning.ai 第五课发布，该系列课程最终结课。Mahmoud Badry 在 GitHub 上开源了五项课程的完整笔记，介绍了包括序列模型在内的详细知识点。机器之心简要介绍了该项目，并重点描述了第五项课程序列模型。 项目地址：https://github.com/mbadry1/DeepLearning.ai-Summary 上周吴恩达在推特上展示了一份由 TessFerrandez 完成的深度学习专项课程信息图，这套信息图优美地记录了深度学习课程的知识与亮点。这一份信息图的详细介绍请查看：  。 Deeplearning.ai 课程开课以来，一直受到大家的关注，也有众多读者积极的参与到学习中。机器之心在这段时间内也介绍了多篇该系列课程的学习笔记，以下为前四课的笔记与学习心得： 吴恩达 Deeplearning.ai 课程学习全体验：深度学习必备课程（已获证书） 入门 | 吴恩达 Deeplearning.ai 全部课程学习心得分享 资源 | 吴恩达 deeplearning.ai 第四课学习心得：卷积神经网络与计算机视觉  Mahmoud Badry 完成的笔记主要分为五部分，分别对应神经网络与深度学习基础、提升 DNN 性能的技巧与方法等、结构化机器学习项目、卷积神经网络和序列模型五门课程。值得注意的是，该项目完成的笔记十分详细，基本上五门课程的知识点全都覆盖到了。例如第一项课程以每周不同主题为序记录了从神经网络简介到 Goodfellow 采访等基本知识点。 由于前四课很多知识点都已经介绍过，因此本文我们着重介绍第五课的笔记概要，读者可自行查阅 GitHub 阅读完整的笔记，也可查看机器之心往期发过的一些课程资料。 第五课序列模型简介 本课程将讲授如何构建自然语言、音频和其他序列数据的模型。在深度学习的帮助下，序列算法比两年前效果更好，用于大量有趣的应用，如语音识别、音乐合成、聊天机器人、机器翻译、自然语言理解等。学完本课，你将： 了解如何构建和训练循环神经网络（RNN）及其常用变体，如 GRU 和 LSTM。 使用序列模型处理自然语言问题，如文本合成。 将序列模型应用到音频应用中，如语音识别和音乐合成。 这是 Deep Learning Specialization 课程的第五课，也是最后一课。 适用人群： 学完第一、二、四课的学习者。同样推荐大家学习第三课。 已经对神经网络（包括 CNN）具备深厚理解，并想学习如何开发循环神经网络的人。 该课程介绍循环神经网络（RNN）、自然语言处理和词嵌入还有序列模型和注意力机制等，以下将简要介绍 Mahmoud Badry 所完成的序列模型笔记。 序列模型 序列模型（如 RNN 和 LSTM）极大地改变了序列学习，序列模型可通过注意力机制获得增强。序列模型在语音识别、音乐生成、情感分类、DNA 序列分析、机器翻译、视频活动识别、命名实体识别等方面得到应用。 循环神经网络出现于 20 世纪 80 年代，最近由于网络设计的推进和图形处理单元上计算能力的提升，循环神经网络变得越来越流行。这种网络尤其是对序列数据非常有用，因为每个神经元或者单元能用它的内部存储来保存之前输入的相关信息。在语言的案例中，「I had washed my house」这句话的意思与「I had my house washed」大不相同。这就能让网络获取对该表达更深的理解。 RNN 有很多应用，在自然语言处理（NLP）领域表现良好。下图是一个用于解决命名实体识别任务的 RNN 网络。 RNN 架构中的反向传播，w_a、b_a、w_y、b_y 被序列中的所有元素共享。 这里使用交叉熵损失函数： 其中第一个公式是序列中一个元素的损失函数，整个序列的损失是每个元素的损失之和。 在上图中沿时间反向传播中，激活值 a 从一个序列元素向另一个元素传播。 RNN 的类型 RNN 的梯度消失 「梯度消失」指的是随着网络深度增加，参数的梯度范数指数式减小的现象。梯度很小，意味着参数的变化很缓慢，从而使得学习过程停滞。循环神经网络在语言建模等序列问题上有非常强大的力量，但同时它也存在很严重的梯度消失问题。因此像 LSTM 和 GRU 等基于门控的 RNN 有非常大的潜力，它们使用门控机制保留或遗忘前面时间步的信息，并形成记忆以提供给当前的计算过程。 门控循环单元（GRU） GRU 旨在解决标准 RNN 中出现的梯度消失问题。GRU 背后的原理与 LSTM 非常相似，即用门控机制控制输入、记忆等信息而在当前时间步做出预测，表达式如下： GRU 有两个有两个门，即一个重置门（reset gate）和一个更新门（update gate）。从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。使用门控机制学习长期依赖关系的基本思想和 LSTM 一致，但还是有一些关键区别： GRU 有两个门（重置门与更新门），而 LSTM 有三个门（输入门、遗忘门和输出门）。 GRU 并不会控制并保留内部记忆（c_t），且没有 LSTM 中的输出门。 LSTM 中的输入与遗忘门对应于 GRU 的更新门，重置门直接作用于前面的隐藏状态。 在计算输出时并不应用二阶非线性。 为了解决标准 RNN 的梯度消失问题，GRU 使用了更新门（update gate）与重置门（reset gate）。基本上，这两个门控向量决定了哪些信息最终能作为门控循环单元的输出。这两个门控机制的特殊之处在于，它们能够保存长期序列中的信息，且不会随时间而清除或因为与预测不相关而移除。 以下展示了单个门控循环单元的具体结构。 使用传统的通过时间的反向传播（BPTT）或实时循环学习（RTTL/Real Time Recurrent Learning），在时间中反向流动的误差信号往往会爆炸（explode）或消失（vanish）。但 LSTM 可以通过遗忘和保留记忆的机制减少这些问题。 LSTM 单元一般会输出两种状态到下一个单元，即单元状态和隐藏状态。记忆块负责记忆各个隐藏状态或前面时间步的事件，这种记忆方式一般是通过三种门控机制实现，即输入门、遗忘门和输出门。 以下是 LSTM 单元的详细结构，其中 Z 为输入部分，Z_i、Z_o 和 Z_f 分别为控制三个门的值，即它们会通过激活函数 f 对输入信息进行筛选。一般激活函数可以选择为 Sigmoid 函数，因为它的输出值为 0 到 1，即表示这三个门被打开的程度。 若我们输入 Z，那么该输入向量通过激活函数得到的 g(Z) 和输入门 f(Z_i ) 的乘积 g(Z) f(Z_i ) 就表示输入数据经筛选后所保留的信息。Z_f 控制的遗忘门将控制以前记忆的信息到底需要保留多少，保留的记忆可以用方程 c*f（z_f）表示。以前保留的信息加上当前输入有意义的信息将会保留至下一个 LSTM 单元，即我们可以用 c' = g(Z)f(Z_i) + cf(z_f) 表示更新的记忆，更新的记忆 c' 也表示前面与当前所保留的全部有用信息。我们再取这一更新记忆的激活值 h(c') 作为可能的输出，一般可以选择 tanh 激活函数。最后剩下的就是由 Z_o 所控制的输出门，它决定当前记忆所激活的输出到底哪些是有用的。因此最终 LSTM 的输出就可以表示为 a = h(c')f(Z_o)。 双向 RNN（BRNN） 双向 RNN 和深度 RNN 是构建强大序列模型的有效方法。下图是一个命名实体识别任务的 RNN 模型： BRNN 架构 BRNN 的缺点是在处理之前需要整个序列。 深度 RNN 深度 RNN 可帮助构建强大的序列模型。 RNN 的反向传播 在现代深度学习框架中，你只需实现前向传播，框架会执行反向传播，因此大部分机器学习工程师不需要担心反向传播。但是，如果你是微积分专家，想了解 RNN 中反向传播的细节，可参考该 notebook：https://www.coursera.org/learn/nlp-sequence-models/notebook/X20PE/building-a-recurrent-neural-network-step-by-step。 词表征在自然语言处理中是必不可少的部分，从早期的 One-Hot 编码到现在流行的词嵌入，研究者一直在寻找高效的词表征方法。Mahmoud Badry 在笔记中详细记录了词嵌入方法，包括用于命名实体识别、人脸识别和翻译系统的词嵌入等，下图展示了用于人脸识别的词嵌入结构： 在这种词嵌入方法中，我们可以将不同的人脸编码压缩为一个向量，进而根据该向量比较是不是同一张脸。 词嵌入有非常多的优秀属性，例如给定一张词嵌入表。该笔记用案例来解释词嵌入的语义相近关系，如下图所示，男性变化到女性与国王变化到女王的反向在嵌入空间内是相同的，这意味着词嵌入捕捉到了词与词之间的语义相关性。 一般来说，Word2Vec 方法由两部分组成。首先是将高维 one-hot 形式表示的单词映射成低维向量。例如将 10,000 列的矩阵转换为 300 列的矩阵，这一过程被称为词嵌入。第二个目标是在保留单词上下文的同时，从一定程度上保留其意义。Word2Vec 实现这两个目标的方法有 skip-gram 和 CBOW 等，skip-gram 会输入一个词，然后尝试估计其它词出现在该词附近的概率。还有一种与此相反的被称为连续词袋模型（Continuous Bag Of Words，CBOW），它将一些上下文词语作为输入，并通过评估概率找出最适合（概率最大）该上下文的词。 对于连续词袋模型而言，Mikolov 等人运用目标词前面和后面的 n 个词来同时预测这个词。他们称这个模型为连续的词袋（CBOW），因为它用连续空间来表示词，而且这些词的先后顺序并不重要。CBOW 可以看作一个具有先知的语言模型，而 skip-gram 模型则完全改变将语言模型的目标：它不像 CBOW 一样从周围的词预测中间的词；恰恰相反，它用中心语去预测周围的词。 Mahmoud Badry 还展示了另一种学习词嵌入的方法 GloVe，该方法虽然不像语言模型那样使用广泛，但它精简的结构非常容易理解： 序列模型与注意力机制 最后一部分作者着重介绍了注意力机制，包括编码器解码器架构的缺陷和引入注意力机制的解决方案。下图展示了使用语境向量 C 或注意力权重编码信息的过程。 其实当我们翻译一个句子时，会尤其关注于正在翻译的单词。神经网络可以通过注意力来实现同样的行为，即关注所收到信息子集的一部分。 我们通常是使用基于上下文的注意力生成注意力分布。参与的 RNN 会生成一个描述它想关注内容的查询。每一个条目和这个查询做点乘来产生一个分数，这个分数描述这个条目与查询匹配程度。这些分数被输入一个 softmax 来生成注意力分布。 "
137,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739143&idx=1&sn=f51551128021e7747205f87971f4762e&chksm=871ad5b9b06d5caf0a5b4601863adc213eb42573bbdc2773e54ff1c585b3954a8644df024015&scene=27,从起源到具体算法，这是一份适合所有人读的深度学习综述论文,"自 2012 年多伦多大学 Alex Krizhevsky 等人提出 AlexNet 以来，「深度学习」作为一种机器学习的强大方法逐渐引发了今天的 AI 热潮。随着这种技术被应用到各种不同领域，人们已经开发出了大量新模型与架构，以至于我们无法理清网络类型之间的关系。近日，来自 University of Dayton 的研究者们对深度学习近年来的发展历程进行了全面的梳理与总结，并指出了目前人们面临的主要技术挑战。机器之心觉得这是一份非常详细的综述论文，既适合从零开始了解深度学习的人，又适合有基础的学习者。 论文地址：https://arxiv.org/abs/1803.01164 近年来，深度学习作为机器学习的新分支，其应用在多个领域取得巨大成功，并一直在快速发展，不断开创新的应用模式，创造新机会。深度学习方法根据训练数据是否拥有标记信息被划分为监督学习、半监督学习和无监督学习。实验结果显示了上述方法在图像处理、计算机视觉、语音识别、机器翻译、艺术、医学成像、医疗信息处理、机器人控制和生物、自然语言处理（NLP）、网络安全等领域的最新成果。本报告简要概述了深度学习方法的发展，包括深度神经网络（DNN）、卷积神经网络（CNN）、循环神经网络（RNN）（包括长短期记忆（LSTM）和门控循环单元（GRU））、自 编码器（AE）、深度信念网络（DBN），生成对抗网络（GAN）和深度强化学习（DRL）。此外，本文也涵盖了深度学习方法前沿发展和高级变体深度学习技术。此外，深度学习方法在各个应用领域进行的探索和评估也包含在本次调查中。我们还会谈到最新开发的框架、SDK 和用于评估深度学习方法的基准数据集。然而，这些论文并没有讨论某些大型深度学习模型和最新开发的生成模型方法 [1]。 自 20 世纪 50 年代以来，作为人工智能子领域的机器学习已经开始革新若干个领域，而诞生自机器学习的深度学习实现了迄今为止最大的原创性突破，几乎在每个应用领域取得了显著成功。图 1 给出了 AI 的谱系。深度学习（学习或分层学习方法的深层架构）是从 2006 年兴起的一类机器学习技术。在深度学习中，学习即是评估模型参数，使学习模型（算法）可执行特定任务。例如，在人工神经网络（ANN）中，参数是权重矩阵 。另一方面，深度学习在输入层和输出层之间包含若干个隐层，使得不同阶段的非线性处理单元具有层级结构，以用于特征学习和模式分类 [1, 2]。基于数据表征的学习方法也被称为表征学习 [3]。根据最新文献，基于深度学习的表征学习涉及特征或概念的层次结构，其中高级概念可以从低级概念定义，低级概念可以从高级概念定义。在一些文章中，深度学习也被描述为一种通用学习方法，可以解决不同应用领域的几乎所有问题（不局限于特定任务）[4]。   A. 深度学习方法的类型 像机器学习一样，深度学习方法可以分为以下几类：监督、半监督、部分监督以及无监督。此外，还有另一类学习方法称为强化学习（Reinforcement Learning）或深度强化学习（Deep Reinforcement Learning），它们经常在半监督或非监督学习方法的范围内讨论。   1) 监督学习 一种使用标注数据的学习技术。在其案例中，环境包含一组对应的输入输出  。比如，输入是 x_t，智能体预测 ，则会获得损失值 。接着智能体不断迭代调整网络参数，从而更好地近似期望输出。成功训练之后，智能体可对环境问题做出正确回答。监督学习主要有以下几种：深度神经网络 (DNN)、卷积神经网络 (CNN)、循环神经网络（包含 LSTM）以及门控循环单元（GRU）。上述网络将分别在 2、3、4、5 章节中详述。 2) 半监督学习 一种使用部分标注数据的学习技术（通常被称之为强化学习）。本文第 8 节调查了其方法。在一些案例中，深度强化学习（DRL）和生成对抗网络（GAN）常被用作半监督学习技术。此外，包含 LSTM 的 RNN 和 GRU 也可划分为半监督学习。GAN 将在第 7 节讨论。 3) 无监督学习 一种不使用标注数据的学习技术。在这种情况下，智能体学习内部表示或重要特征以发现输入数据中的未知关系或结构。无监督学习方法通常有聚类、降维和生成技术等。有些深度学习技术擅长聚类和非线性降维，如自编码器（AE）、受限玻尔兹曼机（RBM）和 GAN。此外，RNN（比如 LSTM）和 RL 也被用作半监督学习 [243]。本文第 6、7 节将分别详述 RNN 和 LSTM。 4) 深度强化学习（DRL） 一种适用于未知环境的学习技术。DRL 始于 2013 年谷歌 Deep Mind[5,6]。从此，人们基于 RL 提出了几种先进的方法，例如：如果环境样本输入：agent〜ρ，agentpredict： ，agentreceivecost： ，其中 P 是未知概率分布，环境向智能体提出问题，并给其一个有噪音的分值作为答案。有时这种方法也被称为半监督学习。许多半监督和无监督学习方法已经基于这个概念实施（第 8 节）。在 RL 中，我们没有一个简单的前向损失函数，因此与传统的监督方法相比，这使得机器学习变得更困难。RL 和监督学习之间的根本区别在于：首先，我们无法获取你正在优化的函数，而必须通过交互来查询它；其次，我们正在与基于状态的环境交互：输入 x_t 取决于先前的动作。 B. 特征学习 传统机器学习和深度学习之间的关键区别在于如何提取特征。传统机器学习方法通过应用几种特征提取算法，包括尺度不变特征变换（SIFT）、加速鲁棒特征（SURF）、GIST、RANSAC、直方图方向梯度（HOG）、局部二元模式（LBP）、经验模式分解（EMD）语音分析等等。最后，包括支持向量机（SVM）、随机森林（RF）、主成分分析（PCA）、核主成分分析（KPCA）、线性递减分析（LDA）、Fisher 递减分析（FDA）等很多学习算法都被人们应用于分类和提取特征的任务。此外，其他增强方法通常多个应用于单个任务或数据集特征的学习算法，并根据不同算法的多个结果进行决策。 另一方面，在深度学习中，这些特征会被自动学习并在多个层上分层表示。这是深度学习超越传统机器学习方法的原因。上表展示了不同特征学习方法与不同学习步骤之间的关系。 C. 应用深度学习的时机和领域 人工智能在以下领域十分有用，深度学习在其中扮演重要角色： 1. 缺乏人类专家（火星导航）； 2. 人们尚无法解释的专业知识（演讲、认知、视觉和语言理解）； 3. 问题的解决方案随时间不断变化（追踪、天气预报、偏好、股票、价格预测）； 4. 解决方案需要适应特定情况（生物统计学、个性化）； 5. 人类的推理能力有限，而问题的规模却很大（计算网页排名、将广告匹配到 Facebook、情感分析）。 目前，深度学习几乎在各个领域都有应用。因此，这种方法有时也被称为通用学习方法。图 4 显示了一些示例应用程序。 D. 深度学习的前沿发展 深度学习在计算机视觉和语音识别领域有一些突出的成就，如下所述： 1）ImageNet 数据集上的图像分类 深度学习在图像分类领域的应用基准被称为大规模视觉识别挑战（LSVRC）。基于深度学习和卷积神经网络技术，深度学习在 ImageNet 测量精确度中有很好的表现 [11]。近日，Russakovsky 等人发表了一篇关于 ImageNet 数据集的文章及近年来研究者们实现的最高精确度 [285]。下图显示了 2012 年深度学习技术的发展历程。时至今日，我们开发的方法在 ResNet-152 上只有 3.57％的误差，低于人类约 5% 的误差。 2）自动语音识别   深度学习通过 TIMIT 数据集（通用数据集通常用于评估）完成的小规模识别任务是深度学习在语音识别领域的初次成功体现。TIMIT 连续声音 - 语音语料库包含 630 位来自美国的八种主要英语口音使用者，每位发言人读取 10 个句子。下图总结了包括早期结果在内的错误率，并以过去 20 年的电话错误率（PER）来衡量。条形图清楚地表明，与 TIMIT 数据集上以前的机器学习方法相比，最近开发的深度学习方法（图顶部）表现更好。 E. 为什么要使用深度学习 1）通用学习方法 深度学习有时被称为通用学习，因为它几乎可以应用于任何领域。 2）鲁棒性 深度学习方法不需要提前设计功能。其自动学习的功能对于当前的任务来说是最佳的。结果是，任务自动获得对抗数据自然变化的鲁棒性。 3）泛化 相同的深度学习方法可以用于不同的应用程序或不同的数据类型，这种方法通常被称为迁移学习。另外，这种方法在可用数据不足时很有用。根据这个概念研究学者已经发表了多篇论文（在第 4 节中会有更详细地讨论）。 4）可扩展性 深度学习方法具有高度可扩展性。在 2015 年的一篇论文中，微软描述了一个名为 ResNet 的网络 [11]。该网络包含 1202 个层，并且通常由超级计算规模部署。美国的劳伦斯利弗莫尔国家实验室（LLNL）正在为这样的网络开发框架，该框架可以实现数千个节点 [24]。 F. 深度学习面临的挑战： 使用深度学习进行大数据分析 深度学习方法要有可扩展性 在数据不可用于学习系统的情况下（尤其是对于计算机视觉任务，例如反向图形），生成数据的能力非常重要。 特殊用途设备的低能耗技术，如移动端智能，FPGA 等。 多任务和迁移学习（泛化）或多模块学习。这意味着要从不同的领域或不同的模型一起学习。 在学习中处理因果关系。 其次，大部分针对大规模问题的案例，其解决方案正在高性能计算机（HPC）系统（超级计算机、集群，有时被视为云计算）上部署，这为数据密集型商业计算提供了巨大的潜力。但随着数据在速度，多样性，准确性和数量上的爆炸式增长，我们越来越难以使用企业级服务器进行存储和提升计算性能。大多数论文考虑到这些需求，并提出了使用异构计算系统的高效 HPC。例如：劳伦斯利弗莫尔国家实验室（LLNL）开发了一个框架：Livermore Big Artificial Neural Networks（LBANN），用于大规模部署深度学习（超级计算规模），这一项目明确地回答了深度学习是否可扩展的问题 [24]。 第三，生成模型是深度学习的另一个挑战，其中一个例子是 GAN，它是一种优秀的数据生成方法，可以生成具有相同分布数据 [28]。第四，我们在第七节讨论过的多任务和迁移学习。第四，我们对网络架构和硬件方面的高效率深度学习方法进行了大量的研究。第 10 节讨论了这个问题。   我们可以制作出适用于多领域、多任务的通用模型吗？出于对多模式系统的关注，最近，谷歌提交的论文《One Model To Learn Them All》[29] 介绍了一种新方法，其可以从不同的应用领域学习，包括 ImageNet、多种翻译任务、图像标题（MS-COCO 数据集）、语音识别语料库和英语解析任务。我们将通过这次调查讨论主要挑战和相应的解决方案。在过去几年中，人们还提出了其他多任务技术。 最后，图形模型是一个具有因果关系的学习系统，用于定义如何根据数据推断因果模型。最近，已经出现了解决此类问题的深度学习方法 [33]。但是，在过去几年中，还有其他许多具有挑战性的问题仍未得到有效地解决。例如：图像或视频字幕 [34]，使用 GAN [35] 从文本到图像合成 [36] 以及其他从一个域到另一个域的风格迁移。 最近，一些研究者完成了很多关于深度学习的调查，其中有一篇非常高质量的总结，但它没有涉及最近开发的 GAN 的生成模型 [28]。此外，它提及了强化学习的话题，但没有涉及深度强化学习方法的近期趋势 [1,39]。大多数情况下，调查是依据深度学习的不同方法来分类的。本报告的主要目标是介绍深度学习的总体思路及其相关领域，包括深度监督（如 DNN、CNN 和 RNN）、无监督（如 AE、RBM、GAN）（有时 GAN 也用于半监督学习任务）和深度强化学习的思路。在某些情况下，深度强化学习被认为是半监督/无监督的方法。我们考虑了该领域的最新发展趋势以及基于该技术开发的应用。此外，我们还囊括了评估深度学习技术常用的框架和基准数据集，会议和期刊的名称也包括在内。 本论文的其余部分的组织方式如下：第二节讨论 DNN 的详细调查，第三节讨论 CNN；第四节介绍了不同的先进技术，以有效地训练深度学习模型； 第五节讨论 RNN； AE 和 RBM 在第六节中讨论； GAN 及其应用在第七节讨论；强化学习在第八节中介绍；第九节解释迁移学习； 第十节介绍了深度学习的高效应用方法和硬件； 第十一节讨论了深度学习框架和标准开发工具包（SDK）； 第十二节给出了不同应用领域的基准测试结果；第十三节为结论。 "
138,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739090&idx=4&sn=bcb5e3edd470ce1363b0eb5f3405a531&chksm=871ad5ecb06d5cfa93f08424fc86ca2fc5b795d9e22c71f8619cbe0827981cfb3fab0a47c076&scene=27,CVPR 2018 | 中科大&微软提出立体神经风格迁移模型，可用于3D视频风格化,"近年来，在自然图像上再现名画风格的风格转换技术成为内容创作的热门话题。例如，最近的电影「至爱梵高」是第一部完全由训练有素的艺术家制作的动画电影。然而，目前还没有将风格转换应用于立体图像或视频的技术。现有的风格迁移方法会使左右视图不一致的风格化纹理，研究者通过解决该问题，突破了立体风格迁移的一大瓶颈。 立体 3D 正在成为一种大众消费媒体，例如 3D 电影、电视以及游戏。现在，随着头戴式 3D 显示器（例如 AR/VR 眼镜）和双镜头智能手机的发展，立体 3D 越来越受关注，并激发了许多有趣的研究工作，例如立体修复 [36,27]、视频稳定 [15]，以及全景 [39]。在这些研究中，创建立体 3D 内容总是令人感兴趣的。 近年来，在自然图像上再现名画风格的风格转换技术成为内容创作的一个热门话题。例如，最近的电影「至爱梵高」是第一部完全由训练有素的艺术家制作的动画电影。受卷积神经网络 ( CNN ) 能力的启发，Gatys 等人的开创性工作 [ 13 ] 提出了一种将指定作品的风格自动转换为任何图像的通用解决方案。[ 21，19，34，12，11 ] 提出了许多后续工作，以改进或扩展该项目。这些技术还被应用于许多成功的工业应用 (例如，Prisma [ 1],Ostagram [2] 和 Microsoft Pix [3])。 然而，据作者介绍，目前还没有将风格转换应用于立体图像或视频的技术。在这篇论文中，作者通过首次提出立体神经风格转换算法来应对这种新兴 3D 内容的需求。他们首先独立地仔细检验了现有的风格转换方法在左视图和右视图上的简单应用。 他们发现往往这些方法很难在两个视图上产生几何一致的风格化纹理。结果就是，它会引起有问题的深度感知，并且造成观看者的 3D 疲劳（如图 1 所示）。因此我们需要生成和和两个视图一致的风格化纹理。此外，还需要一个快速的解决方案，尤其是在实际的实时 3D 显示中（例如 AR/VR 眼镜）。最后但不是最不重要的一点，作为进一步扩展的立体视频中的风格转换应当同时满足时间的连贯性。 本文提出了第一个用于快速立体风格转换的前馈网络。除了广泛使用的风格损失函数 [13,19] 之外，作者还引入了一个附加的视差一致性损失，用它来惩罚风格化结果在非遮挡区域的偏差。具体而言，在给定双向视差和遮挡掩膜的情况下，可以建立左视图和右视图之间的对应关系，并且惩罚了两个视图中都可见的重叠区域的风格不一致。 作者首先在基于优化的解决方案 [13] 中验证了这个新的损失项。如图 1 所示，通过在优化过程中联合考虑风格化和视差一致性，该方法可以为两个视图生成更加一致的风格化结果。然后作者进一步将这种新的视差损失结合在了为立体风格化所设计的前馈深度网络中。 本文提出的网络由两个子网络组成。一个是风格化子网络 StyleNet，它使用了和 [19] 中一样的架构。另一个是视差子网络 DispOccNet，它可以估计出输入立体图像对的双向视差图和遮挡掩膜。这两个子网络被集成在特征级别的中间域中。它们首先分别接受各自任务的独立训练，然后作为一个整体接受联合训练。 该新视差子网络具有两个优点： 1 ) 与使用缓慢全局优化技术的一些最先进的立体匹配算法 [ 33，22 ] 相比，它能够实现实时处理； 2 ) 它是第一个同时估计双向视差和遮挡掩模的端到端网络，而其它方法 [ 26，38 ] 在每个前向过程中仅估计单个双向视差图，并且需要后处理步骤来获得遮挡掩模。文章的 5.2 部分展示了这种双向设计优于单向设计的原因。 该网络还可以很容易地通过集成 [10] 中的子网络扩展到立体 3D 视频中。这样，最终的风格化结果不仅可以保持每个时间步的水平空间一致性，而且可以保持相邻时间步之间的时间连贯性。这项工作可能会启发电影创作者考虑自动地将 3D 电影或电视转变为名画风格。 实验结果表明，该方法无论在数量上还是在质量上都优于这个领域中的基准结果。总体而言，本文主要贡献由以下四部分组成： 通过将新的视差一致约束结合到原始的风格损失函数中，本文提出了第一个立体风格转换算法。 本文提出了第一个用于快速立体风格转换的前馈网络，它把风格化、双向视差和遮挡掩码结合成了一个端到端系统。 本文提出的视差子网络是第一个可以同时估计双向视差图和遮挡掩码的端到端网络。 考虑到视差一致性和时间连贯性，本文通过集成一个附加的子网络把该方法扩展到了立体视频的风格转换上。 本文的剩余部分将首先总结一些相关的工作。在该方法中，研究者使用了基于基线优化的方法验证了新提出的视差约束，然后介绍了快速立体风格转换的前馈网络，并将其扩展到立体视频。实验结果表明了该方法的有效性，还展示了对该方法的一些控制变量分析。在本文最后作者作了进一步讨论。 图 2. 左视图、右视图的风格化比较（第一行）；结合了一致性约束的风格化结果（中间行）。前一种方法（没有结合一致性约束）往往会在遮挡掩码边界附近产生纹理不连续。最下行是右视图遮挡掩码和放大的风格化图片。 4. 立体风格转换网络 本文提出了一个快速立体风格转换的前馈网络。整个网络由两个子网络组成:一个是与现有的风格化网络 [ 10,11,12,16 ] 相似的风格化网络 StyleNet，另一个是同时估计双向视差图和遮挡掩码的 DispOccNet。这两个子网络被集成在一个特征级中间域中，使左视图和右视图完全对称。 StyleNet：作者使用了 [19] 最早提出的，并在其他工作中 [10, 11, 12, 16] 得到广泛应用的默认风格网络结构。该架构基本上类似图像自动编码器，它由若干个指定步幅的卷积层 (将图像编码到特征空间中)、五个残差模块和少数指定步幅的卷积层 (将特征解码为图像) 组成。在该实现中，遵循了与 [ 10 ] 相同的设置，其中第三个残差块 (包括第三个残差块) 之前的层被视为编码器，而剩余层被视为解码器。 DispOccNet：最近，Mayer 等人引入了称为 DispNet 的端到端卷积网络，它被用于视差估计 [26]。然而，它只能预测每个前向的单向视差图 DI ( l→r )。在本文中，作者使用类似的网络结构，但在扩展部分中为每个分辨率 ( 1 / 64，...1 / 2 ) 增加了三个分支。这三个分支用于回归视差 Dr 和双向遮挡掩码 Ml 和 Mr。 图 3. 快速立体风格迁移的总体网络结构。它包含两个子网络：StyleNet 和 DispOccNet，它们被集成在特征级别中间域 H 中。 图 4. 立体视频风格迁移的总体结构。左边是时间网络的简化工作流。右边是用于结合上述立体网络和左边附加的时间网络的递归公式。 图 6. 与使用 [10] 中所用方法（第一行）的一个类似变体的结果对比，该变体方法存在重影和风格化不一致的问题。中间行是使用了本文的方法构成掩码替换的结果，重影消失了，但是不一致性仍然存在。相比之下，本文的结果（最底行）没有上述问题。 图 7. 与真实街道视图立体图像对的基准进行比较。第一行中带有红色标记框是基准结果，底行含有对应的绿色框的是本文的结果。显然，本文的结果具有更好的视差一致性。 论文：Stereoscopic Neural Style Transfer（立体神经风格转换器） 论文链接：https://arxiv.org/abs/1802.10591 摘要：本文首次尝试实现立体神经风格转换，以应对 3D 电影或 AR/VR 的新需求。我们首先仔细检验了将现有的分别应用于立体图像的左视图和右视图的单目风格转换方法，表明在最终的风格化结果中不能很好地保持原始的视差一致性，这给观看者造成了 3D 疲劳。为了解决这个问题，我们通过在非遮挡区域中加强双向视差约束，向广泛采用的风格损失函数中加入了一个新的视差损失。为了得到实用的实时性解决方案，我们提出了第一个前馈网络：它联合训练一个风格化子网络和一个视差子网络，并将它们集成在一个特征级的中间域中。我们的视差子网络也是用于同时估计双向视差和遮挡掩码的首个端到端网络。最后，综合考虑时间连贯性和视差一致性，我们将该网络有效地扩展到立体视频上。实验结果表明，该方法无论在数量上还是质量上都明显优于基准算法。 "
139,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739090&idx=3&sn=bb6443f4a4c0c69e765c4063c23bf428&chksm=871ad5ecb06d5cfa97b7bf2d77b63704b96d565b3e9d46bb3d82481fc18bb316554dbb0b18f0&scene=27,观点 | 如何可视化卷积网络分类图像时关注的焦点,在我们使用 CNN 进行图片分类时，模型到底关注的是图像的哪个区域？Grad-CAM 利用卷积网络最后一个特征图的信息，并加权对应的梯度而构建模型分类的热力图，通过该热力图，我们可以清楚地了解哪一块区域对于类别是最重要的。 你在训练神经网络进行图片分类时，有没有想过网络是否就是像人类感知信息一样去理解图像？这个问题很难回答，因为多数情况下深度神经网络都被视作黑箱。我们喂给它输入数据进而得到输出。整个流程如果出现问题很难去调试。尽管预测的已经相当精准，但这并不能说明他们足以和人类感知的方式媲美。 为何会这样？ 假设你需要对大象和企鹅进行二分类（我知道这个任务十分简单）。现在你已经获取了数据集，训好了模型并完成部署。这个模型想必是适用于绝大多数数据的，但是总有可能会出现误判。有人可能会把它看作是一个极端情况，但是你觉得对于 CNN 来说，什么时候物体才是明确可辨的？ 结合上述内容，显然在图像中，大象常伴着草木出现，企鹅常伴着冰雪出现。所以，实际上模型已经学会了分辨草木与冰雪的颜色/形状，而不是真的学会了按对象分类。 由上文案例知，如颜色通道统计那样的简单图像处理技术，与训练模型是一样的。因为在没有智能的情况下，模型只能依靠颜色辩物。现在你或许会问，如何知道 CNN 究竟在寻找什么？答案就是，Grad-CAM。 加权梯度类激活映射（Grad-CAM） 我们在本篇博客中实现了加权梯度类激活映射。首先，我们要知道这不是唯一的解决方案。原作说， 加权梯度类激活映射 (Grad-CAM) 通过任意目标概念的梯度（比如说类别「狗」的分对数甚至是「狗」这个字），将这些知识传递到最后的卷积层进而产生一张粗略的定位图，用于凸显图像中对于预测相关概念至关重要的区域。 通俗点讲，我们只取最终卷积层的特征图，然后将该特征中的每个通道通过与该通道相关的类的梯度进行加权。这种方法只不过是输入图像如何通过每个通道对于类的重要性来激活不同的通道，最重要的是它不需要对现有架构进行任何重训练或更改。 特定类的特征空间得分就是对应类的输出值 y^c 关于特征图 A_ij 的偏导在 i 和 j 维上的特征进行全局平均池化操作。然后，我们将结果与特征图沿其通道轴 k 相乘。最后，将结果在通道维度 k 上求平均/池化。因此，特征空间的得分凸的大小是 i×j。Σ 符号用于描述池化和平均操作。 实现 为了达到本篇博客的目的，我们套用一个预训练好的 VGG 模型，并导入一些必要包开始实现代码。 我们使用 Keras 自带的 VGG16 模型。并加载一些有助于加载和处理图像的函数。 我们先初始化模型并通过命令行参数加载图片。VGG 网络只接受 (224×224×3) 大小的图片，所以我们要把图片放缩到指定大小。由于我们只通过网络传递一个图像，因此需要扩展第一个维度，将其扩展为一个大小为 1 的批量。然后，我们通过辅助函数 preprocess_input 从输入图像中减去平均 RGB 值来实现图像的归一化。 此处，我们来看看顶部预测的特征图。所以我们得到图像的预测，并给得分靠前的类做个索引。请记住，我们可以为任意类计算特征图。然后，我们可以取出 VGG16 中最后一个卷积层的输出 block5_conv3。得到的特征图大小应该是 14×14×512。 如上所述，我们计算相类输出值关于特征图的梯度。然后，我们沿着除了通道维度之外的轴对梯度进行池化操作。最后，我们用计算出的梯度值对输出特征图加权。 然后，我们沿着通道维度对加权的特征图求均值，从而得到大小为 14*14 的热力图。最后，我们对热力图进行归一化处理，以使其值在 0 和 1 之间。 最后，我们使用 OpenCV 来读图片，将获取的热力图放缩到原图大小。我们将原图和热力图混合，以将热力图叠加到图像上。 从上面的图片可以清楚地看到 CNN 在图像中寻找的是区分这些类的地方。这种技术不仅适用于定位，还可用于视觉问答、图像标注等。 此外，它在调试建立精确模型的数据需求方面非常有帮助。虽然此技术并未过多涉及调参，但我们可以使用额外的数据和数据增强技术更好地泛化模型。 
140,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739176&idx=3&sn=ac0a55022def4d23ab618b8d388c381d&chksm=871ad596b06d5c809be9c975048ba82a3bc2b1c33b35a237cc72f0e4a9d09f3e7ceda00dc8b5&scene=27,入门 | 今天是雾霾，明天是什么？马尔可夫链告诉你,"towardsdatascience 什么是马尔可夫链？什么时候应该使用它们？它们是如何运作的？ 马尔可夫链是一个相当常见、相当简单的对随机过程进行统计建模的方式。它们被应用在很多领域，从文本生成到金融建模。一个比较流行的例子是 SubredditSimulator，它使用马尔可夫链自动创建整个 subreddit 的内容。总之，马尔可夫链在概念上是非常直观，并且易于理解的，不使用任何高级的统计或者数学概念就可以实现。马尔可夫链是入门概率建模和数据科学技术的很好的开端。 简介 首先，我们用一个很常见的例子来描述它们： 试想有两种可能的天气状态：晴天或者阴天。你总是可以直接地观察当前的天气状态，而且保证是之前提及的两者之一。现在，你决定预测明天的天气。假设在这个过程中有一个潜在的转移，因为当前的天气会对第二天的天气状态有所影响。因此，作为一个敬业的人，你收集了几年的天气数据，然后计算得到阴天之后出现晴天的概率是 0.25。你还注意到，广泛地讲，阴天之后发生阴天的概率是 0.75，因为只有两种可能的天气状态。你现在可以利用这个分布，根据当地目前的天气状态去预测未来几天的天气。 这个例子描述了马尔可夫链的很多关键概念。马尔可夫链本质上是由一系列满足马尔可夫性质的转移组成，这些转换服从某种概率分布。 我们来观察一下在这个例子中，如何仅仅通过观察从当天到第二天的转换就得到概率分布。这其实说的就是马尔可夫性，即马尔可夫过程独有的让状态转移没有记忆的性质。这通常使它们无法成功地生成会出现某些期望潜在趋势的序列。例如，马尔可夫链可能根据词频来模仿一个作者的写作风格，但是它无法生成包含深层含义的文本或者蕴含某种主题意义的文本，因为这些文本都是基于更长的文本序列开发的。因此，它们缺乏生成语境相关内容的能力，因为它们无法考虑到之前的整条状态链。 模型 形式上，马尔可夫链是一个概率自动机。状态转移的概率分布通常表示为马尔可夫链的转移矩阵。如果马尔可夫链有 N 个可能的状态，那么这个转移矩阵就是 N*x*N 的矩阵，使得元素 (I, J) 代表从状态 I 转移到状态 J 的概率。此外，状态转移矩阵必须是随机矩阵，它的每一行元素之和必须是 1。这完全是能够讲得通的，因为每一行代表它自己的概率分布。 此外，马尔可夫链也会有一个初始状态向量，由一个 N x 1 的向量表示，用这个向量来描述从 N 个状态中的某个状态开始的概率分布。初始向量中的元素 I 代表该马尔可夫链从 I 状态开始的概率。 这两个实体通常就是用来描述一个马尔可夫链所需的全部内容了。 我们知道如何获得从一个状态转移到另一个状态的可能性，但是如何知道经过多个步骤后发生转移的概率呢？为了将这个也形式化，我们现在要定义在 M 个步骤中从状态 I 转移到状态 J 的概率。事实证明，这是很容易的。给定一个状态转移矩阵 P，这可以通过计算矩阵 P 的 M 次幂中的元素 (I, J) 来决定。然而，对于 M 值比较大的情况，如果您对简单的线性代数比较熟悉，更有效的方法是先将矩阵对角化，然后再计算它的 M 次幂。 结论 既然你已经了解了马尔可夫链的基本知识，现在就应该能够用你选择的语言轻松地实现它们。如果你不擅长编程，还有许多更高级的马尔可夫链和马尔可夫过程的属性可以深入研究。在我看来，马尔可夫链沿着理论路线的自然发展将是隐马尔可夫过程或 MCMC（马尔可夫链蒙特卡罗）。简单的马尔可夫链是其他更复杂的建模技术的基本组成，因此，掌握了这些知识，你现在可以去尝试更多这种主题的技术，例如信念建模和采样。 "
141,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738971&idx=3&sn=0f50eb9c8530b942c1bc66a5e0b174da&chksm=871ad565b06d5c73bf331e59311a8d052354d7c7206000e87c16d2d98fbd9f1dbc06a33882fc&scene=27,前沿 | 区块链分享医疗数据：AI学者提出新型匿名数据收集方式,选自Nature 除比特币之外，区块链匿名与安全的特性还可以帮助医疗领域的研究者们收集用于训练 AI 算法的数据，其方法不会透露数据提供者的隐私信息。Nature 近日报道了来自 UCSF 等大学的新型区块链技术。 Dexter Hadley 认为，如可用数百万张标记好的 X 光片训练筛查算法，人工智能（AI）在乳腺癌检测上或许比医生更具优势。那么研究人员如何获得如此大量的数据？很多国家从法律上保护公民隐私，因此敏感医疗信息依然是研究人员和科技公司的难以接近的地带。 为解决这个问题，加州大学旧金山分校的医学和计算生物学家 Hadley 正为此设计解决方案。他的团队正在构建一个系统，让人们可以轻松、安全地与研究人员分享自己的医疗数据，并保留对数据的控制权。他们的方法基于近来最流行的加密货币——比特币基础上的区块链技术，新方法很快就将投入实际测试中。预计在今年 5 月，Hadley 和他的同事们将开展一项研究，训练一种新的乳腺癌检测 AI 算法，因此他们希望收集 300-500 万美国妇女的乳房 X 光照片。 该团队加入了越来越多使用区块链技术的科学家和初创企业的行列——研究人员正希望以此让共享医疗图像，医疗记录和基因数据更具吸引力、也更加高效。有些项目甚至会对分享数据的用户付款。许多团队的最终目标是使用区块链系统收集所需的数据进行 AI 算法训练。 数据安全 由于基于机器学习的人工智能技术依赖于大量数据进行训练，随着 AI 的兴起，公众正在越来越关注科技巨头是如何从个人医疗信息等数据中挖掘有价值信息的。曾有新闻披露英国国家卫生服务部门（NHS）的一个分支机构在未经充分同意的情况下获得了 160 万份患者病历，包括姓名和敏感信息，例如一个人是否患有性病。在 2016 年，这一事件让谷歌母公司 Alphabet 旗下的人工智能公司 DeepMind 陷入了争议。 麻省麻省理工学院（MIT）计算机科学家 Andrew Lippman 表示：「现在，谷歌和 Facebook 收集到关于你的数据超乎想象。但医药领域没有 Facebook，」他补充道，「使用区块链来保护和分享分散的医疗信息，可能是数据身份控制的典范。」 区块链是一种分布式电子记账系统，可以记录难以改变的「区块链「中的交易。如果黑客要侵入一个区块，他必须独立地篡改所有与之相关的区块——这是一项几乎无法完成的任务。 在 Hadley 的研究中，区块链将起到开关的功能指导参与者，临床医生和研究人员之间的数据流动。参与其中的女性可以使用在线门户网站 breastwecan.org 提供或撤销其数据访问权限，breastwecan.org 依靠区块链来保护存储在云中的数据。 研究人员计划在数百万健康女性和乳腺癌患者的乳房 X 线照片上训练他们的 AI 算法，其目的是最终发展出比人类医生能够更加精确地为肿瘤分类的计算机算法。训练数据越多，算法就越准确，正如放射科医师鉴别肿瘤的能力会随着经验的增长而提高一样。 Hadley 希望女性能够分享他们的乳房 X 光片来提高乳腺癌筛查的普遍性，并借此获得超过医疗机构所掌握的信息量。根据组织密度，年龄和其他已知因素，参与研究的妇女将能够在 breastwecan.org 上查看他们的扫描结果以及对其乳腺癌风险的标准临床解释。 连锁反应 越来越多的机构正在开发区块链市场来代理个人、公司以及学术研究人员之间的数据交换，并提供付款方式。其中一项工作就是 Nebula Genomics 和哈佛大学遗传学家 George Church 共同创立的初创公司。Nebula 旨在联系能够提供基因组测序的人与愿意为该服务付费的公司，以换取访问结果数据。支付自己测序费用的人将能够使用 Nebula 出售他们的遗传信息，支付将以数字代币的形式出现并可以兑换成美元。  Nebula 会确保其合作伙伴公司所作的任何承诺，例如公司保存个人数据的时间。相比之下，当加利福尼亚州山景城的 23andMe 等基因组测序公司的客户同意分享他们的数据进行研究时，用户们很大程度上放弃了对数据使用方式的控制。许多测序公司甚至会向生物技术和制药公司出售散装匿名遗传数据。 「更好地利用医疗记录可以让我们获得更快速、更完善的治疗，」Lippman 表示。他和他的学生正在开发一种被称为 MedRec 的区块链健康记录共享系统，它将于今年在波士顿的 Beth Israel Deaconess 医疗中心进行测试。该系统允许用户将信息加入到自己的医疗记录中，其中可以包含可穿戴电子设备（如 Fitbits）上收集到的信息。临床医生和研究者可以在得到允许之后使用这些额外的数据，为人们量身定制医疗解决方案。 只有在被充分分享和研究之后，人们收集到的大量医疗数据才能让医疗领域获得进步。「我们需要吸引用户，让他们分享自己的数据，」Hadley 表示，「所以我们需要寻找能够完美掌控数据的新技术，而区块链正好是其中之一。」 
142,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738971&idx=2&sn=5751ef5e00347a7f9c6bc09f531179ac&chksm=871ad565b06d5c73d1c639bf875f8c1b5df47b83a9c4bb28f497928a95157fcce5927b64ced9&scene=27,业界 | 超越最强GPU：谷歌云TPU开放测试版实力对比评测,选自RiseML Blog Elmar Haußmann 今年 2 月 12 日，谷歌 ，价格约为每云 TPU 每小时 6.50 美元。此举意味着这种曾支持了围棋程序 AlphaGo 的芯片已可以被人们用来训练自己的机器学习模型。不过，兴奋之余我们还有疑问：TPU 和目前流行的 GPU 相比谁更强大？性价比又如何？近日，来自创业公司 RiseML 的 Elmar Haußmann 为我们进行了简单的对比评测，经过众多读者的审阅和建议，本文修正了初版的一些错误。 大多数人认为，今天的深度学习仍基于英伟达的 GPU，几乎没有可替代的相关实际选择。谷歌专门为深度学习定制的芯片张量处理单元（Tensor Processing Unit，TPU）有望改变这种情况。 首次发布九个月后，谷歌终于在谷歌云平台上向早期测试版用户发布了 TPUv2。我们进行了几个快速基准测试。下面，我们分享一下这次评测的经验和初步结果。 我们一直希望深度学习硬件市场的竞争越来越激烈，日益激烈的竞争有望打破英伟达对深度学习硬件的垄断。除此之外，这还将定义未来深度学习的基础架构。 但要记住：正如谷歌在许多地方提到的，TPU 仍处于早期测试阶段，所以我们讨论的一些事情未来可能发生变化。 谷歌云上的 TPU 第一代芯片 TPUv1 面向推断（inference），而第二代芯片的重点在于加速学习。TPUv2 的内核中有一个广泛应用于深度学习的脉动阵列，负责执行矩阵乘法。Jeff Dean 在报告中称，每个 Cloud TPU 设备具备四个「TPUv2 芯片」。每个芯片有 16GB 内存和两个内核，每个内核中有两个矩阵乘法单元。两个内核一共提供 45 万亿次浮点运算（45 TFLOPs），整个 TPU 设备可提供 180 TFLOPs、64GB 内存。而目前这一代 Nvidia V100 GPU 可提供 125 TFLOPs、16GB 内存。 要在谷歌云平台上使用 TPU，你需要启动 Cloud TPU（在获得 quota 命令后执行此操作）。无需（或无法）将 Cloud TPU 分配给特定的 VM 实例，通过网络即可发现实例中的 Cloud TPU。每个 Cloud TPU 都被分配了一个名称和 IP 地址，你需要将二者提供给 TensorFlow 代码。 TPU 仅接受 TensorFlow 1.6 版本（发行候选版）的支持。除此之外，由于与 TPU 通信所需的各种代码都由 TensorFlow 提供，所以你的 VM 实例上不需要任何驱动程序。在 TPU 上执行的代码经过优化，且通过 TensorFlow 中的 XLA 进行实时编译。 为了高效使用 TPU，你的代码应该建立在高级别的 Estimator 抽象上。然后，你可以使用 TPUEstimator 来执行必要任务，提高使用 TPU 的效率，如为 TPU 设置数据排队并在不同核上进行并行计算。当然有使用 TPUEstimator 的方法，但目前我们还不知道例子或文档。 完成所有设置后，即可正常地运行 TensorFlow 代码，你会在启动过程中发现 TPU，计算图被编译并传输至 TPU。有趣的是，TPU 可以直接从云存储中读取和写入，以存储检验点（checkpoint）或事件摘要（event summary）。为了实现这一点，你需要提供支持 Cloud TPU 写入云存储的服务账户。 基准测试 有趣的地方不在于 TPU 的速度有多快。TensorFlow 有一个 TPU 模型的 GitHub 库，效果很好。下面，我们介绍在 TPU 上运行 ResNet 和 Inception 的实验。我们想看一个没有针对 TPU 进行优化的模型是如何运作的，因此我们在 TPU 上运行一个基于 LSTM 的文本分类模型。通常，谷歌推荐使用较大的模型，而这个模型较小，所以看看这种情况下 TPU 是否仍然具有优势很有意思。 对于所有的模型，我们将单个 Cloud TPU 的训练速度与单个 Nvidia P100 和 V100 GPU 进行了对比。我们注意到，一个完全的对比除了吞吐量之外，还应包括模型的最终质量和收敛性能。我们的实验算是最初的尝试，将对未来的研究留下详细的分析。 TPU 和 P100 的实验在谷歌云平台上的 n1-standard-16 实例（16 vCPU Intel Haswell，60 GB 内存）上运行。对于 V100 GPU，我们在 AWS 上使用 p3.2xlarge 实例（8 vCPU，60 GB 内存），所有系统都运行 Ubuntu 16.04。对于 TPU，我们从 PyPi 存储库安装了 TensorFlow 1.6.0-rc1。GPU 实验则使用 nvidia-docker 运行，nvidia-docker 使用包含 CUDA 9.0 和 cuDNN 7.0 的 TensorFlow 1.5 镜像（tensorflow: 1.5.0-gpu-py3）。 TPU 优化模型 我们先来看看正式针对 TPU 进行优化的模型的性能。下图中，你可以看到它们每秒处理图像的性能。 在 ResNet-50 上，单个 Cloud TPU（包含 4 个 TPUv2 和 64GB 内存）的速度约是单个 P100 的 7.3 倍，V100 的 2.8 倍。对于 InceptionV3，加速几乎相同（分别为〜7.6 和〜2.5）。V100 具备更高精度（fp32），因此速度稍慢。注意：在 TPU 上训练精度为 fp32 的模型完全不可能，因为它只支持混合精度计算。 显然，除了速度之外，还必须考虑价格。该表显示按秒计费的按需定价的标准化性能，TPU 仍然明显领先。 自定义 LSTM 模型 我们的自定义模型是一个具备 1024 个隐藏单元的双向 LSTM 文本分类模型。LSTM 是目前 NLP 的一个基本构造块，所以这与基于计算机视觉的官方模型形成了很好的对比。 源代码已经使用了 Estimator 框架，所以调整它来使用 TPUEstimator 非常简单。但是一定要注意：在 TPU 上，我们无法让模型收敛，而在 GPU 上的相同模型（批量大小等相同）运行得很好。我们认为这是由于我们的代码或 TensorFlow 中存在 bug。由于模型无法收敛，这里我们决定不报告初步结果（因此删去了上一版中的初步结果），而是介绍我们在另一篇博文中的发现。 结论 在我们测试的模型上，TPU 与最新一代的 GPU 相比性能更好，也更加便宜。这与之前的报道（https://www.forbes.com/sites/moorinsights/2018/02/13/google-announces-expensive-cloud-tpu-availability/#d1ec931359f1）形成了鲜明对比。总之，就测试版而言。使用 TPU 和适应 TensorFlow 代码的经验已经很不错了。 我们认为，一旦 TPU 可以面向更多用户，它们就可以真正替代英伟达的 GPU。  
143,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738971&idx=4&sn=05a39148f01ab55e94cd734b5a8ec31f&chksm=871ad565b06d5c730dc892a9fe8c3e1aa714f1d12d34560ae7d34ea0e7977054a1093af48120&scene=27,教程 | 简单实用的pandas技巧：如何将内存占用降低90%,"选自DATAQUEST Josh Devlin pandas 是一个 Python 软件库，可用于数据操作和分析。数据科学博客 Dataquest.io 发布了一篇关于如何优化 pandas 内存占用的教程：仅需进行简单的数据类型转换，就能够将一个棒球比赛数据集的内存占用减少了近 90%，机器之心对本教程进行了编译介绍。 当使用 pandas 操作小规模数据（低于 100 MB）时，性能一般不是问题。而当面对更大规模的数据（100 MB 到数 GB）时，性能问题会让运行时间变得更漫长，而且会因为内存不足导致运行完全失败。   尽管 Spark 这样的工具可以处理大型数据集（100 GB 到数 TB），但要完全利用它们的能力，往往需要更加昂贵的硬件。而且和 pandas 不同，它们缺少丰富的用于高质量数据清理、探索和分析的功能集。对于中等规模的数据，我们最好能更充分地利用 pandas，而不是换成另一种工具。   在这篇文章中，我们将了解 pandas 的内存使用，以及如何只需通过为列选择合适的数据类型就能将 dataframe 的内存占用减少近 90%。 处理棒球比赛日志   我们将处理 130 年之久的美国职业棒球大联盟（MLB）比赛数据，这些数据来自 Retrosheet：http://www.retrosheet.org/gamelogs/index.html。   这些数据原来分成了 127 个不同的 CSV 文件，但我们已经使用 csvkit 合并了这些数据，并在第一行增加了列名称。如果你想下载本文所用的这个数据版本，请访问：https://data.world/dataquest/mlb-game-logs。   让我们首先导入数据，并看看其中的前五行： 下面我们总结了一些重要的列，但如果你想了解所有的列，我们也为整个数据集创建了一个数据词典：https://data.world/dataquest/mlb-game-logs/workspace/data-dictionary。 date - 比赛时间 v_name - 客队名 v_league - 客队联盟 h_name - 主队名 h_league - 主队联盟 v_score - 客队得分 h_score - 主队得分 v_line_score - 客队每局得分排列，例如： 010000(10)00. h_line_score - 主队每局得分排列，例如： 010000(10)0X. park_id - 比赛举办的球场名 attendance- 比赛观众   我们可以使用 DataFrame.info() 方法为我们提供关于 dataframe 的高层面信息，包括它的大小、数据类型的信息和内存使用情况。 默认情况下，pandas 会近似 dataframe 的内存用量以节省时间。因为我们也关心准确度，所以我们将 memory_usage 参数设置为 'deep'，以便得到准确的数字。 我们可以看到，我们有 171,907 行和 161 列。pandas 会自动为我们检测数据类型，发现其中有 83 列数据是数值，78 列是 object。object 是指有字符串或包含混合数据类型的情况。 为了更好地理解如何减少内存用量，让我们看看 pandas 是如何将数据存储在内存中的。 dataframe 的内部表示   在 pandas 内部，同样数据类型的列会组织成同一个值块（blocks of values）。这里给出了一个示例，说明了 pandas 对我们的 dataframe 的前 12 列的存储方式。 你可以看到这些块并没有保留原有的列名称。这是因为这些块为存储 dataframe 中的实际值进行了优化。pandas 的 BlockManager 类则负责保留行列索引与实际块之间的映射关系。它可以作为一个 API 使用，提供了对底层数据的访问。不管我们何时选择、编辑或删除这些值，dataframe 类和 BlockManager 类的接口都会将我们的请求翻译成函数和方法的调用。   在 pandas.core.internals 模块中，每一种类型都有一个专门的类。pandas 使用 ObjectBlock 类来表示包含字符串列的块，用 FloatBlock 类表示包含浮点数列的块。对于表示整型数和浮点数这些数值的块，pandas 会将这些列组合起来，存储成 NumPy ndarray。NumPy ndarray 是围绕 C 语言的数组构建的，其中的值存储在内存的连续块中。这种存储方案使得对值的访问速度非常快。   因为每种数据类型都是分开存储的，所以我们将检查不同数据类型的内存使用情况。首先，我们先来看看各个数据类型的平均内存用量。 看能否改进数值列的内存用量。 理解子类型（subtype）   正如我们前面简单提到的那样，pandas 内部将数值表示为 NumPy ndarrays，并将它们存储在内存的连续块中。这种存储模式占用的空间更少，而且也让我们可以快速访问这些值。因为 pandas 表示同一类型的每个值时都使用同样的字节数，而 NumPy ndarray 可以存储值的数量，所以 pandas 可以快速准确地返回一个数值列所消耗的字节数。   pandas 中的许多类型都有多个子类型，这些子类型可以使用更少的字节来表示每个值。比如说 float 类型就包含 float16、float32 和 float64 子类型。类型名称中的数字就代表该类型表示值的位（bit）数。比如说，我们刚刚列出的子类型就分别使用了 2、4、8、16 个字节。下面的表格给出了 pandas 中最常用类型的子类型：   一个 int8 类型的值使用 1 个字节的存储空间，可以表示 256（2^8）个二进制数。这意味着我们可以使用这个子类型来表示从 -128 到 127（包括 0）的所有整数值。   我们可以使用 numpy.iinfo 类来验证每个整型数子类型的最大值和最小值。举个例子： 这里我们可以看到 uint（无符号整型）和 int（有符号整型）之间的差异。这两种类型都有一样的存储能力，但其中一个只保存 0 和正数。无符号整型让我们可以更有效地处理只有正数值的列。   使用子类型优化数值列   我们可以使用函数 pd.to_numeric() 来对我们的数值类型进行 downcast（向下转型）操作。我们会使用 DataFrame.select_dtypes 来选择整型列，然后我们会对其数据类型进行优化，并比较内存用量。 def mem_usage (pandas_obj) 我们可以看到内存用量从 7.9 MB 下降到了 1.5 MB，降低了 80% 以上。但这对我们原有 dataframe 的影响并不大，因为其中的整型列非常少。   让我们对其中的浮点型列进行一样的操作。 我们可以看到浮点型列的数据类型从 float64 变成了 float32，让内存用量降低了 50%。   让我们为原始 dataframe 创建一个副本，并用这些优化后的列替换原来的列，然后看看我们现在的整体内存用量。 尽管我们极大地减少了数值列的内存用量，但整体的内存用量仅减少了 7%。我们的大部分收获都将来自对 object 类型的优化。   在我们开始行动之前，先看看 pandas 中字符串的存储方式与数值类型的存储方式的比较。 数值存储与字符串存储的比较   object 类型表示使用 Python 字符串对象的值，部分原因是 NumPy 不支持缺失（missing）字符串类型。因为 Python 是一种高级的解释性语言，它对内存中存储的值没有细粒度的控制能力。   这一限制导致字符串的存储方式很碎片化，从而会消耗更多内存，而且访问速度也更慢。object 列中的每个元素实际上都是一个指针，包含了实际值在内存中的位置的「地址」。   下面这幅图给出了以 NumPy 数据类型存储数值数据和使用 Python 内置类型存储字符串数据的方式。   在前面的表格中，你可能已经注意到 object 类型的内存使用是可变的。尽管每个指针仅占用 1 字节的内存，但如果每个字符串在 Python 中都是单独存储的，那就会占用实际字符串那么大的空间。我们可以使用 sys.getsizeof() 函数来证明这一点，首先查看单个的字符串，然后查看 pandas series 中的项。 你可以看到，当存储在 pandas series 时，字符串的大小与用 Python 单独存储的字符串的大小是一样的。   使用 Categoricals 优化 object 类型   pandas 在 0.15 版引入了 Categorials。category 类型在底层使用了整型值来表示一个列中的值，而不是使用原始值。pandas 使用一个单独的映射词典将这些整型值映射到原始值。只要当一个列包含有限的值的集合时，这种方法就很有用。当我们将一列转换成 category dtype 时，pandas 就使用最节省空间的 int 子类型来表示该列中的所有不同值。 为了了解为什么我们可以使用这种类型来减少内存用量，让我们看看我们的 object 类型中每种类型的不同值的数量。 大概看看就能发现，对于我们整个数据集的 172,000 场比赛，其中不同（unique）值的数量可以说非常少。   为了了解当我们将其转换成 categorical 类型时究竟发生了什么，我们拿出一个 object 列来看看。我们将使用数据集的第二列 day_of_week.   看看上表，可以看到其仅包含 7 个不同的值。我们将使用 .astype() 方法将其转换成 categorical 类型。 如你所见，除了这一列的类型发生了改变之外，数据看起来还是完全一样。让我们看看这背后发生了什么。   在下面的代码中，我们使用了 Series.cat.codes 属性来返回 category 类型用来表示每个值的整型值。 你可以看到每个不同值都被分配了一个整型值，而该列现在的基本数据类型是 int8。这一列没有任何缺失值，但就算有，category 子类型也能处理，只需将其设置为 -1 即可。   最后，让我们看看在将这一列转换为 category 类型前后的内存用量对比。 9.8 MB 的内存用量减少到了 0.16 MB，减少了 98%！注意，这个特定列可能代表了我们最好的情况之一——即大约 172,000 项却只有 7 个不同的值。   尽管将所有列都转换成这种类型听起来很吸引人，但了解其中的取舍也很重要。最大的坏处是无法执行数值计算。如果没有首先将其转换成数值 dtype，那么我们就无法对 category 列进行算术运算，也就是说无法使用 Series.min() 和 Series.max() 等方法。   我们应该坚持主要将 category 类型用于不同值的数量少于值的总数量的 50% 的 object 列。如果一列中的所有值都是不同的，那么 category 类型所使用的内存将会更多。因为这一列不仅要存储所有的原始字符串值，还要额外存储它们的整型值代码。你可以在 pandas 文档中了解 category 类型的局限性：http://pandas.pydata.org/pandas-docs/stable/categorical.html。   我们将编写一个循环函数来迭代式地检查每一 object 列中不同值的数量是否少于 50%；如果是，就将其转换成 category 类型。 和之前一样进行比较： 在这个案例中，所有的 object 列都被转换成了 category 类型，但并非所有数据集都是如此，所以你应该使用上面的流程进行检查。   object 列的内存用量从 752MB 减少到了 52MB，减少了 93%。让我们将其与我们 dataframe 的其它部分结合起来，看看从最初 861MB 的基础上实现了多少进步。 Wow，进展真是不错！我们还可以执行另一项优化——如果你记得前面给出的数据类型表，你知道还有一个 datetime 类型。这个数据集的第一列就可以使用这个类型。 你可能记得这一列开始是一个整型，现在已经优化成了 unint32 类型。因此，将其转换成 datetime 类型实际上会让内存用量翻倍，因为 datetime 类型是 64 位的。将其转换成 datetime 类型是有价值的，因为这让我们可以更好地进行时间序列分析。   pandas.to_datetime() 函数可以帮我们完成这种转换，使用其 format 参数将我们的日期数据存储成 YYYY-MM-DD 形式。 在读入数据的同时选择类型   现在，我们已经探索了减少现有 dataframe 的内存占用的方法。通过首先读入 dataframe，然后在这个过程中迭代以减少内存占用，我们了解了每种优化方法可以带来的内存减省量。但是正如我们前面提到的一样，我们往往没有足够的内存来表示数据集中的所有值。如果我们一开始甚至无法创建 dataframe，我们又可以怎样应用节省内存的技术呢？   幸运的是，我们可以在读入数据的同时指定最优的列类型。pandas.read_csv() 函数有几个不同的参数让我们可以做到这一点。dtype 参数接受具有（字符串）列名称作为键值（key）以及 NumPy 类型 object 作为值的词典。   首先，我们可将每一列的最终类型存储在一个词典中，其中键值表示列名称，首先移除日期列，因为日期列需要不同的处理方式。 现在我们可以使用这个词典了，另外还有几个参数可用于按正确的类型读入日期，而且仅需几行代码： 通过优化这些列，我们成功将 pandas 的内存占用从 861.6MB 减少到了 104.28MB——减少了惊人的 88%！   分析棒球比赛   现在我们已经优化好了我们的数据，我们可以执行一些分析了。让我们先从了解这些比赛的日期分布开始。 我们可以看到在 1920 年代以前，星期日的棒球比赛很少，但在上个世纪后半叶就变得越来越多了。   我们也可以清楚地看到过去 50 年来，比赛的日期分布基本上没什么大变化了。 让我们再看看比赛时长的变化情况： 从 1940 年代以来，棒球比赛的持续时间越来越长。 总结和下一步   我们已经了解了 pandas 使用不同数据类型的方法，然后我们使用这种知识将一个 pandas dataframe 的内存用量减少了近 90%，而且也仅使用了一些简单的技术：   将数值列向下转换成更高效的类型 将字符串列转换成 categorical 类型   如果你还想使用 pandas 处理更大规模的数据，可以参与这个交互式课程：https://www.dataquest.io/m/163/optimizing-dataframe-memory-footprint/16/next-steps。 "
144,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738971&idx=5&sn=e255e825067bdcee9fef63096674c1f6&chksm=871ad565b06d5c73ca78071bf2bd87a661e3270871da5293ebe784888c22ad706eee02ffd3fb&scene=27,ICASSP 2018 | 阿里巴巴Oral论文：用于语音合成的深度前馈序列记忆网络,"语音领域的顶会 ICASSP 2018 将于 4 月 15-20 日在加拿大阿尔伯塔卡尔加里市举行。据机器之心了解，国内科技巨头阿里巴巴语音交互智能团队有 5 篇论文被此大会接收。本文介绍了其中一篇 oral 论文《Deep Feed-Forward Sequential Memory Network for Speech Synthesis》。 论文标题： 论文地址：https://arxiv.org/pdf/1802.09194.pdf 摘要 我们提出了一种基于深度前馈序列记忆网络的语音合成系统。该系统在达到与基于双向长短时记忆单元的语音合成系统一致的主观听感的同时，模型大小只有后者的四分之一，且合成速度是后者的四倍，非常适合于对内存占用和计算效率非常敏感的端上产品环境。 研究背景 语音合成系统主要分为两类，拼接合成系统和参数合成系统。其中参数合成系统在引入了神经网络作为模型之后，合成质量和自然度都获得了长足的进步。另一方面，物联网设备（例如智能音箱和智能电视）的大量普及也对在设备上部署的参数合成系统提出了计算资源的限制和实时率的要求。本工作引入的深度前馈序列记忆网络可以在保持合成质量的同时，有效降低计算量，提高合成速度。   我们使用基于双向长短时记忆单元（BLSTM）的统计参数语音合成系统作为基线系统。与其他现代统计参数语音合成系统相似，我们提出的基于深度前馈序列记忆网络（DFSMN）的统计参数语音合成系统也是由 3 个主要部分组成，声音合成器（vocoder），前端模块和后端模块，如上图所示。我们使用开源工具 WORLD 作为我们的声音合成器，用来在模型训练时从原始语音波形中提取频谱信息、基频的对数、频带周期特征（BAP）和清浊音标记，也用来在语音合成时完成从声学参数到实际声音的转换。前端模块用来对输入的文本进行正则化和词法分析，我们把这些语言学特征编码后作为神经网络训练的输入。后端模块用来建立从输入的语言学特征到声学参数的映射，在我们的系统中，我们使用 DFSMN 作为后端模块。 深度前馈序列记忆网络 紧凑前馈序列记忆网络（cFSMN）作为标准的前馈序列记忆网络（FSMN）的改进版本，在网络结构中引入了低秩矩阵分解，这种改进简化了 FSMN，减少了模型的参数量，并加速了模型的训练和预测过程。   上图给出了 cFSMN 的结构的图示。对于神经网络的每一个 cFSMN 层，计算过程可表示成以下步骤①经过一个线性映射，把上一层的输出映射到一个低维向量②记忆模块执行计算，计算当前帧之前和之后的若干帧和当前帧的低维向量的逐维加权和③把该加权和再经过一个仿射变换和一个非线性函数，得到当前层的输出。三个步骤可依次表示成如下公式。   与循环神经网络（RNNs，包括 BLSTM）类似，通过调整记忆模块的阶数，cFSMN 有能力捕捉序列的长程信息。另一方面，cFSMN 可以直接通过反向传播算法（BP）进行训练，与必须使用沿时间反向传播算法（BPTT）进行训练的 RNNs 相比，训练 cFSMN 速度更快，且较不容易受到梯度消失的影响。 对 cFSMN 进一步改进，我们得到了深度前馈序列记忆网络（DFSMN）。DFSMN 利用了在各类深度神经网络中被广泛使用的跳跃连接（skip-connections）技术，使得执行反向传播算法的时候，梯度可以绕过非线性变换，即使堆叠了更多 DFSMN 层，网络也能快速且正确地收敛。对于 DFSMN 模型，增加深度的好处有两个方面。一方面，更深的网络一般来说具有更强的表征能力，另一方面，增加深度可以间接地增大 DFSMN 模型预测当前帧的输出时可以利用的上下文长度，这在直观上非常有利于捕捉序列的长程信息。具体来说，我们把跳跃连接添加到了相邻两层的记忆模块之间，如下面公式所示。由于 DFSMN 各层的记忆模块的维数相同，跳跃连接可由恒等变换实现。   我们可以认为 DFSMN 是一种非常灵活的模型。当输入序列很短，或者对预测延时要求较高的时候，可以使用较小的记忆模块阶数，在这种情况下只有当前帧附近帧的信息被用来预测当前帧的输出。而如果输入序列很长，或者在预测延时不是那么重要的场景中，可以使用较大的记忆模块阶数，那么序列的长程信息就能被有效利用和建模，从而有利于提高模型的性能。 除了阶数之外，我们为 DFSMN 的记忆模块增加了另一个超参数，步长（stride），用来表示记忆模块提取过去或未来帧的信息时，跳过多少相邻的帧。这是有依据的，因为与语音识别任务相比，语音合成任务相邻帧之间的重合部分甚至更多。   上文已经提到，除了直接增加各层的记忆模块的阶数之外，增加模型的深度也能间接增加预测当前帧的输出时模型可以利用的上下文的长度，上图给出了一个例子。 实验 在实验阶段，我们使用的是一个由男性朗读的中文小说数据集。我们把数据集划分成两部分，其中训练集包括 38600 句朗读（大约为 83 小时），验证集包括 1400 句朗读（大约为 3 小时）。所有的语音数据采样率都为 16k 赫兹，每帧帧长为 25 毫秒，帧移为 5 毫秒。我们使用 WORLD 声音合成器逐帧提取声学参数，包括 60 维梅尔倒谱系数，3 维基频的对数，11 维 BAP 特征以及 1 维清浊音标记。我们使用上述四组特征作为神经网络训练的四个目标，进行多目标训练。前端模块提取出的语言学特征，共计 754 维，作为神经网络训练的输入。 我们对比的基线系统是基于一个强大的 BLSTM 模型，该模型由底层的 1 个全连接层和上层的 3 个 BLSTM 层组成，其中全连接层包含 2048 个单元，BLSTM 层包含 2048 个记忆单元。该模型通过沿时间反向传播算法（BPTT）训练，而我们的 DFSMN 模型通过标准的反向传播算法（BP）训练。包括基线系统在内，我们的模型均通过逐块模型更新过滤算法（BMUF）在 2 块 GPU 上训练。我们使用多目标帧级别均方误差（MSE）作为训练目标。   所有的 DFSMN 模型均由底层的若干 DFSMN 层和上的 2 个全连接层组成，每个 DFSMN 层包含 2048 个结点和 512 个投影结点，而每个全连接层包含 2048 个结点。在上图中，第三列表示该模型由几层 DFSMN 层和几层全连接层组成，第四列表示该模型 DFSMN 层的记忆模块的阶数和步长。由于这是 FSMN 这一类模型首次应用在语音合成任务中，因此我们的实验从一个深度浅且阶数小的模型，即模型 A 开始（注意只有模型 A 的步长为 1，因为我们发现步长为 2 始终稍好于步长为 1 的相应模型）。从系统 A 到系统 D，我们在固定 DFSMN 层数为 3 的同时逐渐增加阶数。从系统 D 到系统 F，我们在固定阶数和步长为 10,10,2,2 的同时逐渐增加层数。从系统 F 到系统 I，我们固定 DFSMN 层数为 10 并再次逐渐增加阶数。在上述一系列实验中，随着 DFSMN 模型深度和阶数的增加，客观指标逐渐降低（越低越好），这一趋势非常明显，且系统 H 的客观指标超过了 BLSTM 基线。   另一方面，我们也做了平均主观得分（MOS）测试（越高越好），测试结果如上图所示。主观测试是通过付费众包平台，由 40 个母语为中文的测试人员完成的。在主观测试中，每个系统生成了 20 句集外合成语音，每句合成语音由 10 个不同的测试人员独立评价。在平均主观得分的测试结果表明，从系统 A 到系统 E，主观听感自然度逐渐提高，且系统 E 达到了与 BLSTM 基线系统一致的水平。但是，尽管后续系统客观指标持续提高，主观指标只是在系统 E 得分的上下波动，没有进一步提高。 结论 根据上述主客观测试，我们得到的结论是，历史和未来信息各捕捉 120 帧（600 毫秒）是语音合成声学模型建模所需要的上下文长度的上限，更多的上下文信息对合成结果没有直接帮助。与 BLSTM 基线系统相比，我们提出的 DFSMN 系统可以在获得与基线系统一致的主观听感的同时，模型大小只有基线系统的 1/4，预测速度则是基线系统的 4 倍，这使得该系统非常适合于对内存占用和计算效率要求很高的端上产品环境，例如在各类物联网设备上部署。  "
145,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738971&idx=1&sn=64cd488d7b5fd597ad0a6be23f22ee4d&chksm=871ad565b06d5c73609fa643f0e6f8f43516cc2307d0842db8c02ce08ff435d38a364b3ec750&scene=27,谁来治好AI的「幻觉」？面对众多对抗样本攻击，深度神经网络该何去何从,"选自Wired 2 月 3 日，来自 MIT、UC Berkeley 的 Athalye 等人宣布其研究攻破了 ICLR 2018 大会的接收论文中的 7 篇有关防御对抗样本的研究。之前，轻微扰动导致停车标志被无视、把熊猫认成长臂猿、把校车认成鸵鸟等等各种案例层出不穷。那么关于 AI 容易被「幻觉」干扰的现象，研究者又有什么看法？深度神经网络该何去何从？Geoffrey Hinton 的 Capsule 能够解决这一问题吗？ 科技公司正在借助机器学习的强大能力，将人工智能推向世界的每个角落。但是令人激动不已的深度神经网络有一个很大的弱点：轻微变动图像、文本或语音数据就可以欺骗这些系统，造成感知误判。 这对机器学习产品是一个大问题，尤其是在视觉方面，比如自动驾驶汽车。研究人员正努力应对上述问题——但结果证明这很有难度。 案例：今年一月，人工智能顶级会议 ICLR 2018 公布了 11 篇将在 4 月份会议上展示的新论文，全部是关于如何防御或检测对抗性攻击的。仅仅在三天之后， ，其中不乏大机构的论文，比如谷歌、亚马逊、斯坦福。Athalye 说：「一个有创意的攻击者总能绕过这些防御」。该项目由 Athalye 与 Nicholas Carlini、David Wagner 共同完成，后两者分别是伯克利的一名毕业生和教授。 学界对这个三人小组项目的特定细节进行了反复探讨。但有一点几乎没有异议：目前还不清楚如何保护基于深度神经网络的产品（比如消费品和自动驾驶）免受「幻觉」（hallucination）的侵袭。「所有这些系统都很脆弱，机器学习社区没有评估安全性的方法论。」Battista Biggio 说道，他是意大利卡利亚里大学的助理教授，用大约十年时间思考机器学习的安全问题。 人类读者可轻松识别 Athalye 创建的下图，图片上是两个正在滑雪的男人。但是在周四上午的测试中，谷歌的 Cloud Vision 服务却认为 91% 的概率是一只狗。类似的还有如何使停车标志不可见，或者人类认为内容良性的语音却被软件转录为「好的谷歌，浏览不良网站」（okay google, browse to evil.com）。 「目前，此类攻击只发生在实验室中，而没有公开测试。但是我们仍需严肃对待」，加州大学伯克利分校博士后 Bo Li 说。自动驾驶汽车的视觉系统和能够执行消费的语音助手、过滤网络不良内容的机器学习系统都必须是可靠的。「这实际上非常危险。」Li 认为，她去年参与了「在停车标志上粘贴纸从而使机器学习系统无法识别」的研究。 Athalye 及其合作者实验过的论文就包括 Li 作为共同作者撰写的一篇论文。她和加州大学伯克利分校的同仁介绍了一种分析对抗攻击的方法，该方法可用于检测对抗攻击。Li 辩证地看待 Athalye 关于防御可攻破的项目，认为此类反馈可以帮助研究者进步。「他们的攻击表明我们还需要考虑很多问题。」Li 说道。 Yang Song，Athalye 分析中涉及的一篇斯坦福研究的一作，拒绝对此进行评价，他的这篇论文仍在接受另一个重要会议的审核。Zachary Lipton，卡内基梅隆大学教授，也是另一篇论文的共同作者（作者还包括来自亚马逊的研究者），称他尚未仔细看 Athalye 的分析，但是所有现有防御可被规避是合理的。谷歌拒绝对此分析作出评价，称计划更新其 Cloud Vision 服务，以抵御此类攻击。 要想更好抵御此类攻击，机器学习研究者可能需要更加严苛。Athalye 和 Biggio 认为应该应该借鉴安防研究的实践经验，更加严谨地测试新防御技术。「机器学习研究者倾向于信任彼此，」Biggio 说道，「而安全问题恰恰相反，你必须一直对可能性保持警惕。」 上个月来自 AI 和国家安全研究者的一份报告给出了类似的建议。该报告建议机器学习研究者更多地考虑他们所创造的技术被错误使用／利用的情况。 防御对抗攻击可能对一些 AI 系统来讲较为简单。Biggio 称，用于检测恶性软件的学习系统更易增强鲁棒性，原因之一是恶性软件是功能性的，限制了其变化程度；保护计算机视觉系统更加困难，因为自然世界变化万千，图像包含那么多像素。 解决该问题（该问题对自动驾驶汽车的设计者也是一个挑战）可能需要更加彻底地反思机器学习技术。「我认为基本问题在于深度神经网络与人脑的巨大差异。」Li 说道。 人类无法对感官欺骗完全免疫。我们会被视错觉蒙蔽，谷歌近期发布的一篇论文创建了一张图像，既可以欺骗机器，也能够迷惑人类（在不到 1/10 秒时间内看到该图像的人错把猫认成了狗）。但是我们在解析图像时看到的不止是像素模式，还要考虑图像不同组件之间的关系，如人脸的特征，Li 说道。 谷歌最杰出的机器学习研究者 Geoff Hinton  。他认为 capsule network 这种新方法允许机器学习从少量图像中识别物体，而不是从数千张图像种学习。Li 认为具备更接近人类视角的机器应该会更少地受到幻觉影响。她和加州大学伯克利分校的同仁已与神经学家和生物学家展开合作，尝试从大自然中获取启发。  https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/ "
146,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738943&idx=3&sn=358c6dab3737bede0c7181d3ec269576&chksm=871ad481b06d5d97ff255eab56537616eb8cd7ef410a6130f48f01526ebc4b0a0c09df562386&scene=27,资源 | 图像配对数据集TTL：展现人类和机器判断图像相似性的差异,"选自arXiv Amir Rosenfeld等 人类对图像相似性的知觉判断依赖于丰富的内部表征，现有的计算机视觉技术应用的信号类型可能过于狭隘。本文介绍了新型图像配对数据集 TTL，该数据集收集了很多人类在视觉上认为很相似的图像，而深度学习模型无法通过特征提取重构出相似的配对。该结果为未来的图像表征研究指出了新的方向。 数据集地址：https://sites.google.com/view/totally-looks-like-dataset 人类对图像的感知远远超出了物体、形状、纹理和轮廓这些因素。人们看到一个场景时通常会唤醒和当前场景在总体特征或关系上类似的其它场景。这种特性的实现依赖于大脑中的图像空间的丰富表征，包括场景结构、语义以及使用观察场景的表征来唤醒海量存储记忆中相似场景的机制。虽然尚未被完全理解，但人类的大脑的记忆容量是相当惊人的 [1,2]。对于近期深度学习在计算机视觉所有领域（包括图像检索和对比 [6]）的爆炸式发展 [3,4,5]，人们可能会认为计算机视觉的表征能力已经接近甚至超越了人类。为了探索这个问题，本文的研究测试了深度神经网络在一个新数据集（Totally-Looks-Like，TTL）的图像对上的相似性判断行为。如图 1 所示。 该数据集基于一个娱乐性的网站，用户可以发布一对认为很相似的图片，并让网友发表看法。这些图片通常在低层特征上的相似性是很低的。这些图像对的类型包括（但不限于）多种画风的物体、场景、模式、动物和人脸，有素描、卡通以及自然图像。网站上还有用户评级功能（「赞」或「踩」），展示了网友对此图像对的相似性同意度。虽然该数据集规模不是很大，但其中图像的多样性和复杂度隐含地捕捉到了人类对图像相似性感知的很多层面。 网站链接：http://memebase.cheezburger.com/totallylookslike 作者以图像检索任务的形式，评估了多个当前最佳模型在该数据集上的表现，并将结果与人类的相似性判断行为进行了对比。该研究不仅构成了特征评估的一类新基准，并且揭示了当前深度学习表征方法的具体弱点，为未来研究指出了新的方向。作者还实施了人类评估实验来验证收集数据的一致性。虽然在一些实验中为深度学习模型设置了很好的条件，它们仍然无法正确地重构出人类选择的匹配图像。 论文：Totally Looks Like - How Humans Compare, Compared to Machines 论文地址：https://arxiv.org/abs/1803.01485 摘要： 人类对图像相似性的知觉判断依赖于丰富的内部表征，包括低级特征、高级特征、场景特性，甚至文化联想等。试图解释知觉相似性的已有方法和数据集使用的刺激信号并没有覆盖影响人类判断的所有因素。我们在这里介绍基于一个娱乐性网站构建的新数据集 Totally-Looks-Like（TTL），该数据集收集了很多人类在视觉上认为很相似的图像，其中包含了网站上采集的 6016 个图像对，拥有对人类而言足够的多样性和复杂度。我们做了实验试图从当前最佳的深度卷积神经网络提取的特征重构图像对，还做了人类判断实验以验证收集数据的一致性。虽然在一些实验中人工地为深度学习模型设置了很好的条件，但结果表明它们仍然无法通过提取的特征正确地重构和人类选择的匹配图像。我们讨论和分析了这些结果，为未来的图像表征研究指出了新的方向。  "
147,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738943&idx=5&sn=da81e236bdc102da3ba8c5d8ee4b0cac&chksm=871ad481b06d5d973c1c0c7c92f8e4b0b360938671c7df5876befd8b73ffb5e56951576c97b7&scene=27,机器之心「AI00」二月榜单：张钹院士担任首席科学家的深醒科技,"恭喜机器之心 AI00 金融领域上榜公司  。据不完全统计，AI00榜单中已有API.ai、Maluuba、Kitt.ai、KUKA Robotics、Mobileye、nuTonomy等公司被高价收购。 We believe AI should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible. －OpenAI 这不仅是一份榜单，更是一个开源项目，主要基于以下几点： 人工智能是一个复杂庞大的体系，涉及众多学科，也关乎技术、产品、行业和资本等众多要素，报告的写作团队只代表他们的专业观点，有自己的局限性，需要更多行业专家参与进来加以修正和完善。 人工智能技术和行业的发展瞬息万变，而报告的制作周期较长，其中的内容和数据势必会落后于行业的最新进展，无法同时满足时效性和高质量的要求。而领域内参与者的及时更新可以解决这个问题。 我们深刻地理解在没有专业用户反馈的情况下所做出报告的质量局限性，所以希望用工程界「Agile Development」的理念来对待我们的报告，不断收集专业反馈来持续提升报告质量。 人工智能是一个永恒命题，我们不仅会把「100 家公司」这个主题持续做下去，还会陆续开展其他主题。这个过程需要人工智能领域不同的参与者加入进来。 向 OpenAI 、「斯坦福人工智能百年研究」和「Open Source」致敬。 为此，我们邀请人工智能领域的科学家、技术专家、产业专家、专业投资人和读者加入进来，共同完成这项人工智能的长期研究。 如果你对「AI00」感兴趣，可在公众号对话框回复「AI00」（注：字母 AI 加数字 00）查看本开源项目的具体参与方式。 「AI00」第一期的榜单得到了来自投资界、人工智能产业界和学界的众多反馈，我们也依此对榜单的信息作出了一些订正和调整。在前几期的榜单中，我们先后增加了 Waymo、Neurala、Graphcore、云从科技、Citadel 与 Petuum 、竹间智能、Gamalon 和 DataRobot 等公司，同时也移除了一些公司。 在最新一期的 AI00 榜单中，我们加入了专注于机器视觉和深度学习技术的深醒科技。 公司官网： http://www.sensingtech.com.cn/ 深醒科技以深度学习、机器视觉为核心技术，拥有国内自主知识产权的系列智能算法，致力于将人工智能技术应用于安防、金融、教育、医疗、交通等各个领域。据介绍，目前深醒科技已经在全国 15 个省、30 多个城市、上千个点实现了技术落地，实战环境下识别准确率在 90% 以上、实验室识别准确率在 99.99% 以上。 在研发实力上，深醒科技拥有一支国内外机器视觉、人工智能领域顶尖的核心算法团队，由中国科学院院士、清华大学教授、计算机科学专家张钹担任首席科学家，多位科学院、工程院院士担任技术顾问，主要成员由清华大学和北京航空航天大学的博士、硕士组成。   目前深醒科技已经完成亿元级别的 A 轮融资，由昆仲资本、经纬中国双领投，清科创投等机构跟投。 以下为 AI00 二月榜单： 搜狗 中国 人工智能综合研究 搜索引擎、知识图谱、输入法、语音交互产品等 估值约50亿美元 美国 自然语言处理 个性化智能助理 三轮融资共获得 3.4 亿美元 中国 语音技术和自然语言处理 智能家居、车载、电信等行业解决方案 市值约 393 亿人民币 中国 智能语音交互和自然对话 车载、智能家居和智能机器人等智能硬件的语音交互服务 B 轮：2 亿人民币 中 / 美 声源分离、声音增强、声纹识别、麦克风阵列 会议转录，通讯，机器人，智能家居，虚拟现实，增强现实，混合现实 未透露 中国 情感对话机器人、语音情感技术、多模态情感识别 竹间个人助理机器人小影、金融机器人、客服机器人 2500万美元融资 美国 语音识别技术、自然语言处理技术（NLP） 电话语言反馈、预测销售结果、自动信息检索 1400万美元的A轮融资 美国 智能客服 理解和分类用户请求 1200万美元的A轮融资 美国 计算机视觉 图像及视频识别 API B 轮： 3000 万美元 美国 计算机视觉和深度学习 实时面部表情分析和情绪识别解决方案 四轮融资共获得 3372 万美元 新加坡 计算机视觉、视觉搜索、图像识别 电子商务、移动商务、 在线广告等图像识别解决方案 两轮融资共获得 1400 万美元 美国 计算机视觉、深度学习和数据科学 分析海量卫星图像，用于经济趋势分析和公益研究 C轮融资共 5千万美元 美国 计算机视觉和数据科学 将卫星图像识别用于农业、城市规划和灾害响应等 2015 年 5 月融资 1830 万美元 美国 计算机视觉和深度学习 通过 DLFP 平台为农业提供数据分析和预测的解决方案 B轮 3千万美元 中国 计算机视觉和深度学习 人脸识别、危险品识别、行为检测、车辆检测等的安防监控系统 4.1亿美元最新融资 中国 计算机视觉和深度学习 Face++ 人脸识别云服务平台、Image++ 图像识别平台、VisionHacker 移动游戏工作室 C轮4.6亿美元融资 中国 计算机视觉和深度学习 基于图像理解的信息获取和人机交互服务 3.8亿元C轮融资 中国 计算机视觉和深度学习 图像识别、视频鉴黄、智能审核、图片增值等云服务 新一轮千万美元融资 波士顿 深度学习、计算机视觉 帮助机器人和智能设备学习和适应环境的软件 A 轮融资约1400 万美元 中国 人脸识别、计算机视觉 金融机构人脸识别应用、公安系统实时布控、追逃等 B 轮 5亿人民币 中国 计算机视觉、人脸识别 智能安防、智慧金融、智慧楼宇等领域 A 轮亿元级 公司 美国 深度学习芯片架构 DPU (Dataflow Processing Unit) 3轮融资共5900万美元 美国 人工智能芯片技术 用于机器学习的第二代神经网络软件框架 CDNN2 纳斯达克上市，市值 9.12 亿美元 美国 基于 FPGA，针对于服务器端的高性能深度学习平台 被移动设备直接嵌入的深度学习模块 未透露 中国 深度学习 中国首款神经网络处理器 1亿美元 A 轮融资 中国 深度学习 DPU 平台 深度学习 DPU 平台 A+轮约4000万美元融资 英国 深度学习硬件和软件开发 开源软件框架 Poplar 和「智能处理器」IPU A 轮融资 3000 万美元 Groq 美国 深度学习硬件、芯片 暂无信息 1030 万美元融资 瑞士 机器人及自动化技术 工业机器人、智能设备 468.95 亿美元市值 日本 机器人及自动化技术 工业机器人 4.4兆日元市值 德国 机器人及自动化技术 工业机器人 美的 272 亿美元拿下库卡 94.55% 的股份 美国 机器人、人机交互 智能机器人 1800万美元新一轮融资 丹麦 机器人及自动化技术 工业机器人 未透露 美国 计算机视觉、机器人技术 无人机，软件服务 五轮融资超 1 亿 2 千 600 万美元 日本 可穿戴设备 医疗助理机器人 2083.56 亿日元市值 美国 计算机视觉、机器人技术 电子产品，家用机器人 27亿美元市值 德国 计算机视觉、机器人技术 代步机器人，残障专用智能设备 2000万美元市值 英国 智能机器，自动视觉定位及室内地图构建 清洁机器人 未透露 中国 机器人及自动化技术 工业机器人和行业解决方案 约 289 亿市值 中国 机器人 工业机器人、智能装备和行业解决方案 未透露 中国 计算机视觉、无人机控制、环境及障碍感知、视觉跟随、自动寻路 无人机航拍和图像传输 估值约100亿美元 美国 可以接入机器人的智能模块，工厂、仓库等自动化技术 智能设备 种子轮 700 万美元融资 Arterys 美国 深度学习系统生成医疗图像 深度学习分析系统 Arterys System 1200 万美元 A 轮融资 美国 深度学习、大数据、图像检测 癌症检测系统 三轮融资共 1500 万美元 VoxelCloud（体素科技） 美国 深度学习 医疗影像分析云服务 千万美金 A 轮融资 美国 深度学习和大数据 癌症诊疗 1000万美元最新融资 美国 大数据和机器学习技术 通过数据分析为放射肿瘤学家提供临床决策支持，用于个性化医疗。 两轮融资共 132 万美元 美国 深度学习 药物发现 4500万美元A轮 美国 深度学习 通过药物研发平台 DUMA™来评估大型公共和私有数据集，以迅速识别药物，并对药物和疾病的匹配度按照概率进行排序。 种子轮 340 万美元 美国 深度学习 Interrogative Biology® 平台结合病人生物学和人工智能分析来进行药物发现、开发和诊断等。 未透露 美国 机器学习、自然语言处理 拥有 MedxExchange 、MedxInsights 和 MedxCare 三款服务产品的医疗人工智能平台，提供数据、医疗洞见和健康管理服务。 融资 660 万美元 加拿大 深度学习、基因生物学 精准医疗 A 轮 1600 万美元 中国 大数据、人工智能 通过数据挖掘和机器分析提供个人性健康指数分析和预测。 A 轮融资近 10 亿人民币 美国 数据挖掘、预测分析 本地分析软件和云服务 五轮融资 4430 万美元 美国 数据挖掘、机器学习 信用服务 四轮共 1.12 亿美元 英国 信用评级 金融产品的信用评级 四轮共 700 万美元 美国 数据挖掘、机器学习 AppBank、金融业务自动化 市值 867.10 亿美元 中国 人工智能、数据挖掘 智能助理、信用评级和风险管理等应用 估值 600 亿美元 Citadel 美国 人工智能、数据挖掘 对冲基金 目前掌管至少 260 亿美元资产 美国 自动驾驶 自动驾驶汽车 谷歌无人驾驶项目开始以独立公司的身份运营 美国 自动驾驶 电动汽车 498.35 亿美元市值 美国 深度学习 自动驾驶汽车 5000 万美元B轮融资 以色列 智能 3D 传感、传感器融合和精准地图和定位等核心自动驾驶技术 物美价廉的高清晰度固态激光雷达 6500万美元B轮融资 美国 雷达和专用短程通信安全系统 自动驾驶卡车 最新一轮6000万美元融资（2017 年 4 月） 美国 计算机视觉、数据挖掘 交通安全和表现的智能解决方案 六轮融资共 1.8 亿美元 美国 全自动驾驶 全新的自动驾驶汽车 3 轮融资共 2.9 亿美元 中国 计算机视觉和深度学习 自动驾驶 5500 万美金的 C 轮融资 中国 计算机视觉、智能驾驶 辅助驾驶系统 A 轮数千万人民币 Argo AI 美国 人工智能和自动驾驶技术发 自动驾驶汽车 福特将持续注资 10 亿美元 美国 人工智能和自动驾驶技术 自动驾驶汽车Level 4 无人配送车 A 轮融资 9200 万美元 英国 利用机器学习和独家算法来检测和响应以前未识别的网络威胁 Darktrace 的核心产品为「企业免疫系统」(EIS) 三轮共融资 1.045 亿美元 美国 自动驾驶、机器学习、数据挖掘 自动驾驶汽车、智能交通和智能出行应用 12 轮融资 87.1 亿美元，估值 660 亿美元 美国 机器学习 开源 AeroSolve 机器学习框架、智能助手、智能推荐、定价 9 轮共融资 20 亿 9 千万美元 美国 云计算、深度学习、数据处理 CRM 解决方案 市值约 638.37 亿美元 美国 机器学习 企业通讯应用，bots 平台 总融资 5.4 亿美元，估值约 38 亿美元 美国 人工智能、大规模分布式计算 解决复杂商业问题的综合智能系统 1.03 亿美元 C 轮融资，三轮共 1.3578 亿美元 美国 数据挖掘 基于社交网络的数据分析服务 1.3 亿美元 D 轮融资。五轮共 1.83 亿美元 美国 认知计算、深度学习、自然语言处理 法务研究智能辅助工具 未透露 中国 自动驾驶、机器学习、数据挖掘 自动驾驶汽车、智能交通和智能出行应用 超55亿美元新一轮融资 （2017 年 4 月） 中国 深度学习、自然语言处理、图像识别 媒体产品的应用 估值约 120 亿美元 中国 基于云端的深度神经网络算法、图像、语音、自然语言理解和运动控制、技术集成 智能机器解决方案 A+ 轮近亿美元 美国 机器学习、数学科学 使用人工智能来预防网络攻击 已融资 1.77 亿美元 美国 机器学习 利用人工智能/机器学习来开发网络安全应用的公司 4 轮共 5360 万美元融资 SparkCognition 美国 机器学习、人工智能、数据分析 使用机器学习和人工智能技术来分析预测网络安全漏洞与系统故障 新一轮3250万美元融资 英国 人工智能基础研究 AlphaGo、医疗健康、谷歌内部产品应用。 以 4 亿英镑（约 5.32 亿美元）被谷歌收购 美国 人工智能基础研究 新的计算机视觉系统，机器人视觉 五轮获得 7200 万美元融资 美国 深度学习 Inkling 脚本语言和集成开发环境 Mastermind 760 万美元A轮融资 日本 深度学习 深度学习操作系统 Chainer，机器学习在物联网的应用 9500 万美元最新一轮融资 美国 深度学习 深度学习企业应用包 SKIL、开源框架 Deeplearning4j 种子轮融资 300 万美元 美国 机器学习 开源机器学习平台和商业化支持 四轮融资 3360 万美元 美国 数据挖掘、机器学习 为铁路、建筑等大行业提供数据预测分析 SaaS 服务 B轮融资5000万美元（2017 年 4 月） 美国 机器学习 为数据科学家提供图像、文本的识别和分析的工具 4轮融资共 438 万美元 中国 机器学习 金融应用和「先知」平台 三大国有银行联合投资金额未公开 美国 机器学习 Bayesian Program Synthesis 可以自行编写代码，用最优的方法解释收集到的数据 来自 DARPA 的 770 万美元投资、来自 Felicis Ventures 的 450 万美元种子轮融资 美国 机器学习 机器学习平台公司，DataRobot 平台上有数百个开源机器学习算法 5400 万美元 C 轮融资 美国 机器学习与人工智能平台 PetuumOS、Poseidon 框架、Petuum Healthcare Solutions 9300万美元B轮融资 美国 算法 类似于苹果 App Store 的「算法应用」商店 1050 万美元 A 轮融资 美国 人工智能综合研究 TensorFlow 等开源框架，Google Photos、Now、Inbox 和搜索等多项产品和服务、硬件 市值 6701 亿美元 美国 人工智能综合研究 多个开源框架和硬件平台，Messenger、社交网络和定向广告等多项产品和服务 市值 4296 亿美元 美国 人工智能综合研究 云服务、Echo 等智能家居、机器人、电商产品应用 市值 4696 亿美元 美国 人工智能综合研究 CNTK 等开源框架，Cortana、小冰等多项产业和服务，硬件 市值 5362 亿美元 美国 人工智能综合研究 Watson、行业认知计算解决方案、量子计算机等 市值 1434 亿美元 美国 人工智能综合研究 基于智能手机等硬件的多项产品和硬件、智能助手、智能家居、医疗等 市值 8067 亿美元 中国 人工智能综合研究 开源框架 PaddlePaddle、百度大脑、自动驾驶、互联网应用 市值 849.78 亿美元 中国 人工智能综合研究 云服务、人工智能平台 DT PAI、电商产品应用 市值3079 亿美元 中国 人工智能综合研究 互联网应用 25683.98亿人名币 美国 人工智能硬件 GPU、深度学习超级计算机 DGX-1、自动驾驶超级计算机 Xavier 市值 832.07 亿美元 美国 人工智能硬件 CPU、Xeon Phi、Nervana 市值 1706.30 亿美元 美国 人工智能硬件 移动智能设备芯片 市值约 899.36 亿美元 美国 全可编程技术和器件 All Programmable FPGA、SoC 和 3D IC 提供商 160.70 亿美元 华为 中国 人工智能综合研究、硬件 人机交互设备应用、芯片等 2017 年以 785.108 亿美元营业收入首次打入《财富》前百强 京东 中国 人工智能综合研究 电商产品应用、金融 市值约 649.90 亿美元 「 AI00 开源项目」参与方式： "
148,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738943&idx=2&sn=b81017623ab0cd9e8e7919777456c36f&chksm=871ad481b06d5d9717656ce55ba3cc9bd9ed7883ffb5a14f7d0d930e0db76fe1db62f2197ee3&scene=27,重磅 | 今日头条&机器之心联合发布：斯坦福AI指数2017年度报告官方中文版,机器之心发布   由斯坦福大学人工智能百年研究（AI100）推出的「人工智能指数」（AI Index）是一个追踪 人工智能行业动态与发展的非营利性项目，其研究覆盖了百年以来人工智能的总体情况， 目标是基于数据来推动人工智能的广泛交流和有效对话。2017 年，AI Index 推出了首份年度报告，从多个角度观察和解读了人工智能领域的动态和进展。 经「人工智能指数」项目委员会授权，今日头条联合机器之心对此报告做了中文翻译（译文错误由翻译方负责），官方中文版报告近期正式发布。 中文版本报告下载链接：http://cdn.aiindex.org/AI-Index-2017-Chinese-Translation.pdf 学术领域 1.论文发表数量 下图统计了 Scopus 学术论文库中标注关键词「人工智能」的计算科学论文数量。 自 1996 年至今，每年发布的 人工智能论文数量增加了 9 倍多。 这里是各类学术论文年发表率与其 1996 年发表率的比较。图表显示了各领域论文、 计算机科学领域论文以及计算机领域内人工智能论文年发表率的增速。 数据揭示了人工智能论文发表率的增长不仅仅是出于对更广泛计算机科学领域兴趣的 增长。具体来说，尽管自 1996 年以来整体计算机科学领域内的论文数量已经增长了 6 倍，同时期人工智能领域每年发表的论文数量已经增长了 9 倍多。 2.课程选修人数 除了论文发表数以外，课程的参与人数也能体现这个领域的活力。以下展示的是斯坦 福大学每年选修人工智能与机器学习导论课程的学生数量。机器学习是人工智能的子领域。我们着重关注机器学习导论课程的参与度是因为目前人工 智能领域很多成果都基于机器学习的算法与理论。 自 1996 年以来，选修斯坦福大学人工智能导论课程的人数已经增长了 11 倍。 注:斯坦福大学 2016 学年机器学习入学人数的下降是基于当年的行政问题而非学生兴趣。详情请见附录。 本报告之所以着重突出斯坦福大学导论课程的选修人数是因为其数据最全面。不过如 下所示，其它高校导论课程的选修趋势也与斯坦福相似。 注:许多大学从上世纪 90 年代起开设人工智能课程。上图展示的是可获取数据的年份的情况。 注:许多大学从上世纪 90 年代起开设机器学习课程。上图展示的是可获取数据的年份的情况。 需要注意的是，这些图表展示了高等教育领域中的一个侧面，这些数据并不一定代表 学术机构总体的发展趋势。 3.学术会议出席情况 以下展示了人工智能领域有代表性的学术会议的参会情况，其中既有如 AAAI、IJCAI 和 ICML 这样的大型综合性会议 (按 2016 年参会人数超过 1000 人为标准)，也有像 CVPR、ACL、ICRA 那样专注于计算机视觉、自然语言处理和机器人的小型会议 (2016 年参 会人数不足 1000 人)。 注:大多数学术会议自 1980 年代起即开始举办，上图展示的是参会人数有记录的年份的情况。 研究重心转移:上图的参会人数同样表明了研究重点已经从符号推理转向了机器学习与深 度学习。 下图展示了参会人数少于 1000 人的小型学术会议的参会情况，其中需要注意的是 ICLR，该会议专注于深度学习领域，第一次会议于 2013 年由深度学习先驱 Yann LeCun 及 Yoshua Bengio 主办。 稳步前进:尽管学术界研究重点近年来已转移至机器学习及深度学习，仍有一小部分 研究者继续在符号推理方法上进行探索并取得进展。  产业领域 1.AI 领域创业公司  下图展示了得到风投资本支持并开发了人工智能系统的美国活跃创业公司的数量。 这一数量自 2000 年以来已增加了 14 倍。 2.AI 领域风险投资  下图为风投资本对美国人工智能创业公司所有融资阶段的年投资总额。 这一金额自 2000 年以来增加了 6 倍。 3.工作机会 下图分别展示了两个在线招聘网站 Indeed 和 Monster 上需要人工智能技能的工作数量的增 长。我们通过标题和工作描述的关键词区分出需要人工智能技能的工作。 下图是 Indeed 网站上美国需要人工智能技能的工作数量的增长数据。涨幅是基于 2013 年 1 月 Indeed 网站上美国要求人工智能技能的就业岗位所占份额的增长倍数。 自 2013 年以来，在美国需要人工智能技能的工作比重增长了 4.5 倍。 下图为 Indeed.com 平台报告的多个国家需要人工智能技能的工作比重的增长趋势。 注:虽然在加拿大和英国 人工智能就业市场增长很快，但 Indeed.com (http://indeed.com/) 称相对来说它们在 绝对规模上仍然只有美国 AI 就业市场的 5% 和 27%。 下图为 Monster 平台发布的按照所需的特定技能划分的一年内人工智能工作机会总量。 注:一份与人工智能相关的工作可能出现被计算两次的情况 (属于不同的类别)。比如，一份工作可能尤其需要自然语言处理和计算机视觉两种技能。   4.自动化及机器人应用 工业机器人进口到北美和全球的数量。  工业机器人进口到北美和全球的数量增长趋势。  开源生态 1.GitHub 项目统计 下图展示了 GitHub 上 TensorFlow 和 Scikit-Learn 软件包被收藏 (star) 的次数。二者都是深度 学习和机器学习的常用软件包。 软件开发者在 GitHub 上收藏 (Star) 软件项目以表示感兴趣并希望快速导航至该项目。收藏 可以代表开发者对软件和软件使用的兴趣。 下图展示了 GitHub 上不同人工智能和机器学习软件包被收藏的次数。 注:GitHub 库的 fork 数量遵循几乎同样的趋势 (尽管每个库的 fork 量和 star 量不同)。 公众认知及媒体报道 1.舆论倾向 下图展示了包含关键词「人工智能」的大众媒体文章的百分比，文章根据其意见倾向性 被分为正面报道或负面报道。 计算机视觉 1.物体检测 下图展示了 LSVRC 竞赛 (Large Scale Visual Recognition Challenge) 中人工智能系统在物体检 测任务上的性能表现。 图像标注的误差率从 2010 年的 28.5% 降至低于 2.5%。 2.视觉问答 下图展示了人工智能系统在针对图像问题提供开放式回答任务上的表现。 注:VQA 1.0 数据集已经被 VQA 2.0 数据集超越，目前尚不明确 VQA 1.0 数据集在未来会 获得多少关注。 自然语言处理 1.解析 下图展示了人工智能系统在确定句子句法结构任务上的表现。   2.机器翻译 下图展示了人工智能系统在英德新闻互译任务中的表现。 3.问答 下图展示了人工智能系统在从文档中找到问题答案任务上的表现。 4.语音识别 下图展示了人工智能系统在语音识别上的表现。 定理证明 可处理度 (tractability) 是指自动定理证明器在大量定理的数据集上的平均可处理程度。它可 以被用来衡量部分最先进的自动定理证明器。参见附录以获取与「可处理度」有关的更多信息。 注:引进最先进的证明器虽然可以解决新问题，但由于其在处理其他证明器擅长解决的问题上表现糟糕，平均可处理度可能会下降。  SAT 求解 这里指的是 SAT 求解系统解决问题 (那些可应用到产业实践中的问题) 的百分比。 通过研究不同流行趋势之间的关系，我们可以从前述章节中的评估中获得进一步的领 悟。本章展示了人工智能指数收集的数据可以如何被应用到进一步的分析中，以及这些数 据如何推动了一个全新、精确的衡量指标的发展。 由于这是一个案例研究板块，我们会着眼于横跨学术圈与产业界的流行趋势去探究其 之间的动态关系。进一步，我们会将这些标准整合成一个联合的人工智能活力指数。 学术界-产业界的动态关系 为了研究学术界与产业界人工智能相关活动的关系，我们首先从之前章节中选择了部 分具有代表性的评估结果。特别地，我们考察了人工智能论文的发布情况与斯坦福大学人 工智能与机器学习导论课程的修读情况，此外还考察了风投资本对人工智能创业公司的投 资情况。 论文发表数、注册学生数和投资金额这些数量指标并不能直接比较。为了分析这些趋 势之间的关系，我们首先以 2000 年为起始为每个测量指标设定了时间标准。这使得我们 可以来比较这些指标随时间的增长情况变化，而不是仅仅从最后的绝对值入手分析。 注:注册学生数在 2016 年有所下降，这反映了学校行政上的某些问题，并非没有足够的学生对课程感兴趣。具体细节可参考附录 A2。 数据显示，首先，学术活动数量 (论文发表与注册学生数) 在稳步上升。在 2010 年 左右，投资者便开始注意到了这个领域，到 2013 年，投资者已经成为了推进该领域发展 的核心驱动力。此后，学术界逐渐赶上了产业界的步伐。 人工智能活力指数 人工智能活力指数整合了来自学术界和产业界的各类数据 (论文发表量、课程注册学 生数、风险资本投资) 来量化整个人工智能领域的活力。为了计算人工智能活力指数，我 们按照时间对来自论文发表、学生课程注册和投资领域的数据进行了归一化平均处理。 我们希望这份简要调查可以激发大家在研究如何进一步分析人工智能指数中数据类别 方面的兴趣，也希望可以引起讨论来研究出一个可以长期追踪的有价值的测量方法。 
149,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739090&idx=1&sn=1fb2e59693bb5a9f7e5ceec140080980&chksm=871ad5ecb06d5cfa7bf1e9cad4359dc0b63e17b2bf5751e94346254c74991faeb2f84d2a2c18&scene=27,从七桥问题开始：全面介绍图论及其应用,"图论是计算机科学中最重要、最有趣的领域之一，同时也是最容易被误解的。本长文从图论最基础的七桥问题开始，进而结合推特与 Facebook 实例解释无向图与有向图。此外，本文还是用大量的实例解释表征图、搜索树、哈希表等关键概念。最后本文描述了基于深度的搜索和基于广度的搜索等十分流行的图算法。 理解和使用图帮助我们成为更好的程序员。用图思考帮助我们成为最好的，至少我们应该那么思考。图是很多节点 V 和边 E 的集合，即可以表示为有序对 G=(V, E)。 尽管尝试研究过图论，也实现了一些算法，但是我还是非常困惑，因为它实在太无聊了。 事实上，理解一件事物的最佳方式是理解其应用。我们将展示图论的多个应用，最重要的是，有很多插图。 让我们首先从《图论的起源》中的「柯尼斯堡（Königsberg）的七座桥」开始。在加里宁格勒（Kaliningrad）有七座桥，连接着由普雷戈里亚（Pregolya）河分割而成的两个岛屿和两大陆地。 在 18 世纪，这里被称为柯尼斯堡，隶属普鲁士，这一区域有很多桥。当时，有一个与柯尼斯堡的桥相关的脑筋急转弯：如何只穿过桥一次而穿过整个城市。下图为柯尼斯堡七座桥的简化图。 你可以尝试一下，在穿过每座桥仅一次的情况下穿过这个城市。每座桥，意味着所有桥都被穿过；只穿过一次，意味着每座桥不能被穿越两次及以上。如果你对这一问题有所了解，就知道这不可能。 有时候，放弃这一问题是合理的。这就是 Leonhard Euler 的解决方法，他没有试图解决这一问题，而是证明其不可解决。让我们试着去理解 Euler 的内在想法，做到像 Euler 一样思考。首先我们从下图开始。 图中有四块彼此分隔的区域，两个岛屿和两块陆地，以及七座桥。探讨每一区域的桥数是否有一定模式很有趣。 如图所示，每块区域的桥数皆为奇数。如果你只能穿过桥一次，区域有两座桥，那么你就可以进入并离开该区域。 通过图示很容易发现，如果你通过一座桥进入一个区域，那么你也要通过第二座桥离开它。但是当第三座桥出现，则无法只穿过桥一次而离开。所以对于一块区域，当桥数为偶时，则可以每座桥只穿过一次而离开；当桥数为奇时，则不能。请牢记。 让我们再添加一座新桥，如下图所示，看看其是否能解决问题。 现在我们有两个偶数和两个奇数。让我们在添加新桥的图上画一条新路线。 我们已经看到了桥的奇偶数是重要的。这里有个问题：桥的数量解决问题了吗？难道这个数不应该一直是偶数吗？后来发现不是的。这就是 Euler 做的，他发现了一个显示桥数量很重要的办法。更有意思的事，有奇数个连接点的「陆地」也很重要。这时候 Euler 开始把陆地和桥转化成我们看得懂的图。下面是一幅表示了哥尼斯堡七桥（Königsberg bridges）的图（注意：我们「临时」加的桥不在这里）： 问题的泛化和提取是需要注意的。当你解决一个特定问题时，最重要的是为类似的问题概括答案。在这个实际问题里，Euler 的任务是泛化过桥问题从而在将来可以解决类似的问题。比如：对于世界上所有的桥。可视化也可以帮助我们从另一个角度看问题，如下面的图也全是七桥问题的抽象： 所以，可视化图是解决该问题的好选择，因此我们需要去找出哥尼斯堡七桥问题是怎样被这张图解决的。注意从圈里面向外出来的线。因此我们命名圈为节点（或节点），连接他们的线为边。你也许看到了字母表达法，V 是节点（vertex），E 是边（edge）。 下一个重要的事是所谓节点自由度（Degree），即连接到节点的边数量。在我们上面的例子里，连接陆地和桥的边的数量可以被表达成节点的自由度。 在 Euler 的努力下，他证明了在图上（城市里）每次只走过一条边（桥）并且走过每一条边是严格取决于节点自由度。由这样的边组成的路径被叫做 Euler 路径（Euler path），Euler 路径的长度就是边的数量。 有限无向图 G(V,E) 的 Euler 路径是指 G 的每一个边都只出现一次的路径。如果 G 有一条 Euler 路径，它就被称之 Euler 图。[注释 1] 定理：有且仅有两个确定的节点存在奇数自由度，其它的节点都有偶数自由度，那么该有限无向图为 Euler 图。【1】 左图：有两个节点有奇数自由度的图像。右图：所有节点都有奇数自由度。 首先，让我们分清楚上面定理和理论中的新名词。 有限图（Finite graph）是指有限数量的边和节点的图。 图可以为有向的或无向的，这也是图非常有趣的性质。你肯定看到过将 Facebook 和 Twitter 的作为有向图和无向图的例子。Facebook 朋友关系也许可以很简单地表示为一个无向图，因为如果 Alice 是 Bob 的朋友的话，Bob 也必须是 Alice 的朋友。 而且也要注意「Patrick」节点，因为它没有连接一条边（edges）。虽然它还是图的一部分，但在这个案例中我们可以说该图没有连接上，这是个失联图（disconnected graph）（「John」、「Ashot」和「Beth」也是同样的，因为它们是和别的节点都是分离的）。在一个连接的图里没有到达不了的节点，这里必须在每一对节点之间有一条路。 与 Facebook 的例子相反的是，如果 Alice 在 Twitter 上关注了 Bob，Bob 并不需要关注 Alice。所以「关注」关系必须是有向的连接，其表示节点（用户）有一条有向边（关注）连接到其它的节点（用户）。 现在，我们了解了什么是有限无向图，让我们再一次思考 Euler 图： 所以为什么我们最开始就讨论了哥尼斯堡七桥问题和 Euler 图呢？在接触答案之前接触一下问题背后的因素（节点、边、有向、无向）也能避免枯燥的理论方法。我们现在应该更关注于用电脑表示图，因为这是我们最大的兴趣。用电脑程序表示图将使我们设计出一个算法来跟踪图路径（graph path），这样就能发现它是不是 Euler 路径了。 这是一个很沉闷的任务，要有耐心。记得数组和链表之间的战争吗？用如果你需要快速访问元素就用数组，如果你需要快速插入/删除元素就用链表等。我很难相信你会在像「怎样表示列表」这样的问题上纠结。当然，在图论中真正的表达是非常无聊的，因为首先你应该决定你将怎样确切地表达图。 现在我们以一个树来开始。你肯定已经至少一次见到了二叉树（下面的不是二叉搜索树）。 因为它是由节点和边构成的，所以它就是图。你也要想到一般最常见的二叉树是怎样表示的（至少在教科书上）。 这个对于已经非常熟悉树的人来说太详细了，但是我必须确保我们在同一阶段。（注意我们还是在用伪代码）。 如果你不是新手，仔细的读上面的伪代码然后阅读以下图解： 当一个二叉树是简单的节点「集合」，每一个父节点有左子节点和右子节点的节点。二叉树在应用简单规则的时候是非常有意义的，例如允许快速的关键字查找。二叉搜索树（BST）按序储存他们的关键字。我们可以根据任何规则实现二叉树（即使它会根据不同的规则而有不同的名字，比如，min—heap 或者 max——heap），最常见的 BST 规则是它符合二项搜索性质（也是名字的由来），即「任意节点的键值必须比它左边子树的键值要大，比右边子树上的键值要小。「更大」是 BST 重要的本质，当你把它改成「比更大或一样」时，你的 BST 可以在插入新节点时解决复制键值得问题，除此之外它将只保留唯一键值的节点。你可以在网上找到很好的二项树的文章，我们不会提供一个二元搜索树的全面实现，但我们将展示一个简单的二元搜索树。 Airbnb 树是非常有用的数据结构，你也许还没有实现过树型结构，但你也许无意间用过它们。像你注意到的，二叉搜索树（Binary Search Tree）中有「搜索」，简单来说，所有需要快速查找的事，应该被放到二叉搜索树中。「应该」不意味着一定，在编程中最重要的事情是用合适的工具去解决问题。这里有很多案例可以看到简单链表（O(N) 复杂度）搜索相比 BST（O(logN) 复杂度）搜索更受欢迎。一般来说我们可以用一个库来实现一个 BST，但是在这个教程中我们可以重新发明我们自己的轮子（BST 是基本在所有多用途编程语言库都有实现）。接近了「一个真实世界例子」，这里是我们试着去处理的问题： Airbnb 房源搜索一瞥： 怎样用滤波器基于词条尽可能快的搜索房源，这是一项很难的任务。如果我们考虑到 Airbnb 储存了几百万条表格的情况下，这个任务更难了。 所以当用户搜索房源时，他们也许就会「接触」到四百万条数据库中的记录。的确，在网站主页上能够展现的「top listings」有限，而用户对浏览百万条列表也并不感兴趣。我没有任何 Airbnb 的分析记录, 但我们可以用编程语言中叫做「假设」的强大工具，所以我们假设单个用户查看最多 1 千个房源就会发现中意的房源。并且最重要的因子是即时用户的数量，因为它会影响数据结构、数据库和项目构架的选择。就像这看起来的那么明显，如果这里总共有 100 个用户，我们就不用去操心。相反，如果即时用户数量超过了百万级，我们必须去思考每一个决定到底对不对。每个决策都被正确的使用，这是为什么巨头们雇佣最好的人才，为提供卓越的服务而努力的原因（Google、Facebook、Airbnb、Netflix、Amazon、Twitter 和许多其他公司都在处理大量的数据；招聘正确的工程师来做正确的选择，为数百万实时用户每秒处理百万级字节的数据。这就是为什么我们码农纠结于可能遇见的数据结构，算法和问题处理，因为需要的是工程师有能力快速、有效地解决这样大的问题）。 所以在 Airbnb 的案例里，用户浏览了他们的房源主页，Airbnb 试着去过滤房源来找出最适合的。我们怎样处理这个问题呢？（注意这个问题是后端的，所以我们不需要管前端或者网络流量或者 https over http 或者 Amazon EC2 over home cluster 等。首先，因为我们已经熟悉了程序员仓库中最强大的工具（在说假设而不是抽象），我们假设处理的是完全适配 RAM 的数据。然后你也可以假设我们的 RAM 是足够大的。足够大去支持，但这是多大呢？这是另一个非常好的问题。需要多大的内存来存储真正的数据呢？如果我们处理的是四百万单元的数据（还是假设），如果我们大概知道每一个单元的大小，之后我们可以简单地驱动需要的内存，就是 4M*sizeof(one_unit)。考虑下「房源」及其性质（properties），事实上，至少考虑一下处理这一问题必要的性质（一个「房源」是我们的单元）。我们需要用 C++结构伪代码来表示一些问题，你可以简单地将他们转化为一个 MongoDB 略图目标或者任何你想要的形式, 我们只讨论性质的名字和类别。（试着去想象这是在空间经济里用字位段或者位集合） 假设。上面的结构不是完美的（很显然），而且这里有很多假设或者不完整的地方，去再读一下免责声明。我只是看了下 Airbnb 的过滤器和应该存在的符合搜索查询的设计性产权表。这只是个例子。现在我们应该能计算每一个 AirbnbHome 对象会在内存中占用多少空间。name 是一个 wstring 来支持多语言的名字/头衔的，这个意味着每一个字符占了 2 字节（我们不想担心字符大小如果我们需要用其他的语言，但在 C++中，char 是 1 字节然后 wchar 是 2 字节）。 快速的看一下 Airbnb 的表可以让我们估计房源的名字可以占到最多 100 个字符（虽然最多的是 50 个左右，而不是 100 个），我们会认为 100 个字符是最多的量，这占了差不多 200 字节的内存。uint 是 4 字节，uchar 是 1 字节,ushort 是 2 字节（还是假设）。假设图片是在储存服务旁边，像 Amazon S3（目前据我所知，这个假设对于 Airbnb 来说是最可能实现的，当然这也是假设）而且我们有这些照片的 URL，而且考虑这里没有 URL 的标准尺寸的限制，但这事实上有一个众所周知的上线-2083 字符，我们将要用这个当成任何 URL 的最大尺寸。所以考虑到这个，平均每个房源有 5 张照片，这可以占到 10Kb 内存。 让我们重新想一下，一般储存用同样的基础 URL 服务，像 http(s)://s3.amazonaws.com/<bucket>/<object>，这样就是说，这里有一个基本的模式来建 URL 并且我们只需要储存真实照片的 ID，让我们说用了一些独特的 ID 生成器，可以为图片对象返回 20 字节长度的独特的字符 ID，看起来像 https://s3.amazonaws.com/some-know-bucket/。这提供了我们好多空间效率，所以为了 5 张照片储存字符 ID，我们只需要 100 字节内存。 同样的「手段」可以用 host_id 来做，就是说，房主的用户 ID，占了 20 字节的内存（实际上我们可以就让用户用数字 ID，但考虑到一些 DB 系统像 MongoDB 有非常详细的 ID 生成器，我们假设 20 字节的字符 ID 是「中位」长度，已经可以用很小的改动就能适用于大部分 DB 系统了。Mongo 的 ID 长度是 24 字节）。最后，我们将用一个最多 4 字节 32 位大小对象的位集合和一个比 32 位大的 64 位小的位集合一起作为一个 8 字节的独享。注意这是个假设。我们在这个例子里为了所有的表达了枚举的性质用了位集合，但位集合可以取不止一个值，换种说法这是一种多种选择的多选框。 举例，每一个 Airbnb 房源都有一些便利工具列表，比如：「熨斗」、「洗衣机」、「电视」、「wifi」、「挂衣架」、「烟雾探测器」甚至「适用于笔记本电脑的书桌」等。这里也许有超过 20 种便利工具，我们用 20 这个数字是因为 Airbnb 网站上可以用 20 个选项过滤。如果一个房源有所有的上述便利措施（在截图中看），我们在对应的位集合的位置上标注 1 就好。 举例，检查一个房源有没有「洗衣机」： 或者更专业一点： 你可以修正代码或修复编译错误，我们只想强调该问题背后用向量表征特征的观念。同样的观念可以用在「入住守则」、「房间类型」和其它特征上。 最后，对于国家编码和城市名。像上面代码注释中提到的一样（看标注），我们不会储存经纬度来避免地理-空间问题，我们储存国家代码和城市名字来缩小用地名搜索的范围（为了简介省略街名，原谅我）。国家代码可以被用两个字符，3 个字符或者 3 个数字来表示，我们会用 ushort 储存数字表达。不幸的是，城市比国家多了太多，所以我们不能使用「城市编码」，我们只会储存真实的城市名，保留平均 0 为 50 字节的城市名字和特别的名字。我们最好用一个附加的布尔变量来表示这是不是一个特别长的名字，所以我们将会加上附加的 32 字节来保证最终的结构大小。我们也假设在 64 位系统上工作，即使我们为 int 何 short 选择了非常紧凑的值。 所以，420 字节加上 32 个多出的字节，若我们四舍五入到 500 字节。所以每一个对象「home」最多占 500 字节，并且对于所有房源列表，500 字节*4 百万=1.86Gb ~2Gb。我们在搭建结构时做了很多假设，让储存在内存中更便宜。无论我们对这数据做什么，我们需要至少 2Gb 内存。如果你无聊，忍着，我们刚开始。 现在是任务最难的地方了，为这个问题选择合适的数据结构（来尽可能有效的过滤表）不是最难的任务。最难的是（对我而言）按照一系列滤波器搜索列表。如果只有一个搜索键（key）（即只有一个滤波器），我们可以很轻易地解决这个问题。假设用户只关心价格，我们需要的只是在给定的范围以价格下降的顺序找到 Airbnbhome 对象（合适的家）。如果我们要用二元搜索树来解决这个问题，则可按下图形式执行。 如果需要遍历所有的 4 百万个对象，这搜索树会长得很大很大。另外，需要占用的内存也会越来越多。这只是因为我们用了二元搜索数来存储对象，每一个树节点给它的左右子树两个额外的指针，加起来是每个子指针有 8 个额外的比特（假设是 64 位系统）。对于 400 百万节点它加起来就是 62Mb，对于 2Gb 对象的数据来说是很小的，但还是不能轻易地「忽略」。 目前为止，上面展示的树表明了任何物品都可以很轻易地在 O（logN）复杂度内被找到。如果你对这些概念不熟悉，我们会在接下来解释清楚，或者跳过复杂度讨论的副章节。 算法复杂度：我们在这里快速地简要介绍，在之后的文章「Algorithmic Complexity and Software Performance: the missing manual」我会进行详细的解释。在大部分情况里，找到一个算法的「大 O」复杂度是简单的。首先要注意到，我们总是考虑最差的情况，即：一个算法要最多算多少次才能产生一个合适的结果（用来解决问题）。 假设一个有 100 个元素的未排序数列，要做多少次的比较才能让它找到任意的元素，这里也要考虑到需要的元素可能缺失的情况？它最多需要与匹配的数字比较 100 次才能发现，尽管有时候第一个在数列中的元素就是要找的数字（意味着单次比较就找到答案了），但我们只考虑最差的可能情况（元素可能丢失了或者在最后的位置）。 计算算法复杂度的重点是找到运算次数与输入规模之间的依赖关系，举例来说，上面的数列有 100 个元素，运算的次数也是 100，如果数列的数量（输入）增长到 1423，运算次数也会增长到 1423（最差的情况）。所以，输入和运算次数之间的关系在这里是非常清晰的，它也被叫做线性关系，运算次数和数列的增长一样快。增长是复杂度的关键，我们说在一个未排序的数列中搜索需要 O（N）次运算，来强调寻找它需要最多用 N 次运算，（甚至最多到 N 的常数倍数运算，比如 3N 次）。另一方面，访问数列上的任意元素只需要常数时间，比如 O（1）。这是因为数列的结构是一个连续结构，并且包含相同类型的元素，所以访问特定的元素只需要计算它与数列第一个元素的相对位置。 有一点非常明确，二元搜索树按排序顺序保存其节点。那么在二元搜索树中搜索元素的算法复杂度是多少呢？我们应该在最坏的情况下计算查找元素所需的操作次数。 见上图，当我们开始在根部搜索时，第一次匹配可能会导致三种情况： （1）目标节点被发现； （2）如果要找的值小于节点的值，则匹配将继续到节点的左子树； （3）如果要找的值大于节点的值，则匹配将继续到节点的右子树。 在每一步中我们都把节点的数量减半。在二元搜索树中查找元素所需的操作数等于树的高度。树的高度是最长路径上节点的数量。在这一案例中，高度是 4。所以高度等于 logN+1（底数为 2），搜索复杂度是 O（logN+1）=O（logN）。这意味着在 4 百万个节点里搜索元素需要 log1000000=~22 次比较（最差的情况）。 回到树搜索的问题，二元搜索树中的元素搜索时间为 O（logN）。为什么不使用哈希表？哈希表有常数的访问时间，这使得在几乎任何地方使用哈希表都是合理的。 在该问题中，我们必须考虑一个重要的需求，即执行范围搜索，如搜索价格区间在$80 到 $162 之间的房源。在二元搜索树的情况下，获取区间中所有节点很简单，只需对树执行顺序遍历，保存计数即可。而哈希表的计算稍微昂贵，在这种情况下使用坚持二元搜索树更好一些。尽管还存在另一个因素，使我们需要重新考虑哈希表：密度。价格不会「一直」上涨，大部分房源处于固定的价格区间内。截图中的柱状图显示了价格的真实分布，数百万房源处于同一区间（$18—$212），它们具备同样的平均价格。简单的数组可能起到很好的效果。假设数组的索引是价格，则我们能够在（几乎）常数时间内获取任意价格区间。如下图所示： 就像一个哈希表，我们通过房源的价格来匹配每一套房子。所有具有相同价格的房源都归入单独的二元搜索树。如果我们存储房源的 ID 而不是上面定义的完整对象（AirbnbHome 结构），也可以节省一些空间。最可能的情况是将所有房源的完整对象保存在哈希表，并将房源 ID 映射到房源的完整对象中，以及保存另一个哈希表（或更好的，一个数组），该哈希表将价格与房源 ID 进行映射。因此，当用户请求价格范围时，我们从价格表中获取房源 ID，将结果裁剪成固定大小（即分页，通常在一页上显示 10-30 个项目），然后使用每个房源 ID 获取完整的房源对象。请记得，要注意平衡。平衡对二元搜索树至关重要，因为它是在 O（logN）内完成树操作的唯一保证。当你按排序顺序插入元素时，二元搜索树不平衡的问题就会很明显，最终，树将变成连接列表，这显然会导致线性时间复杂度。现在假设我们所有的搜索树都是完美平衡的。再看看上面的图。每个数组元素代表一棵大树。如果我们改变这个树，变成图呢？ 这是一个「更接近真实」的图。这个图展示了最隐蔽的数据结构和图，带领我们到了（下文）。 图论的缺点是缺乏单独定义，这就是为什么你无法在库中找到 std::graph。我们已经尝试过表示「特别的」图 BST。重点在于，树是图，但图未必是树。最后一张示例图表明在一个抽象图下面有很多树，「价格 vs 房源」和一些节点的类型不同，价格只有价格值的图节点，表示满足特定价格的房源 ID（房源节点）的树。这很像混合数据结构，不同于我们在教科书示例中见到的简单图形。图表示的重点：不存在固定、「权威」的图表示结构（这与 BST 不同，后者用左／右子指针代表基于节点的特定表示，尽管你可以用一个数组表示 BST）。你可以用最便捷的方式表示一个图，重点在于你把它「看作」是图。「看图」的意思是使用适用于图的算法。 N-ary 树更像是模拟一个图。 首先，将 N-ary 树节点表示为： 该结构仅表示树的一个节点，完整的树如下所示： class NTree public 该类别是围绕单个树节点 root_ 的抽象。我们可以用它构建任意大小的树。这是树的起点。如果要添加新的树节点，我们就要为其分配内存，将该节点添加至树的根节点处。 图与 N-ary 树很像，只有细微的不同。我们来看一下。 这是图吗？我认为，是的。但这幅图与前面的 N-ary 相同，只不过稍微旋转了一下。只要你看到一棵树，无论它是苹果树、柠檬树，还是二叉搜索树，你都可以确定它也是一张图。因此，通过设计图节点（图节点）结构，我们能够提出同样的结构： 这样可以创建图吗？还不够。看下面两幅图，找不同： 左侧的图没有可以「进入」的点（与其说是树，它更像森林），相反，右侧的图没有不可达的节点，听起来很熟悉。 如果图中任意两点都是连通的，那么该图被称作连通图。 虽然图中不明显，但是我们假设价格不能互相连接，那么在「价格 vs 房源」图中并不是每对节点之间都有路径相连，这说明我们无法使用单个 GraphNode 结构构建图的例子，但是在很多案例中我们必须这样处理非连通图。看下面这个类别： class ConnectedGraph public 类似围绕单个节点（根节点）构建的 N-ary 树，连通图也可以围绕根节点构建。树有「根」，即它们有起始点。连通图可以用带有根节点的树来表示（当然还有其他属性），不过要注意，实际的表示可能会随着算法或具体问题发生变化。但是，考虑基于节点的图本质，非连通图可以按照下面的方式来表示： class DisconnectedGraphOrJustAGraph public 树状图可以清晰自然地表示 DFS/BFS 这样的图遍历。但是，高效路径追踪等需要不同的表示方法。还记得欧拉图吗？为了追踪图的「eulerness」（真实性），我们应该追踪图中的欧拉路径。这意味着遍历每个边一次就要访问所有节点；如果追踪结束后我们仍有未遍历的边，则该图没有欧拉路径，因此它不是欧拉图。更快的方法是：检查节点的度（假设每条边都保存了度），如定义所述，如果图的节点度是奇数，则它不是欧拉图。该检查的复杂度是 O(|V|)，其中 |V| 是图节点的数量。我们可以在插入新的边缘的同时追踪节点的奇数／偶数度，同时插入新的边以增加奇数／偶数度检查的复杂度到 O(1)。下面介绍图表示和返回路径的 Trace() 函数。 class ConnectedVELOGraph public 注意 bug，bug 到处都是。该代码包含大量假设，比如标签，我们可以通过节点理解字符串标签，确保你可以将其更新成任意事物。接下来，是命名。如注释中所述，VELOGraph 仅适用于 Vertex Edge Label Only Graph。重点在于，该图表示包括一个将节点标签映射至节点关联边的表、一个包含与边相连的两个节点的边列表，和一个仅用于 Trace() 函数的 flag。查看 Trace() 函数实现，它使用边的 flag 来标记已经遍历过的边（在任意 Trace() 调用之后应该重置 flag）。 示例：Twitter 另一种表示叫做相邻矩阵，它在有向图中有用，就像我们在 Twitter 关注图中所使用的那样。 推特案例中有 8 个节点，所以我们需要使用|V|x|V|的二维矩阵表征这个图（其中 |V|分别代表行数和列数）。如果从 v 到 u 有一条有向边，那么我们称矩阵的元素 [v][u] 为真，否则为假。 如你所见，这是一个十分稀疏的矩阵，其中的值代表是否有单向连接路径。如果我们需要了解 Patrick 是否关注了 Bob 的推特，那么我们只需要查看矩阵中 [""Patrick""][""Sponge Bob""] 的值是不是等于 1。而要查看 Ann 推特的关注者，我需要获得「Ann」的整个列；同样查看 Sponge Bob 正在关注的人只需要查看「Sponge Bob」的行就行。此外，邻接矩阵（Adjacency matrix）可以用来描述无向图，它不会在 v 到 u 的边选择值「0 或 1」来表示是否有连接，它会同时设置两个方向的值都为 1，即 adj_matrix[v][u] = 1 和 adj_matrix[u][v] = 1。因此，无向图的邻接矩阵为对称矩阵。 注意，在通常情况下我们在邻接矩阵中并不会只储存 0 和 1，我们可以储存一些更具信息的值，例如边权重等。最好的案例可能是带有距离信息的地图。 上图表示了 Patrick 和 Sponge Bob 等人之间的距离（也称为加权图）。如果节点间没有没有直接路径，那么我们就把它设为无穷大，它既不意味着根本没有路径，也不意味着一定有路径。它可以在应用算法搜索两个节点间路径时定义。当然，我们还有更好的方法来储存节点和边之间的关系，如关联矩阵。 尽管邻接矩阵对推特关注关系有很好的表征，但将 3 亿用户（每月活跃用户）储存在矩阵中需要 300*300*1（百万字节/布尔值）的储存空间。也就是约有 82000Tb（Terabyte）或需要 1024 * 82000 Gb 的储存空间。BitBoard 可以帮助我们减少一些空间需求，大约可以降低到 10000Tb，但还是太大。如上所述，邻接矩阵稀疏要求我们提供比实际需求更多的空间，这也就是为什么边的列表映射到节点可能会很有用。重点是，邻接矩阵允许保持关注和不关注的信息，而我们需要的仅仅是知道如下内容： 右图表示邻接列表（adjacency list），每个列表描述了图中的一组邻近节点。在图表中，我们突出了哈希表的用法，因为任何节点的访问复杂度都是 O（1）。而且对于邻近节点列表，我们并没有提到任何具体的数据结构，也不会从列表转化为向量。重点是，如果要确定 Patrick 是否关注 Liz，我们应该遍历哈希表中每一个元素（常数时间），而邻近矩阵需要查看每一个与 Liz 相关的元素（线性时间）。线性时间在这一点上并不是那么糟，因为我们经需要循环与 Patrick 相关的固定数目的节点。 如果我们用空间复杂度表示推特，这需要 3 亿个哈希表记录，每个记录指向一个向量（选择向量以避免链表的左/右指针所产生的内存开销）。此外，虽然没有统计数据，但平均推特关注的人数是 707 个。所以如果我们考虑每个哈希表记录指向一个有 707 个用户 ID 的数组，且每个 ID 有 8 个字节，那么现在我们可以计算出储存空间约为 12TB。 现在少很多了，但我们仍然不确定 12TB 是否是一个合理的数字。如果在 32Gb RAM 的专用服务器上一个月需要 30 美元，那么 12TB 加上一些控制服务器和备用服务器等（约需要额外两倍数量）核算下来需要 1500 台服务器，每月的成本达到了 45K 美元。这对于我们来说当然是难以接受的价格，但对于推特来说就非常便宜了。但推特需要提高响应的速度，即用户发的推文需要第一时间发送给关注的人。但理想的时间是多少？我们并不能作出任何假设和抽象，因此我们可以探讨一下现实世界产品系统的响应。以下是我们在推文时经常遇到情况。 同样我们并不知道一条推文需要多少时间才能发送到所有的关注者，但公开的数据表明每天约有 500 亿条推文。所以经验看来一般推文的时间在 5 秒内，同时我们还要注意哪些拥有超百万粉丝的名人，推特可能会分配更多的资源来推送名人「超级有用」的内容。 为了解决推文的分发问题，我们并不需要以下的图，我们需要关注者的列表。前面的哈希表和一些列表允许我们高效地搜索特定关注者关注的所有用户，但它并不允许高效地搜索关注特定用户所有关注者，因此我们必须扫描所有的哈希表键值。这也就是为什么我们应该构建另一个图，它与以下我们展示的图对称相反。这个新的图由包含 3 亿个节点的哈希表组成，每个节点指向相邻节点的猎鸟（结构相同），但是这次相邻节点的列表将表示关注者。 因此基于该示例，无论何时 Liz 发推特，Spone Bob 和 Ann 都必须在他们的时间线上找到特定的推文。一项普遍使用的解决该问题的技术是为每个用户的时间线保持独立的结构。假设推特有 3 亿的用户，我们可以假定至少存在 3 亿个时间线（每人一个）。简单的说，无论何时发推，我们应该找到他的关注者并且更新他们的时间轴，时间线可以被表征成连结串列或平衡树（以推文的日期作为节点关键字）。 这仅仅是基本思想，从真实的时间线表征抽象得到。当然如果使用多线程，我们可以将实际的传送过程变得更快。这对于大规模案例非常关键，因为对于百万级别的关注者，接近列表终点的用户在处理上通常慢于接近列表前面的用户。以下的伪代码将尝试解释该多线程传送思想。 因此无论关注者在什么时候刷新推特，他们都能收到新的推文。当然，我们仅仅讨论了 Airbnb 或推特面临问题的冰山一角，还有很多问题需要各位读者共同探讨。 推特的推文分发问题关键在于对图的利用，即使我们不使用任何的图算法，仅用图表示。而真正的图算法是相当复杂的。在使用图表示之前，我们讨论了 Airbnb 房源和高效过滤的问题，主要困难在于当过滤器关键字超过一个的时候，就无法高效地过滤家园。那么使用图算法能带来什么好处吗？值得一试。我们可以将每个过滤器表示成一个独立的节点，每个过滤器可以代表特定的属性（价格、城市名、国家、生活设施等）。 我们还可以通过添加高层的节点，例如「生活设施」节点来连接所有生活设施类的节点（WiFi、电视等），使该集合更容易理解。 现在，我们可以将 Airbnb 房源（home）表示成节点，然后将这些节点和对应的属性节点连接起来。 这个插图的微妙变化使它更像一种特殊类型的图形，称为偶图（bipartite graph）。 偶图的节点可以分为两个不相交和独立的集合，这样每个边就将一个集合的节点连接到另一个集合的节点。在我们的例子中，其中一个表征过滤器（我们用 F 表示），另一个表征房源集合（H）。例如，如果有价值 62 美元的 10 万个房源，则标记为「$ 62」的价格节点将具有 10 万条边入射到每个房源的节点。 如果我们测量空间复杂度的最坏情况，即每个家庭具有满足所有过滤器的所有属性，则要存储的边总量将为 7 万×400 万。如果我们将每个边表示为一个 ID 对：{filter_id; home_id}，如果我们重新考虑 ID，并使用 4 个字节（int）数字 ID 为过滤器 ID，8 个字节（long）ID 为房源使用的 ID，那么每个边缘至少需要 12 个字节。 因此，存储 7 万* 400 万个 12 字节值需要大约 3TB 的内存。我们在计算中犯了一个小错误，由于在 Airbnb 中有 65,000 个活跃城市，因此过滤器的数量约为 7 万个（统计数据）。 好消息是，同一个家庭不能位于不同的城市。也就是说，我们实际与城市边配对的数量是 400 万（每个家庭位于一个城市），因此我们将计算 70k-65k = 5000 个过滤器，这意味着我们需要 5000 * 400 万* 12 个字节的内存，小于 0.3Tb。听起来不错。但是什么给了我们这个偶图？最常见的网站/移动请求将由多个过滤器组成，例如： 因此我们只需要找到上述所有「过滤器」的节点，并处理与其邻近所有「房源」的节点。 图算法 或许，任何用图执行的计算过程都可以分类为「图算法」，你可以实现一个输出图的所由节点的函数，然后将其命名为「<你的名字>的节点输出算法」。但真正可怕的是教科书中列出的图算法： Coloring Hopcroft–Karp Hungarian Prüfer coding Tarjan's off-line least common ancestors Topological sort 我们尝试使用偶图匹配算法，例如 Hopcroft–Karp 算法到 Airbnb 房源过滤问题： 给定一个 Airbnb 房源（H）的偶图和过滤器（F），其中 H 的每个节点可以多于 F 的一个相邻节点（共享一个公共边）。寻找 H 的由节点构成的子集，该子集和 F 的子集的节点相邻。 问题的定义很难理解，并且目前我们还不确定 Hopcroft-Karp 算法可以解决该问题，但我们可以在求解的过程中学到很多图算法的关键思想。这个过程不会很短，需要你有耐心。Hopcroft-Karp 算法以二分图为输入，并生成最大基数匹配的输出，该输出是一个包含尽可能多的边的集合，其中没有任何两条边共享同一个端点。熟悉该算法的读者已经注意到，这并不能解决我们的问题，因为匹配过程的条件是没有任何两条边共享同一个节点。我们来看一个示例展示，其中只有 4 个过滤器和 8 个房源（为简单起见）。这些房源用字母 A 到 H 标记，过滤器是随机选择的。从 A 到 H 的所有房源都是 50 美元每晚的价格和一张床，但很少有供应 WiFi 和/或电视的。因此以下的示例过程将尝试找到满足四个条件的房源（拥有 4 个过滤器）。 对该问题的求解需要找到和特定的房源连接的所有边，该房源节点和相同的过滤器子集关联。而 Hopcroft-Karp 算法将移除公共端点的边，并生成和两个子集都关联的边。 查看上图，我们需要寻找的是房源 D 和 G，它们满足了所有四个过滤器值。我们真正需要的是找到共享端点的所有匹配边。我们可以为该方法设计一个算法，但其处理时间可以说和用户需求并不相关（用户需求=快速）。也许创建一个平衡的多分类关键字的二值搜索树会更快，差不过类似于数据库索引文件，其将主关键字和外键映射到满足条件的记录集合。我们将在另一篇文章中独立讨论平衡二值搜索树和数据库索引，到时会再次返回到 Airbnb 房源问题上。 Hopcroft-Karp 算法（以及很多其它算法）都基于 DFS（深度优先搜索）和 BFS（广度优先搜索）的图遍历算法。说实话，这里介绍 Hopcroft-Karp 算法的真正原因是逐渐转换到图遍历算法的讨论，相比从二值树开始讨论会更好。 二值树遍历非常漂亮，这大多是因为它们的递归本质。有三种基本的遍历方式称为中序（in-order）、后序（post-order）和前序（pre-order）。如果你曾经遍历过连结串列，这些概念是很好懂的。在连结串列中，你只需要输出当前节点的值（在下方的代码中称为 item），并继续到达下一个节点。 这和二值树几乎相同，输出节点的值，然后到达下一个节点，但在这里，「下一个」指的是两个节点，左节点和右节点。因此你需要分别到达左节点和右节点。不过你有三个不同的选择： 输出节点值，然后到达左节点，再到达右节点。 到达左节点，然后输出节点值，再到达右节点。 到达左节点，然后到达右节点，再输出节点值。 很明显递归函数的形式很优雅，虽然其计算成本很高。每次我们递归地调用一个函数，也就意味着我们调用了一个完全的新函数（如上图所示）。其中「新」的意思是函数变量和局域变量需要分配其它的堆栈内存空间。这正是为什么递归调用的成本如此高（额外的堆栈空间分配和多函数调用）和危险（堆栈溢出），很明显地使用迭代实现会更好。在关键任务（航空、NASA 探测车等）的系统编程中，递归调用是完全禁止的。 实例：Netflix 假设我们要将所有 Netflix 电影存储在二进制搜索树中，并将电影标题作为排序键。所以无论何时用户输入类似「Inter」的内容，我们都会返回一个以「Inter」开头的电影列表，举例，[「Interstellar」,「Interceptor」,「Interrogation of Walter White」]。如果我们将返回标题中包含「Inter」的所有电影（不仅仅是以「Inter」开头的电影）那就太好了，并且该列表将根据电影的评分或与该特定用户相关的内容进行排序（喜欢惊悚片比戏剧更多）。这个例子的重点在于对 BST 进行有效的范围查询，但像往常一样，我们不会深入探讨其余部分。基本上，我们需要通过搜索关键字进行快速查找，然后获得按关键字排序的结果列表，这很可能应该是电影评级和/或基于用户个性化数据的内部排名。我们会尽可能地坚持 KISK 原则（Keep It Simple，Karl）。 「KISK」或「让我们保持它的简单」或「为了简单起见」，这是教程编写者从真实问题中抽象出来的超级借口，并通过在伪代码中引入「abc」简单示例及其解决方案来做出大量假设，并且这些答案在很老的笔记本电脑上也能工作。 这个问题可以很容易地应用到亚马逊的产品搜索上，因为我们通常通过输入描述我们兴趣的文本（如「图算法」）来搜索亚马逊的东西，并根据产品的评分获得结果（我没有在亚马逊的个性化结果中体验过搜索结果，但我很确定亚马逊也是这样做的）。所以，为了公平将这个子标题改为... Netflix 和亚马逊。Netflix 提供电影服务，亚马逊提供产品，我们会将它们命名为「物品」，所以每当你阅读「物品」时，都会想到 Netflix 中的电影或亚马逊的任何 [合格] 产品。这些物品最常用的是解析其标题和描述（我们只处理标题），所以如果一个操作员（通常是一个人通过管理仪表板将项目的数据插入 Netflix / Amazon 数据库）插入新项目到数据库中，它的标题正在被一些「ItemTitleProcessor」处理以产生关键字。 每一个物品都有专属 ID，这个 ID 也链接到了标题之中的关键字。这也是搜索引擎在爬全世界的网站时做的。他们分析每个文档的内容，对其进行标记（将其分解为更小的实体和单词）并添加到表中，该表将每个标记（词）映射到标记已被「看到」的文档标识（网站）。因此，无论何时搜索「hello」，搜索引擎都会获取映射到关键字「hello」的所有文档（实际情况非常复杂，因为最重要的是搜索相关性，这就是为什么谷歌搜索非常棒）。所以 Netflix /亚马逊的类似表格可能看起来像这样（再次，在阅读物品时想一想电影或产品）。 哈希表，再提一次。是的，我们将为此倒排索引（索引结构存储来自内容的映射）保留哈希表。哈希表会将关键字映射到物品的 BST。为什么选择 BST？因为我们希望保持它们的排序并同时提供连续排序的部分（响应前端请求），例如一次请求（分页）中的 100 个物品。这并不能说明 BST 的强大功能，但假设我们还需要在搜索结果中进行快速查找，例如，你需要关键字为「机器」的所有 3 星电影。 请注意，可以在不同的树中复制物品，因为通常可以使用多个关键字找到物品。我们将使用如下面定义的物品进行操作： 每次将新物品插入数据库时，其标题都将被处理并添加到大型索引表中，该表将关键字映射到物品。可能有许多物品共享相同的关键字，因此我们将这些物品保存在按照评分排序的 BST 中。当用户搜索某个关键字时，他们会得到按其评分排序的物品列表。我们如何从排序的树中获取列表？通过按顺序遍历。 这里是一种 InOrderProduceVector() 实现： class BST public 但是呢，我们首先需要最高评价的物品，来替换掉按顺序遍历生成最低评级的物品。这是因为它的性质，是从低到高的顺序遍历物品，「自下而上」。为了得到我们想要的东西，即列表按降序而不是升序排列，我们应该仔细查看顺序遍历实现。我们所做的是通过左节点，然后打印当前节点的值和通过右边的节点。当我们第一次通过左节点时，这就是为什么我们首先获得了「最左」节点（最左节点），这是具有最小值的节点。因此，简单地将实现更改为首先通过正确的节点将导致我们按照列表的降序排列。我们会像其他人一样将其命名，这是一种逆序的遍历。让我们更新上面的代码（引入单个列表、警告、错误）： class BST public 这就对了，我们可以非常快速地提供物品搜索结果。如上所示，反转索引在搜索引擎中最常用，例如谷歌搜索。虽然谷歌搜索引擎非常复杂，它确实利用了某些简单的思想，来将搜索查询匹配到文档上，并尽可能快速地提供结果。 我们使用了树遍历来以分类排序提供结果。在这里，前序/顺序/后序遍历可能太多了，但有时候我们也需要应用其它类型的遍历。让我们来解决这个著名的编程面试问题：「如何按等级输出一个二值树等级？」 DFS vs. BFS 如果你对这个问题不熟悉，想想你在遍历树的时候可用于存储节点的数据结构。如果对比分层遍历树和上文介绍的其他方式（前序/顺序/后序遍历），就会发现两种主要的图遍历方法：深度优先搜索（DFS）和广度优先搜索（BFS）。 深度优先搜索寻找最远的节点，广度优先搜索先寻找最近的节点。 深度优先搜索——更多动作，更少思考。 广度优先搜索——在进一步行动之前先仔细观察四周。 DFS 很像前序/顺序/后序遍历，而 BFS 用于分层输出树节点。我们需要一个队列（数据结构）来存储图的「层级」，同时输出（访问）其「父级」。在之前的插图中，节点是队列中浅蓝色的点。每一层的节点被从队列中取走，同时在访问每个被取走的节点时，我们还应该将其子节点插入队列（为下一层做准备）。下列代码很简单，可以帮助大家了解 BFS。代码假设图是连通的，尽管我们可以修改代码，使其应用于非连通图。 在基于节点的连通图表示上可以轻松地了解基本思想。记住图遍历的实现因图表示而异。BFS 和 DFS 在解决图搜索问题中是重要的工具（但是存在大量图搜索算法）。尽管 DFS 具备优雅的递归实现，但进行迭代实现更合理。我们使用队列进行 BFS 的迭代实现，而 DFS 则需要堆栈。图领域中一个最流行的问题，同时也可能是你阅读本文的原因之一是寻找图节点之间的最短路径。这需要我们进行最后一个实验。 示例：Uber Uber 有 5000 万用户、700 万司机，对于 Uber 来说，最重要的事情之一就是高效匹配司机和乘客。该问题首先是定位的问题。后端要处理数百万份用户请求，将每份请求发送至一或多（通常是多）位附近的司机。尽管将用户请求发送至所有附近司机更加简单，有时也更加智能，但是预处理通常会有所帮助。 除了处理请求、基于用户坐标确定定位、寻找最近坐标的司机以外，我们还需要寻找最适合这趟行程的司机。为了避免地理空间请求处理（对比司机当前坐标和用户坐标，来获取附近车辆），我们假设已经分割了用户和多辆附近车辆的地图，如下图所示： 黄色路线是车辆到用户处的可能路径。问题在于计算车辆到达用户的最小距离，即寻找最短路径。尽管这更多地涉及谷歌地图而不是 Uber，我们仍然尝试解决这一特定和简化案例，其原因主要在于通常存在多辆车，Uber 可能需要计算离用户最近的车。就这张插图而言，这意味着计算全部三辆车的最短路径并确定哪辆车最适合该行程。为了使问题简洁，我们将讨论只有一辆车的情况。下图显示了一些到达用户的可能路径。 我们将该地图分割表示为一个图： 这是一个无定向的加权图（更具体地说是，边加权）。为了找到从 B（车）到 A（用户）的最短途径，我们应该找到他们之间的一条边权重最小的路。你可以自由的设计你自己版本的解决方案，我们还是使用 Dijkstra 的版本。下面的步骤是从维基百科上找到的 Dijkstra 的算法的步骤。 让我们把起始的节点叫做初始节点。节点距离 Y 表示初始节点到 Y 的距离。Dijkstra 的算法将分配一些初始距离值，并尝试一步步地改善它们。 1. 标记所有未访问节点。创建所有未访问节点的集合「unvisited set」。 2. 为每个节点分配一个实验距离值：初始节点的距离值设置为 0，其他节点设置为无穷大。将初始节点设置为当前节点。 3. 对于当前节点，考虑其所有未访问近邻，通过当前节点计算它们的实验距离。对比新计算的实验距离和当前分配的值，分配较小的值。例如，如果当前节点 A 的距离是 6，连接 A 与近邻 B 的边长度为 2，则经过 A 到 B 的距离是 6 + 2 = 8。如果 B 之前标记的距离大于 8，则将其更改为 8。反之，保留当前值。 4. 当我们考虑完当前节点的所有近邻之后，将当前节点标记为已访问，并从 unvisited set 中移除。已访问节点无需再检查。 5. 如果目标节点已经标记为已访问（当规划两个特定节点之间路线的时候），或 unvisited set 中节点之间的最小实验距离是无穷大（当规划完整遍历时，初始节点和其余未访问节点之间没有连接时），则停止，算法结束。 6. 反之，选择标记有最小实验距离的未访问节点，将其设置为新的「当前节点」，并返回第 3 步。 在我们的示例中，我们首先将节点 B（车辆）设置为初始节点。前两步： 我们的 unvisited set 包含了所有的节点，同时也要注意图左边里显示的表格。所有的节点都包括了到 B 和到之前节点的最短距离。例如，从 B 到 F 的最短距离是 20，之前节点是 B。 我们把 B 标注为访问过的然后移动到它的近邻 F。 现在，我们把 F 标注已访问过的，然后在最小实验距离下选下一个没被访问过的节点，就是 G。 就像算法里说的，如果目标节点已经被标注为访问过的（当规划两个特定的节点间的路线时）那么我们可以停止。所以我们下一步用下面的值去停止算法。 所以我们已经有了从 B 到 A 并且经过 F 与 G 的两条最短距离。 这真的是 Uber 里最简单的问题的例子，和冰山类比相比较，我们在冰山的山尖上。然而，这对于我们探索图论应用的真实场景是个好的开始。我没有完成我一开始计划的文章，但在不久的将来这个文章最有可能继续下去（也包括数据库内部索引）。 关于图论有很多内容需要去学习，这篇文章只是冰山一角，非常感谢各位读者有耐心能阅读完 ~ "
150,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738943&idx=1&sn=8c9c1122f1f6b5d6ae13942a37217401&chksm=871ad481b06d5d97d1c84e1d1360afe31f4adb360a3ef82a573138dad50f937f5ed8c51624c1&scene=27,二维码太丑？用风格迁移生成个性二维码了解一下,"Mingliang Xu等 手机二维码太普通，换来换去还是不好看。何不自定义一个自己喜欢的呢？近日，郑州大学、浙江大学、微软亚洲研究院、北京航空航天大学的研究者发布论文，提出一种设计个性化的艺术风格二维码的方法，通过三步自定义独特二维码，并且能保持扫描成功率。 随着互联网和智能移动设备不断普及，二维码（Quick Response code）已成为世界上应用最为广泛的信息载体之一。一般的二维码观感并不好，带有人眼无法识别的单调的黑/白编码模块。最近，二维码的视觉美化在学界和业界兴起了一波热潮。如图 1 所示，当前方法可分为四类： i) 嵌入式 [1]–[5]，嵌入图像以覆盖二维码中的附加位元； ii) 变形类型 [5]，改变二维码模块的形状/颜色，比如把正方块转化为圆形、三角形或星状； iii) 手动型 [5]，来自手工设计和渲染； iv) 融合型 [6]–[12]，把图像融合进二维码。其中，融合型方法的视觉效果最好，也最为吸引眼球。 虽然已有的融合类型的方法可以提升二维码的视觉品质，但仍存在如下几个有待提升的方面：（1）多样化和个性化，主流方法通过改变融合图像来生成不同外观的艺术二维码，但实际上，用户希望对独特的融合图像（例如 logo、个人照片或商标等）生成多样化和个性化的艺术二维码；（2）艺术品质，一方面，大多数的方法将「美」定义为「和融合图像更加相似」，即理想的结果是原始图像不被艺术元素「反客为主」。另一方面，已有的融合类型二维码通常直接、机械地将图像和黑/白编码模块相结合，导致编码模块外观不可变动和单调乏味，即使用很漂亮的图像融合也是这样（如图 4(a)-(d)）。（3）鲁棒性，在不影响可读性的前提下将艺术元素融合到二维码并不容易，大多数已有方法缺乏错误校正机制来保证结果的鲁棒性，从而导致二维码被错误解码。 解决这些问题而不牺牲其它特性是一项很大的挑战。幸运的是，通过基于 CNN 的风格迁移网络，我们找到了有效的解决方案。本文提出了一种自动化生成鲁棒艺术风格二维码的方法——SEE QR Code。对于问题（1），如图 3 所示，我们的 SEE QR Code 是风格导向的艺术二维码，用户可以通过嵌入单张图像生成多种艺术风格的二维码。因此，用户可以按照自己的偏好和需求进行个性化选择。对于问题（2），我们的 SEE QR Code 应该被称为艺术品，而不仅仅是原始图像的风格化；此外，该方法可直接风格化基线艺术二维码（如图 2(a)(b)），可同时为编码模块和融合图像赋予艺术元素，增强其视觉吸引力（如图 4(e)-(l)）。对于问题（3），我们设计了一个错误校正机制以量化和平衡两个竞争要素，即视觉品质和可读性，从而得到鲁棒的结果。 上述现有工作仅聚焦于二维码美化的第一阶段，即通过改变嵌入图（参见图 2(a)(d)）产生有美感的二维码。相比之下，我们的方法主要美化基线审美的二维码（参见图 2(b)）。总结一下，我们的主要贡献有： 我们提出了一种艺术风格审美的二维码——SEE 二维码，在以下方面表现优于现有方法：多样性与个性化、美学品质和鲁棒性。 我们设计了一种高效算法，可在基线艺术二维码中设置模块安排的优先性，从而最小化黑/白编码模块和融合图像之间的视觉对比。 我们调整风格迁移网络以适应把基线艺术二维码作为内容目标，这不仅有效避免了像噪音一样的编码模块造成的视觉影响，还减少了由网络引起的误差模块的数量。 我们提出了一种基于迭代更新的误差修正机制，并通过平衡互相竞争的两个要素：视觉质量和可读性，来保证二维码的鲁棒性。 如下所示为整个系统的架构，在 Stage A 中，我们结合图片与编码信息的二维码而生成艺术二维码。随后再把需要迁移的风格与前面生成的艺术二维码结合，并在 Stage C 中与抽出来的信息编码做误差修正而提升解码鲁棒性。 后面将简要介绍该系统的各个阶段与实验结果，因此我们需要了解表 1 所描述的符号意义： A. 方法概览 如图 6 所示，我们的方法共分为三步：Stage A、Stage B 和 Stage C。在 Stage A 中，我们生成一个优化过的艺术二维码 Q_a。在 Stage B 中，我们使用一个调整过的神经风格迁移网络对 Q_a 进行风格化，并输出非鲁棒的艺术风格二维码 Q_b。最后，在 Stage C 中，我们通过误差修正机制修复 Q_b 的可读性，获得鲁棒的结果 Q_c。以下将分别介绍这三个基本步骤。 二维码基于 RS 纠错码的编码规则，表达为方形的编码模块。[13] 中证明我们可以使用 Gauss-Jordan 消去法在有限范围内修改模块的颜色（即黑色或白色），而无需考虑机器可读性。主流研究（[7] [8] [10]）通常使用 [13] 中的方法，通过考虑融合图像的局部视觉特征（如显著性图、边缘图或感兴趣区域）来选择可变模块。在 Stage A 中，我们提出一种高效策略来设置可变模块的优先级，即根据混合图像 I 的全局特征进行选择，并最终输出基线艺术二维码 Q_a，其最小化图像 I 和类似噪点黑白模块之间的视觉对比。 关于融合图像 I^g 的灰度，每个像素的灰度值在 [0, 255] 区间内，黑白模块的灰度值分别是恒定值 0 和 255。因此，我们认为当模块的颜色最接近图像 I 中的对应区域时视觉对比度最小。也就是说，将图像 I 中最黑／最白颜色的区域赋予高权重来对应黑白模块，将大大优化 Q_a 的视觉感知度。基于此，我们将 Stage A 分为计算权重矩阵和融合图像两个步骤。 本论文使用的风格迁移架构如下图 8 所示，我们大致 yan 用了由 J. Johnson[16] 等人提出的框架。对于风格迁移来说，我们输入需要迁移风格的目标图像 a_c 和风格图像 a_s，而输出的 a hat 应该组合 a_c 的内容和 a_s 的风格特征。 在 Stage B 中，基线艺术二维码 Q_a 有非常密集的黑白编码模块作为需要迁移风格的内容，那么这就要求我们解决两个问题：1）为了更强的鲁棒性，所产生的错误模块数需要最小化；2）为了视觉质量，避免类似噪点的模块带来视觉影响是非常重要的。因此为了解决这两个问题，如下我们对内容重构层和风格重构层等网络架构做了进一步的修改： 虽然我们在 Stage B 中显著地优化了鲁棒性弱的问题，然而，在 Q_b 中仍然存在少量误差模块（error-modules）。因此，在 Stage C，我们设计了一个误差修正机制，通过平衡鲁棒性和视觉品质来检测和校正 Q_b 的误差模块，以生成鲁棒的结果 Q_c。 B 编码模块的鲁棒性评估 C 误差修正机制 论文：Stylize Aesthetic QR Code 论文链接：https://arxiv.org/pdf/1803.01146.pdf 摘要： 随着智能移动设备的持续发展，二维码的使用越来越广泛。现有研究试图美化二维码的外观，并开发了一系列相关技术。但是，这些研究仍有很大改进空间，如视觉多样性、美学质量、灵活性、通用性质和鲁棒性。为了解决这些问题，本论文提出了一种美化二维码的新方法 SEE（Stylize aEsthEtic），只需三步即可自动生成鲁棒的艺术风格二维码。具体来说，第一步，我们提出一种方法来生成优化的基线艺术二维码，减少噪声类黑白模块和融合图像之间的视觉对比度。第二步，为获取艺术风格的二维码，我们调整一个合适的神经风格迁移网络，给基线艺术二维码增加一些抽象化的艺术元素。第三步，我们设计了一种误差修正机制，通过平衡两种这两种存在竞争的元素：视觉质量和可读性，以确保性能的鲁棒性。大量实验证明 SEE 二维码在外观和鲁棒性方面都保持高质量，同时使用户拥有更多个性化选择。  "
151,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738847&idx=4&sn=11769c4b7532be4c0cb884253187ca67&chksm=871ad4e1b06d5df7f75c19cc4d6c6a13f3edddf251c2936519e0fd1f8fbcb2debe7e1cc80fae&scene=27,CVPR 2018 | UNC&Adobe提出模块化注意力模型MAttNet，解决指示表达的理解问题,"选自 北卡教堂山分校 (UNC) 虞立成等人近日发表的 CVPR 2018 论文提出了模块化注意力模型 MAttNet，将 Referring Expression（指示表达）分解为三个模块：主语、位置和关系，并基于句子和图片的联合注意力解析，解决基于指示表达的目标定位问题。实验表明 MAttNet 在基于自然语句的目标检测和分割两种任务上都大幅优于前文的方法。该论文已被 CVPR 2018 录取，并提供了代码以及 demo。 代码链接：https://github.com/lichengunc/MAttNet Demo 链接：http://vision2.cs.unc.edu/refer/comprehension 任务 Referring Expression（指示表达）指描述图片中某一特定物体的一句自然语句。与 Image Captioning 任务不同的是 Referring Expression 具有唯一性和区域性，比如「穿红色毛衣的女人」或「右边的男孩」。在日常生活中，无论人与人之间的交流或是人机交互，都会涉及这种指示性的短语表达。所以理解它们并准确定位所描述的物体成为自然交互的必要条件。 指示表达理解的问题可以被定义成：从图片 I 里的各个物体〖O={o_i }〗_(i=1)^N 中选出那个「最」能被指示表达 r 贴切描述的物体 o^*。过往在此问题上的研究可以分为两类：基于 CNN-LSTM 的框架建模的 P(r|o)，以及基于联合概率框架建模的 P(r, o)。本文使用后者。 模型与方法 文章对数据集进行统计后发现根据目标物体和其他物体的差异性，对它的指示表达可以涉及不同类型的信息。例如，如果目标物体为十个黑球中的一个红球，那么对它的指示表达可以很简单得说「红球」。如果相同的红球被放在其他三个红球中，那么基于位置的信息可能变得更加重要，例如「右边的红球」。或者，如果在场景中有 100 个红球，那么球与其他物体之间的关系可能是最明显的信息，例如「猫旁边的红球」。因此，将指示语句的理解模型进行模块化分析便显得非常自然直观。本文利用主语，位置和关系模块来解析输入的指示语句，然后触发不同的视觉模块进行处理。 本文提出的模块化注意力模型 (MAttNet) 如图 1 所示。模型首先将输入的指示表达分解成三个短语表征，作为三个视觉模块的输入，这些视觉模块用不同的注意力模型分别计算与其对应的短语的匹配分数。最后，三个模块的匹配分数的加权总和成为整体匹配得分。整个模型可以无缝链接 Mask R-CNN 框架，因而可以做目标定位和目标分割两个任务。 具体来分析模型的细节。首先是自然语言的分解模块，作者提出的 Language Attention Network（语言注意力模型）对输入的指示表达进行主语，位置和关系的三个模块的拆解。每个拆解后的成分会有两个输出，其一是该模块的词向量表征，其二是该模块占整句句子的权重。然后是三个视觉模块，给定某个备选物体 (candidate object)，我们计算它与三个词向量表征的匹配分数。其中，主语的视觉模块抽取物体的 R-CNN 特征，并使用该模块内部的软注意力模型抽取与词向量相关的区域，计算匹配分数；位置的视觉模块抽取物体的位置特征，将其映射到高维空间后与位置词向量进行匹配；关系的视觉模块抽取其周边其他物体的特征，使用多示例学习 (Multiple Instance Learning) 选取与关系词向量最相关的周边物体，计算得分。最后，三个匹配得分会和上述的三个权重进行内积，得到最终匹配得分。 实验结果 实验中，首先为了和前文进行公平比较，使用了 VGG16 特征，在目标定位上超过前文~2%。配合上 Mask R-CNN 的特征后，优势扩展为~7%。在目标分割上，基本达到前文最佳精度的两倍。此外，模型具有很好的解释性，作者展示了三个模块各自注意到的相关单词和视觉区域。如图 2 所示。 结束语 指示表达的应用很广，人与人的对话系统和人机交互系统都经常会涉及对图片或场景里的某个目标物体进行自然语言的描述。从作者提供的 demo 来看，方向性的指示表达和对「人」这一类的指示表达，理解得都还比较准确；但对其他物体的指示理解还有进一步的提升空间。作者在提供的代码的最后写了一些 Notes，可以作为后续工作的思考方向。   图 2：MAttNet 在 RefCOCOg 上的结果：第一列为原图，第二列为输入的指示表达与其分解结果，第三列为主语模块的注意区域；第四列为主语模块的 attribute 输出；第五列为目标定位（蓝筐）；第六列为目标分割。 "
152,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738943&idx=4&sn=e4c383d9c4c0dc8fcfd3b621de38db0e&chksm=871ad481b06d5d97ee22dc18f2953731af1cb0a231028739b7623673b84946e304379e03bc98&scene=27,学界 | 多任务学习概述论文：从定义和方法到应用和原理分析,"选自National Science Review 张宇、杨强 多任务学习是一个很有前景的机器学习领域，相关的理论和实验研究成果以及应用也在不断涌现。近日，香港科技大学计算机科学与工程系的杨强教授和张宇助理教授在《国家科学评论（National Science Review）》2018 年 1 月份发布的「机器学习」专题期刊中发表了题为《An overview of multi-task learning》的概述论文，对多任务学习的现状进行了系统性的梳理和介绍。机器之心对该论文的主干内容进行了编译介绍，欲更深入了解多任务学习的读者可参阅原论文及论文中列出的相关参考文献。 论文链接：https://academic.oup.com/nsr/article/5/1/30/4101432 摘要 多任务学习（MTL）是一个很有前景的机器学习领域，其目标是通过利用多个相关学习任务之间的有用信息来提升它们的表现。我们在本论文中对 MTL 进行了概述。首先我们将介绍 MTL 的定义，然后会介绍多种不同的 MTL 设置，其中包括多任务监督学习、多任务无监督学习、多任务半监督学习、多任务主动学习、多任务强化学习、多任务在线学习和多任务多视角学习。对于每种设置，我们将给出代表性的 MTL 模型。我们还将介绍用于加速学习过程的并行和分布式 MTL 模型。我们也将概览使用 MTL 来提升性能的诸多应用领域（包括计算机视觉、生物信息学、健康信息学、语音、自然语言处理、网络应用和普适计算）和一些代表性的成果。最后我们将介绍近期对 MTL 的理论分析。 引言 利用历史数据中的有用信息来帮助分析未来数据的机器学习，通常需要大量有标签数据才能训练出一个优良的学习器。深度学习模型是一种典型的机器学习模型，因为这类模型是带有很多隐藏层和很多参数的神经网络，所以通常需要数以百万计的数据样本才能学习得到准确的参数。但是，包括医学图像分析在内的一些应用无法满足这种数据要求，因为标注数据需要很多人力劳动。在这些情况下，多任务学习（MTL）可以通过使用来自其它相关学习任务的有用信息来帮助缓解这种数据稀疏问题。 MTL 是机器学习中一个很有前景的领域，其目标是利用多个学习任务中所包含的有用信息来帮助为每个任务学习得到更为准确的学习器。我们假设所有任务（至少其中一部分任务）是相关的，在此基础上，我们在实验和理论上都发现，联合学习多个任务能比单独学习它们得到更好的性能。根据任务的性质，MTL 可以被分类成多种设置，主要包括多任务监督学习、多任务无监督学习、多任务半监督学习、多任务主动学习、多任务强化学习、多任务在线学习和多任务多视角学习。 多任务监督学习任务（可能是分类或回归问题）是根据训练数据集（包含训练数据实例和它们的标签）预测未曾见过的数据的标签。多任务无监督学习任务（可能是聚类问题）是识别仅由数据构成的训练数据集中的有用模式。多任务半监督学习任务与多任务监督学习类似，只是其训练集中不仅包含有标签数据，也包含无标签数据。多任务主动学习任务是利用无标签数据来帮助学习有标签数据，这类似于多任务半监督学习，其差异在于主动学习会选择无标签数据来主动查询它们的标签从而降低标注开销。多任务强化学习任务是选择动作以最大化累积奖励。多任务在线学习任务是处理序列数据。多任务多视角学习任务是处理多视角数据——其中每个数据实例都有多组特征。 MTL 可以看作是让机器模仿人类学习行为的一种方法，因为人类常常将一个任务的知识迁移到另一个相关的任务上。例如，根据作者自身经验，打壁球和打网球的技能可以互相帮助提升。与人类学习类似，（机器）同时学习多个学习任务是很有用的，因为一个任务可以利用另一个相关任务的知识。 MTL 也与机器学习的某些其它领域有关，包括迁移学习、多标签学习和多输出回归，但 MTL 也有自己不同的特点。比如说，类似于 MTL，迁移学习的目标也是将知识从一个任务迁移到另一个任务，但不同之处在于迁移学习希望使用一个或多个任务来帮助另一个目标任务，而 MTL 则是希望多个任务彼此助益。当多任务监督学习的不同任务使用了同样的训练数据时，这就变成了多标签学习或多输出回归。从这个意义上看，MTL 可以被看作是多标签学习和多输出回归的一种泛化。 我们在本论文中对 MTL 进行了概述。我们首先将先给出 MTL 的定义。然后我们将探讨不同的 MTL 设置，其中包括多任务监督学习、多任务无监督学习、多任务半监督学习、多任务主动学习、多任务强化学习、多任务在线学习和多任务多视角学习。对于每种 MTL 设置，我们会介绍代表性的 MTL 模型。当任务数量很多或不同任务的数据位于不同的机器中时，就必需使用并行和分布式 MTL 模型；我们将会介绍几种这样的模型。作为一种大有前景的学习范式，MTL 已经在多个领域得到了应用，其中包括计算机视觉、生物信息学、健康信息学、语音、自然语言处理、网络应用和普适计算；我们将会给出每个领域的一些有代表性的应用。此外，我们也将回顾对 MTL 的理论分析，这能为我们提供对 MTL 的深度理解。 多任务学习 首先，我们给出 MTL 的定义。 定义 1（多任务学习）：给定 m 个学习任务，其中所有或一部分任务是相关但并不完全一样的，多任务学习的目标是通过使用这 m 个任务中包含的知识来帮助提升各个任务的性能。 基于这一定义，我们可以看到 MTL 有两个基本因素。 第一个因素是任务的相关性。任务的相关性是基于对不同任务关联方式的理解，这种相关性会被编码进 MTL 模型的设计中。 第二个因素是任务的定义。在机器学习中，学习任务主要包含分类和回归等监督学习任务、聚类等无监督学习任务、半监督学习任务、主动学习任务、强化学习任务、在线学习任务和多视角学习任务。因此不同的学习任务对应于不同的 MTL 设置，这也是后面的章节所关注的重点。在后面的章节中，我们将回顾不同 MTL 设置中的代表性 MTL 模型。 多任务监督学习 多任务监督学习（MTSL）意味着 MTL 中的每个任务都是监督学习任务，其建模了从数据到标签的函数映射。 基于特征的 MTSL 在这一类别中，所有 MTL 模型都假设不同的任务都具有同样的特征表示，这是根据原始特征学习得到的。根据这种共有的特征表示的学习方式，我们进一步将多任务模型分为了三种方法，包括特征变换方法、特征选择方法和深度学习方法。特征变换方法学习到的共有特征是原始特征的线性或非线性变换。特征选择方法假设共有特征是原始特征的一个子集。深度学习方法应用深度神经网络来为多个任务学习共有特征，该表征会被编码在深度神经网络的隐藏层中。 特征变换方法 在这种方法中，共有特征是原始特征的一种线性或非线性变换。 特征选择方法 特征选择方法的目标是选择原始特征的一个子集来作为不同任务的共有特征。 深度学习方法 与特征变换方法中的多层前向神经网络模型类似，深度学习方法中的基本模型包括卷积神经网络和循环神经网络等高级神经网络模型。但是，不同于仅有少数隐藏层（比如 2 层或 3 层）的多层前向神经网络，深度学习方法涉及到的神经网络有数十乃至数百个隐藏层。此外，与多层前向神经网络类似，这一类别中的大多数深度学习模型都会将某一隐藏层的输出作为共有特征表征。[23] 中提出的 cross-stitch 网络则与这些深度模型不同，而是会将来自两个任务的隐藏特征组合起来构建更强大的隐藏特征。关于 cross-stitch 网络的示意图参见图 1。 基于参数的 MTSL 基于参数的 MTSL 使用模型参数来关联不同任务的学习。根据不同任务的模型参数的关联方式，我们可将其分成 5 种方法，包括低秩方法、任务聚类方法、任务关系学习方法、脏方法（dirty approach）和多层次方法。具体而言，因为假设任务是相关的，所以参数矩阵 W 很可能是低秩的，这是低秩方法提出的动机。任务聚类方法的目标是将任务分成多个集群，并假设其中每个集群中的所有任务都具有同样的或相似的模型参数。任务关系学习方法是直接从数据中学习任务间的关系。脏方法假设参数矩阵 W 可以分解成两个分量矩阵，其中每个矩阵都由一种稀疏类型进行正则化。多层次方法是脏方法的一种泛化形式，是将参数矩阵分解成两个以上的分量矩阵，从而建模所有任务之间的复杂关系。 低秩方法 相似的任务通常具有相似的模型参数，这使得 W 很可能是低秩的。 任务聚类方法 任务聚类方法是应用数据聚类方法的思想来将任务分成多个簇，其中每个簇中的任务具有相似的模型参数。 任务关系学习方法 这种方法使用任务关系来反映任务相关性，任务关系的例子包括相似度和协方差。 脏方法 脏方法假设参数矩阵 W 可以分解为两个分量矩阵：W = U + V，其中 U 和 V 各自包含了任务相关性的不同部分。在这种方法中，不同模型的目标函数可以被统一成一个目标函数，此目标函数最小化所有任务上的训练集损失以及 U 和 V 的两个正则化项 g(U) 和 h(V)。因此，这种方法的不同类型主要体现在 g(U) 和 h(V) 的选择上，具体可参见表 1。 多层次方法 多层次方法是脏方法的一种泛化形式。它是将参数矩阵 W 分解成多个（多于两个）分量矩阵。多层次方法有助于建模复杂的任务结构。 基于实例的 MTSL 这一类别的研究很少，其中 [61] 提出的多任务分布匹配方法是其中的代表。具体来说，它首先评估每个实例来自其自己的任务的概率和来自所有任务的混合的概率之比。在确定了此比率之后，这种方法会利用此比率针对每一个任务来加权所有任务的数据，并利用加权的数据来学习每一个任务的模型参数。 讨论 基于特征的 MTSL 可以为不同的任务学习常见的特征表示，而且更适合原始特征信息不多和区分度不大的应用，比如计算机视觉、自然语言处理和语音。但是，基于特征的 MTSL 容易轻易被与其它无关的离群任务（outlier task）影响，因为它难以为彼此无关的任务学习共有特征。在有良好的特征表征时，基于参数的 MTSL 可以学习到更加准确的模型参数，而且它也对离群任务更为鲁棒。因此，基于特征的 MTSL 和基于参数的 MTSL 可以相互补充。基于实例的 MTSL 目前还处于探索阶段，它与其它两种方法并行发展。 总而言之，在 MTL 研究中，MTSL 是最重要的，因为这是其它设置研究的基础。在 MTL 领域已有的研究工作中，有大约 90% 都是关于 MTSL 设置；而在 MTSL 设置中，基于特征和基于参数的 MSTL 得到的关注最多。 多任务无监督学习 不同于每个数据实例都关联了一个标签的多任务监督学习，多任务无监督学习的训练集仅由数据样本构成，其目标是挖掘数据集中所包含的信息。典型的无监督学习任务包括聚类、降维、流形学习（manifold learning）和可视化等，而多任务无监督学习主要关注多任务聚类。聚类是指将一个数据集分成多个簇，其中每簇中都有相似的实例，因此多任务聚类的目的是通过利用不同数据集中包含的有用信息来在多个数据集上同时执行聚类。 多任务半监督学习 在很多应用中，数据通常都需要很多人力来进行标注，这使得有标签数据并不很充足；而在很多情况下，无标签数据则非常丰富。所以在这种情况下，可以使用无标签数据来帮助提升监督学习的表现，这就是半监督学习。半监督学习的训练集由有标签和无标签的数据混合构成。在多任务半监督学习中，目标是一样的，其中无标签数据被用于提升监督学习的表现，而不同的监督学习任务则共享有用的信息来互相帮助。 多任务主动学习 多任务主动学习的设置和多任务半监督学习几乎一样，其中每个任务的训练集中都有少量有标签数据和大量无标签数据。但是不同于多任务半监督学习，在多任务主动学习中，每个任务都会选择部分无标签数据来查询一个 oracle 以主动获取其标签。因此，无标签数据的选择标准是多任务主动学习领域的主要研究重点。 多任务强化学习 受行为心理学的启发，强化学习研究的是如何在环境中采取行动以最大化累积奖励。其在很多应用上都表现出色，在围棋上击败人类的 AlphaGo 就是其中的代表。当环境相似时，不同的强化学习任务可以使用相似的策略来进行决策，因此研究者提出了多任务强化学习。 多任务在线学习 当多个任务的训练数据以序列的形式出现时，传统的 MTL 模型无法处理它们，但多任务在线学习则可以做到。 多任务多视角学习 在计算机视觉等一些应用中，每个数据样本可以使用不同的特征来描述。以图像数据为例，其特征包含 SIFT 和小波（wavelet）等。在这种情况下，一种特征都被称为一个视角（view）。多视角学习就是为处理这样的多视角数据而提出的一种机器学习范式。与监督学习类似，多视角学习中每个数据样本通常都关联了一个标签。多视角学习的目标是利用多个视角中包含的有用信息在监督学习的基础上进一步提升表现。多任务多视角学习是多视角学习向多任务的扩展，其目标是利用多个多视角学习问题，通过使用相关任务中所包含的有用信息来提升每个多视角学习问题的性能。 并行和分布式 MTL 当任务数量很大时，如果我们直接应用一个多任务学习器，那可能就会有很高的计算复杂度。现在计算机使用了多 CPU 和多 GPU 架构，其计算能力非常强大。所以我们可以使用这些强大的计算设备来设计并行 MTL 算法，从而加速训练过程。[82] 中设计了一种并行 MTL 方法来解决 MTRL 模型的一个子问题，这个子问题也会出现在很多属于任务关系学习方法的正则化方法中。具体而言，这种方法利用了 FISTA 算法设计了一种针对所有任务的可分解的代理函数（surrogate function），这个代理函数可以并行化，从而实现学习过程加速。此外，[82] 还研究了三种损失函数（hinge、ε-insensitive 和平方损失），让这种并行方法适用于 MTSL 中的分类和回归问题。 在某些情况中，用于不同任务的训练数据可能存在不同的机器中，这会使传统的 MTL 模型难以工作。如果将所有的训练数据都可转移到一台机器上，这会造成额外的传输和存储成本。设计能够直接处理分布在多台机器上数据的分布式 MTL 模型是更好的选择。[83] 提出了一种基于 debiased lasso 模型的分布式算法，该算法在一台机器上学习一个任务，并实现了高效的通信。 多任务学习的应用 包括计算机视觉、生物信息学、健康信息学、语音、自然语言处理、网络应用和普适计算在内的很多领域都在使用 MTL 来提升各自的应用的性能。 理论分析 学习理论（learning theory）用来研究机器学习的学习模型（包括 MTL 模型）的理论基础。MTL 领域的理论分析主要关注的是 MTL 模型的泛化边界（generalization bound）。众所周知，MTL 的主要关注点是 MTL 在测试数据上的泛化表现。但是因为难以对底层的数据分布建模，所以泛化表现难以直接计算。因此泛化边界被提出来用于提供泛化表现的上界。 [133] 首次为通用 MTL 模型推导出了泛化边界，然后很多研究分析了不同 MTL 方法的泛化边界，包括针对特征变换方法的泛化边界 [7,134]、针对特征选择方法的泛化边界 [135]、针对低秩方法的泛化边界 [24,135–138]、针对任务关系学习方法的泛化边界 [136] 和针对脏方法的泛化边界 [138]。 结论 在本文中，我们对 MTL 进行概述。首先，我们给出 MTL 的定义。在此基础上，我们介绍了多任务监督学习、多任务无监督学习、多任务半监督学习、多任务主动学习、多任务强化学习、多任务在线学习和多任务多视角学习等多种不同的 MTL 设置。对于每种设置，我们介绍了其代表性模型。然后讨论了并行和分布式 MTL 模型，这可以帮助加快学习过程。最后，我们回顾了 MTL 在各个领域的应用，并对 MTL 进行了理论分析。 最近，深度学习在诸多应用领域中广为流行，并且深度学习十分适用于 MTL。几乎所有的深层模型都会为不同的任务共享隐藏层；当各种任务非常相似时，这种在任务之间共享知识的方式非常有用，但是一旦这种假设被违背，模型性能则显著恶化。我们认为，多任务深度模型的未来发展方向是设计更加灵活的架构，可以容纳不相关的任务甚至异常的任务。此外，深度学习，任务聚类和多层次方法缺乏理论基础，需要更多的分析来指导这些方面的研究。  英文原文 2017 年 9 月发表于《国家科学评论》(National Science Review, NSR)，原标题为「An overview of multi-task learning」。《国家科学评论》是科学出版社旗下期刊，与牛津大学出版社联合出版。机器之心经《国家科学评论》和牛津大学出版社授权刊发该论文文中文翻译。 "
153,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738847&idx=3&sn=20121a74c657fa3d69ae2835463bd663&chksm=871ad4e1b06d5df7114f79482782b8622d09edbecb939e4bcbbb3b85b2489bcb5e71aa8cdf3c&scene=27,业界 | OpenAI提出Reptile：可扩展的元学习算法,"选自OpenAI Blog 近日，OpenAI 发布了简单元学习算法 Reptile，该算法对一项任务进行重复采样、执行随机梯度下降、更新初始参数直到习得最终参数。该方法的性能可与 MAML（一种广泛应用的元学习算法）媲美，且比后者更易实现，计算效率更高。 元学习是学习如何学习的过程。元学习算法会学习任务的一个分布，每项任务都是学习问题，并输出快速学习器，学习器可从少量样本中学习并进行泛化。一个得到充分研究的元学习问题是 few-shot 分类，其中每项任务都是分类问题，学习器只能看到 1-5 个输入-输出样本（每个类别），之后学习器必须对新输入进行分类。下面，你可以尝试 OpenAI 的 1-shot 分类交互 Demo，其使用了 Reptile。 点击「Edit All」按钮，绘制三种不同的形状或符号，然后在后侧的输入区域绘制其中一个形状，就可以看到 Reptile 的分类效果。前三个图是标注样本：每个定义一类。最后的图表示未知样本，Reptile 输出其属于每个类别的概率。（请点击原文链接体验交互） Reptile 的工作原理 和 MAML 类似，Reptile 会学习神经网络的参数初始化方法，以使神经网络可使用少量新任务数据进行调整。但是 MAML 通过梯度下降算法的计算图来展开微分计算过程，而 Reptile 在每个任务中执行标准形式的随机梯度下降（SGD）：它不用展开计算图或计算任意二阶导数。因此 Reptile 比 MAML 所需的计算量和内存都更少。伪代码如下： 最后一步也可以把 Φ−W 作为梯度，将其插入如 Adam 等更复杂的优化器。 很令人震惊，该方法运行效果很好。如果 k=1，该算法对应「联合训练」（joint training）：在多项任务上执行 SGD。尽管联合训练在很多情况下可以学到有用的初始化，但在 zero-shot 学习不可能出现的情况下（如输出标签是随机排列的）它能学习的很少。Reptile 要求 k>1，更新依赖于损失函数的高阶导数。正如 OpenAI 在论文中展示的那样，k>1 时 Reptile 的行为与 k=1（联合训练）时截然不同。 为了分析 Reptile 的工作原理，OpenAI 使用泰勒级数逼近更新。Reptile 更新最大化同一任务中不同小批量的梯度内积，以改善泛化效果。该发现可能在元学习之外也有影响，如解释 SGD 的泛化性能。OpenAI 的分析结果表明 Reptile 和 MAML 可执行类似的更新，包括具备不同权重的相同两个项。 在 OpenAI 的实验中，他们展示了 Reptile 和 MAML 在 Omniglot 和 Mini-ImageNet 基准上执行 few-shot 分类任务时具备类似的性能。Reptile 收敛速度更快，因为其更新具备更低的方差。OpenAI 关于 Reptile 的分析表明，我们可以使用不同的 SGD 梯度组合获取大量不同的算法。在下图中，假设我们在不同任务中使用不同批量大小的 SGD 执行 K 个更新步，产生 g_1,g_2,…,g_k k 个梯度。下图展示了在 Omniglot 上的学习曲线，且它由梯度的和作为元梯度而绘制出。g_2 对应一阶 MAML，即原版 MAML 论文提出的算法。由于方差缩减，使用更多的梯度会导致更快的学习或收敛。注意仅使用 g_1（对应 k=1）如预测那样在这个任务中没有什么提升，因为我们无法改进 zero-shot 的性能。 实现 实现的 GitHub 地址：https://github.com/openai/supervised-reptile 该实现应用 TensorFlow 进行相关的计算，代码可在 Omniglot 和 Mini-ImageNet 上复现。此外，OpenAI 也发布了一个更小的基于 JavaScript 的实现（https://github.com/openai/supervised-reptile/tree/master/web），其对使用 TensorFlow 预训练的模型进行了调整——以上 demo 就是基于此实现的。 最后，下面是一个 few-shot 回归的简单示例，预测 10(x,y) 对的随机正弦波。该示例基于 PyTorch： 论文：Reptile: a Scalable Metalearning Algorithm  地址：https://d4mucfpksywv.cloudfront.net/research-covers/reptile/reptile_update.pdf 摘要：本论文讨论了元学习问题，即存在任务的一个分布，我们希望找到能在该分布所采样的任务（模型未见过的任务）中快速学习的智能体。我们提出了一种简单元学习算法 Reptile，它会学习一种能在新任务中快速精调的参数初始化方法。Reptile 会重复采样一个任务，并在该任务上执行训练，且将初始化朝该任务的已训练权重方向移动。Reptile 不像同样学习初始化的 MAML，它并不要求在优化过程中是可微的，因此它更适合于需要很多更新步的优化问题。我们的研究发现，Reptile 在一些有具备完整基准的 few-shot 分类任务上表现良好。此外，我们还提供了一些理论性分析，以帮助理解 Reptile 的工作原理。 原文链接：https://blog.openai.com/reptile/ "
154,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738847&idx=5&sn=3eb6e4f12d0ca3fc4153ffcdc7f7955a&chksm=871ad4e1b06d5df7ce1aba5cd9b549722eeb20826fe75e71425bccfc561232c15843fafd4d23&scene=27,活动 | 欢迎报名参加TalkingData全球广告反欺诈算法大赛,"广告无处不在，如果广告点击都是假的，我们的世界将会怎样？ 图片来自：Medium 广告无处不在，它早已是我们生活中不可或缺的一部分，但你是否想过，如果广告点击是假的，我们的世界会怎样？  或许有人会庆幸，广告点击是假的，就无所谓广告主投放的广告需要与目标受众匹配，心急火燎地追剧时不必关心等待数十秒的广告是否会有惊喜，刷朋友圈时也无需再动动手指查看“赞助商提供的广告信息”，点击“我不感兴趣”。 但要知道，广告可是不少科技巨头的重要收入来源，无论国外的 Google、Facebook，还是国内 BAT，广告都是其营收的重要组成部分，且有逐年上涨的趋势。 图片来自：Visual Capitalist 2017 年 Business Insider 援引 Visual Capitalist 发布的报告称，2016 年 Google 母公司 Alphabet 及 Facebook 的收入构成中，广告占比分别高达 88% 和 97%。 如果这些广告点击都是假的，引发的经济问题和社会问题恐怕难以想象。 广告创造如此多的商业利润，却难掩其尴尬之处，美国百货商店之父 John Wanamaker 那句广为流传的名言一语道破天机： 我知道我在广告上的投资有一半浪费了，但问题是我不知道是哪一半。 事实上，在巨额的商业利益驱动下，广告带来的不止尴尬，还有欺诈，浪费的广告费也远不止一半。 图片来自：TalkingData 2017 移动广告行业报告 TalkingData 2017 移动广告行业报告显示，2017 年移动效果广告点击总量相比 2016 年增长超过 17 倍，其中 iOS 平台点击量同比增长 1776.2%，安卓平台点击量同比增长 366.2%。 而与此同时，2017 年移动效果广告推广激活总量同比增长 41.9%，其中 iOS 平台同比增长 17.1%，安卓平台同比增长 123.2%。 点击量增速远超激活量，虚假点击刷量严重。 一方面是广告创造的巨额收益，另一方面广告欺诈触目惊心，广告反欺诈刻不容缓。在这样的背景下，TalkingData 联合 Kaggle 共同发起 TalkingData 全球广告反欺诈算法大赛。 本次大赛将于  UTC 时间 3 月 5 日至 5 月 7 日 举行，赛题已于北京时间今日凌晨正式在 Kaggle 平台上线，5 月 28 日，获奖团队将进行结果展示。 正如之前的 预告所说 （可点击查看），本次大赛前三名共将获得  25,000 美元 奖金，其中第一名获得奖金  12,500 美元 。除此之外，你还有机会解决真正的商业问题。 解决真正的商业问题 让面向 2 万多广告主的数据驱动的广告监测产品更有效 https://edu.talkingdata.com/competition#/  查看我们的中文介绍，然后点击“我要参赛”跳转至 Kaggle 官网赛题页面，登录你的 Kaggle 账号即可参赛。 如果你之前没有参加过 Kaggle 比赛，别担心，我们来为你答疑解惑。 3 月 28 日 晚 20:00-21:00 我们将举行一场线上微信群文字直播，为大家答疑解惑。我们邀请了去年获得京东金融数据探索者大赛商业组前五名的  TalkingData Sniper 战队 （可点击查看） 给大家分享他们以往参加数据竞赛的经验。 怎么参加直播？扫描下方二维码入群即可。 如果群成员人数已达上限，您可以扫描下方二维码或搜索  TDU2018  添加 TDU 微信号，备注： 姓名+学校+专业 或者 姓名+公司+职位 ，直播开始前，我们会统一拉大家入群。 同时，我们也为大家准备了一些 TalkingData 周边，包括两只 TD 兔和三个 TalkingData 定制水杯，直播结束后在群里为大家抽奖。 有奖品，有奖金，赶快 参赛吧！ 题图来自：Medium 参考资料： Business Insider : The Tech Takeover of Advertising in One Chart http://www.businessinsider.com/the-tech-takeover-of-advertising-in-one-chart-2017-9 Wikipedia : John Wanamaker https://zh.wikipedia.org/wiki/%E7%BA%A6%E7%BF%B0%C2%B7%E6%B2%83%E7%BA%B3%E6%A2%85%E5%85%8B TalkingData 2017 移动广告行业报告 http://mi.talkingdata.com/report-detail.html?id=711 "
155,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738847&idx=1&sn=46a2acb5ce367ddd9cede5cb6aae0322&chksm=871ad4e1b06d5df7b1a9f2ac6130dc64698f3753852649fd9af75aed2a212ce37604dd68d5cd&scene=27,百度宣布成立量子计算研究所，量子科学家段润尧教授出任所长,"机器之心报道   3 月 8 日，百度宣布成立量子计算研究所，开展量子计算软件和信息技术应用业务研究，悉尼科技大学量子软件和信息中心创办主任段润尧教授出任百度量子计算研究所所长，直接向百度总裁张亚勤汇报。百度计划在五年内组建世界一流的量子计算研究所，并逐步将量子计算融入到业务中，成为阿里巴巴、腾讯之后，又一进军量子计算的国内巨头之一。   量子纠缠特性与应用领域专家段润尧 段润尧，本科和博士均就读于清华大学计算机系，师从应明生教授。悉尼科技大学终身教授，澳大利亚研究理事会（ARC）Future Fellow，自 2016 年 9 月 15 日起担任量子软件和信息中心创办主任。主要从事量子计算和量子信息论，特别是有关量子纠缠特性与应用以及量子通信信道容量等方面的研究。在量子状态/操作分辨、利用有噪量子信道进行精确通信、量子纠缠转换理论等课题上做出一系列重要贡献。 2016 年，他与来自巴塞罗那自治大学 (UAB) 的 Andreas Winter 教授合作首次用量子信息论方法给出著名的 Lovász number 的完整信息论解释，从而解决了信息论和图论中自 1979 年以来一直悬而未决的公开问题。 截至目前，他已经在国际顶级学术期刊会议上发表论文 80 余篇。曾主持或作为主要参与人完成量子计算方面多项国家自然科学基金项目，一个 863 项目，以及两项 ARC 项目。曾于 2013-2016 年担任 QIP 会议（量子计算和量子信息科学理论方面最顶级的学术会议）管委会委员和 2015 年主席，并作为组委会主席成功在悉尼举办 QIP2015。 段润尧出任百度量子计算研究所所长，将直接向百度总裁张亚勤汇报。段润尧对加盟百度深感荣幸，表示将全力推动""百度量子、量子百度""的研究规划，计划五年时间里在百度组建世界一流的量子计算研究所，并在之后五年将量子计算逐渐融入百度的业务中来。 继阿里巴巴、腾讯之后进军量子计算   量子计算是基于量子力学的全新计算模型，与传统计算理论不同，它的运行基于量子比特，利用量子叠加和量子纠缠等独特的量子效应进行信息处理，可以极大提高计算效率，同时也将对信息安全提出了严峻挑战。   量子计算目前主要应用于复杂的大规模数据处理与计算难题，以及基于量子加密的网络安全服务，随着人工智能对计算能力的需求不断提升，量子计算提供了一种从根本上增强计算能力的思路，其核心优势是可以进行高速并行计算。同时，量子计算机可以完美地解决传统计算机模拟量子系统时遇到的海量存储和指数时间问题，是进行高效量子模拟的天然选择。   量子计算机一方面在运行机器学习算法时可以更快、更高效，另一方面通过量子辅助优化，可以解决现有许多重要优化问题，包括基于随机梯度下降的各类算法等，克服了速度与成本问题，将是迈向强人工智能的重要道路。   量子计算机在金融、医药、化学、材料、人工智能等领域具有广阔的应用空间，有望应用于人工智能（如自动驾驶软件）、药物发现、天气预报、金融建模、以及高效全局的最优搜索（如解决交通拥堵问题、快速信息检索）等。 国内 BAT 三家如今已全部入局量子计算。 在 2015 年，阿里云就和中国科学院共同成立「中国科学院 - 阿里巴巴量子计算实验室」，开展量子计算的前瞻性研究。2017 年 9 月 11 日施尧耘正式宣布加入阿里巴巴，随后担任阿里云量子计算的首席科学家，负责组建阿里云量子计算实验室 (AQL)。在去年云栖大会首日，施尧耘与中科院潘建伟院士共同推出了由阿里云与「中国科学院 - 阿里巴巴量子计算实验室」联合开发的「量子计算云平台」。   今年 2 月 22 日下午，在安徽合肥中科大举办的「中国科学院量子信息与量子科技创新研究院 2018 年度工作会议」上，潘建伟院士正式发布中科院联合阿里云打造的 11 量子比特超导量子计算的云平台。 阿里之后，腾讯 SNG 也于 2017 年 12 月宣布成立量子实验室，任用了张胜誉作为其首位杰出科学家。该实验室的目标是网罗量子相关的算法、复杂性、通讯、模拟、量子物理、量子化学等各方面的人才，甚至有计划跟人工智能技术相结合。 国外，谷歌、IBM、微软等巨头正在「量子霸权」上你追我赶；国内，BAT 的纷纷入局也拉开了量子计算的抢滩战。 "
156,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738847&idx=2&sn=005115a912244b5d7aa83d1eaaa36cb4&chksm=871ad4e1b06d5df76c24e26d79391f64e55d3af2286407769b0acf17486b41fdb32f2ea08ca4&scene=27,"业界 | 对话郑宇: 做城市计算比AlphaGo难多了, 但这就是我在京东金融继续All in的事",机器之心原创 京东金融和城市计算的关系是什么？这要从刚刚加入公司的郑宇说起。 郑宇说，京东金融的城市计算，是一盘很大的棋。 2018 年 2 月 13 日，城市计算领军人物郑宇正式加入京东金融。此前，他曾是微软亚洲研究院城市计算领域的负责人，毫不夸张地说，「城市计算」一词，就是他提出来的。 郑宇有一连串响当当的头衔——上海交通大学讲座教授（Chair Professor）、香港科技大学客座教授、ACM Transactions on Intelligent Systems and Technology（TIST）主编、全球杰出青年创新者（MIT TR35）、2014 年《财富》杂志中国 40 位 40 岁以下商界精英...... 除了这些，郑宇还在 Google Scholar 城市计算领域世界学者的论文引用排名中名列榜首。 和很多人一样，在得知郑宇加入京东金融这一新闻时，我的第一反应是，为什么要去京东金融？京东金融作为金融科技公司，为什么要做城市计算？毕竟，郑宇这次所带领的，是一个一级事业部。 带着这样的疑问，机器之能采访到郑宇，希望找到问题的答案。以下为采访实录，机器之能做了不改变原意的整理。 京东金融给了我很大的空间，成立了一个一级事业部，从数据到研究到开发到市场，能够形成一个闭环。这种闭环的空间不是什么地方都可以给的。 你加入京东金融这件事在业内反响很大，是什么原因让你做出这个决定？ 我觉得有几点原因，首先看大环境，现在人工智能与实体经济结合是大趋势，智慧城市是抓手。这个事情一定要有人来做，我觉得现在是一个很好的时间点。 第二，京东金融给了我很大的空间，也给了我足够的信任来做这件事。包括成立独立的一级事业部等等。另外我们还会建立城市计算研究院，我还兼任整个京东金融的首席数据科学家。我觉得这是非常难得的机会。 一级事业部不像一个小组或者研究院，它是一个独立的事业部，可以发展得很大。我们从数据到研究到开发到市场，能够形成一个闭环。这种闭环的空间不是什么地方都可以给的。 从战略制高点看的话，一级事业部的成立也说明了整个京东金融对城市计算这件事的重视，同时让我感受到京东金融有足够的信心和勇气来对城市计算进行投入。 要做城市计算这件事，一定需要非常大的空间和信任，同时我需要面对的挑战也会更大。以前我可能只关注研究、开发和落地，那现在还要关注市场、数据、资源、客户的对接，各方面的事情都要考虑。 最后一点是，我比较认可京东的文化和价值观，踏踏实实地干事情，然后努力成功。确实有很多企业给我抛过橄榄枝，但是我最后还是选择了京东金融。 你所负责的这个事业部预计要招多少人？ 这个暂时要保密，再过一段时间之后，我们会有一个比较官方的宣布。 可以说的是，我们现在正在建立二级部门，分别着眼于环境、交通规划等方面。所以京东金融的城市计算是一个很大的架构，不是说只是一个小团队。 预计会建几个二级部门？ 这个到时候大家就知道了，可以说大家关心的我们都会有，像交通、环境都会有，但这个具体数字是多少，得等到具体的情况来看。 京东金融是一家服务金融行业的科技公司，为什么要做城市计算？ 这里有两个关键词，金融和科技。京东金融其实是科技公司，它的商业模式是 B2B2C。当然，很多人认为京东金融是一个金融公司，但其实不是这样的，它的业务是为金融行业提供一些科技技术。 比如它为银行提供风控模型，让银行更好地为 C 端客户提供贷款服务。在这里京东金融是第一个 B，银行是第二个 B，然后终端客户是 C。京东金融并不是说直接放贷给个人，它更多地是扮演了一个技术赋能者的角色。 这种 B2B2C 的模式我非常认可。因为我们可以把中间的这个 B 想象成 G。我们给相关主管机构提供技术，让他们更好地为百姓服务。 你可以认为，城市计算是京东金融作为一家科技公司，对现有业务的拓展。TO G 的业务需求会着眼于交通、环境等等，这会反哺京东金融的科技公司属性。 我们的定位是要成为国内最好的智慧城市技术平台和解决方案提供方，同时我们也希望成为产学研一体化的典范。 京东金融当前的业务与城市计算最紧密的连接点在哪里？ 智能商业。大家对城市计算了解得比较多的方面可能是环境、交通、规划等，但其实智能商业也是城市计算的很大一部分。包括商业选址、地产价格评估，还有甚至帮银行做一些 B 端的风控等。 举个例子，一个企业到银行贷款，要盖一个娱乐城，那银行就要对这个项目进行风险评估，确保到了还款时间能按时收回贷款。以前银行的方法可能是去评估这个企业本身的一些征信资质，比如坏账率，诚信度等等。 但是企业资质良好，不代表它开发的每个项目都没有风险，因为这个风险不是企业本身决定的，而是由这个地方的发展环境和消费水平决定的。这些东西只能从别的数据反映，银行本身也没有。可以从项目周边居民的消费水平反映，从人们出行的方式反映，从地区周边的配套反应，甚至包括基础设施比如路网、车站是不是变得越来越完善来反映，这些因素综合决定了这个地域本身的发展程度。 这其实代表了城市计算的一个愿景，用多元的、不同的时空数据融合来做一些事情。不管是分析预测，还是因果分析，还是异常检测，都能够从更多维度、更准确地进行。 在京东金融的城市计算布局上你有怎样的规划？ 我们的定位是要成为国内最好的智慧城市技术平台和解决方案提供方，同时我们也希望成为产学研一体化的典范。这两点很重要，我们不希望只是一个工程公司，赚很多钱，而是希望真的作为一个科技公司，把产学研带动起来。这是国家一直强调的东西，只有这样产业才有长远的生命力。 这两点定位之后，我们也就知道了应该怎么做。一方面给国家相关机构提供服务，一方面给一些大型国有企业提供服务，帮助他们解决行业痛点，还有就是我们会跟高校进行合作，建立一些人才联合培养机制，联合实验室，也会合作发表一些论文，攻克一些重点难题。我们这三方面会一起做。 至于布局，各个地方的业务需求是不一样的，有些地方可能比较关心环境，有的地方可能比较关心交通，有的地方可能关心他们自己的经济 GDP 问题、脱贫问题等等。所以我们会针对于各个地方的需求，尽量利用我们一个比较通用性的平台，来满足各地差异的需求。 所以是要看具体的需求才能决定业务开展的优先级？还是已经有规划？ 我们已经有规划，确定了大概哪几个方向是一定要做的，先把它做起来，然后根据各个地方的需求进行定制化。交通、环境、规划、智能商业这几个方向是一定要做的。 是否有多长时间推进多少个城市这样的计划？ 这种量化数字跟我们具体的团队和人数有关系。我觉得其实这不是重点，我相信未来一段时间大家会看到我们更多的布局。 在城市计算这件事上，京东金融的优势是什么？ 其实京东金融本身有海量的数据。根据最新财报，京东集团的活跃用户已经有 2.925 亿。我们不仅有京东集团的商城数据，包括产品本身数据、用户交易数据、物流数据，还有京东金融本身的一些理财、支付和消费数据，构成了一个很大的数据群体。 这些数据可以从侧面描述城市的经济维度，甚至跟别的数据融合之后可以反映这个城市的发展趋势，因为材料样本已经足够大。特别是还有物流的数据在里面，其实也反映了城市的一些动态的韵律，跟城市和城市之间的一些消费的交往，以及物流需求的一些往来。所以说京东的数据是很好的，很难得的。 再就是京东集团在地方上跟很多管理部门已经形成了有效配合，它的渠道和品牌效应可以帮助我们比较顺利地开展很多事情。 劣势呢？ 我觉得不能说是劣势，就是我们未来会在哪方面进行努力。我们会把科技含量提高，然后我们会跟更多的高校合作，跟政府和企业加强联系合作。有了我们这个事业部之后，我相信城市计算这件事情会更加顺畅，变得更好。 你刚刚说到，京东金融在城市计算的智能商业领域有很好的条件，那在像环境、交通等领域，京东金融会怎样参与？ 首先，京东金融作为科技公司，我们用科技服务于各个其他的 B。可以是银行，也可以是有关主管部门，也可以是其下属的大型国有企业。这样的话我们就可以做交通服务、环境服务，我们并没有把自己限制在金融场景里面。 可能现在最初的一些项目是在与金融相关的消费场景中，后面慢慢地我们会建立企业信用，做企业服务，也会和政府有一些项目合作。再往后可能政府以外、金融以外的很多项目也都是我们希望做的。 所以，未来京东金融的定位是科技公司，其根本是 B2 B2 C 的模式。我们强调科技服务，而不强调金融本身。以后大家会越来越发现这个趋势。 京东金融的城市计算和阿里的城市大脑会有怎样的不同？ 对于其他公司我不作评价。我在这个领域的工作十几年前就开始了，这十几年来也一直坚持扎根在这个领域。我有这样的一个情怀，希望能用毕生所学真正地服务中国。 这也是我出来最根本的原因。我只希望我们能够实实在在地把城市中的问题解决掉，帮助到城市建设，同时我们也很愿意跟其他公司共同合作来推进这个进程。 所以您认为城市计算的市场容量是很大的，合作大于竞争？ 对，行业需求非常大，现在更多的应该是推动行业共同发展。另外，城市计算有它自己特殊的业务逻辑。要做城市计算这个行业，其实不是那么容易，它有它的门槛。除了技术本身的门槛之外，还有业务逻辑和场景的门槛，还有地域的限制。这就导致了为什么到现在还没看到一家特别大的城市计算公司。很少有人能够做到这一点，我们希望能够做成这个最难的事。 城市计算问题状态空间大、动作空间也大、还是一个开放系统，问题解决起来比 AlphaGo 难多了。 城市计算问题和其它人工智能相关问题相比，有哪些特有的技术难点？ 第一，城市计算是时空数据，它不是一般的视频图像文本，它所用到的数据管理办法和人工智能方法和其他问题都不太一样，不是说拿一个 CNN 或者 LSTM 就能解决的。它有自己特殊的时空属性，包括时间的趋势、周期和邻近性，包括空间距离和空间层次性。这些特殊的时空特性用普通算法是抓不住的。 还有就是在真正的城市计算里面，会用到多个数据源，而不是单一数据源。比如刚刚我们说到的娱乐城案例，需要用到 POI、路网数据点，加上环境消费等等很多种数据，融合这么多数据才能判断这个地方未来的变化。 多元数据融合本身就是一个难点，在机器学习当中也是一个相对比较新兴的学科和研究方向。来自于不同领域的数据如何做到 1+1 大于 2 的知识发现，这个其实是一件很困难的事情。 同时，城市计算也不是一个简单的云计算问题，现在云计算平台对这种时空数据的支持都不足够好。时空数据的数据结构查询方法，以及刚刚说的多元数据的融合、索引机制都不存在，所以市面上任何一家公司的云直接拿来做城市计算都不太合适。必须要经过特殊的技术积淀，使得云能够具备对时空大数据的管理、分析和挖掘能力，并且形成动态闭环，这非常难，门槛也很高。 所以最后会有一个京东金融城市计算云？ 不，我们事业部不做云，我们希望让市场上的云计算公司拥有挖掘分析城市大数据的能力。可以是京东金融自己的云，可以是微软的云，也可以是腾讯的云，华为的云，都可以。我们会让云厂家快速拥有对城市大数据的分析、挖掘和管理能力，并且能够快速形成闭环，做一些实时动态的分析挖掘业务。这是我们的能力所在，也是我们的门槛。 能否举例具体说明一下城市计算之「复杂」？ 我讲个具体的例子，比如说交通信号灯的控制问题，就比其他任务，比如 AlphaGo 要难太多。AlphaGo 是一个 19×19 的网格，每个格子上的状态只有黑棋、白棋和没棋三种，状态空间就这么大。 但是和 19×19 个格子相比，整个北京的红绿灯路口数量有几万，而且每个路口的状态和动作也会更多，比如这个道路的交通流量速度是 40 公里每小时，另一个是 45，还有 30 的；信号灯的控制的变化可能是红灯 30 秒，绿灯 20 秒，都是连续变量，这个空间状态就比刚才的三个状态要大多了。 而且很多数据是缺失的。可能这一时刻刚好这条路上没有人，或者没有车经过，或者没有埋传感器，那我们就拿不到这条路的数据，拿到的是一个不完整的观察。 另外，还有道路是一个开放系统，在围棋里面，我说一句话，在旁边走两步路，不会影响围棋的结果。但是在道路上，有一个人过马路，哪怕一条狗穿过马路，都会改变道路的状态。 所以城市计算问题状态空间大、动作空间也大、还是一个开放系统，那问题解决起来肯定比 AlphaGo 难多了。 我再举个例子，城市人流量的预测，我们把城市分成很多个网格，要预测每一个网格里面，未来会有多少人进和出。 一个网格中的人流量既跟它自身前一个小时有多少人进和出有关系，也跟他周边那些邻居格子有多少人进和出有关系，因为有人会从旁边过来。还跟离格子很远的地方，某个区域的人的进和出有关系。因为当你这里有大事情发生的时候，会有很多人从很远的地方，坐地铁从地下钻出来，这个时候你如果只是靠网格周边的人员变化，就不能捕捉到大事情发生时的异常，就像一些地方发生的踩踏事件一样。 如果你想对每一个格子都能准确预测人流量，也就意味着你要把城市的所有其他格子的状态都作为输入，来预测这个变化。而且格子和格子之间是相互依赖的，你不能说我先做 A 格子再做 B 格子。因为 A 格子进入了多少来自 B 格子的人，也就意味着 B 格子出来了多少人到 A，这是一个相互的东西，所以要同时做。如果你把每个格子单独拿出来做，那这个模型会相对简单，可是你发现这样做出来，总人数加到一起可能都不等于全城总人数。 要把这么多格子一起做起来的话，有一个传统的方法，就是用 graphic model，把每个格子当作一个节点，每个格子跟其他格子建立边的关系，那这个复杂度就非常非常高了，所以以前的机器学习根本做不了，直到现在深度学习出来之后，才有解决方案。 除了格子本身的数据之外，还有天气、事件等能够影响人流量的数据，都是要动态输入的。这么多数据，需要在秒级或者分钟级得出结果。比如交通量预测，或者预警任务，对时效性要求很高。这么多数据的处理，没有特殊的方法的话，可能一个小时过去了，特征都没提完。 现在我们看像人脸识别这样的任务，就是一张图片和特征，是单一数据源的单一样例，而城市计算是多个数据源的多个样例，所以复杂度是不在同一个量级上的。 京东金融在计算资源上的投入一定很大。 我们有一些特殊的方法，如果没有很好的方法的话，那就只能靠堆机器，必然是不可行的，对国家资源也是一种浪费。我们用特殊的方法，结合分布式技术和时空索引技术，能够用十分之一的机器实现十倍的效率。 说到踩踏事件，当时你发微博说可以用城市计算解决这个问题，最终还发了一篇论文，但是受到了很大的争议，说是在蹭热点，你本人对这件事怎么看？ 当时我在微博上说，完全可以通过大数据和人工智能方法提前预测，加强管控，避免踩踏。有些人支持我，相信我一定能把这个问题解决掉。也有一拨人说，「这个东西根本就不需要你预测，谁都知道会有很多人来。」 但其实仔细想一想，很多人是多少人？50 万人很多，100 万人也很多。可是应对 100 万人和 50 万人的安全级别戒备是完全不一样的。另外，就算你知道 100 万人要来，那这 100 万人是分成五个小时均匀来，还是说在某两个小时峰值到达？这两种情况所采取的安全措施也是完全不一样的。所以只能大概知道有很多人要来，但不知道会有多少人、具体什么时候来。这个时候就需要用技术的方法来解决。 现在，我们可以预测各地区的人流进和出，能够提前几个小时告诉城市主管部门，这边可能会有问题，你可以加强管控，提前预防，比如通过媒体通知疏散人流。这些事情一定是越早做越好，人家都来了，你再让人家回去，这也不现实。 城市计算已经在我们的实际生活中解决了哪问题？ 很多，比如 2014 年我给环保部做的雾霾预测，那段时间是北京空气越来越糟糕的时候，我们定义了问题。 第一，怎么根据环保部门设立的这些有限的空气质量监测站点，把那些没有监站的地方的空气质量给估计出来。 城市空气是不均匀的，空气质量会受很多复杂因素的影响，包括周边交通流量，周边是否有厂矿，扩散条件怎么样，是公园绿地还是建筑群等等。这些东西都不是线性的，不是均匀变化的，甚至每个小时差异都特别大。 环保部门只设立了有限个监测点。为什么不多建？因为站点很贵，一个国产的站点大概需要 100 万人民币，进口的要 200 万人民币的样子。而且还要维护，站点的占地面积还蛮大的，它不是一个小盒子就可以做的，一定是要在一个大的地方部署一个很大的设备才能做这件事情。如果真的在北京每一平方公里都放这样一个设备的话，大概需要 3000 多个站点，根本没法维护，所以只能设立有限的站点。 站点有限，空气质量又不均匀，那怎么办？只能用人工智能和大数据的方法，结合交通气象以及地理信息的变化来学习。这就是一个非常典型的半监督学习的案例，只用有限的样本，来推断所有地区的空气质量。 这是第一步，然后第二步还要预测，刚刚是搞清楚现状，现在是预测未来。空气质量会受天气的影响，受地区绿化的影响，受人类地面活动排放的影响，所以看天、看地、还要看人。 另外，我们拿到的天气预报的数据都是不准确的，现在天气预报的准确率只有 40% 左右，你的模型要在很多这种不确定的数据上训练，大数据和人工智能就有这种能力，把很多不确定的数据加在一起，形成确定的数据；把很多稀疏的数据叠在一起，得到一个明确答案。 这篇论文我们发表在 KDD 2013 上，是领域内第一篇用大数据做空气质量分析和预测的文章，引用率特别高。同时技术也已经落地，已经覆盖到中国 300 多个城市。 像今年过年某些地区游客滞留这件事情，能够怎样通过城市计算进行解决？ 游客滞留是一大类问题，大家在某段时间大家扎堆过去，但是景区旅客接待能力不足，或者说运力不足导致游客滞留在某地，不能及时返回。这个问题是我一直想做的问题，我觉得很有意思。 国家旅游局希望能够预测未来一段时间，比如黄金周，各个景点的人数。你如果只是把某个地区单独拎出来，根据往年数据来看，预测一定不准。为什么？因为这是一个相关性问题。可能前一年去这个地方的人很多，第二年就没人去了。可能突然某个景区火了，带动了邻近的景区。还有一些景色相似的地区，比如一个地区出现问题了，大家都不去了，导致去另一个相似地区的人突然变多了。或者出国便宜了，出国游的人变多了，国内游的人就变少了。原因有很多，不能单独看，要多维度联动一起看。这些维度有些是关联存在的关系，有些是增强关系，有些是互相的排斥关系，都要搞清楚。 一个厉害的人大概同时能把十个项目撑起来，而一个平庸的团队可能铺一百个人下去，也讨论不出来一个结果。 目前城市计算所面临的最主要的挑战是什么？ 缺数据科学家。很多人觉得自己已经将 AI 掌握得很成熟，算法研究的很好，但这不代表他能够解决行业问题。要解决行业问题，还需要对行业本身的了解。 举个例子，比如说空气质量分析和预测工作，你必须知道是什么因素导致了空气污染，这样才能想到用什么样的数据来分析和预测。当反映某个因素的数据不存在的时候，你要想是不是能够有别的数据来替代。还有这个行业里面的传统方法是什么样子？有什么样好的思想可以借鉴？有什么地方是不行的？为什么不行？我们要怎样做才能让它行？你的方法比原来的方法好在哪里？这些问题都要搞得很清楚。 否则即便你做出一个东西，看起来模型结果比较好，行业的人不见得会认可你。因为你讲不清楚，你没有跟别人沟通的语言。业务场景是在别人那，你最终要落到业务场景里去。 比如我接到过的城市需求是发展大数据产业。你要自己找到问题，比如充电桩选址是个问题，你要提出方法，用什么数据，怎样实现。在这方面北京和上海会好一些，但是如果我们要求全国各城市都具备计算机问题的分析能力，那要求太高了。政府是城市管理部门，不是科研部门。 所以现在我不担心硬件不发达，或者算法上不去，或者算法本身研究不出来，这都不是问题，我相信总会有突破。怎样把这些 AI 大数据人工智能算法跟行业的问题对接，是需要一大批数据科学家来支撑的。 合格的数据科学家需要有怎样的特质？ 既要深度地了解行业，也需要懂数据背后的知识，而不是数据表面的格式。要懂各种各样的模型，不单单要具备底层的机器学习能力，还要有可视化、数据管理能力。他要懂得云平台怎么用，因为最后产品会落到云平台上来，做一个好的数据科学家是站在云平台上看问题，想数据关联模型，把模型有机地组合到一起，部署到我们的云平台上。 我觉得以后我们国家要加大力培养数据科学家，而不是简单地培养 AI 人才。在学校里上几门课，学学算法，学学工具，这些都不难。难的是学了之后要到各行各业里面去工作，解决行业问题，这中间有很高的门槛。这种经验的积累不是在学校里能够学得到的，需要靠数据和项目去喂养，在实战中去修炼。 所以我很鼓励高校的一些学生，特别是高年级的研究生们，能够多出来接触实际的企业，看到实际的问题，用真实的数据做一些东西。除了理论研究之外，我们也可以看看这个研究怎么落地应用，并从中得到一些反馈，同时你也会发现新的问题，甚至可能是核心的研究问题，是以前没想到的问题。所以我觉得这是一个产学研一体化的东西。 我们需要多长时间才能拥有足够多的数据科学家？ 按照以往经验的话，7 到 10 年培养这样的一个人。五年的博士加上两年的工作经验，如果博士全在学校里面，那可能还不行。 我发现在我带的学生里面，有很多人有这样的问题。你给他培训得很细致，让他做了很多项目，但是他没有环境转换能力，换了就不懂。因为他没有快速的学习能力、高度的抽象能力、以及举一反三的能力。这可能跟悟性是有关的，还需要数据敏感性。有的人一看到问题，马上就能想到什么数据可以反映这个问题，但很多人就很难建立这个思路。 比如他看到出租车数据就想到出租车，想不到可以变成车流、人流，还能反映区域的经济、交通环境的变化。这个思路穿过来之后，就能用领域 A 的数据解决领域 B 的问题。 这个真的是看悟性，有时候你教都教不来。我从这么多的学生中观察，成才率很低。培养数据分析师很容易，可能数据分析师国家有几千几万名，但是数据科学家真的非常少。所以说中国一定要加大力度培养这种人才，才能在世界上立于不败之地。 在这方面我们的团队还好，因为我们团队已经磨合了很多年了，在这个方向我们也研究了很久，经验积累比较多。然后我自己学生跟我读书也读了四五年的博士，所以相对来说他们是有一些经验的。 一家公司想做城市计算的话，需要有怎样的素质？ 我觉得需要有数据和团队。首先，做城市计算需要有良好的数据基础和数据资源。大家都以为政府数据特别多，但其实不是这样的，很多时候政府也需要行业数据来支撑他们的决策，共同解决问题。 还有一个就是团队。其实我觉得人才是最关键的，现在都说 AI 大战其实是人才大战。但不是说 AI 比赛是千军万马的比赛，不是说我有一百个人就比你十个人要厉害。很多时候能不能解决问题，往往靠一个人灵光一现。比如说问题卡在那，某一种数据缺失，怎么想都搞不定。这种时候往往是靠某一个人灵光一闪，发现另外一种数据，通过怎样的转变，怎样的使用就能解决问题了。当几个看上去没用的数据组合在一起的时候，在一种特殊场景和模型下面，能够发挥出一些意想不到的优势，这就是人才的作用。 一个厉害的人大概同时能把十个项目撑起来，而一个平庸的团队可能铺一百个人下去，也讨论不出来一个结果。所以 AI 真的不是一个靠千军万马过河的学科。京东在这方面已经意识到了战略储备的重要性，也在花大力气投入引进尖端人才。 
157,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739090&idx=2&sn=53657e6c5df27bc2926fbfc3f9f901ab&chksm=871ad5ecb06d5cfa696dab51e5d14c9f018f887942ff420b1e97d4dd8fc9c34c59a41ab6c6b4&scene=27,深度 | 级联MobileNet-V2实现人脸关键点检测（附训练源码）,"为了能在移动端进行实时的人脸关键点检测，本实验采用最新的轻量化模型——MobileNet-V2 作为基础模型，在 CelebA 数据上，进行两级的级联 MobileNet-V2 实现人脸关键点检测。首先，将 CelebA 数据作为第一级 MobileNet-V2 的输入，经第一级 MobileNet-V2 得到粗略的关键点位置；然后，依据第一级 MobileNet-V2 的输出，采取一定的裁剪策略，将人脸区域从原始数据上裁剪出来作为第二级 MobileNet-V2 的输入；最后，经第二级 MobileNet-V2 输出最终人脸关键点定位信息。经初步训练，最终网络单模型不到 1M，仅 956KB，单张图片 inference 耗时 6ms（采用 GTX1080 在未优化的 Caffe）。实验结果表明，MobileNet-V2 是一个性能极佳的轻量化模型，可以采用较少的参数获得较好的性能；同时，级联的操作可达到从粗到精的关键点定位。 人脸关键点检测也称为人脸关键点检测、定位或者人脸对齐，是指给定人脸图像，定位出人脸面部的关键区域位置，包括眉毛、眼睛、鼻子、嘴巴、脸部轮廓等和人脸检测类似，由于受到姿态和遮挡等因素的影响，人脸关键点检测是一个富有挑战性的任务。 人脸关键点检测方法大致分为三种，分别是基 ASM(Active Shape Model)[1] 和 AAM (Active Appearnce Model)[2,3] 的传统方法；基于级联形状回归的方法 [4]；基于深度学习的方法 [5-10]。在深度学习大行其道的环境下，本实验将借鉴深度学习以及级联思想进行人脸关键点检测。 随着手机的智能化及万物物联的兴起，在移动端上部署深度学习模型的需求日益增大。然而，为获得更佳的性能，卷积神经网络的设计，从 7 层 AlexNet[11] 到上千层的 ResNet[12] 和 DenseNet[13]，网络模型越来越大，严重阻碍了深度学习在移动端的使用。因此，一种轻量的，高效率的模型——MobileNet-V1[14] 应运而生。MobileNet-V1 最早由 Google 团队于 2017 年 4 月公布在 arXiv 上，而本实验采用的是 MobileNet-V2[15]，是在 MobileNet-V1 基础上结合当下流行的残差思想而设计的一种面向移动端的卷积神经网络模型。MobileNet-V2 不仅达到满意的性能（ImageNet2012 上 top-1:74.7%），而且运行速度以及模型大小完全可达到移动端实时的指标。因此，本实验将 MobileNet-V2 作为基础模型进行级联。 2.1 整体框架及思路 本实验采用两级级联 MobileNet-V2，分别称之为 level_1 和 level_2，由于个人精力有限，level_1 和 level_2 采用完全相同的网络结构。level_1 进行粗略的关键点定位；依据 level_1 输出的关键点进行人脸区域裁剪，获得人脸区域图像作为 level_2 的输入，最终关键点定位信息由 level_2 进行输出。流程如下图所示： 通常进行人脸关键点检测之前，需要进行人脸检测，即将人脸检测获得的人脸图像区域作为人脸关键点检测模型的输入。然而进行人脸检测是相当耗时的，所以，在特定场景下（即确定有且仅有一个人的图像）可以采用 level_1 代替人脸检测步骤，通过 level_1 可获得人脸区域，从而提高整个任务效率和速度。本实验正是采用 level_1 剔除非人脸区域，从而使得后一级的网络可以更为精准的进行关键点定位。 本实验仅做了两级级联，其实还可像 DCNN[5] 那样继续级联，对眼睛，鼻子，嘴巴分别进行预测，或者是学习 Face++[6] 那样，在 68 点的关键点定位中，将 68 点划分为两个区域分别预测。这些都是很好的想法，十分值得借鉴，但个人精力有限，在此仅做了两级级联作为学习，希望大家可以基于 MobileNet-V2 去改进上述两种方法。 2.2 基础模型——MobileNet-V2 MobileNet-V2[15] 由 Google 团队于 2018 年 1 月公布在 arXiv 上，是一种短小精悍的模型，仅数 M 的模型就在 ImageNet 上获得 74.7% 的准确率（top-1），具体分类性能如下： 原版 MobileNet-V2 网络结构如下图所示： 其中：t 表示「扩张」倍数，c 表示输出通道数，n 表示重复次数，s 表示步长 stride。  解释一下原文的有误之处：  1. 第五行，也就是第四个 bottleneck，stride=2，分辨率应该从 28 降低到 14；要么就是 stride=1；  2. 文中提到共计采用 19 个 bottleneck，但是这里只有 17 个。 结合本实验任务的需求，设计了一个输入尺寸为 48*48 的 MobileNet-V2（level_1 和 level_2 均采用此网络结构），网络结构如下图所示： 3.1 数据集简介 实验数据采用 CelebFaces Attributes Dataset (CelebA) 公开数据集 (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)，CelebA 数据集是香港中文大学的开放的数据集，此数据集包含 10177 个名人，共计 202599 张图片，每张图片含 5 个关键点标注信息以及 40 个属性标签。经过实验结果分析，此数据集在人脸关键点检测上的困难点在于： 1. 人脸区域面积占图片面积大小不一，有部分图片中的人脸占比相当小。 例如： 2. 人脸角度多样性强，俯仰角（pitch）、偏航角 (yaw) 和横滚角 (roll) 均有，但绝大多数数据为正向无偏角，从而导致正向无偏角检测效果较好，复杂偏角检测效果较差。 3. 噪音图片，图片「不完整」，例如存在大部分黑色区域图片。 3.2 评价指标 这里采用相对误差作为评价指标，相对误差计算公式为：，其中表示输出 landmark 与真实 landmark 的欧氏距离，表示图片对角线的长度。一开始，本实验采用的是双眼距离作为归一化因子，但是 CelebA 数据集并不适合采用双眼距离作为归一化因子，因为在计算时会出现 inf！即两眼在同一位置，导致分母为零，这是 LFW,ALFW 数据集不会出现的情况。两眼距离为零的图片示意图； 3.3 模型训练 level_1 与 level_2 采用相同网络结构，solver 也一致。solver 为： level_1 的训练 loss 曲线如图所示： Train loss 大概是在 0.005~0.006，Test loss 是在 0.007~0.008（注：训练的时候 loss_weighs 设置为 100，因此上图数量级上不一致） 训练和测试的平均误差分别为 1.01% 和 1.17%。5 个点的平均误差如下图所示： 训练集和测试集各点误差分布如图： 从误差分布图可以看到，误差分布呈高斯分布状，训练与测试的分布形状基本一致，通过误差分布图可知，在训练集中，绝大部分误差小于 5%，因此在选取 level_1 的裁剪阈值时，可参照此图进行调试。 level_1 训练完毕，需要进行图片裁剪，以供 level_2 进行训练。这里采取的裁剪策略为，以 level_1 预测到的鼻子为中心，裁剪出一个正方形，这个正方形的边长为四倍鼻子到左眼的距离。以下图为例，绿色点为 level_1 的预测关键点，红色方框即为裁剪框： 在裁剪时，还需要对部分图片进行丢弃，以此确保裁剪之后的图片能包含完整的人脸。本实验将丢弃阈值设置为 10%，即误差大于 10% 的图片进行丢弃，然而实验结果表明 10% 并不是一个较好的阈值，应该设置为 5% 比较合理。 经过裁剪操作可获得 level_2 的训练数据，接着训练 level_2，level_2 的 loss 曲线如图： level_2 与 level_1 的曲线有着明显的不同，即曲线出现了很大的浮动，有尖点存在。这是为什么呢？其实是因为 level_1 剪裁出了问题，在裁剪时，将误差小于 10% 的图片保留，用以裁剪。其实这个阈值（10%）还是太大了，以至于裁剪出这一些「不合格」图片。例如： level_2 训练完毕，即可进行级联检测，forward 耗时如图所示： 级联检测效果如下图所示，其中，绿色点为真实 landmark，红色点为 level_1 的预测，蓝色点为 level_2 的预测。可以看到蓝色点比红色点更靠近绿色点： 本实验在 CelebA 数据集上，采用最新的轻量化网络——MobileNet-V2 作为基础模型，进行级联卷积神经网络人脸关键点检测实验，初步验证 MobileNet-V2 是短小精悍的模型，并且从模型大小以及运行速度上可知，此模型可在移动端实现实时检测。此实验只是对级联思想的一个简单验证，若要获得更高精度的人脸关键点模型，还有很多可改进的地方。在此总结此实验已知的不足之处，供大家参考并改进： 1.	level_1 的输入可为 24*24 甚至更小，只要保证能依据输出的 landmark 来裁剪处人脸区域即可； 2.	level_1 的裁剪策略相对「拙劣」，可依据具体应用场景，提出不同的裁剪策略，确保输入到 level_2 的图片包含整个人脸； 3.	模型训练不足，未完全收敛，可对 solver 进行修改，获得更好的模型； 4.	可增加第三级模型，分别对眼睛，鼻子，嘴巴进行检测，从而获得更精确定位点； 5.	对于具体应用场景，应依据困难样本，需要针对性的做数据增强，例如 CelebA 中，正向无偏角人脸较多，而有俯仰角、偏航角和横滚角的图片占少数，从而导致模型对含偏角的图片预测效果较差； 6.	未进行模型压缩，若需要部署，则要进行模型压缩，从而获得更小的模型； 7.	Caffe 对 depth-wise convolution 的支持并不友好，从而在 Caffe 下体现不出 MobileNet-V2 的高效，可尝试 TensorFlow 下进行； 8.	欢迎大家补充 下一步工作： 1. 针对以上不足进行改进； 2. 寻找有兴趣的朋友进行 68 点关键点检测实验，并进行模型压缩，获得更小更好的网络。 本实验所有代码可在 GitHub 上获得：https://github.com/tensor-yu/cascaded_mobilenet-v2 详细训练步骤可参见博客：http://blog.csdn.net/u011995719/article/details/79435615 欢迎大家提出宝贵的意见和建议。 "
158,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738808&idx=4&sn=0f846d169252eeee661b99ca8f482a7b&chksm=871ad406b06d5d100abd3f52f2c735af5d749608dba3885e11c0cfb84a8799129fe13628abcc&scene=27,学界 | 精准防御对抗性攻击，清华大学提出对抗正则化训练方法DeepDefense,"选自 本文提出了一个名为 DeepDefense 的训练方案，其核心思想是把基于对抗性扰动的正则化项整合进分类目标函数，从而使模型可以学习直接而精确地防御对抗性攻击。在 MNIST、CIFAR-10 和 ImageNet 上的扩展实验证明了该方法可以显著提高不同深度学习模型对高强度对抗攻击的鲁棒性，同时还不会牺牲准确率。 虽然深度神经网络（DNN）在许多挑战性的计算机视觉任务中都取得了当前最优的表现，但在对抗样本（在人类感知上和真实图像很相似，但却能欺骗学习模型做出错误预测的生成图像）面前，它们仍然非常脆弱 [32]。 合成对抗样本的通常方法是应用最坏情况的扰动到真实图像上 [32,7,26]。通过适当的策略，仅有真实图像像素值 1/1000 的扰动幅度就可以成功欺骗 DNN 模型，这种扰动通常对于人类来说是不可感知的。有研究称即使是当前最佳的 DNN 模型也会被这类对抗样本所欺骗，得出高信度的错误分类结果 [19]。更糟糕的是，对抗扰动还可以迁移到不同的图像和网络架构上 [25]。这种迁移性使得黑箱攻击变得可行，即不需要任何模型架构或参数的知识就可以实现欺骗 [28]。 虽然 DNN 的这种特性很有趣，但其还会导致现实世界应用的潜在问题（例如，自动驾驶汽车和人脸识别支付等）。和对抗随机噪声的不稳定性不同（已被证明理论上和实践上都不是很重要 [6,32]），深度学习的对抗扰动脆弱性仍然是很严重的问题。目前有许多研究都尝试对其进行分析和解释 [32,7,5,12]。例如，Goodfellow 等人 [7] 称 DNN 的脆弱性的主要原因在于线性本质（而不是非线性）以及过拟合。基于该解释，他们设计了一种高效的线性扰动，并在进一步研究中将其结合到对抗训练中 [32]，以优化正则化效果。最近，Cisse 等人 [5] 探索了基于 DNN 分类器的 Lipschitz 常数，并提出了 Parseval 训练法对该常数进行控制，从而提高 DNN 分类器的鲁棒性。然而，和某些以前提出的基于正则化的方法类似 [8]，Parseval 训练法需要对其理论最优约束做一些近似，限制了其对非常强的对抗攻击的有效性。 本文提出了 DeepDefense，这是一种用于训练 DNN 提高模型鲁棒性的对抗正则化方法。与很多已有的使用近似和优化非严格边界的方法不同，研究者准确地将一个基于扰动的正则化项结合到分类目标函数中。从理论的角度看，这使得 DNN 模型可以直接从对抗扰动中学习并进一步对其进行防御。具体来说，就是给正确分类的样本分配更大的正则化项值，给错误分类的样本分配更小的正则化项值，来惩罚对抗扰动的范数。作为正则化项，它将和原始的学习目标函数联合优化，并且整个问题将被当做训练一个类似递归型的网络而高效地求解。在 MNIST、CIFAR-10 和 ImageNet 上的扩展实验证明了该方法可以显著提高不同 DNN 对高强度对抗攻击的鲁棒性，同时还不会牺牲准确率。 图 2：MNIST 上的收敛曲线：(a) MLP 的测试精度，(b) MLP 的测试ρ_2 值，(c) LeNet 的测试精度，(d) LeNet 的测试 ρ_2 值。「Clean」表征无扰动图像上的精调。 图 3：一张来自 MNIST 测试集并标注为「0」的图像 (x_k)，并基于 DeepFool 生成对抗样本以欺骗不同的模型，包括：(b) 参考模型，(c)-(e)：带有对抗性训练的精调模型、Parseval 训练以及我们的 DeepDefense。图中上方的箭头表示实例被错误分类的类别结果，下方的数字表示 的值。上半部分是为 MLP 模型生成，下半部分是为 LeNet 模型生成。模型（即，动量：0.9，权重衰减：0.0005）。 论文：DeepDefense: Training Deep Neural Networks with Improved Robustness 论文链接：https://arxiv.org/abs/1803.00404 摘要： 尽管深度神经网络（DNNs）对于很多计算机视觉任务很有效，但很容易受到对抗性攻击，限制了其在安防系统的应用。最近工作已表明不可感知的扰动图像输入（即对抗样本）存在欺骗良好训练的 DNN 模型做出任意预测的可能性。为解决这一问题，我们提出了一个名为 DeepDefense 的训练方案，其核心思想是把基于对抗性扰动的正则化项整合进分类目标函数，从而使模型可以学习直接而精确地防御对抗性攻击。整个优化问题可以按训练递归网络的方式得到解决。实验结果表明我们的方法在不同数据集（包含 MNIST、CIFAR-10 和 ImageNet）和 DNN 架构上明显优于当前最佳方法。我们将很快公开发布再现这一结果的代码和模型。 "
159,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738808&idx=5&sn=c6a6d3577bbf9d9e4ecaffde24fc90ec&chksm=871ad406b06d5d107bd6a20b3e88c88f3fdba944448d3da486474d7d70883f6747311038f375&scene=27,活动 | INTERFACE#4 解读搜狗机器翻译技术，体验搜狗旅行翻译宝产品,2018 年 1 月 24 日，搜狗 CEO 王小川在搜狗 2018 伙伴大会上发布了两款新产品「搜狗旅行翻译宝」与「搜狗速记翻译笔」，而且 「搜狗旅行翻译宝」 将于 3 月 12 日京东首发。对这两款全新的 AI 硬件你是否充满好奇？在机器之心 2018 年首期 INTERFACE（总第四期）分享中，我们邀请到了搜狗语音交互技术中心研发总监陈伟、 搜狗 IOT 事业部产品负责人 李健涛，从技术到产品让我们体验搜狗的机器翻译。 分享主题一：搜狗语音翻译技术是怎样炼成的？ 分享者：搜狗语音交互技术中心研发总监陈伟 陈伟：搜狗语音交互技术中心研发总监，语音技术负责人，负责搜狗语音识别、语音合成、机器翻译、声纹识别、手写识别等多项技术的研发工作，同时负责搜狗知音引擎语音技术的研发，致力于通过技术和产品的创新提升语音交互品质，为用户提供优质的语音使用体验。 分享主题二：搜狗旅行翻译宝产品研发理念   李健涛： 搜狗 IOT 事业部产品负责人 ，负责搜狗旅行翻译宝、速记翻译笔、糖猫儿童智能手表等智能硬件产品研发工作。他在 2005 年加入搜狐，负责博客开发，经历了社交网络大爆炸、移动互联网浪潮，又投身智能硬件领域，致力于通过产品创新提升智能硬件产品使用体验，为用户提供优质的服务。 活动流程 2018 年 03 月 17 日 13:00-14:00 签到、产品体验 14:00-14:30 陈伟分享 14:30-15:00 李健涛分享  15:00-15:30 现场提问 15:30-16:00 现场交流、产品体验 地址：北京市海淀区中关村东路搜狐网络大厦 8 层 
160,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738808&idx=3&sn=61448875d2beac4b36b855e747f7b92c&chksm=871ad406b06d5d10ee1a6de19a0c0572ce3cac9060b6a127b9a6a3bf0140244e5b9b4018b6f8&scene=27,业界 | 为主流价位移动设备加入AI计算：ARM发布新一代Mali解决方案,机器之心报道 3 月 6 日下午，移动设备芯片设计公司 Arm 在北京举行了产品发布会，向全球发布了其人工智能计算的最新解决方案，其中包括 Mali-G52、G31 移动端显卡，Mali-D51 显示处理器以及 Mali-V52 视频处理器。 随着人工智能技术的逐渐实用化，人们对于机器学习算力的需求正在飞速增长，除英特尔、英伟达等传统芯片厂商以外，谷歌、亚马逊等公司都在致力于打造自己的专用 AI 处理器。为了保持在移动芯片领域的领先地位，Arm 也在沿着自己的道路不断推进。2 月 14 日，该公司宣布启动 Trillium 项目，其中包括未来的专用机器学习处理器、目标检测处理器，以及神经网络软件库的 Arm IP 套件。而昨天在北京发布的新一代硬件产品则宣示了 Arm 在主流市场上已开始了自己的行动。 「目前在移动的世界里，消费者的使用行为已经发生了变化，」Arm 计算事业群市场营销资深总监 Ian Smythe 说道。「今天，手机是人通向世界的大门，Arm 作为一家企业的使命是让技术以无形的方式来让人连接世界。」 在 Arm 看来，今天移动端设备消费者行为的变化带来了三大新需求：机器学习应用、3D 视频游戏、和虚拟/增强现实。 这或许意味着未来芯片市场的格局会出现翻天覆地的变化。据估算，至 2018 年底，中国将设计和制造超过 10 亿台智能手机并销往世界，中国厂商在海外市场的份额正在以每年翻倍的速度快速增长。这样的速度使得 Arm 分外重视中国市场，该公司正在积极寻求与国内厂商在新架构上的合作。 目前，Arm 对于自己在市场上的领先地位感到满意。「目前市场上大部分移动技术都是基于 Arm 的，」Ian Smythe 介绍道，「其中包括 Cortex-A 的处理器——世界上绝大部分的智能手机都装载了 Cortex-A 系列处理器。同时 Arm 拥有一个多媒体方面的品牌 Mali，它也是全球 GPU 出货量最大的 GPU。简而言之，Arm 在全球的合作伙伴去年共计出货基于 Mali 的片上系统（SoC）达到了 12 亿片。」 AI 和机器学习是现在最热门的话题，而作为被用到最多的计算设备，手机等移动设备则是机器学习的重要应用场景。很多用户已经在不知情的情况下享受到了机器学习带来的便利。事实上，在搜索、机器翻译和照相等应用的背后都有机器学习的身影。 作为最大的移动端芯片设计者，Arm 的产品是机器学习计算的重要承载设备。根据 IDC 统计，目前市场上 90% 的 AI 计算由 Arm 来实现的。 与华为、苹果推出的深度学习芯片相对应，Arm 在 2017 年推出了 DynamIQ 异构计算解决方案。DynamIQ 完全改变了 Cortex 系列处理器的部署方式，它为 CPU 引入了一系列全新指令集，极大提升了人工智能计算的性能。同时它也极大地提高了多核计算的灵活性，在 8 核处理器系统中（1 大核加 7 小核），DynamIQ 实现了在同样的硅面积之下，总体线程性能翻一番，与此同时，它也能实现高性能的双四核布局。 在发布会上，Arm 发布了全新的 Mali-G52、G31 移动端显卡，Mali-D51 显示处理器以及 Mali-V52 视频处理器设计。据介绍，新一代芯片设计在机器学习计算性能上有了大幅提升，适用于主流市场上的手机、智能电视等设备。 Mali G52 GPU 的上一代 G51 是正好在一年以前发布的，时隔一年，新一代的产品就已获得了性能密度 30% 的提升，性能效率比 15% 提升，而在机器学习能力上，新一代设计更是获得了 3.6 倍的性能提升。 Mali-G52 采用典型的四核布局。其执行引擎由 G51 中的四线程扩展到了八线程。因此在复杂的指令上，它能够实现两倍的性能，因此在芯片面积上 G52 是 G51 的 1.2 倍，但性能上前者却是后者的两倍。为了更好地支持机器学习，Arm 也加入了一些具体的指令，其中包含英特尔一直以来支持的一些指令，在 ImageNet 图像分类与 Yolo network 卷积层性能测试中，G52 的性能相对前代产品都有了非常大的提升。 在显示芯片上，Arm 配套推出了 Mali D51，它是去年 DP650 的继承者。相较前代产品性能密度提升了一倍，通过使用 Offload 技术，它的性能效率则有 30% 的提升，同时内存时延减少了 50%。 通过把 Mali-D51 和 Assertive Display5、HDR10 结合在一起。Arm 的解决方案可以让目前主流价位设备的显示屏都能实现 HDR4K 画质，从而进一步提升产品的竞争力。 Mali-V52 则是基于 Arm 多核 Video 的视频输出解决方案。与前代产品 Mali-V61 相比较，其在解码性能方面有了一倍提升，硅晶片面积降低了近 40%，同时在视频的质量上提升了 20%。针对于一些特定的使用场景，Arm 能够提供优化解决方案。如在智能电视应用上，基于 Mali-V52 的设备可以在一个电视显示屏上同步展示多达 16 个视频流。 与此同时，Arm 还推出了面向低端市场的新一代 GPU Mail-G31，它也是 G30 系列，Bifrost 架构家族中的第一款 GPU。它和 Mali-G51 采用的架构技术是一样的，但总硅面积降低了 20%，在性能密度上也有 20% 的提高，同时在 UI 性能方面有 12% 的提升。 至此，Arm 的下一代中/低端视频处理芯片解决方案的组件已全部推出，这家公司正在积极推进新设计的产品落地。Ian Smythe 表示，目前已经有很多合作伙伴正在开发基于 Mali V52、D51 的产品。在 GPU 方面，预计在今年晚些时候我们就将在市场上看到搭载 Mali G31 芯片的设备面世，而搭载 G52 的设备则可能会于 2019 年出现在市场上。 
161,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738808&idx=2&sn=c10ae82563200bdfa0e2ec6556947ed4&chksm=871ad406b06d5d10977bf6b8e3a0ab3d66455a4ab90e59dcd7df6d5bc1291fe4db1a10494db8&scene=27,业界 | 谷歌推出神经网络可视化库Lucid，推进模型的可解释性工作（附GitHub）,选自 Chris Olah 近日，谷歌在 distill 上发文《The Building Blocks of Interpretability》探讨了如何结合特征可视化和其他可解释性技术来理解神经网络如何做出决策，同时谷歌还发布了 Lucid（及其 GitHub），一套研究神经网络可解释性的基础架构和工具和 colab notebooks，使得借助 Lucid 再现文中的可视化变得极其简单。 2015 年，谷歌可视化神经网络理解图像方式的早期尝试导致了迷幻图像（psychedelic images）的产生。不久之后，谷歌开源了 DeepDream 代码，从而衍生出一种小型艺术流派，产生的作品令人惊叹。但是谷歌并未放弃 DeepDream 背后的研究思路，并尝试解决深度学习中最令人激动的问题之一：神经网络的自身运行方式是什么？ 去年，谷歌在线上期刊 Distill 上介绍了这些技术如何展示神经网络中单个神经元的行为，而不是 DeepDream 中给神经网络带来有趣的部分。这使我们看到网络中神经元如何检测所有事物（纽扣、布片、建筑），及其如何构建成为更复杂的神经网络层。 尽管可视化神经元令人兴奋，但是这项研究遗漏了一些重要的东西：这些神经元与神经网络实际行为之间的关系。 今天，谷歌发布《The Building Blocks of Interpretability》，这篇新的 Distill 文章探讨了如何结合特征可视化和其他可解释性技术来理解神经网络如何决策。这种结合允许我们稍微「站在神经网络内部」，看到神经网络在某一具体时刻如何决策，及其如何影响最终输出。例如，我们可以看到神经网络如何检测耷拉的耳朵，以提高其判断一张图像是「拉布拉多犬」还是「比格犬」的概率。 谷歌探索了理解神经网络中哪些神经元处于激活状态的技术。如果我们询问哪些神经元被激活，通常会得到一些没用的答案，如「神经元 538 有一点兴奋」，而这对专家来说也没有什么帮助。谷歌的技术通过可视化每个神经元，使事情变得更加有意义，能够看到「耷拉的耳朵检测器被激活」。这可以说是神经网络的 MRI 了。 谷歌同样可以缩小图像，展示如何按照不同层对图像进行「观察」。这允许看到网络从检测简单的边连接到细致的纹理、三维结构，再到高级结构（如耳朵、鼻子、头和腿）的转换。 这些见解本身已经非常令人振奋了，但是当把其与神经网络的最终决策联系起来时，事情更加让人激动。我们不仅看到网络检测出耷拉的耳朵，还看到这一检测如何提高图像被标注为拉布拉多犬的概率。 除了论文之外，谷歌还发布了 Lucid，一个建立在 DeepDream 上的神经网络可视化库。Lucid 是研究神经网络可解释性的一套基础架构和工具。具体来说，它提供顶尖的特征可视化技术实现和灵活的抽象，使探索新的研究方向变得非常简单。除了更艺术化的 DeepDream 图像，Lucid 还允许进行上述的特征可视化。 谷歌还发行了 colab notebooks，它们使得用 Lucid 再现谷歌文章中的可视化变得极其简单。只要打开 notebook，点击按钮运行代码即可，无需安装。 GitHub 地址：https://github.com/tensorflow/lucid 这一工作只是触及用某些可能的接口帮助理解神经网络的表面。期待社区的下一步动作，并为共同致力于加深对人们对神经网络的理解而感到兴奋。 
162,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738808&idx=1&sn=950b3a51af28a0434018343a2810bc82&chksm=871ad406b06d5d1000a246ea73acb9bec34e9b1de8f19fc90e473f071a2c40667676a174eb1b&scene=27,这是一份优美的信息图，吴恩达点赞的deeplearning.ai课程总结,"吴恩达在推特上展示了一份由 TessFerrandez 完成的深度学习专项课程信息图，这套信息图优美地记录了深度学习课程的知识与亮点。因此它不仅仅适合初学者了解深度学习，还适合机器学习从业者和研究者复习基本概念。机器之心认为这不仅仅是一份课程笔记，同时还是一套信息图与备忘录。下面，我们将从深度学习基础、卷积网络和循环网络三个方面介绍该笔记，并提供信息图下载地址。 信息图下载地址： https://pan.baidu.com/s/1DtYg3TyplXQOVZ-YmplJaw 深度学习基础 1 深度学习基本概念 监督学习：所有输入数据都有确定的对应输出数据，在各种网络架构中，输入数据和输出数据的节点层都位于网络的两端，训练过程就是不断地调整它们之间的网络连接权重。 左上：列出了各种不同网络架构的监督学习，比如标准的神经网络（NN）可用于训练房子特征和房价之间的函数，卷积神经网络（CNN）可用于训练图像和类别之间的函数，循环神经网络（RNN）可用于训练语音和文本之间的函数。 左下：分别展示了 NN、CNN 和 RNN 的简化架构。这三种架构的前向过程各不相同，NN 使用的是权重矩阵（连接）和节点值相乘并陆续传播至下一层节点的方式；CNN 使用矩形卷积核在图像输入上依次进行卷积操作、滑动，得到下一层输入的方式；RNN 记忆或遗忘先前时间步的信息以为当前计算过程提供长期记忆。 右上：NN 可以处理结构化数据（表格、数据库等）和非结构化数据（图像、音频等）。 右下：深度学习能发展起来主要是由于大数据的出现，神经网络的训练需要大量的数据；而大数据本身也反过来促进了更大型网络的出现。深度学习研究的一大突破是新型激活函数的出现，用  ReLU  函数替换  sigmoid  函数可以在反向传播中保持快速的梯度下降过程，sigmoid 函数在正无穷处和负无穷处会出现趋于零的导数，这正是梯度消失导致训练缓慢甚至失败的主要原因。要研究深度学习，需要学会「idea—代码—实验—idea」的良性循环。 2 logistic 回归 左上：logistic 回归主要用于二分类问题，如图中所示，logistic 回归可以求解一张图像是不是猫的问题，其中图像是输入（x），猫（1）或非猫（0）是输出。我们可以将 logistic 回归看成将两组数据点分离的问题，如果仅有线性回归（激活函数为线性），则对于非线性边界的数据点（例如，一组数据点被另一组包围）是无法有效分离的，因此在这里需要用非线性激活函数替换线性激活函数。在这个案例中，我们使用的是 sigmoid 激活函数，它是值域为（0, 1）的平滑函数，可以使神经网络的输出得到连续、归一（概率值）的结果，例如当输出节点为（0.2, 0.8）时，判定该图像是非猫（0）。 左下：神经网络的训练目标是确定最合适的权重 w 和偏置项 b，那这个过程是怎么样的呢？ 这个分类其实就是一个优化问题，优化过程的目的是使预测值 y hat 和真实值 y 之间的差距最小，形式上可以通过寻找目标函数的最小值来实现。所以我们首先确定目标函数（损失函数、代价函数）的形式，然后用梯度下降逐步更新 w、b，当损失函数达到最小值或者足够小时，我们就能获得很好的预测结果。 右上：损失函数值在参数曲面上变化的简图，使用梯度可以找到最快的下降路径，学习率的大小可以决定收敛的速度和最终结果。学习率较大时，初期收敛很快，不易停留在局部极小值，但后期难以收敛到稳定的值；学习率较小时，情况刚好相反。一般而言，我们希望训练初期学习率较大，后期学习率较小，之后会介绍变化学习率的训练方法。 右下：总结整个训练过程，从输入节点 x 开始，通过前向传播得到预测输出 y hat，用 y hat 和 y 得到损失函数值，开始执行反向传播，更新 w 和 b，重复迭代该过程，直到收敛。 3 浅层网络的特点 左上：浅层网络即隐藏层数较少，如图所示，这里仅有一个隐藏层。 左下：这里介绍了不同激活函数的特点： sigmoid：sigmoid 函数常用于二分分类问题，或者多分类问题的最后一层，主要是由于其归一化特性。sigmoid 函数在两侧会出现梯度趋于零的情况，会导致训练缓慢。 tanh：相对于 sigmoid，tanh 函数的优点是梯度值更大，可以使训练速度变快。 ReLU：可以理解为阈值激活（spiking model 的特例，类似生物神经的工作方式），该函数很常用，基本是默认选择的激活函数，优点是不会导致训练缓慢的问题，并且由于激活值为零的节点不会参与反向传播，该函数还有稀疏化网络的效果。 Leaky ReLU：避免了零激活值的结果，使得反向传播过程始终执行，但在实践中很少用。 右上：为什么要使用激活函数呢？更准确地说是，为什么要使用非线性激活函数呢？ 上图中的实例可以看出，没有激活函数的神经网络经过两层的传播，最终得到的结果和单层的线性运算是一样的，也就是说，没有使用非线性激活函数的话，无论多少层的神经网络都等价于单层神经网络（不包含输入层）。 右下：如何初始化参数 w、b 的值？ 当将所有参数初始化为零的时候，会使所有的节点变得相同，在训练过程中只能学到相同的特征，而无法学到多层级、多样化的特征。解决办法是随机初始化所有参数，但仅需少量的方差就行，因此使用 Rand（0.01）进行初始化，其中 0.01 也是超参数之一。 4 深度神经网络的特点 左上：神经网络的参数化容量随层数增加而指数式地增长，即某些深度神经网络能解决的问题，浅层神经网络需要相对的指数量级的计算才能解决。 左下：CNN 的深度网络可以将底层的简单特征逐层组合成越来越复杂的特征，深度越大，其能分类的图像的复杂度和多样性就越大。RNN 的深度网络也是同样的道理，可以将语音分解为音素，再逐渐组合成字母、单词、句子，执行复杂的语音到文本任务。 右边：深度网络的特点是需要大量的训练数据和计算资源，其中涉及大量的矩阵运算，可以在 GPU 上并行执行，还包含了大量的超参数，例如学习率、迭代次数、隐藏层数、激活函数选择、学习率调整方案、批尺寸大小、正则化方法等。 5 偏差与方差 那么部署你的机器学习模型需要注意些什么？下图展示了构建 ML 应用所需要的数据集分割、偏差与方差等问题。 如上所示，经典机器学习和深度学习模型所需要的样本数有非常大的差别，深度学习的样本数是经典 ML 的成千上万倍。因此训练集、开发集和测试集的分配也有很大的区别，当然我们假设这些不同的数据集都服从同分布。 偏差与方差问题同样是机器学习模型中常见的挑战，上图依次展示了由高偏差带来的欠拟合和由高方差带来的过拟合。一般而言，解决高偏差的问题是选择更复杂的网络或不同的神经网络架构，而解决高方差的问题可以添加正则化、减少模型冗余或使用更多的数据进行训练。 当然，机器学习模型需要注意的问题远不止这些，但在配置我们的 ML 应用中，它们是最基础和最重要的部分。其它如数据预处理、数据归一化、超参数的选择等都在后面的信息图中有所体现。 6 正则化 正则化是解决高方差或模型过拟合的主要手段，过去数年，研究者提出和开发了多种适合机器学习算法的正则化方法，如数据增强、L2 正则化（权重衰减）、L1 正则化、Dropout、Drop Connect、随机池化和提前终止等。 如上图左列所示，L1 和 L2 正则化也是是机器学习中使用最广泛的正则化方法。L1 正则化向目标函数添加正则化项，以减少参数的绝对值总和；而 L2 正则化中，添加正则化项的目的在于减少参数平方的总和。根据之前的研究，L1 正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于 0，因此它常用于特征选择设置中。此外，参数范数惩罚 L2 正则化能让深度学习算法「感知」到具有较高方差的输入 x，因此与输出目标的协方差较小（相对增加方差）的特征权重将会收缩。 在中间列中，上图展示了 Dropout 技术，即暂时丢弃一部分神经元及其连接的方法。随机丢弃神经元可以防止过拟合，同时指数级、高效地连接不同网络架构。一般使用了 Dropout 技术的神经网络会设定一个保留率 p，然后每一个神经元在一个批量的训练中以概率 1-p 随机选择是否去掉。在最后进行推断时所有神经元都需要保留，因而有更高的准确度。 Bagging 是通过结合多个模型降低泛化误差的技术，主要的做法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。而 Dropout 可以被认为是集成了大量深层神经网络的 Bagging 方法，因此它提供了一种廉价的 Bagging 集成近似方法，能够训练和评估值数据数量的神经网络。 最后，上图还描述了数据增强与提前终止等正则化方法。数据增强通过向训练数据添加转换或扰动来人工增加训练数据集。数据增强技术如水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转通常应用在视觉表象和图像分类中。而提前终止通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。因此，提前终止通过确定迭代次数解决这个问题。 7 最优化 最优化是机器学习模型中非常非常重要的模块，它不仅主导了整个训练过程，同时还决定了最后模型性能的好坏和收敛需要的时长。以下两张信息图都展示了最优化方法需要关注的知识点，包括最优化的预备和具体的最优化方法。 以上展示了最优化常常出现的问题和所需要的操作。首先在执行最优化前，我们需要归一化输入数据，而且开发集与测试集归一化的常数（均值与方差）与训练集是相同的。上图也展示了归一化的原因，因为如果特征之间的量级相差太大，那么损失函数的表面就是一张狭长的椭圆形，而梯度下降或最速下降法会因为「锯齿」现象而很难收敛，因此归一化为圆形有助于减少下降方向的震荡。 后面的梯度消失与梯度爆炸问题也是十分常见的现象。「梯度消失」指的是随着网络深度增加，参数的梯度范数指数式减小的现象。梯度很小，意味着参数的变化很缓慢，从而使得学习过程停滞。梯度爆炸指神经网络训练过程中大的误差梯度不断累积，导致模型权重出现很大的更新，在极端情况下，权重的值变得非常大以至于出现 NaN 值。 梯度检验现在可能用的比较少，因为我们在 TensorFlow 或其它框架上执行最优化算法只需要调用优化器就行。梯度检验一般是使用数值的方法计算近似的导数并传播，因此它能检验我们基于解析式算出来的梯度是否正确。 下面就是具体的最优化算法了，包括最基本的小批量随机梯度下降、带动量的随机梯度下降和 RMSProp 等适应性学习率算法。 小批量随机梯度下降（通常 SGD 指的就是这种）使用一个批量的数据更新参数，因此大大降低了一次迭代所需的计算量。这种方法降低了更新参数的方差，使得收敛过程更为稳定；它也能利用流行深度学习框架中高度优化的矩阵运算器，从而高效地求出每个小批数据的梯度。通常一个小批数据含有的样本数量在 50 至 256 之间，但对于不同的用途也会有所变化。 动量策略旨在加速 SGD 的学习过程，特别是在具有较高曲率的情况下。一般而言，动量算法利用先前梯度的指数衰减滑动平均值在该方向上进行修正，从而更好地利用历史梯度的信息。该算法引入了变量 v 作为参数在参数空间中持续移动的速度向量，速度一般可以设置为负梯度的指数衰减滑动平均值。 上图后面所述的 RMSProp 和 Adam 等适应性学习率算法是目前我们最常用的最优化方法。RMSProp 算法（Hinton，2012）修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。RMSProp 是 Hinton 在公开课上提出的最优化算法，其实它可以视为 AdaDelta 的特例。但实践证明 RMSProp 有非常好的性能，它目前在深度学习中有非常广泛的应用。 Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。 8 超参数 以下是介绍超参数的信息图，它在神经网络中占据了重要的作用，因为它们可以直接提升模型的性能。 众所周知学习率、神经网络隐藏单元数、批量大小、层级数和正则化系数等超参数可以直接影响模型的性能，而怎么调就显得非常重要。目前最常见的还是手动调参，开发者会根据自身建模经验选择「合理」的超参数，然后再根据模型性能做一些小的调整。而自动化调参如随机过程或贝叶斯优化等仍需要非常大的计算量，且效率比较低。不过近来关于使用强化学习、遗传算法和神经网络等方法搜索超参数有很大的进步，研究者都在寻找一种高效而准确的方法。 目前的超参数搜索方法有： 依靠经验：聆听自己的直觉，设置感觉上应该对的参数然后看看它是否工作，不断尝试直到累趴。 网格搜索：让计算机尝试一些在一定范围内均匀分布的数值。 随机搜索：让计算机尝试一些随机值，看看它们是否好用。 贝叶斯优化：使用类似 MATLAB bayesopt 的工具自动选取最佳参数——结果发现贝叶斯优化的超参数比你自己的机器学习算法还要多，累觉不爱，回到依靠经验和网格搜索方法上去。 因为篇幅有限，后面的展示将只简要介绍信息图，相信它们对各位读者都十分有帮助。 9 结构化机器学习过程 我们需要按过程或结构来设定我们的机器学习系统，首先需要设定模型要达到的目标，例如它的预期性能是多少、度量方法是什么等。然后分割训练、开发和测试集，并预期可能到达的优化水平。随后再构建模型并训练，在开发集和测试集完成验证后就可以用于推断了。 10 误差分析 在完成训练后，我们可以分析误差的来源而改进性能，包括发现错误的标注、不正确的损失函数等。 11 训练集、开发集与测试集 上图展示了三个分割数据集及其表现所需要注意的地方，也就是说如果它们间有不同的正确率，那么我们该如何修正这些「差别」。例如训练集的正确率明显高于验证集与测试集表明模型过拟合，三个数据集的正确率都明显低于可接受水平可能是因为欠拟合。 12 其它学习方法 机器学习和深度学习当然不止监督学习方法，还有如迁移学习、多任务学习和端到端的学习等。 13 卷积神经网络基础 计算机视觉任务涉及的数据体量是特别大的，一张图像就有上千个数据点，更别提高分辨率图像和视频了。这时用全连接网络的话，参数数量太大，因而改用卷积神经网络（CNN），参数数量可以极大地减小。CNN 的工作原理就像用检测特定特征的过滤器扫描整张图像，进行特征提取，并逐层组合成越来越复杂的特征。这种「扫描」的工作方式使其有很好的参数共享特性，从而能检测不同位置的相同目标（平移对称）。 卷积核对应的检测特征可以从其参数分布简单地判断，例如，权重从左到右变小的卷积核可以检测到黑白竖条纹的边界，并显示为中间亮，两边暗的特征图，具体的相对亮暗结果取决于图像像素分布和卷积核的相对关系。卷积核权重可以直接硬编码，但为了让相同的架构适应不同的任务，通过训练得到卷积核权重是更好的办法。 卷积运算的主要参数： padding：直接的卷积运算会使得到的特征图越来越小，padding 操作会在图像周围添加 0 像素值的边缘，使卷积后得到的特征图大小和原图像（长宽，不包括通道数）相同。 常用的两个选项是：『VALID』，不执行 padding；『SAME』，使输出特征图的长宽和原图像相同。 stride：两次卷积操作之间的步长大小。 一个卷积层上可以有多个卷积核，每个卷积核运算得到的结果是一个通道，每个通道的特征图的长宽相同，可以堆叠起来构成多通道特征图，作为下一个卷积层的输入。 深度卷积神经网络的架构： 深度卷积神经网络的架构主要以卷积层、池化层的多级堆叠，最后是全连接层执行分类。池化层的主要作用是减少特征图尺寸，进而减少参数数量，加速运算，使其目标检测表现更加鲁棒。 14 经典卷积神经网络 LeNet·5：手写识别分类网络，这是第一个卷积神经网络，由 Yann LeCun 提出。 AlexNet：图像分类网络，首次在 CNN 引入 ReLU 激活函数。 VGG-16：图像分类网络，深度较大。 15 特殊卷积神经网络 ResNet：引入残差连接，缓解梯度消失和梯度爆炸问题，可以训练非常深的网络。 Network in Network：使用 1x1 卷积核，可以将卷积运算变成类似于全连接网络的形式，还可以减少特征图的通道数，从而减少参数数量。 Inception Network：使用了多种尺寸卷积核的并行操作，再堆叠成多个通道，可以捕捉多种规模的特征，但缺点是计算量太大，可以通过 1x1 卷积减少通道数。 16 实践建议 使用开源实现：从零开始实现时非常困难的，利用别人的实现可以快速探索更复杂有趣的任务。 数据增强：通过对原图像进行镜像、随机裁剪、旋转、颜色变化等操作，增加训练数据量和多样性。 迁移学习：针对当前任务的训练数据太少时，可以将充分训练过的模型用少量数据微调获得足够好的性能。 基准测试和竞赛中表现良好的诀窍：使用模型集成，使用多模型输出的平均结果；在测试阶段，将图像裁剪成多个副本分别测试，并将测试结果取平均。 17 目标检测算法 目标检测即使用边界框检测图像中物体的位置，Faster R-CNN、R-FCN 和 SSD 是三种目前最优且应用最广泛的目标检测模型，上图也展示了 YOLO 的基本过程。 18 人脸识别 人脸识别有两大类应用：人脸验证（二分分类）和人脸识别（多人分类）。 当样本量不足时，或者不断有新样本加入时，需要使用 one-shot learning，解决办法是学习相似性函数，即确定两张图像的相似性。比如在 Siamese Network 中学习人脸识别时，就是利用两个网络的输出，减少同一个人的两个输出的差别，增大不同人的两个输出之间的差别。 19 风格迁移 风格迁移是一个热门话题，它会在视觉上给人耳目一新的感觉。例如你有一副图，然后将另一幅图的风格特征应用到这幅图上，比如用一位著名画家或某一副名画的风格来修改你的图像，因此我们可以获得独特风格的作品。 20 循环神经网络基础 如上所示，命名实体识别等序列问题在现实生活中占了很大的比例，而隐马尔可夫链等传统机器学习算法只能作出很强的假设而处理部分序列问题。但近来循环神经网络在这些问题上有非常大的突破，RNN 隐藏状态的结构以循环形的形式成记忆，每一时刻的隐藏层的状态取决于它的过去状态，这种结构使得 RNN 可以保存、记住和处理长时期的过去复杂信号。 循环神经网络（RNN）能够从序列和时序数据中学习特征和长期依赖关系。RNN 具备非线性单元的堆叠，其中单元之间至少有一个连接形成有向循环。训练好的 RNN 可以建模任何动态系统；但是，训练 RNN 主要受到学习长期依赖性问题的影响。 以下展示了 RNN 的应用、问题以及变体等： 循环神经网络在语言建模等序列问题上有非常强大的力量，但同时它也存在很严重的梯度消失问题。因此像 LSTM 和 GRU 等基于门控的 RNN 有非常大的潜力，它们使用门控机制保留或遗忘前面时间步的信息，并形成记忆以提供给当前的计算过程。 21 NLP 中的词表征 词嵌入在自然语言处理中非常重要，因为不论执行怎样的任务，将词表征出来都是必须的。上图展示了词嵌入的方法，我们可以将词汇库映射到一个 200 或 300 维的向量，从而大大减少表征词的空间。此外，这种词表征的方法还能表示词的语义，因为词义相近的词在嵌入空间中距离相近。 除了以上所述的 Skip Grams，以下还展示了学习词嵌入的常见方法： GloVe 词向量是很常见的词向量学习方法，它学到的词表征可进一步用于语句分类等任务。 22 序列到序列 序列到序列的方法使用最多的就是编码器解码器框架，其它还有束搜索等模块的介绍。 编码器解码器架构加上注意力机制可以解决非常多的自然语言处理问题，以下介绍了 BLEU 分值和注意力机制。它们在机器翻译的架构和评估中都是不能缺少的部分。 以上是所有关于吴恩达深度学习专项课程的信息图，由于它们包含的信息较多，我们只介绍了一部分，还有很多内容只是简单的一笔带过。所以各位读者最好可以下载该信息图，并在后面的学习过程中慢慢理解与优化。 "
163,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738741&idx=4&sn=894fa577b92d93da56e47331de0a12e8&chksm=871ad44bb06d5d5df31e0b5f4839357827f65789c41cdfa00f3b04300c4fde063eb5dd8dc9a7&scene=27,业界 | 谷歌推出72-qubit量子处理器Bristlecone，意图实现「量子霸权」,选自Google Research Blog 今天，谷歌在洛杉矶举办的美国物理学会年度会议上推出了他们的 72-qubit 超导量子处理器 Bristlecone。本文介绍了他们的成果、近期目标和未来展望。 谷歌量子 AI 实验室的目标是构建一个可以解决实际问题的量子计算机，其策略是使用与大规模通用纠错量子计算机向前兼容的系统探索近期应用。 。关键的是，处理器在读出和逻辑运算时也必须具备低错误率，比如单比特量子门和两比特量子门。 谷歌量子 AI 实验室：https://research.google.com/pubs/QuantumAI.html 今天，谷歌在洛杉矶举办的美国物理学会年度会议上推出了他们的量子处理器 Bristlecone，这一基于门的超导系统的目的是为研究系统错误率和量子比特技术的可扩展性以及量子模拟、优化和机器学习应用提供一个测试平台。 该处理器的指导设计原则是保留谷歌先前 9 个量子比特线性阵列技术的物理学基础，展示其最佳结果：读出错误率 1%、单量子比特门错误率 0.1%，以及最为重要的两比特量子门错误率 0.6%。该处理器使用了相同的耦合、控制和读出方案，但已扩展至 72 个量子比特的矩形阵列。之所以选择这一规模的处理器是为了在未来实现「量子霸权」，使用表面代码探索一阶和二阶纠错，并推动量子算法在实际硬件上的部署。 在探索具体的应用之前，量化量子处理器的功能很重要。谷歌的理论团队为这项任务开发了基准测试工具。通过应用随机量子电路到设备上并检查采样的输出分布，与经典模拟进行对比，可以确定单个系统的误差。如果量子处理器有足够低的误差，它可以在定义明确的计算机科学问题上超越经典超级计算机，即实现「量子霸权」。 谷歌表示量子处理器超越超级计算机的实验证明将是该领域的分水岭，这也一直是其主要目标之一。 谷歌近期的目标是在 Bristlecone 的 72 量子比特上实现近似 9 量子比特设备的最佳错误率。他们相信 Bristlecone 将作为建立更大规模量子计算机的引人入胜的原理验证。在低系统误差下操作 Bristlecone 这样的设备，需要从软件、电子控制设备到处理器本身的完整技术堆栈的互相协调。实现真正的工作需要系统工程的多次迭代。 谷歌乐观地认为 Bristlecone 可以实现「量子霸权」，他们期望在未来继续共享研究成果，与合作者一起运行实验。 
164,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738741&idx=5&sn=b6edeac97df06eed3ccb942fb12bea48&chksm=871ad44bb06d5d5de91abdd06e0f24e1be9c02ba5e2a43b2000a8cecff4f0d648ce511168e72&scene=27,学界 | 结合主动学习与迁移学习：让医学图像标注工作量减少一半,"选自arXiv 和普通图像的标注不一样，生物医学图像的标注需要有专业知识和技能的人来做，因此难以获得大型的有标注数据集供卷积神经网络学习。近日，IEEE 一篇论文提出可以将主动学习和迁移学习结合起来降低标注任务的工作量，实验结果也证明了这种方法的有效性。机器之心对该论文进行了编译介绍，详细的数学过程和结果分析请参阅原论文。 在 ImageNet 和 Places 等大规模有标注数据集的帮助下，卷积神经网络（CNN）已经为计算机视觉领域带来了革命性的发展。正如 IEEE TMI 专刊 [4] 和最近的两本书 [5,6] 谈到的那样，人们对在生物医学图像分析中应用 CNN 有着广泛且浓厚的兴趣；但由于生物医学领域缺乏如此大量的有标注数据集，所以 CNN 在这一领域的成功之路还有所阻碍。标注生物医学图像不仅耗时耗力，而且需要高成本的、特定专业的知识和技能，这些都不易取得。因此，我们希望解答这个重要问题：如何显著降低将 CNN 应用于生物医学图像的标注成本；另外我们也想解答一个附属问题：给定一个有标签数据集，如何确定它充分覆盖了不同的相关对象。为此，我们提出了一种名为 AFT* 的全新方法，可以自然地将主动学习（active learning）和迁移学习（transfer learning）整合成单一一个框架。我们的 AFT* 方法首先会使用一个预训练 CNN 来寻找未标注数据中的「显著」样本来进行标注，然后这个（经过微调的）的 CNN 会根据新标注的样本以及所有误分类的样本而持续得到调整改进。我们在三种不同的应用上评估了我们的方法，其中包括结肠镜检查帧分类、息肉检测和肺栓塞（PE）检测；结果表明标注成本至少可以减少一半。 这种出色表现主要得益于一个简单而又强大的观察结果：为了提升 CNN 在生物医学图像上的表现，通常会通过数据增强方法为每个候选数据自动生成多个图块（patch）；这些根据同一候选数据生成的图块具有同样的标签，所以当它们被放入训练集中时，自然能够预见当前 CNN 会为它们给出相似的预测结果。因此，它们的熵和多样性能提供有关候选数据的「能力」的有用指标，从而可帮助提升当前 CNN 的表现。但是，自动数据增强无可避免地会为某些候选数据生成「困难的」样本，注入有噪声的标签；因此，为了显著增强我们的方法的稳健性，我们会根据当前 CNN 的预测结果，通过选择每个候选数据的一小部分图块来计算 熵和多样性。 有些研究已经表明对生物医学图像分析 CNN 进行微调是有用的，但这些研究只执行了一次微调，也就是说，只使用所有可用的训练样本对预训练的 CNN 进行一次微调，而不涉及任何主动选择过程。就我们所知，我们提出的方法是首次以连续的方式将主动学习整合到 CNN 的微调中，可以使 CNN 对生物医学图像分析更加友好，实现极大降低标注成本的目标。算法 1 给出了我们的方法的概览；与传统的主动学习相比，我们的方法有 8 项优势： 从全空的有标签数据集开始，不需要任何初始的有标签候选数据； 通过持续的微调而非反复的再训练来逐步提升学习器的表现； 通过自然地利用每个候选数据中图块之间的预期一致性来主动选择信息最丰富和最有代表性的候选数据； 在每个候选数据中的少量图块上局部地计算选择标准，从而能节省可观的计算时间； 通过多数选择（majority selection）自动处理有噪声标签； 自动平衡不同类的训练样本； 将新选择的候选数据与误分类的候选数据结合到一起，去除简单样本以提升训练效率，重点关注困难样本以防止灾难性遗忘（catastrophic forgetting）； 将随机性纳入主动选择过程，以在探索（exploration）和利用（exploration）之间达到接近最优的平衡。 更重要的是，我们的方法有望对使用生物医学成像的计算辅助诊断（CAD）产生重要影响，因为当前法规要求 CAD 系统必须部署在「封闭」环境中，其中所有的 CAD 结果都要经过放射科医生的检查，如有错误就会得到纠正；由此，所有的假正例结果都应该被移除，所有的假负例结果都应该补充上来，这是一种即时的在线反馈，也许能让 CAD 系统能够具备自我学习能力并且可能能在我们的方法的持续性微调能力的帮助下在部署之后继续改进。 AFT* 是在生物医学成像计算机辅助诊断（CAD）的背景下设计的。CAD 系统通常有一个候选数据生成器，可以快速生成一个候选数据集合，其中有些是真正例，有些是假正例。在生成候选数据之后，任务目标是训练一个分类器来尽可能地去除假正例结果同时尽可能地保留真正例结果。为了训练分类器，必须对每个候选数据进行标注。我们假设每个候选数据都要取多个可能标签中的一个。为了提升用于 CAD 系统的 CNN 的表现，通常要通过数据增强为每个候选数据自动生成多个图块；这些根据同一候选数据生成的图块会继承该候选数据的标签。换句话说，所有标签都是在候选数据层面上获取的。 但是，AFT* 是通用型的，可以应用于计算机视觉和图像分析领域中的很多任务。为了说明清楚，我们将使用 Places 数据库在自然图像中的场景解读任务上阐述 AFT* 背后的思想，其中不需要候选数据生成器，因为每张图像都可以直接被当作是候选数据。为了说明简单同时不失一般性，我们将其限制到了 3 种类别（厨房、客厅和办公室），并且将每一类中的 Places 图像都分成了训练集（14 000 张图像）、验证集（1000 张图像）和测试集（100 张图像），这三个子集之间没有重叠。 设计主动学习算法涉及两个关键问题：（1）如何确定一个标注候选数据的「价值度（worthiness）」；（2）如何更新分类器/学习器。 3.1 主动候选数据选择 图 3 给出了用于多类分类的主动候选数据选择过程，同时为了便于理解，表 1 用二元分类情况对其进行了阐述。如表 1 第 1 行所示，二元分类情况有 7 种典型预测模式。 3.2 寻找有价值的候选数据 主动学习的关键是开发用于确定标注候选数据「价值度」的指标。我们的标准基于一个简单而强大的观察结果：所有根据同一候选数据增强得到的图块都具有同样的标签；预计当前 CNN 对它们的预测也相似。因此，它们的熵和多样性能提供有关候选数据的「能力」的有用指标，从而可帮助提升当前 CNN 的表现。直观上讲，熵代表了分类的确定性——更高的不确定性值表示更高程度的信息（比如，表 1 中的模式 A）；而多样性是指在一个候选数据的多个图块上所得到的预测的一致性——多样性值更高说明预测不一致性程度更高（比如，表 1 中的模式 C）。 3.3 通过多数选择处理噪声标签 自动数据增强对提升 CNN 的表现而言至关重要，但却无可避免地会为某些候选数据生成「困难的」样本（如图 4(c) 所示），注入有噪声的标签；因此，为了显著增强我们的方法的稳健性，我们会根据当前 CNN 的预测结果，通过选择每个候选数据的一小部分图块来计算熵和多样性。 3.4 将随机性注入主动选择 如 [41] 中讨论的那样，简单的随机选择在一开始时的表现可能优于主动选择，因为主动选择依赖于当前模型来选择用于标注的范例；因此，在早期阶段做出的糟糕选择可能会对后续选择的质量造成不良影响；而随机选择则更不易受到糟糕假设的约束。也就是说，主动选择重在利用从已获得的标签中取得的知识以探索决策边界，而随机选择则重在探索，所以能够定位到特征空间中分类器表现糟糕的区域。因此，有效的主动学习策略必须找到探索和利用之间的平衡。为此，我们通过根据采样概率主动选择而向我们的方法中注入了随机性。 3.5 比较多种学习策略 根据上面的讨论，可以推导出几种主动学习策略，如表 2 所示。我们对这些策略进行了全面的比较，结果表明：（1）AFT' 不稳定；（2）AFT'' 需要仔细调整参数；（3）AFT 与 AFT' 和 AFT'' 相比是最可靠的，但需要在每一步使用所有目前可用的有标注样本从一开始就对原始模型进行微调。为了克服这个短板，我们开发了一种优化版本 AFT*，可以使用新标注的候选数据以及被误分类的候选数据来持续优化当前模型。有些研究者已经证明微调能带来更好的表现，而且比从头开始训练更加稳健。此外，我们的实验表明 AFT* 的收敛速度比反复微调原来的预训练的 CNN 更快，从而可以节省训练时间；AFT* 还能通过去除简单样本，重点关注困难样本，防止灾难性遗忘来提升性能。 图 2(a) 比较了使用 Places 数据库的 AFT* 和 RFT。RFT 通过系统性的随机采样生成了 6 个不同的序列。最后的曲线是根据 6 次运行的平均结果绘制的。如图 2(a) 所示，在 AUC（曲线下面积）方面，仅使用了 2906 个候选数据查询的 AFT* 可以实现使用了 4452 个候选数据查询的 RFT 的表现；同时 AFT* 仅使用 1176 个候选数据查询就能实现使用全部 42000 个候选数据的完全训练（full training）的表现。因此，AFT* 相比于 RFT 能节省 34.7% 的标注成本，相比于完全训练能节省 97.2% 的标注成本。当使用了大约 100% 的训练数据时，其表现仍然在继续增长；因此，考虑到 GoogLeNet 架构有 22 层，所以这个数据集的大小还是不够。AFT* 是一种通用算法，不仅可用于生物医学数据集，而且也能用于其它数据集；AFT* 可用于有很多类别的问题。 我们将我们的 AFT 和 AFT* 方法应用到了三种应用上，包括结肠镜检查帧分类、息肉检测和肺栓塞（PE）检测。 4.1 结肠镜检查帧分类 4.2 息肉检测 4.3 肺栓塞检测 4.4 比较所有方法 4.5 在所选模式上的观察结果 4.6 正例-负例比例的自动平衡 图 9：AFT*、AFT 和 RFT 所选择的候选数据中的正例-负例比例。注意，RFT 的比例大致能代表整个数据集的比例 。 4.7 AFT* 在 CNN 架构中的泛化性 图 10：基于三种应用，在 GoogLeNet 上比较 AFT* 和 AFT 的表现。（a）结肠镜检查帧分类、（b）息肉检测和（c）肺栓塞检测。结果表现出了与 AlexNet（见图 5）一样的模式。 AFT*：整合主动学习与迁移学习以减少标注工作（Integrating Active Learning and Transfer Learning to Reduce Annotation Efforts） 论文地址：https://arxiv.org/abs/1802.00912 卷积神经网络（CNN）在计算机视觉领域的辉煌成功很大程度上可以归功于大型有标注数据集的可用性，比如 ImageNet 和 Places。但是，在生物医学成像领域，创建如此大型的有标注数据集是非常艰难的，因为标注生物医学图像不仅枯燥费力和费时，而且需要高成本的专业技能，这并不容易取得。为了大幅降低标注成本，本论文提出了一种用于将主动学习和迁移学习（微调）自然地整合成一个单一框架的全新方法 AFT*。该方法首先是使用一个预训练的 CNN 来为标注寻找「有价值的」样本，然后通过持续性的微调来增强（微调过的）CNN。我们在三种不同的生物医学成像应用中评估了我们的方法，结果表明与之前最佳的方法相比，这至少可以降低一半的成本。这种表现得益于我们方法的先进的主动连续学习能力的多种优势。尽管 AFT* 最初是在生物医学成像中的计算机辅助诊断背景中设计的，但这种方法是通用的，可用于计算机视觉和图像分析中的很多任务；我们使用 Places 数据库在自然图像的场景解读任务上阐释 AFT* 背后的关键思想。 注：据机器之心了解，本文作者之一梁建明是AI医疗创业公司体素科技的研究开发副总裁，美国亚利桑那州立大学（ASU）副教授，美国梅奥医学中心首届入驻教授，发表了超过70篇论文并获得13项专利。 "
165,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738741&idx=2&sn=1e3cc2934d5eda89610ccd116a84a372&chksm=871ad44bb06d5d5da02fc30e22de53a55d0096daf71d2a4a857fa5e4ec2f8b2fc554028d4b50&scene=27,学界 | 南京大学宣布成立人工智能学院,机器之心报道 据南京大学官网信息，南大宣布成立人工智能学院，将致力于建设一流的人工智能基础研究基地和人才培养基地。 据机器之心了解，南京大学昨日晚上已经发布通知，宣布成立南京大学人工智能学院。今日，南京大学新闻网上正式发布相关新闻。 在新闻中，南京大学写道，「近年来，随着全球人工智能技术突飞猛进，人工智能已成为各国彰显创新实力的必争之地。2017 年国务院印发《新一代人工智能发展规划》，从国家层面对我国人工智能的发展道路进行了战略部署。2018 年 3 月 5 日李克强总理的政府工作报告中四次提及「智能」，并特别指出要『加强新一代人工智能研发应用』、『发展智能产业』。成立人工智能学院，是南京大学顺应国际科技发展趋势、打造学科发展生态体系、更好地服务国家与地方建设的又一重大举措。」 目前，在全球大学计算机科学与人工智能排名  （http://csrankings.org）上，南京大学的计算机科学名列亚洲第 14 位。 而周志华教授带领的机器学习与数据挖掘研究所 (LAMDA) 是南京大学为外界所熟知的 AI 研究部门。据官网介绍，LAMDA 的含义是「Learning And Mining from DatA」。LAMDA 的主要研究兴趣包括机器学习、数据挖掘、模式识别、信息检索、演化计算、神经计算，以及相关的其他领域。目前的主要研究内容包括：集成学习、半监督与主动学习、多示例与多标记学习、代价敏感和类别不平衡学习、度量学习、降维与特征选择、结构学习与聚类、演化计算的理论基础、增强可理解性、基于内容的图像检索、 Web 搜索与挖掘、人脸识别、 计算机辅助医疗诊断、生物信息学等。 除了负责人周志华教授，我们也能在 CSRankings 上看到姜远教授、李武军副教授等 LAMDA 研究所的成员。 南京大学并非是国内最早成立人工智能学院的院校。2017 年 5 月 28 日，中国科学院大学发文成立人工智能技术学院。这是我国人工智能技术领域首个全面开展教学和科研工作的新型学院。 2017 年 11 月 2 日，西安电子科技大学人工智能学院正式揭牌成立，该学院系国内教育部直属高校首个致力于人工智能领域高端人才培养、创新成果研发和高层次团队培育的实体性学院。 
166,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738741&idx=3&sn=ad3fc7e9baa65591b0c432c9647d1655&chksm=871ad44bb06d5d5dfef108d4c7ffa615e2370e334e1f6d19d096e8894da22688699fc608a0cf&scene=27,解读 | 起底语音对抗样本：语音助手危险了吗？,机器之心原创 想要让深度学习系统走向大街小巷、走进千家万户，就要在算法研发阶段给出系统的鲁棒性检验。对于图像对抗性攻击的讨论正是如火如荼，攻防双方都是妙手频出的状态。例如，来自 MIT 和 UC Berkeley 的两位博士生，Anish Athalye 和 Nicholas Carlini 就接连攻破了 7 篇 ICLR 2018 接收的对抗防御文章，指出，你们的防御策略不过都是基于「混淆梯度」（obfuscated gradient）现象的「虚假安全感」。虽然 Ian Goodfellow 回应称「 混淆梯度 」不过是之前「梯度遮挡」（gradient masking）的老调重弹，而且声称「遍历了 ICLR 8 篇对抗防御论文」的作者是不是漏掉了我参与的 ICLR 入选论文《Ensemble Adversarial Training: Attacks and Defenses》呀？但 Ian 的回击仍然不妨碍 Athalye 和 Carlini 成为名噪一时的攻方代表。 图像领域的攻击防御如此热闹，是因为图像识别的应用场景直指安防、自动驾驶这类关乎生命安全的方向，而当 2017 年，当国内外业界里，无论是雄踞一方的巨头还是崭露头角的新秀，都争先恐后地发布「智能音箱」，进而用跳楼价在家居语音入口这个还远不成气候的领域里圈地盘的时候，学界对于语音对抗性攻击的关注，也终于在视觉之后水涨船高了。 比如，上文中提到的「拳打 ICLR」的博士生之一，UC Berkeley 的 Nicholas Carlini 就与其导师一起，在《Audio Adversarial Examples: Targeted Attacks on Speech-to-Text》一文中给出了对 Mozilla 实现的百度 DeepSpeech 论文的一个白箱、定向、需要直接输入的攻击。给定任意一个波形，甚至不必须是语音，音乐乃至无声都可以，就能用优化的办法生成一个 99.9% 相似的、但是会被语音识别系统转写成完全不同的另一段话的新波形。 本文接下来，就将沿着这篇工作展开，简单聊聊对抗样本的分类，然后验证一下作者提供的对抗样本的攻击效果。 首先聊聊对抗样本的分类。 「白箱、定向、需要直接输入」这三个修饰「攻击」的形容词，其实都在从攻击的力度层面对对抗样本进行分类。阅读对抗样本相关工作的时候，我们首先可以问这样三个问题： 1 对抗样本的制造过程中，是否拥有模型结构及参数的知识？ 如果答案是肯定的，那么攻击是「白箱」攻击。如果答案是否定的，则为「黑箱」攻击。 了解模型结构和参数，换言之了解模型的预测行程的过程，就能够有的放矢地进行对抗样本的构建，这无疑是更容易的，因此最先成功的以及当前大部分的对抗样本都是白箱攻击。 是否有成功的黑箱攻击呢？在图像领域，有，Ian Goodfellow 在 2016 年参与的一篇文章，《Practical Black-Box Attacks against Machine Learning》，就给出了一个「利用原模型生成样本及标签，创建合成训练集（synthetic dataset）、利用训练集建立原模型的替代网络（substitute network）、再利用替代网络优化创建对抗样本」的解决方案。 但是在语音领域，尚没有成功的黑箱攻击。本文就是一个白箱攻击，攻击的对象是 Mozilla 去年年底开源的 。DeepSpeech（GitHub: https://github.com/mozilla/DeepSpeech）是对百度硅谷 AI 实验室（SVAIL）2014 年的论文《Deep Speech: Scaling up end-to-end speech recognition》的一个 TensorFlow 实现，在 LibriSpeech clean test corpus 上达到了 6.0% 的词错率（WER）。 插一句，看到这里还是很唏嘘的，百度是最早站在深度学习研究一线的 BAT 了，这篇文章的作者列表里赫然有当年的百度 AI 研究总负责人吴恩达和硅谷 AI 实验室总负责人 Adam Coates。然而去年一年里，吴恩达、  以及吴恩达的继任者林元庆已经先后离开百度，百度的 AI 研究力量，也随着李彦宏「从没说过 All in AI」的宣言，散落天涯了。 2 对抗样本能保证模型错误的类型吗？ 如果答案是肯定的，那么进行的是定向（targeted）攻击，否则则是非定向（untargeted）攻击。 具体一点解释，对于非定向攻击来说，能让一个区分动物的模型把猫识别成任何其他动物，都算是攻击成功。而对于定向攻击来说，构建对抗样本的时候的目标要更加明确：例如，目标是让模型把猫识别成狗，那么如果模型把猫识别成了兔子，仍然要说定向攻击是失败的。 本文是一个定向攻击，如下图所示，给定一个声波 x，可以造出一个干扰项 δ，让 x+δ 能够被机器识别成指定的、与 x 不同的标签。 不过！Mozilla DeepSpeech 在今年 1 月底更新了 v0.1.1，进行了一些依赖项调整，重新训练了参数，但并没有改动主要结构。然而在后文，当我用新版本测试作者给出的对抗样本的时候，发现攻击样本已经不能「指哪儿打哪儿」地篡改语音内容了，而是会输出一段乱码，换言之，一旦白箱设定稍有收缩、参数改动，定向攻击就会退化成非定向攻击。 3 对抗样本可以进入现实场景吗？ 这个问题听起来非常宽泛，但其实是衡量对抗样本威胁性的一个重要指标。 如果答案是否定的，样本需要直接作为输入进入模型，那么其威胁性是有限的。例如本文中的音频样本，只能直接以 WAV 格式交给模型，才能有攻击效果；如果用播放器播放再用麦克收音，攻击就完全失效了——不只是从定向攻击退化成非定向攻击，而是完全没有攻击效果。 如果答案是肯定的，那么语音攻击的样本就可以在不知不觉中唤醒你的语音助手然后进行特定的操作。去年浙江大学的《DolphinAttack: Inaudible Voice Commands》，就利用了谐波以及麦克风和人耳接受声音频率的范围不同，实现在人感知不到的情形下通过麦克风唤醒 Siri、Google Assistant 和 Alexa 等系统并执行相应语音命令的操作。当然，考虑到这类攻击必须利用专业设备、设备距离麦克风不超过 1.5 米，以及供应商可以在系统端通过设置允许频率范围进行防御等等特点，这类攻击真实的「威胁性」并没有那么高，但是它确实是可以进入现实场景的攻击。 图像领域里进入现实场景的样本攻击就更多了，有可以打印出来放在待识别物体周围扰乱分类的贴纸，也有哪怕不是直接进入模型，而是被摄像机捕捉，也仍然能进行攻击的像素改变技术。 总之，白箱还是黑箱、是否定向、是否可以进入现实场景，是根据攻击的威胁性对对抗样本进行分类的三个相对重要的角度，除此之外，也有是否对压缩鲁棒、是否可迁移等其他衡量标准，大家可以阅读论文做详细了解。 解释完一连串的形容词之后来看看对抗样本的测试攻击效果。 作者在个人网站上（https://nicholas.carlini.com/code/audio_adversarial_examples/）公布了一些对抗样本的样例。人耳听起来几乎一样的一段语音，（听起来都是一个带着咖喱口音的大叔在说「that day the merchant gave the boy permission to build the display」，只不过后两个样本中背景里有轻微的杂音）会被系统分别识别为三句截然不同的话。 我利用 Google Colab 薅了一点点 GPU 资源测试了下 DeepSpeech 对攻击的反应。关于 Google Colab 的用法，可以参考之心之前的文章： 。 首先把上面三个音频文件放在 Google Drive 上，并通过右键选择「Get Sharable Link」拿到对应的 id（链接中 id= 后面的部分）。 然后开一个 Colab Notebook，在选项栏 Edit ==》Notebook Setting 里选择 GPU。 安装 DeepSpeech 包： 下载模型 0.1.0 版本： 注意，最新版是 0.1.1，作者用的版本是 0.1.0，不要下错。 验证模型文件的 MD5 sum： 进行授权登录以加载数据： 加载数据： 运行预训练好的模型进行推断： 注意，不同于 github 上给的命令，参数顺序是：模型、音频文件、字母表、lm（非必须）、trie（非必须） 参数说明： extra0a.wav 识别结果： 还是有一点不准的。 extra0b.wav 识别结果： extra0c.wav 识别结果： 复现了对抗结果。 接下来我们试验一下 v0.1.1 对对抗样本的反应。 新版本是一个针对这篇论文进行调整的模型吗？版本的主要改动说明中并未对此进行说明，但是，版本的提交者确实是知晓这篇论文的存在的，他提出了一个至今仍然开放的 issue（GitHub: https://github.com/mozilla/DeepSpeech/issues/1179），但是并没有人回应。 让我们看看新版本的效果： extra0a.wav 识别结果： 分割好了一点。 extra0b.wav 识别结果： 乱码了。 extra0c.wav 识别结果： 仍然是乱码。 模型结构未变，参数变了，对抗样本就从定向变成非定向了。 出于好奇，我还拿讯飞听见的英文听写功能（beta 版）测试了一下： 对抗并没有效果，而且识别效果也还不错。 总体而言，作者能够对一个固定结构、固定参数的模型实现有效的白箱定向攻击，但是这种攻击是不可迁移、不可扩展的。文章的贡献在于在语音方面开始了对「定向」的探索。因为不同于自动驾驶等视觉场景，对于现阶段的语音模型与系统来说，非定向攻击并没有太大的威胁性。在自动驾驶场景里，一个能够让「禁行」、「急转弯」路牌无法被准确识别的非定向攻击，就会造成严重的后果。但是在语音系统中，非定向攻击造成的后果不外乎「语音助手变成了语音废柴」，并不会威胁用户的隐私、财产或者生命安全，从「人工智障时代」一路走来的用户对这种程度的漏洞还是有相当的宽容度的。因此，能够对语音系统产生影响，推动其进步的对抗样本必然是以定向为基础的。我们也期望有更多以定向为基础，穿透语音识别系统中不同模型的集成，更加深入语音识别本质的，对抗样本攻击的出现。 
167,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738741&idx=1&sn=d4ba0d9f640676c11f9a0682ed9816fa&chksm=871ad44bb06d5d5d41fee4611340381542104d5b0f91baabbb95dea1632809c3f4e367f0cd6d&scene=27,终于！Keras官方中文版文档正式发布了,"机器之心整理 今年 1 月 12 日，Keras 作者 François Chollet‏ 在推特上表示因为中文读者的广泛关注，他已经在 GitHub 上展开了一个 Keras 中文文档项目。而昨日，François Chollet‏ 再一次在推特上表示 Keras 官方文档已经基本完成！他非常感谢翻译和校对人员两个多月的不懈努力，也希望 Keras 中文使用者能继续帮助提升文档质量。 这一次发布的是 Keras 官方中文文档，它得到了严谨的校对而提升了整体质量。但该项目还在进行中，虽然目前已经上线了很多 API 文档和使用教程，但仍然有一部分内容没有完成。其实早在官方中文文档出现以前，就有开发者构建了 Keras 的中文文档，而且很多读者都在使用 MoyanZitto 等人构建的中文文档。 Keras 官方文档：https://keras.io/zh/ Keras 第三方文档：http://keras-cn.readthedocs.io/en/latest/ 以下我们将简要介绍这次官方发布的 Keras 文档。 Keras 是一个用 Python 编写的高级神经网络 API，它能够以 TensorFlow、CNTK、或者 Theano 作为后端运行。Keras 的开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。 如果你有如下需求，请选择 Keras： 允许简单而快速的原型设计（用户友好，高度模块化，可扩展性）。 同时支持卷积神经网络和循环神经网络，以及两者的组合。 在 CPU 和 GPU 上无缝运行与切换。 Keras 兼容的 Python 版本: Python 2.7-3.6。 Keras 相对于其它深度学习库非常容易构建：首先它提供一致和简单的 API；其次，它提供独立的、完全可配置的模块构成序列或图表以完成模型；最后，作为新的类和函数，新的模块很容易扩展。这样说可能比较抽象，但正如文档中所描述的，我们甚至在 30 秒就能快速上手 Keras。所以在坑外徘徊或准备入坑 Keras 的小伙伴可以开心地开始你们的 30 秒。 快速开始：30 秒上手 Keras Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 Sequential 模型，它是由多网络层线性堆叠的栈。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。 Sequential 模型如下所示： 可以简单地使用 .add() 来堆叠模型： 在完成了模型的构建后, 可以使用 .compile() 来配置学习过程： 如果需要，你还可以进一步地配置优化器。Keras 的一个核心原则是使事情变得相当简单，同时又允许用户在需要的时候能够进行完全的控制（终极的控制是源代码的易扩展性）。 现在，你可以批量地在训练数据上进行迭代了： 或者，你可以手动地将批次的数据提供给模型： 只需一行代码就能评估模型性能： 或者对新的数据生成预测： 构建一个问答系统，一个图像分类模型，一个神经图灵机，或者其他的任何模型，就是这么的快。深度学习背后的思想很简单，那么它们的实现又何必要那么痛苦呢？ 使用简介 Keras 模型的使用一般可以分为顺序模型（Sequential）和 Keras 函数式 API，顺序模型是多个网络层的线性堆叠，而 Keras 函数式 API 是定义复杂模型（如多输出模型、有向无环图，或具有共享层的模型）的方法。以下将简要介绍两种模型的使用方法： 1.Keras 顺序模型 你可以通过将层的列表传递给 Sequential 的构造函数，来创建一个 Sequential 模型： 也可以使用 .add() 方法将各层添加到模型中： 如下展示了一个完整的模型，即基于多层感知器 (MLP) 的 softmax 多分类： 2. Keras 函数式 API  利用函数式 API，可以轻易地重用训练好的模型：可以将任何模型看作是一个层，然后通过传递一个张量来调用它。注意，在调用模型时，您不仅重用模型的结构，还重用了它的权重。 以下是函数式 API 的一个很好的例子：具有多个输入和输出的模型。函数式 API 使处理大量交织的数据流变得容易。 来考虑下面的模型。我们试图预测 Twitter 上的一条新闻标题有多少转发和点赞数。模型的主要输入将是新闻标题本身，即一系列词语，但是为了增添趣味，我们的模型还添加了其他的辅助输入来接收额外的数据，例如新闻标题的发布的时间等。该模型也将通过两个损失函数进行监督学习。较早地在模型中使用主损失函数，是深度学习模型的一个良好正则方法。 模型结构如下图所示： 让我们用函数式 API 来实现它（详细解释请查看中文文档）： 以上只是一个简单的案例，Keras 函数式 API 还有非常多的应用案例，包括层级共享、有向无环图和残差网络等顶尖视觉模型，读者可以继续阅读中文文档了解更多 文档的后一部分更多是描述 Keras 中常用的函数与 API，包括 Keras 模型、层级函数、预处理过程、损失函数、最优化方法、数据集和可视化等。这些 API 和对应实现的功能其实很多时候可以在实际使用的时候再查找，当然最基本的 API 我们还是需要了解的。以下将简要介绍 Keras 模型和层级 API，其它的模块请查阅原中文文档。 Keras 模型 在 Keras 中有两类模型，顺序模型 和 使用函数式 API 的 Model 类模型。这些模型有许多共同的方法： model.summary(): 打印出模型概述信息。它是 utils.print_summary 的简捷调用。 model.get_config(): 返回包含模型配置信息的字典。通过以下代码，就可以根据这些配置信息重新实例化模型： model.get_weights(): 返回模型权重的张量列表，类型为 Numpy array。 model.set_weights(weights): 从 Nympy array 中为模型设置权重。列表中的数组必须与 get_weights() 返回的权重具有相同的尺寸。 model.to_json(): 以 JSON 字符串的形式返回模型的表示。请注意，该表示不包括权重，只包含结构。你可以通过以下代码，从 JSON 字符串中重新实例化相同的模型（带有重新初始化的权重）： model.to_yaml(): 以 YAML 字符串的形式返回模型的表示。请注意，该表示不包括权重，只包含结构。你可以通过以下代码，从 YAML 字符串中重新实例化相同的模型（带有重新初始化的权重）： model.save_weights(filepath): 将模型权重存储为 HDF5 文件。 model.load_weights(filepath, by_name=False): 从 HDF5 文件（由 save_weights 创建）中加载权重。默认情况下，模型的结构应该是不变的。如果想将权重载入不同的模型（部分层相同），设置 by_name=True 来载入那些名字相同的层的权重。 Keras 层级 所有 Keras 层都有很多共同的函数： layer.get_weights(): 以 Numpy 矩阵的形式返回层的权重。 layer.set_weights(weights): 从 Numpy 矩阵中设置层的权重（与 get_weights 的输出形状相同）。 layer.get_config(): 返回包含层配置的字典。此图层可以通过以下方式重置： 如果一个层具有单个节点 (i.e. 如果它不是共享层), 你可以得到它的输入张量，输出张量，输入尺寸和输出尺寸： layer.input layer.output layer.input_shape layer.output_shape 如果层有多个节点，您可以使用以下函数： layer.get_input_at(node_index) layer.get_output_at(node_index) layer.get_input_shape_at(node_index) layer.get_output_shape_at(node_index) 这些是 Keras 模型与层级基本的函数，文档的中心内容也是这一部分和下面描述的 API 用途与参数，它包括完整模型所需要的各个模块，包括数据、预处理、网络架构、训练、评估和可视化等。但这一部分我们并不会介绍，因为很多时候我们只有在遇到未知的函数时才会详细查阅。 Keras 官方中文文档，欢迎各位徘徊者入坑。 "
168,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738663&idx=2&sn=3ffc86a8fb16f95e607752de88a9d4ed&chksm=871acb99b06d428f152ea4deefbb89533b845d22f167a4965c7083a1b7f966d583da2862317a&scene=27,观点 | 下一步研究目标：盘点NLP领域最具潜力的六大方向,"选自ruder.io 在开始你的研究之前，了解目标领域中最重要的研究方向是很重要的任务。本文中，德国海德堡大学的计算语言学在读博士 Sebastian Ruder 为我们介绍了 NLP 领域里最具潜力的几个研究方向。 目录 独立于任务的 NLP 数据增强 用于 NLP 的 few-shot learning 用于 NLP 的的迁移学习 多任务学习 跨语言学习 独立于任务的架构提升 当开始新领域的研究时，你会发现寻找引人注目的主题并学会问正确的问题是一件很难的事。这种情况在机器学习这种进展很快的领域里尤其突出——你很难找到突破点。 本文旨在向初级研究人员和试图进入研究领域的人提供启发和思路，其中汇集了我感兴趣的一系列研究课题：着重于自然语言处理（NLP）和迁移学习，所以本文不是面向所有读者的。如果你对增强学习感兴趣，OpenAI 提供了一系列有趣的增强学习研究主题（https://blog.openai.com/requests-for-research-2/）。如果你期待与他人合作，或对更广泛的主题感兴趣，请参阅 Artificial Intelligence Open Network（https://ai-on.org/）。 这些研究主题中的大多数目前还没有被人们透彻地思考过；在很多情况下，概括性的描述是非常模糊和主观的，未来研究方向也不甚明确。此外，大多数主题也并不包含低挂果实，因此需要我们非常努力才能得出结论。请注意：这一主题集并不是对于所有文献的综述，所以其覆盖范围可能不全。 希望本文能够激发你的兴趣，并为你自己的研究历程提供灵感。 独立于任务的 NLP 数据增强 数据增强旨在通过转换生产现有训练实例的变体而创建额外的训练数据，以反映现实世界中的实际情况。在计算机视觉中，一般的增强技术有镜像、随机裁剪、剪切等。数据增强在 CV 中超级有用，比如有助于 AlexNet 对抗过拟合，绝大多数当前最优模型也使用了它。此外，数据增强非常直观，因为它使得训练数据更加多元，从而提升模型泛化能力。 然而，NLP 中数据增强并未广泛使用。依我看，原因有两点： 1. NLP 中的数据是分离的。这可防止我们把简单的转换直接应用于输入数据。目前绝大多数的增强方法聚焦于这样的转换，比如领域随机化 (Tobin et al., 2017) [2]。 2. 小的扰动可改变语义。删除一个否定词可能会改变句意，修改段落中的一个词可能无意中改变了关于该段落问题的答案。其实在 CV 中情况并非如此：扰动一个像素并不会改变一个猫或狗的图像，并且甚至明显的变化比如不同图像的插值也是有用的（Zhang et al., 2017）[3]。 我关注的现有方法要么是基于规则的 (Li et al., 2017) [5]，要么是解决特定任务的，比如解析 (Wang and Eisner, 2016) [6] 或零-代名词分辨率 (Liu et al., 2017) [7]。Xie et al. (2017) [39] 通过来自不同分布的样本替代单词以进行语言建模和机器翻译。最近聚焦于创建对抗样本的工作要么是通过替代单词或字符 (Samanta and Mehta, 2017; Ebrahimi et al., 2017) [8, 9]，级联 (Jia and Liang, 2017) [11]；要么是添加对抗扰动 (Yasunaga et al., 2017)。Li et al. (2017) [16] 同样使用了对抗设置，其训练系统生产与人类对话语句无差别的序列。 反向翻译（Back-translation）(Sennrich et al., 2015; Sennrich et al., 2016) [12, 13] 是机器翻译中的常见数据增强方法，有助于吸收单语训练数据。比如，当训练一个英转法系统时，单语法语文本通过法转英系统被翻译成英语；合成的平行数据接着被用于训练。反向翻译同样用于释义 (Mallinson et al., 2017) [14]。释义已被用于 QA (Dong et al., 2017) [15] 的数据增强，但并未发现有其他用处。 另一个方法与释义相近，即通过变分自编码器 (Bowman et al., 2016; Guu et al., 2017) [17, 19] 从连续空间中生成语句。如果按照 Hu et al., 2017 [18] 把表征解开，那么我们离风格迁移 (Shen et al., 2017) [20] 也就不远了。 以下几个研究方向很有趣，值得去跟： 1. 评估学习：评估一系列未广泛使用的现有数据增强方法及技巧，比如应用于一系列不同任务（包括文本分类和序列标注）的释义和风格迁移。确定何种类型的数据增强在所有任务和特定任务中是鲁棒的。这可被打装成软件库以使未来的基准更容易。 2. 带有风格迁移的数据增强：调查风格迁移是否可用于修改训练实例的不同属性以获得更鲁棒的学习。 3. 学习增强：类似于 Dong et al. (2017)，我们可为一个特定任务学习释义或者生成转换。 4. 学习词嵌入空间以增强数据：典型的词嵌入空间同时聚类同义词和反义词。因此在空间中使用最近邻用于替换是不可行的。受最近工作 (Mrkšić et al., 2017) [21] 启发，我们可以具化词嵌入空间以使其更适用于数据增强。 5. 对抗性数据增强：与最近的可解释性工作相关 (Ribeiro et al., 2016) [22]，我们可以改变实例中最重要的单词，即那些模型依赖以做出预测的单词。但是这依然需要保留语义的替换方法。 用于 NLP 的 Few-shot learning  Zero-shot、one-shot、few-shot learning 是最近最为有趣的研究方向之一。通过遵从 Vinyals et al. (2016) [4] 的核心思想，即 few-shot learning 模型应该明确地训练以执行 few-shot learning，我们已取得若干个最新进展 (Ravi and Larochelle, 2017; Snell et al., 2017) [23, 24]。学习若干个标注样本是最艰难的问题之一，以及区分当前机器学习模型生成与更广泛应用的系统的核心能力之一。据我所知，Zero-shot learning 只在学习未知单词的词嵌入的语境中被调查。无数据分类 (Song and Roth, 2014; Song et al., 2016) [25, 26] 是一个有趣的相关方向，它在联合空间中嵌入标签和文件，并需要带有良好描述的可解释性标签。 1. 标准化基准：为 NLP few-shot learning 创建标准化基准。Vinyals et al. (2016) 为 Penn Treebank 引入了 one-shot 语言建模任务。这一任务尽管很有用，但与 CV 基准上的广泛评估相比却相形见绌，并且据我所知没有多少使用。NLP 的 ew-shot learning 基准应该包含大量分类并提供标准化的再现性分割。良好的候选任务将是主题分类或细粒度实体识别。 2. 评估学习：创建这样一个基准之后，下一步是评估 CV 中的现有 few-shot learning 方法在执行 NLP 任务方面表现如何。 3. NLP 的全新方法：给定一个基准数据集和实证评估学习，接着我们可以开始开发执行 NLP few-shot learning 的全新方法。 用于 NLP 的迁移学习 迁移学习已经对计算机视觉（CV）产生了巨大的影响，并大大降低了解决特定 CV 问题的难度门槛。计算机视觉从业者现在不再需要为每个新任务耗费大量的工程，仅需使用少量示例对已在大型数据集上训练好的模型进行微调。 然而，在 NLP 领域里，我们目前仍然只能通过预训练嵌入来预训练模型的第一层。近期一些研究人员提出的方法（Peters et al., 2017, 2018）[31,32] 加入了预训练语言模型嵌入，但是它们仍然需要针对每个任务定制架构。在我看来，若想解锁迁移学习在 NLP 上的真正潜力，我们需要预训练整个模型，并在目标任务上仅需微调，类似于微调 ImageNet 模型。举例来说，在 NLP 上的语言建模可以类比为 CV 上的 ImageNet 分类（Howard and Ruder, 2018）[33]。 这里有一些潜在的研究方向： 1. 识别有用的预训练任务：预训练任务的选择非常重要，即使是对于相关任务进行微调，我们也可能近会收到有限的成功（Mou et al., 2016）[38]。其他任务，如近期关于学习通用句嵌入的研究（Conneau et al., 2017；Subramanian et al., 2018; Nie et al., 2017）[34,35,40] 可能是语言模型预训练的补充，或适用于其他目标任务。 2. 复杂架构的微调：模型应用于多目标时，预训练是最为有效的。然而，目前仍不清楚如何对更为复杂的模型进行预训练，如用于配对分类任务（Augenstein et al., 2018）或推理任务（如 Q&A 和阅读理解）的模型。 多任务学习 多任务学习（Multi-task learning，MTL）在 NLP 领域中已经变得越来越普遍了。有关多任务学习的概述，可参阅此处（http://ruder.io/multi-task/），有关 NTL 在 NLP 中的目标可参阅此处（http://ruder.io/multi-task-learning-nlp/）。对于我们来说，多任务学习还有很多未知等待我们去探寻。 MTL 的主要问题带来了一系列有趣的研究方向： 1. 确定有效的辅助任务：其中一个主要问题是哪些任务对于多任务学习是有用的。标签熵已被证明可以是 MTL 中成功的预测器（Alonso and Plank, 2017）[28]，但这一方向并未结束。在最近的研究中（Augenstein et al., 2018）[27]，我们发现又跟股东数据和更多细化标签的辅助任务更加有效。未来的 MTL 论文或许不仅会提出新的模型或辅助任务，也会试图开始求解为何很多辅助任务会比另一个紧密相关的任务更好。 2. 硬参数共享的替代方案：硬参数共享目前仍然是 MTL 的默认操作方式，但它对模型施加了很多约束，以便用相同的参数压缩与不同任务有关的知识，这往往会使学习变得困难。在 MTL 中，我们需要更加易于使用，且在多种任务上工作稳定的新方法（Misra et al., 2017; Ruder et al., 2017）[29,30]，标签嵌入层（Augenstein et al., 2018）在这一方向上很有潜力。 3. 人工辅助任务：最好的辅助任务是针对主任务目标，且不需要任何附加数据的任务。在这里，我列出了潜在的人工辅助任务列表（http://ruder.io/multi-task-learning-nlp/）。当然，目前我们还不清楚哪些辅助任务可以在多种不同类型的任务上适用，或哪种基于特定任务的变体性能最好。 跨语言学习 构建能够跨语言的模型，将资源丰富语言中的知识迁移到资源贫乏的语言中，一直是 NLP 的重要研究方向之一。最近，学习跨语言表示，将多种不同语言投影到共享嵌入空间的方法有了很大进展。可参阅论文《A Survey of Cross-lingual Word Embedding Models》[36]。 跨语言表示通常根据内部基准测试，或外部下游任务（如文本分类）上的表现进行评估。虽然目前的最先进方法已经有了很多进步，但我们仍对于这些方法在某些任务或语言上的失败原因，以及如何在所有任务上减小这些失败的方法，如加入基于特定任务的约束（Mrkšić et al., 2017）仍未有足够的理解。 独立于任务的架构提升 目前，在各个特定任务上，业内最佳的成绩正在不断地被刷新，旧的架构正不断被淘汰。之前，我已经列出了在不同 NLP 任务上的最佳实践（http://ruder.io/deep-learning-nlp-best-practices/），但如果不对这些架构在不同任务上的性能进行比较，我们很难定义特定架构的能力，也无法得知它们在其他任务上的可用性。 最近涌现出了一个颇具潜力的模型 Transformer（Vaswani et al., 2017）[37]。虽然完整的模型可能不适用于每个任务，但多头注意（multi-head attention）或基于位置的编码（position-based encoding）可以作为模块构建模型，这样就可以适用于很多 NLP 任务了。 结论 希望这一研究方向汇集能够对你有所帮助。如果你有关于如何解决相关研究课题的思路，欢迎在本文下进行讨论。 "
169,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738663&idx=4&sn=8664911d352c4f9cc4d17ac9ea0be45f&chksm=871acb99b06d428fe8b706b510eefa6042004e01d06375d691fc300d739b9e652103a91c1f7e&scene=27,学界 | 综述论文：对抗攻击的12种攻击方法和15种防御方法,"选自arXiv 这篇文章首次展示了在对抗攻击领域的综合考察。本文是为了比机器视觉更广泛的社区而写的，假设了读者只有基本的深度学习和图像处理知识。不管怎样，这里也为感兴趣的读者讨论了有重要贡献的技术细节。机器之心重点摘要了第 3 节的攻击方法（12 种）和第 6 节的防御方法（15 种），详情请参考原文。 尽管深度学习在很多计算机视觉领域的任务上表现出色，Szegedy et al. [22] 第一次发现了深度神经网络在图像分类领域存在有意思的弱点。他们证明尽管有很高的正确率，现代深度网络是非常容易受到对抗样本的攻击的。这些对抗样本仅有很轻微的扰动，以至于人类视觉系统无法察觉这种扰动（图片看起来几乎一样）。这样的攻击会导致神经网络完全改变它对图片的分类。此外，同样的图片扰动可以欺骗好多网络分类器。这类现象的深远意义吸引了好多研究员在对抗攻击和深度学习安全性领域的研究。 自从有了 Szegedy 的发现，机器视觉领域中陆续出现了好几个有意思的受对抗攻击影响的结果。例如，除了在特定图像的对抗性扰动之外，Moosavi-Dezfooli et al. [16] 展示了「通用扰动（universal perturbations）」的存在（如图 1 所示），这种通用扰动可以让一个分类器对所有图片错误分类。同样的，Athalye et al. [65] 展示了即使用 3D 打印的真实世界中存在的物体也可以欺骗深度网络分类器（如图 2 所示）。考虑到深度学习研究在计算机视觉的重要性和在真实生活中的潜在应用，这篇文章首次展示了在对抗攻击领域的综合考察。这篇文章是为了比机器视觉更广泛的社区而写的，假设了读者只有基本的深度学习和图像处理知识。不管怎样，这里也为感兴趣的读者讨论了有重要贡献的技术细节。 第 2 节里列举了机器视觉中关于对抗攻击的常用术语。 第 3 节回顾了针对图片分类任务的对抗攻击。 第 4 节单独介绍了在实际生活场景中对抗攻击的方法。 第 5 节关注对抗攻击的工作焦点和研究方向。 第 6 节讨论了防御对抗攻击的文献。 在第 7 章里，我们以讨论过的文献为基础的展望了未来的研究方向。 第 8 章总结并画上结尾。 论文：Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey 论文地址：https://arxiv.org/abs/1801.00553 深度学习正占据如今飞速发展的机器学习和人工智能领域的心脏地位。在机器视觉领域中，它已经变成了从自动驾驶到监控、安保应用中的主力。然而，即便深度网络已经展示了在处理复杂问题时所取得的现象级成功，最近的研究表明它们对于输入中带有的轻微扰动是很脆弱的，从而导致错误的输出。对于图片来说，这样的扰动经常是太小了从而不能被人类感知，但是它们完全愚弄了深度学习模型。对抗攻击造成了深度学习在实践中成功的一系列威胁，进而引导了大量的研究进入这个方向。这篇文章展示了第一个对抗攻击在机器视觉领域的深度学习中的综合考察。我们回顾了对抗攻击设计的研究，分析了这些攻击的存在性以及提出的防御机制。为了强调对抗攻击在实际场所中存在，我们独立地回顾了实际场景中的对抗攻击。最终，我们引用文献来展望更广阔的研究方向。  本节列举了 12 种生成对抗样本的方法，专门针对分类网络。 1 Box-constrained L-BFGS Szegedy[22] 等人首次证明了可以通过对图像添加小量的人类察觉不到的扰动误导神经网络做出误分类。他们首先尝试求解让神经网络做出误分类的最小扰动的方程。但由于问题的复杂度太高，他们转而求解简化后的问题，即寻找最小的损失函数添加项，使得神经网络做出误分类，这就将问题转化成了凸优化过程。 2 Fast Gradient Sign Method (FGSM)  Szegedy 等人发现可以通过对抗训练提高深度神经网络的鲁棒性，从而提升防御对抗样本攻击的能力。GoodFellow[23] 等人开发了一种能有效计算对抗扰动的方法。而求解对抗扰动的方法在原文中就被称为 FGSM。 Kurakin[80] 等人提出了 FGSM 的「one-step target class」的变体。通过用识别概率最小的类别（目标类别）代替对抗扰动中的类别变量，再将原始图像减去该扰动，原始图像就变成了对抗样本，并能输出目标类别。 3 Basic & Least-Likely-Class Iterative Methods one-step 方法通过一大步运算增大分类器的损失函数而进行图像扰动，因而可以直接将其扩展为通过多个小步增大损失函数的变体，从而我们得到 Basic Iterative Methods（BIM）[35]。而该方法的变体和前述方法类似，通过用识别概率最小的类别（目标类别）代替对抗扰动中的类别变量，而得到 Least-Likely-Class Iterative Methods[35]。 4 Jacobian-based Saliency Map Attack (JSMA) 对抗攻击文献中通常使用的方法是限制扰动的 l_∞或 l_2 范数的值以使对抗样本中的扰动无法被人察觉。但 JSMA[60] 提出了限制 l_0 范数的方法，即仅改变几个像素的值，而不是扰动整张图像。 5 One Pixel Attack 这是一种极端的对抗攻击方法，仅改变图像中的一个像素值就可以实现对抗攻击。Su[68] 等人使用了差分进化算法，对每个像素进行迭代地修改生成子图像，并与母图像对比，根据选择标准保留攻击效果最好的子图像，实现对抗攻击。这种对抗攻击不需要知道网络参数或梯度的任何信息。 6 Carlini and Wagner Attacks (C&W)  Carlini 和 Wagner[36] 提出了三种对抗攻击方法，通过限制 l_∞、l_2 和 l_0 范数使得扰动无法被察觉。实验证明 defensive distillation 完全无法防御这三种攻击。该算法生成的对抗扰动可以从 unsecured 的网络迁移到 secured 的网络上，从而实现黑箱攻击。 7 DeepFool Moosavi-Dezfooli 等人 [72] 通过迭代计算的方法生成最小规范对抗扰动，将位于分类边界内的图像逐步推到边界外，直到出现错误分类。作者证明他们生成的扰动比 FGSM 更小，同时有相似的欺骗率。 8 Universal Adversarial Perturbations 诸如 FGSM [23]、 ILCM [35]、 DeepFool [72] 等方法只能生成单张图像的对抗扰动，而 Universal Adversarial Perturbations[16] 能生成对任何图像实现攻击的扰动，这些扰动同样对人类是几乎不可见的。该论文中使用的方法和 DeepFool 相似，都是用对抗扰动将图像推出分类边界，不过同一个扰动针对的是所有的图像。虽然文中只针对单个网络 ResNet 进行攻击，但已证明这种扰动可以泛化到其它网络上。 9 UPSET and ANGRI Sarkar[146] 等人提出了两个黑箱攻击算法，UPSET 和 ANGRI。UPSET 可以为特定的目标类别生成对抗扰动，使得该扰动添加到任何图像时都可以将该图像分类成目标类别。相对于 UPSET 的「图像不可知」扰动，ANGRI 生成的是「图像特定」的扰动。它们都在 MNIST 和 CIFAR 数据集上获得了高欺骗率。 10 Houdini Houdini[131] 是一种用于欺骗基于梯度的机器学习算法的方法，通过生成特定于任务损失函数的对抗样本实现对抗攻击，即利用网络的可微损失函数的梯度信息生成对抗扰动。除了图像分类网络，该算法还可以用于欺骗语音识别网络。 11 Adversarial Transformation Networks (ATNs) Baluja 和 Fischer[42] 训练了多个前向神经网络来生成对抗样本，可用于攻击一个或多个网络。该算法通过最小化一个联合损失函数来生成对抗样本，该损失函数有两个部分，第一部分使对抗样本和原始图像保持相似，第二部分使对抗样本被错误分类。 12 Miscellaneous Attacks 这一部分列举了更多其它的生成对抗样本的方法，详情请参见原文。 除了 Houdini 这个例外, 在 3.1 节中概述的所有主流对抗攻击直接针对于分类任务——欺骗基于 CNN 的分类器。然而，因为对抗性威胁的严重性，对抗攻击的研究已经超越了分类/识别场景。文中概述了以下分类应用领域之外的攻击深度神经网络的方法： 在自编码器和生成模型上的攻击 在循环神经网络上的攻击 深度强化学习上的攻击 在语义切割和物体检测上的攻击 目前，在对抗攻击防御上存在三个主要方向： 1）在学习过程中修改训练过程或者修改的输入样本。 2）修改网络，比如：添加更多层/子网络、改变损失/激活函数等。 3）当分类未见过的样本时，用外部模型作为附加网络。 第一个方法没有直接处理学习模型。另一方面，另外两个分类是更加关心神经网络本身的。这些方法可以被进一步细分为两种类型：（a）完全防御；（b）仅探测（detection only）。「完全防御」方法的目标是让网络将对抗样本识别为正确的类别。另一方面，「仅探测」方法意味着在对抗样本上发出报警以拒绝任何进一步的处理。详细的分类在图 9 中展示了。剩下的章节是按这个分类来整理的。 1 蛮力对抗训练 通过不断输入新类型的对抗样本并执行对抗训练，从而不断提升网络的鲁棒性。为了保证有效性，该方法需要使用高强度的对抗样本，并且网络架构要有充足的表达能力。这种方法需要大量的训练数据，因而被称为蛮力对抗训练。很多文献中提到这种蛮力的对抗训练可以正则化网络以减少过拟合 [23,90]。然而，Moosavi-Dezfooli[16] 指出，无论添加多少对抗样本，都存在新的对抗攻击样本可以再次欺骗网络。 2 数据压缩 注意到大多数训练图像都是 JPG 格式，Dziugaite[123] 等人使用 JPG 图像压缩的方法，减少对抗扰动对准确率的影响。实验证明该方法对部分对抗攻击算法有效，但通常仅采用压缩方法是远远不够的，并且压缩图像时同时也会降低正常分类的准确率，后来提出的 PCA 压缩方法也有同样的缺点。 3 基于中央凹机制的防御 Luo[119] 等人提出用中央凹（foveation）机制可以防御 L-BFGS 和 FGSM 生成的对抗扰动，其假设是图像分布对于转换变动是鲁棒的，而扰动不具备这种特性。但这种方法的普遍性尚未得到证明。 4 数据随机化方法 Xie[115] 等人发现对训练图像引入随机重缩放可以减弱对抗攻击的强度，其它方法还包括随机 padding、训练过程中的图像增强等。 5 深度压缩网络 人们观察到简单地将去噪自编码器（Denoising Auto Encoders）堆叠到原来的网络上只会使其变得更加脆弱，因而 Gu 和 Rigazio[24] 引入了深度压缩网络（Deep Contractive Networks），其中使用了和压缩自编码器（Contractive Auto Encoders）类似的平滑度惩罚项。 6 梯度正则化/ masking 使用输入梯度正则化以提高对抗攻击鲁棒性 [52]，该方法和蛮力对抗训练结合有很好的效果，但计算复杂度太高。 7 Defensive distillation distillation 是指将复杂网络的知识迁移到简单网络上，由 Hinton[166] 提出。Papernot[38] 利用这种技术提出了 Defensive distillation，并证明其可以抵抗小幅度扰动的对抗攻击。 8 生物启发的防御方法 使用类似与生物大脑中非线性树突计算的高度非线性激活函数以防御对抗攻击 [124]。另外一项工作 Dense Associative Memory 模型也是基于相似的机制 [127]。 9 Parseval 网络 在一层中利用全局 Lipschitz 常数加控制，利用保持每一层的 Lipschitz 常数来摆脱对抗样本的干扰。 10 DeepCloak 在分类层（一般为输出层）前加一层特意为对抗样本训练的层。它背后的理论认为在最显著的层里包含着最敏感的特征。 11 混杂方法 这章包含了多个人从多种角度对深度学习模型的调整从而使模型可以抵抗对抗性攻击。 12 仅探测方法 这章介绍了 4 种网络，SafetyNet，Detector subnetwork，Exploiting convolution filter statistics 及 Additional class augmentation。 SafetyNet 介绍了 ReLU 对对抗样本的模式与一般图片的不一样，文中介绍了一个用 SVM 实现的工作。 Detector subnetwork 介绍了用 FGSM, BIM 和 DeepFool 方法实现的对对抗样本免疫的网络的优缺点。 Exploiting convolution filter statistics 介绍了同 CNN 和统计学的方法做的模型在分辨对抗样本上可以有 85% 的正确率。 13 防御通用扰动 利用一个单独训练的网络加在原来的模型上，从而达到不需要调整系数而且免疫对抗样本的方法。 14 基于 GAN 的防御 用 GAN 为基础的网络可以抵抗对抗攻击，而且作者提出在所有模型上用相同的办法来做都可以抵抗对抗样本。 15 仅探测方法 介绍了 Feature Squeezing、MagNet 以及混杂的办法。 Feature Squeezing 方法用了两个模型来探查是不是对抗样本。后续的工作介绍了这个方法对 C&W 攻击也有能接受的抵抗力。 MagNet:作者用一个分类器对图片的流行（manifold）测量值来训练，从而分辨出图片是不是带噪声的。 混杂方法（Miscellaneous Methods）：作者训练了一个模型，把所有输入图片当成带噪声的，先学习怎么去平滑图片，之后再进行分类。 以下是机器之心报道过的对抗攻击的案例： 既能欺骗机器，也能迷惑人类！Goodfellow 等人提出新一代对抗样本 学界 | 几张贴纸就让神经网络看不懂道路标志，伯克利为真实环境生成对抗样本 学界 | 神奇的面包机！谷歌造出对抗样本的实体版 学界 | 继图像识别后，图像标注系统也被对抗样本攻陷! 修改一个像素，就能让神经网络识别图像出错 "
170,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738663&idx=1&sn=1f55accc61e75baf3ee25e9856a13ac0&chksm=871acb99b06d428f475c00a45d19d2d3c5387b0bd31fdeb8b6f9828332ef5e2632aa7e1d1888&scene=27,南京大学周志华教授综述论文：弱监督学习,"选自NSR 在《国家科学评论》(National Science Review, NSR) 2018 年 1 月份出版的机器学习专题期刊中，介绍了南京大学周志华教授发表的一篇论文《A brief introduction to weakly supervised learning》。机器之心经授权对此论文部分内容做了编译介绍，更完整内容可查看英文论文原文。 摘要 ：监督学习技术通过学习大量训练样本来构建预测模型，其中每个训练样本都有一个标签标明其真值输出。尽管当前的技术已经取得了巨大的成功，但是值得注意的是，由于数据标注过程的高成本，很多任务很难获得如全部真值标签这样的强监督信息。因此，能够使用弱监督的机器学习技术是可取的。本文综述了弱监督学习的一些研究进展，主要关注三种弱监督类型：不完全监督：只有一部分训练数据具备标签；不确切监督：训练数据只具备粗粒度标签；以及不准确监督：给出的标签并不总是真值。 机器学习在各种任务中取得了巨大成功，特别是在分类和回归等监督学习任务中。预测模型是从包含大量训练样本的训练数据集中学习，每个训练样本对应一个事件或对象。训练样本由两部分组成：一个描述事件/对象的特征向量（或示例），以及一个表示真值输出的标签。在分类任务中，标签表示训练样本所属的类别；在回归任务中，标签是一个与样本对应的实数值。大多数成功的技术，如深度学习 [1]，都需要含有真值标签的大规模训练数据集，然而，在许多任务中，由于数据标注过程的成本极高，很难获得强监督信息。因此，研究者十分希望获得能够在弱监督前提下工作的机器学习技巧。 通常来说，弱监督可以分为三类。第一类是不完全监督（incomplete supervision），即，只有训练集的一个（通常很小的）子集是有标签的，其他数据则没有标签。这种情况发生在各类任务中。例如，在图像分类任务中，真值标签由人类标注者给出的。从互联网上获取巨量图片很容易，然而考虑到标记的人工成本，只有一个小子集的图像能够被标注。第二类是不确切监督（inexact supervision），即，图像只有粗粒度的标签。第三种是不准确的监督（inaccurate supervision），模型给出的标签不总是真值。出现这种情况的常见原因有，图片标注者不小心或比较疲倦，或者某些图片就是难以分类。 弱监督学习是一个总括性的术语，涵盖了尝试通过较弱的监督来学习并构建预测模型的各种研究。在本文中，我们将讨论这一研究领域的一些进展，重点放在以不完整、不确切和不准确的监督进行学习的研究。我们将把不同类型的弱监督分开，但值得一提的是，在实际操作中，几种弱监督经常同时发生。为简单起见，在本文中我们以包含两个可交换类 Y 和 N 的二元分类为例。形式化表达为，在强监督学习条件下，监督学习的任务是从训练数据集 中学习 其中 是特征空间， 以及 我们假设 是根据未知的独立同分布 D 生成的；换言之， 是 i.i.d. 样本。 图 1 提供了我们将在本文中讨论的三种弱监督类型的示例。 不完全监督 不完全监督考虑那些我们只拥有少量有标注数据的情况，这些有标注数据并不足以训练出好的模型，但是我们拥有大量未标注数据可供使用。形式化表达为，模型的任务是从训练数据集 中学习 其中训练集中有 l 个有标注训练样本（即给出  的样本）和 u = m - l 个未标注样本；其他条件与具有强监督的监督学习相同，如摘要最后的定义。为便于讨论，我们也将 l 个有标注示例称为「标注数据」，将 u 个未标注示例称为「未标注数据」。 能够实现此目标的主要两类技巧，即，主动学习 [2] 和半监督学习 [3-5]。 主动学习假设存在一个「神谕」（oracle），比如一位人类专家，能够向他查询选定的未标注示例的真值标签。 相比之下，半监督式学习试图在有标注数据之外，自动开发无标注数据以提高学习效果，这个过程不需要人工干预。存在一种特殊的半监督学习，称为直推式学习（transductive learning）；直推式学习和（纯）半监督学习的主要区别在于，它们对测试数据，即训练过的模型需要进行预测的数据，假设有所不同。直推式学习持有「封闭世界」假设，即，测试数据是事先给出的、目标是优化测试数据的性能；换言之，未标注数据正是测试数据。纯半监督式学习则持有「开放世界」假设，即，测试数据是未知的，未标注数据不一定是测试数据。图 2 直观地表示了主动学习、（纯）半监督学习和直推式学习之间的差异。 不确切监督 不确切监督关注于给定了监督信息，但信息不够精确的场景。一个典型的场景是仅有粗粒度的标签信息可用。例如，在药物活性预测 [40] 的问题中，其目标是建立一个模型学习已知分子的知识，来预测一个新的分子是否适合制造一种特定药物。一个分子可以有很多的低能量形状，而这些分子是否能用于制药取决于这些分子是否具有某些特殊的形状。然而即使对于已知的分子，人类专家也仅知道该分子是否适合制药，而不知道其中决定性的分子形状是什么。 形式化表达为，该任务是从训练数据集 中学习 其中 被称为一个包。 是一个示例，m_i 是示例 X_i 的数量， X_i 是一个 positive 包，即 y_i=Y，如果存在 x_ip 是正的，同时 是未知的。其目标是为未见过的包预测标签。该方法被称为多示例学习 [40,41]。 已经有许多有效的算法被开发出来并应用于多示例学习。实际上，几乎所有的有监督学习算法都有对等的多示例算法。大多数算法试图调整单示例监督学习算法，使其适配多示例表示，主要是将其关注点从对示例的识别转移到对包的识别 [42]；一些其他算法试图通过表示变换，调整多示例表示使其适配单示例算法 [43,44]。还有一种类型 [45]，将算法分为三类：一个整合了示例级响应的示例空间范式，一个把 包 视作一个整体的 包 空间范式，以及一个在嵌入特征空间中进行学习的嵌入空间范式中。请注意，这些示例通常被视为 i.i.d. 样本，然而，[46] 表明，多示例学习中的示例不应该被认为是独立的，尽管这些包可以被视为 i.i.d. 样本，并且已经有一些有效的算法是基于此见解进行开发的 [47]。   多示例学习已成功应用于各种任务，如图像分类/检索/注释 [48-50]，文本分类 [51,52]，垃圾邮件检测 [53]，医学诊断 [54]，面部/对象检测 [55,56]，对象类别发现 [57]，对象跟踪 [58] 等。在这些任务中，将真实对象（例如一幅图像或一个文本文档）视为一个包是很自然的。然而，不同于药物活性预测这类包中包含天然示例（分子的各种形状）的例子，需要为每个包生成示例。包生成器制定如何生成示例来构成包。通常情况下，可以从图像中提取许多小的图像块作为其示例，而章节/段落甚至句子可以用作文本文档的示例。尽管包生成器对学习效果有重要影响，但最近才出现关于图像包生成器的全面研究 [59]，研究揭示了一些简单的密集取样包生成器比一些复杂的生成器性能更好。图 5 显示了两个简单而有效的图像包生成器。 多示例学习的初始目标是为未见过的包预测标签；然而，已有研究尝试识别那些之所以让正包变正的关键示例（key instance）[31,60]。这在诸如没有细粒度标记训练数据的感兴趣区域定位的任务中特别有用。值得注意的是，标准的多示例学习 [40] 假定每一个正包必须包含一个关键示例，而还有其它研究假定不存在关键示例，每一个示例都对包标签有贡献 [61,62]；甚至假定存在多个概念，而仅当一个包包含满足所有概念的示例时，该包才是正的 [63]。可以在文献 [41] 中找到更多的变体。 早期的理论结果 [64-66] 表明多示例学习对于包中每个示例都由不同的规则分类的异质（heterogeneous）案例来说，是很难的，对于以相同的规则分类所有示例的同质性（homogeneous）案例就是可学习的。幸运的是，几乎所有的实际多示例任务都属于同质性案例。这些分析假定 bag 中的示例是独立的。而不假定示例的独立性的分析更具挑战性，这类研究也出现得较晚，其揭示了在同质性类中，至少存在某些可以用包间的任意分布来学习的案例 [67]。尽管如此，与其在算法和应用上的繁荣发展相反，多示例学习的理论研究成果非常少，因为分析的难度太大。 某些在包中任意分布的示例是可学习的 [67]。尽管如此，与在算法和应用上的繁荣发展相反，多示例学习的理论研究成果非常少，因为分析的难度太大。 不准确监督 不准确监督关注于监督信息不总是真值的场景，也就是说，有部分信息会出现错误。其形式基本和引言最后部分的表示相同，除了训练数据集中的 y_i 可能是不准确的。 一个典型的场景是在有标签噪声的情况下进行学习 [68]。目前已有很多理论研究 [69-71]，其中大多数假定存在随机的分类噪声，即标签受随机噪声影响。在实践中，基本的思想是识别潜在的误分类样本 [72]，然后尝试进行修正。例如，数据编辑（data-editing）方法 [73] 构建了相对邻域图（relative neighborhood graph），其中每一个节点对应一个训练样本，而连接两个不同标签的节点的边被称为切边（cut edge）。然后，测量 一个切边的权重统计量，直觉上，如果一个示例连接了太多的切边，则该示例是可疑的。可疑的示例要么被删除，要么被重新标记，如图 6 所示。值得注意的是，这种方法通常依赖于咨询邻域信息；由于当数据很稀疏时，邻域识别将变得更不可靠，因此，在高维特征空间中该方法的可靠性将变弱。 近期出现的有趣的不准确监督的场景是众包模式 [74]，这是一种流行的将工作外包给个人的范式。对于机器学习来说，用众包模式为训练数据收集标签是一种经济的方式。具体来说，未标记的数据被外包给大量的工人来标记。在著名的众包系统 Amazon Mechanical Turk 上，用户可以提交一项任务，例如将图片标注为「树」或「非树」，然后职工完成工作以获取少量报酬。通常这些工人来自世界各地，每个人都可以执行多个任务。这些职工通常互相独立，报酬不高，并通过自己的判断标记数据。这些职工的标记质量参差不齐，但标记质量信息对于用户来说是不可见的，因为工人的身份是保密的。在这些职工中可能存在「垃圾制造者」，几乎用随机的标签来标记数据（例如，用机器替代人类赚取报酬），或「反抗者」，故意给出错误的标签。此外，某些任务可能对一些人来说太难而无法完成。使用众包返回的不准确监督信息来保证学习性能是非常困难的。 很多研究尝试用众包标签推断真值标签。多数人投票策略得到了集成方法 [35] 的理论支持，在实践中得到了广泛使用并有很好的表现 [75,76]，因此通常作为基线标准。如果预期可以对工人质量和任务难度建模，那么通过为不同的工人在不同的任务上设置权重，则可以获得更好的效果。为此，一些方法尝试构建概率模型然后使用 EM 算法进行评估 [77,78]。人们也使用了极小极大熵方法 [35]。概率模型可以用于移除垃圾制造者 [79]。近期人们给出了移除低质量工人的一般理论条件 [80]。 在机器学习中，众包通常用于收集标签，在实践中，模型的最终性能，而不是这些标签的质量，才是更重要的。目前已有很多关于从低能老师和众包标签学习的研究 [81,82]，这和用带噪声标签学习是很接近的。但其中的区别在于，对于众包设定而言，人们可以方便地、重复地对某个示例提取众包标签。因此，在众包数据学习中，考虑经济性和最小化众包标签的充分数量是很重要的，即有效众包学习的最小代价 [83]。很多研究专注于任务分配和预算分配，尝试在准确率和标注开销之间取得平衡。为此，非适应性的任务分配机制（离线分配任务 [84,85]）和适应性机制（在线分配任务 [86,87]）都得到了在理论支持下的研究。需要注意的是，多数研究采用了 Dawid–Skene 模型 [88]，其假设不同任务的潜在成本是相同的，而没有探索更复杂的成本设置。 设计一个有效的众包协议也是很重要的。在文献 [89] 中提供了「不确定」选项，从而使工人在不确定的时候不被迫使给出确定的标签。该选项可以帮助标记的可靠性获得有理论支持 [90] 的提升。在文献 [91] 中提出了一种「double or nothing」的激励兼容机制，以确保工人能提供基于其自己的信心的标注，诚实地工作。在假定每位工人都希望最大化他们的报酬的前提下，该协议被证实可以避免垃圾制造者的出现。 结论 监督学习技术在具备强监督信息（如大量具备真值标签的训练样本）的情况中取得了很大成功。然而，在实际任务中，收集监督信息需要大量成本，因此，使用弱监督学习通常是更好的方式。 本文主要介绍三种典型的弱监督：不完全、不确切和不准确监督。尽管三者可以分开讨论，但是实践中它们通常同时出现，如图 1 所示，以往研究中也讨论过此类「混合」案例 [52,92,93]。此外，还存在其他类型的弱监督。例如，主要通过强化学习方法解决 [94] 的延时监督也属于弱监督。由于篇幅限制，本文实际上扮演了更多文献索引而非文献综述的角色。对细节感兴趣的读者请阅读对应参考文献。近期越来越多的研究者关注弱监督学习，如部分监督学习主要关注不完全监督学习 [95]，同时也有很多关于弱监督的其他讨论 [96,97]。 为了简化讨论，本文主要关注二分类，尽管大部分讨论经过稍微改动就可以扩展到多类别或回归学习。注意，多类别任务中可能会出现更复杂的情景 [98]。如果考虑到多标签学习 [99]，即每个样本同时关联到多个标签的任务，则情况更加复杂。以不完全监督为例，除了标注／非标注示例以外，多标签任务可能遇到部分标注示例，即训练示例中，只有部分标签是真值 [100]。即使只考虑标注／未标注数据，其设计选项也比单标签设置多。如对于积极学习而言，给出一个非标注示例，在多标签任务中可以要求给出该示例的所有标签 [101]、特定标签 [102]，或一对标签的相关性排序 [103]。然而，不管是哪种数据和任务，弱监督学习都变得越来越重要。 英文原文 2017 年 8 月发表于《国家科学评论》(National Science Review, NSR)，原标题为「A brief introduction to weakly supervised learning」。《国家科学评论》是科学出版社旗下期刊，与牛津大学出版社联合出版。机器之心经《国家科学评论》和牛津大学出版社授权刊发该论文文中文翻译。 "
171,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738663&idx=3&sn=a7a974a62cd5fa289046b54ab3fa4406&chksm=871acb99b06d428f6bbc1bd87975ef7d061d9ec389eb1a67b8292a3fb6a03215266489746ea0&scene=27,教程 | 如何通过Scikit-Learn实现多类别文本分类？,"选自towardsdatascience 互联网的绝大多数的文本分类都是二进制的，本文要解决的问题更为复杂。作者使用 Python 和 Jupyter Notebook 开发系统，并借助 Scikit-Learn 实现了消费者金融投诉的 12 个预定义分类。本项目的 GitHub 地址见文中。 GitHub 地址：https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb 商业活动中有很多文本分类应用。例如，新闻报道通常是按照主题进行构架；内容或产品通常是根据类别添加标签；可以根据用户如何在线讨论某个产品或品牌将其分为多个群组...... 然而，互联网上绝大多数的文本分类文章和教程都是二进制文本分类，比如垃圾邮件过滤，情感分析。大多数情况下，现实世界的问题更为复杂。因此，这就是我们今天要做的事情：将消费者的金融投诉分为 12 个预定义的类别。 我们使用 Python 和 Jupyter Notebook 开发系统，机器学习方面则借助 Scikit-Learn。如果你想要 PySpark 实现，请阅读下篇文章。 该问题是监督式文本分类问题，我们的目标是调查哪种监督式机器学习方法最适合解决它。 当出现新投诉时，我们希望将其分配到 12 个类别中的一个。分类器假设每个新投诉都被分配到一个且仅一个的类别之中。这是多类别文本分类问题。我迫不及待想看到我们能实现什么！ 数据探索 在深入训练机器学习模型之前，我们首先应该看一些实例，以及每个类别的投诉数量： 对于这个项目，我们只需要两栏——「产品」和「消费者投诉叙述」。 输入： Consumer_complaint_narrative 实例：「我的信用报告中有过时的信息，我以前有争议的是这些信息已超过七年未被删除，并且不符合信用报告的要求」 输出：product 实例：信用报告 我们将删除「消费者投诉叙述」栏中的缺失值，并添加一列来将产品编码为整数，因为分类变量通常用整数表示比用字符串更好。 我们还创建了几个字典供将来使用。 清理完成后，这是我们将要处理的前五行数据： 我们看到每件产品的投诉数量不平衡。消费者的投诉更集中于收取欠款、信用报告和抵押方面。 当我们遇到这样的问题时，我们使用标准算法解决这些问题必然会遇到困难。常规算法往往偏向于多数类别，而不考虑数据分布。在最糟糕的情况下，少数类别被视为异常值并被忽略。对于某些情况，如欺诈检测或癌症预测，我们则需要仔细配置我们的模型或人为地平衡数据集，比如欠采样或过采样每个类别。 但是，在学习不平衡数据的情况下，我们最感兴趣的是多数类。我们想有一个分类器，能够对多数类提供较高的预测精度，同时对少数类保持合理的准确度。因此我们会保持原样。 分类器和学习算法不能直接处理原始形式的文本文档，因为它们大多数都期望大小固定的数字特征向量而不是具有可变长度的原始文本文档。因此，在预处理步骤中，文本被转换为更易于管理的表达。 从文本中提取特征的一种常见方法是使用词袋模型：对于每个文档，我们案例中的投诉叙述、单词的出现（通常是频率）被考虑在内，而它们出现顺序则被忽略。 具体来说，对于我们数据集中的每一项，我们将计算一种被称为词频、反向文档频率的值，其缩写为 tf-idf。我们将使用 sklearn.feature_extraction.text.TfidfVectorizer 为每个消费者投诉叙述计算一个 tf-idf 向量。 sublinear_df 设为 True 从而使用频率的对数形式。 min_df 是单词必须存在的最小文档数量。 norm 设为 l2，以确保我们所有特征向量的欧几里德范数为 1。 ngram_range 设为 (1, 2)，表示我们想要考虑 unigrams 和 bigrams。 stop_words 设为 ""english"" 来删除所有常用代词 (""a"", ""the"", ...) 以减少噪音特征的数量。 (4569, 12633) 现在，4569 个消费者投诉描述中的每一个由 12633 个特征表达，代表不同的 unigrams 和 bigrams 的 tf-idf 分数。 我们可以使用 sklearn.feature_selection.chi2 来查找与每个产品最相关的项： # 『银行账户或服务』:  . 最相关的 unigrams:  . 银行  . 透支  . 最相关的 bigrams:  . 透支费  . 支票账户  # 『消费者贷款』:  . 最相关的 unigrams:  . 车  . 交通工具  . 最相关的 bigrams:  . 交通工具 xxxx  . 丰田金融  # 『信用卡』:  . 最相关的 unigrams:  . 花旗  . 卡  . 最相关的 bigrams:  . 年费  . 信用卡  # 『信用报告』:  . 最相关的 unigrams:  . 益百利  . equifax  . 最相关的 bigrams:  . 全联公司  . 信用报告  # 『讨回欠款』:  . 最相关的 unigrams:  . 收集  . 债务  . 最相关的 bigrams:  . 讨回全款  . 讨债公司  # 『汇款』:  . 最相关的 unigrams:  . wu  . paypal  . 最相关的 bigrams:  . 西联汇款  . 汇款  # 『抵押』:  . 最相关的 unigrams:  . 修正  . 抵押  . 最相关的 bigrams:  . 抵押公司  . 贷款修改  # 『其他金融服务』:  . 最相关的 unigrams:  . 牙齿  . 护照  . 最相关的 bigrams:  . 帮助支付  . 规定支付  # 『发薪日贷款』:  . 最相关的 unigrams:  . 借款  . 发薪日  . 最相关的 bigrams:  . 大图片  . 发薪日贷款  # 『预付卡』:  . 最相关的 unigrams:  . 服务  . 充值  . 最相关的 bigrams:  . 获得资金  . 预付卡  # 『学生贷款』:  . 最相关的 unigrams:  . 学生  . navient  . 最相关的 bigrams:  . student loans  . student loan  # 『虚拟货币』:  . 最相关的 unigrams:  . 手柄  . https  . 最相关的 bigrams:  . xxxx 提供者  . 想要钱  它们都有道理，难道不是吗？ 多类别分类器：特征和设计 为了训练监督式分类器，我们首先将「消费者投诉叙述」转化为数字向量。我们研究了向量表示，例如 TF-IDF 加权向量。 有了这个向量表达的文本后，我们可以训练监督式分类器来训练看不到的「消费者投诉叙述」并预测它们的「产品」。 在完成上述数据转换之后，现在我们拥有所有的特征和，是时候训练分类器了。我们可以使用很多算法来解决这类问题。 朴素贝叶斯分类器：最适合字数统计的是多项式变体： 在拟合好训练集后，让我们做一些预测。 「『收回欠款』」 「『信用报告』」 不是太寒酸！ 我们现在准备尝试不同的机器学习模型，评估它们的准确性并找出潜在问题的根源。 我们将对以下四种模型进行基准测试： Logistic 回归 （多项式）朴素贝叶斯 线性支持向量机 随机森林 模型名称 线性支持向量机：0.822890 Logistic 回归：0.792927 （多项式）朴素贝叶斯：0.688519 随机森林：0.443826 名称：精确度，dtype：float64 线性支持向量机和 Logistic 回归比其他两个分类器执行的更好，前者具有轻微的优势，其中位精度约为 82%。 继续使用我们的最佳模型（LinearSVC），我们将查看混淆矩阵，并展示预测标签和实际标签之间的差异。 正如我们所希望的，绝大多数预测都在对角线结束（预测标签=实际标签）。然而，仍然存在大量错误分类，看看这些是由什么造成的可能很有趣： 如你所见，一些错误分类的投诉涉及多个主题（比如涉及信用卡和信用报告的投诉）。这种错误总是发生。 再次，我们使用卡方检验来找到与每个类别最相关的项： # 『银行账户或服务』:  . 最高的 unigrams:  . 银行  . 账户  . 最高的 bigrams:  . 借记卡  . 透支费用 # 『消费者贷款』:  . 最高的 unigrams:  . 交通工具  . 车  . 最高的 bigrams:  . 个人贷款  . 历史 xxxx # 『信用卡』:  . 最高的 unigrams:  . 卡  . 发现  . 最高的 bigrams:  . 信用卡  . 发现卡 # 『信用报告』:  . 最高的 unigrams:  . equifax  . 全联公司  . 最高的 bigrams:  . xxxx 账户  . 全联公司 # 『讨回欠款』:  . 最高的 unigrams:  . 债务  . 收集  . 最高的 bigrams:  . 账户信用  . 时间提供 # 『汇款』:  . 最高的 unigrams:  . paypal  . 汇款  . 最高的 bigrams:  . 汇款  . 寄钱 # 『抵押』:  . 最高的 unigrams:  . 抵押  . 国际支付宝  . 最高的 bigrams:  . 贷款修改  . 抵押公司 # 『其他金融服务』:  . 最高的 unigrams:  . 护照  . 牙齿  . 最高的 bigrams:  . 规定支付  . 帮助支付 # 『发薪日贷款』:  . 最高的 unigrams:  . 发薪日  . 贷款  . 最高的 bigrams:  . 发薪日贷款  . 发薪日 # 『预付卡』:  . 最高的 unigrams:  . 充值  . 服务  . 最高的 bigrams:  . 预付卡  . 使用卡 # 『学生贷款』:  . 最高的 unigrams:  . navient  . 贷款  . 最高的 bigrams:  . 学生贷款  . sallie mae # 『虚拟货币』:  . 最高的 unigrams:  . https  . tx  . 最高的 bigrams:  . 想要钱  . xxxx 提供者 它们符合我们的预期。 最后，我们打印出每个类的分类报告： 原文链接：https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f "
172,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738591&idx=2&sn=be82005eedd1edf650aee0d6059c2926&chksm=871acbe1b06d42f76efb2e9dd1e13c4c01e4204082b5034d78d61d531af78da524478f78088c&scene=27,教程 | 5种快速易用的Python Matplotlib数据可视化方法,"选自towardsdatascience 数据可视化是数据科学家工作的重要部分。在项目的早期阶段，我们通常需要进行探索性数据分析来获得对数据的洞察。通过数据可视化可以让该过程变得更加清晰易懂，尤其是在处理大规模、高维度数据集时。在本文中，我们介绍了最基本的 5 种数据可视化图表，在展示了它们的优劣点后，我们还提供了绘制对应图表的 Matplotlib 代码。 Matplotlib 是一个很流行的 Python 库，可以帮助你快速方便地构建数据可视化图表。然而，每次启动一个新项目时都需要重新设置数据、参数、图形和绘图方式是非常枯燥无聊的。本文将介绍 5 种数据可视化方法，并用 Python 和 Matplotlib 写一些快速易用的可视化函数。下图展示了选择正确可视化方法的导向图。 选择正确可视化方法的导向图。 由于可以直接看到原始数据的分布，散点图对于展示两个变量之间的关系非常有用。你还可以通过用颜色将数据分组来观察不同组数据之间的关系，如下图所示。你还可以添加另一个参数，如数据点的半径来编码第三个变量，从而可视化三个变量之间的关系，如下方第二个图所示。 用颜色分组的散点图。 用颜色分组的散点图，点半径作为第三个变量表示国家规模。 接下来是代码部分。我们首先将 Matplotlib 的 pyplot 导入为 plt，并调用函数 plt.subplots() 来创建新的图。我们将 x 轴和 y 轴的数据传递给该函数，然后将其传递给 ax.scatter() 来画出散点图。我们还可以设置点半径、点颜色和 alpha 透明度，甚至将 y 轴设置为对数尺寸，最后为图指定标题和坐标轴标签。 def scatterplot (x_data, y_data, x_label= """" , y_label= """" , title= """" , color =  ""r"" , yscale_log=False) 当一个变量随另一个变量的变化而变化的幅度很大时，即它们有很高的协方差时，线图非常好用。如下图所示，我们可以看到，所有专业课程的相对百分数随年代的变化的幅度都很大。用散点图来画这些数据将变得非常杂乱无章，而难以看清其本质。线图非常适合这种情况，因为它可以快速地总结出两个变量的协方差。在这里，我们也可以用颜色将数据分组。 线图示例。 以下是线图的实现代码，和散点图的代码结构很相似，只在变量设置上有少许变化。 def lineplot (x_data, y_data, x_label= """" , y_label= """" , title= """" ) 直方图对于观察或真正了解数据点的分布十分有用。以下为我们绘制的频率与 IQ 的直方图，我们可以直观地了解分布的集中度（方差）与中位数，也可以了解到该分布的形状近似服从于高斯分布。使用这种柱形（而不是散点图等）可以清楚地可视化每一个箱体（X 轴的一个等距区间）间频率的变化。使用箱体（离散化）确实能帮助我们观察到「更完整的图像」，因为使用所有数据点而不采用离散化会观察不到近似的数据分布，可能在可视化中存在许多噪声，使其只能近似地而不能描述真正的数据分布。 直方图案例 下面展示了 Matplotlib 中绘制直方图的代码。这里有两个步骤需要注意，首先，n_bins 参数控制直方图的箱体数量或离散化程度。更多的箱体或柱体能给我们提供更多的信息，但同样也会引入噪声并使我们观察到的全局分布图像变得不太规则。而更少的箱体将给我们更多的全局信息，我们可以在缺少细节信息的情况下观察到整体分布的形状。其次，cumulative 参数是一个布尔值，它允许我们选择直方图是不是累积的，即选择概率密度函数（PDF）或累积密度函数（CDF）。 def histogram (data, n_bins, cumulative=False, x_label =  """" , y_label =  """" , title =  """" ) 如果我们希望比较数据中两个变量的分布，有人可能会认为我们需要制作两个独立的直方图，并将它们拼接在一起而进行比较。但实际上 Matplotlib 有更好的方法，我们可以用不同的透明度叠加多个直方图。如下图所示，均匀分布设置透明度为 0.5，因此我们就能将其叠加在高斯分布上，这允许用户在同一图表上绘制并比较两个分布。 叠加直方图 在叠加直方图的代码中，我们需要注意几个问题。首先，我们设定的水平区间要同时满足两个变量的分布。根据水平区间的范围和箱体数，我们可以计算每个箱体的宽度。其次，我们在一个图表上绘制两个直方图，需要保证一个直方图存在更大的透明度。 def overlaid_histogram (data1, data2, n_bins =  , data1_name= """" , data1_color= ""#539caf"" , data2_name= """" , data2_color= ""#7663b0"" , x_label= """" , y_label= """" , title= """" ) 当对类别数很少（<10）的分类数据进行可视化时，条形图是最有效的。当类别数太多时，条形图将变得很杂乱，难以理解。你可以基于条形的数量观察不同类别之间的区别，不同的类别可以轻易地分离以及用颜色分组。我们将介绍三种类型的条形图：常规、分组和堆叠条形图。 常规条形图如图 1 所示。在 barplot() 函数中，x_data 表示 x 轴上的不同类别，y_data 表示 y 轴上的条形高度。误差条形是额外添加在每个条形中心上的线，可用于表示标准差。 常规条形图 分组条形图允许我们比较多个类别变量。如下图所示，我们第一个变量会随不同的分组（G1、G2 等）而变化，我们在每一组上比较不同的性别。正如代码所示，y_data_list 变量现在实际上是一组列表，其中每个子列表代表了一个不同的组。然后我们循环地遍历每一个组，并在 X 轴上绘制柱体和对应的值，每一个分组的不同类别将使用不同的颜色表示。 分组条形图 堆叠条形图非常适合于可视化不同变量的分类构成。在下面的堆叠条形图中，我们比较了工作日的服务器负载。通过使用不同颜色的方块堆叠在同一条形图上，我们可以轻松查看并了解哪台服务器每天的工作效率最高，和同一服务器在不同天数的负载大小。绘制该图的代码与分组条形图有相同的风格，我们循环地遍历每一组，但我们这次在旧的柱体之上而不是旁边绘制新的柱体。 堆叠条形图 def barplot (x_data, y_data, error_data, x_label= """" , y_label= """" , title= """" ) def stackedbarplot (x_data, y_data_list, colors, y_data_names= """" , x_label= """" , y_label= """" , title= """" ) def groupedbarplot (x_data, y_data_list, colors, y_data_names= """" , x_label= """" , y_label= """" , title= """" ) 0 上述的直方图对于可视化变量分布非常有用，但当我们需要更多信息时，怎么办？我们可能需要清晰地可视化标准差，也可能出现中位数和平均值差值很大的情况（有很多异常值），因此需要更细致的信息。还可能出现数据分布非常不均匀的情况等等。 箱线图可以给我们以上需要的所有信息。实线箱的底部表示第一个四分位数，顶部表示第三个四分位数，箱内的线表示第二个四分位数（中位数）。虚线表示数据的分布范围。 由于箱线图是对单个变量的可视化，其设置很简单。x_data 是变量的列表。Matplotlib 函数 boxplot() 为 y_data 的每一列或 y_data 序列中的每个向量绘制一个箱线图，因此 x_data 中的每个值对应 y_data 中的一列/一个向量。 箱线图示例。 def boxplot (x_data, y_data, base_color= ""#539caf"" , median_color= ""#297083"" , x_label= """" , y_label= """" , title= """" ) 箱线图代码 本文介绍了 5 种方便易用的 Matplotlib 数据可视化方法。将可视化过程抽象为函数可以令代码变得易读和易用。Hope you enjoyed！ 原文地址： https://towardsdatascience.com/5-quick-and-easy-data-visualizations-in-python-with-code-a2284bae952f "
173,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738591&idx=3&sn=2cc89ea618a28e8ec24a1da06c87d2bd&chksm=871acbe1b06d42f7c9d91cda71e051cb92cb594848c615771616775522e2aad53f4ad0f19fd8&scene=27,专栏 | 如何做好文本关键词提取？从三种算法说起,"机器之心专栏 作者：韩伟 在自然语言处理领域，处理海量的文本文件最关键的是要把用户最关心的问题提取出来。 而无论是对于长文本还是短文本，往往可以通过几个关键词窥探整个文本的主题思想。与此同时，不管是基于文本的推荐还是基于文本的搜索，对于文本关键词的依赖也很大，关键词提取的准确程度直接关系到推荐系统或者搜索系统的最终效果。因此，关键词提取在文本挖掘领域是一个很重要的部分。 关于文本的关键词提取方法分为有监督、半监督和无监督三种： 1 有监督的文本关键词提取算法需要 高昂的人工成本 ，因此现有的文本关键词提取主要采用适用性较强的无监督关键词抽取。 其文本关键词抽取流程如下： 图 1 无监督文本关键词抽取流程图 无监督关键词抽取算法可以分为三大类，基于统计特征的关键词抽取、基于词图模型的关键词抽取和基于主题模型的关键词抽取。 基于于统计特征的关键词抽取算法的思想是 通常将文本经过预处理得到候选词语的集合，然后采用特征值量化的方式从候选集合中得到关键词。基于统计特征的关键词抽取方法的关键是采用什么样的特征值量化指标的方式，目前常用的有三类： 基于词权重的特征量化 基于词权重的特征量化主要包括词性、词频、逆向文档频率、相对词频、词长等。 这种特征量化方式是根据文章不同位置的句子对文档的重要性不同的假设来进行的。通常，文章的前N个词、后N个词、段首、段尾、标题、引言等位置的词具有代表性，这些词作为关键词可以表达整个的主题。 基于词的关联信息的特征量化 词的关联信息是指词与词、词与文档的关联程度信息，包括互信息、hits值、贡献度、依存度、TF-IDF值等。 下面介绍几种常用的特征值量化指标。 词性时通过分词、语法分析后得到的结果。现有的关键词中，绝大多数关键词为名词或者动名词。一般情况下，名词与其他词性相比更能表达一篇文章的主要思想。但是，词性作为特征量化的指标，一般与其他指标结合使用。 词频表示一个词在文本中出现的频率。一般我们认为，如果一个词在文本中出现的越是频繁，那么这个词就越有可能作为文章的核心词。词频简单地统计了词在文本中出现的次数，但是，只依靠词频所得到的关键词有很大的不确定性，对于长度比较长的文本，这个方法会有很大的噪音。 一般情况下，词出现的位置对于词来说有着很大的价值。例如，标题、摘要本身就是作者概括出的文章的中心思想，因此出现在这些地方的词具有一定的代表性，更可能成为关键词。但是，因为每个作者的习惯不同，写作方式不同，关键句子的位置也会有所不同，所以这也是一种很宽泛的得到关键词的方法，一般情况下不会单独使用。 互信息是信息论中概念，是变量之间相互依赖的度量。互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息的计算公式如下： 其中，p(x,y)是X和Y的联合概率分布函数，p(x)和p(y)分别为X和Y的边缘概率分布函数。 当使用互信息作为关键词提取的特征量化时，应用文本的正文和标题构造PAT树，然后计算字符串左右的互信息。 词跨度是指一个词或者短语字文中首次出现和末次出现之间的距离，词跨度越大说明这个词对文本越重要，可以反映文本的主题。一个词的跨度计算公式如下： 其中， 词跨度被作为提取关键词的方法是因为在现实中，文本中总是有很多噪声（指不是关键词的那些词），使用词跨度可以减少这些噪声。 一个词的TF是指这个词在文档中出现的频率，假设一个词w在文本中出现了m次，而文本中词的总数为n，那么 一个词的IDF是根据语料库得出的，表示这个词在整个语料库中出现的频率。假设整个语料库中，包含词w的文本一共有M篇，语料库中的文本一共有N篇，则 由此可得词w的TF-IDF值为： 但是，TFIDF算法提取关键词的缺点也很明显，严重依赖语料库，需要选取质量较高且和所处理文本相符的语料库进行训练。另外，对于IDF来说，它本身是一种试图抑制噪声的加权，本身倾向于文本中频率小的词，这使得TF-IDF算法的精度不高。TF-IDF算法还有一个缺点就是不能反应词的位置信息， 基于统计特征的关键词提取算法通过上面的一些特征量化指标将关键词进行排序，获取TopK个词作为关键词。 基于统计特征的关键词的重点在于特征量化指标的计算，不同的量化指标得到的结果也不尽相同。同时，不同的量化指标作为也有其各自的优缺点，在实际应用中，通常是采用不同的量化指标相结合的方式得到Topk个词作为关键词 。 基于词图模型的关键词抽取首先要构建文档的语言网络图，然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，这些短语就是文档的关键词。语言网络图中节点基本上都是词，根据词的链接方式不同，语言网络的主要形式分为四种： 在语言网络图的构建过程中，都是以预处理过后的词作为节点，词与词之间的关系作为边。语言网络图中，边与边之间的权重一般用词之间的关联度来表示。在使用语言网络图获得关键词的时候，需要评估各个节点的重要性，然后根据重要性将节点进行排序，选取TopK个节点所代表的词作为关键词。节点的重要性计算方法有以下几种方法。 综合特征法 综合特征法也叫社会网络中心性分析方法，这种方法的核心思想是节点中重要性等于节点的显著性，以不破坏网络的整体性为基础。此方法就是从网络的局部属性和全局属性角度去定量分析网络结构的拓扑性质，常用的定量计算方法如下。 节点的度是指与该节点直接向量的节点数目，表示的是节点的局部影响力，对于非加权网络，节点的度为： 对于加权网络，节点的度又称为节点的强度，计算公式为： 节点的接近性是指节点到其他节点的最短路径之和的倒数，表示的是信息传播的紧密程度，其计算公式为： 特征向量的思想是节点的中心化测试值由周围所有连接的节点决定，即一个节点的中心化指标应该等于其相邻节点的中心化指标之线性叠加，表示的是通过与具有高度值的相邻节点所获得的间接影响力。特征向量的计算公式如下： 节点的集聚系数是它的相邻的节点之间的连接数与他们所有可能存在来链接的数量的比值，用来描述图的顶点之间阶级成团的程度的系数，计算公式如下： 节点的平局最短路径也叫紧密中心性，是节点的所有最短路径之和的平均值，表示的是一个节点传播信息时对其他节点的依赖程度。如果一个节点离其他节点越近，那么他传播信息的时候也就越不需要依赖其他人。一个节点到网络中各点的距离都很短，那么这个点就不会受制于其他节点。计算公式如下： 因为每个算法的侧重方向的不同，在实际的问题中所选取的定量分析方法也会不一样。同时，对于关键词提取来说，也可以和上一节所提出的统计法得到的词的权重，例如词性等相结合构建词搭配网络，然后利用上述方法得到关键词。 系统科学法 系统科学法进行中心性分析的思想是节点重要性等于这个节点被删除后对于整个语言网络图的破坏程度。重要的节点被删除后会对网络的呃连通性等产生变化。如果我们在网络图中删除某一个节点，图的某些指定特性产生了改变，可以根据特性改变的大小获得节点的重要性，从而对节点进行筛选。 随机游走法 随机游走算法时网络图中一个非常著名的算法，它从给定图和出发点，随机地选择邻居节点移动到邻居节点上，然后再把现在的节点作为出发点，迭代上述过程。 随机游走算法一个很出名的应用是大名鼎鼎的PageRank算法，PageRank算法是整个google搜索的核心算法，是一种通过网页之间的超链接来计算网页重要性的技术，其关键的思想是重要性传递。 在关键词提取领域， Mihalcea 等人所提出的TextRank算法就是在文本关键词提取领域借鉴了这种思想。 PageRank算法将整个互联网看作一张有向图，网页是图中的节点，而网页之间的链接就是图中的边。根据重要性传递的思想，如果一个大型网站A含有一个超链接指向了网页B，那么网页B的重要性排名会根据A的重要性来提升。网页重要性的传递思想如下图所示： 图 2 PageRank简单描述（来自PageRank论文） 在PageRank算法中，最主要的是对于初始网页重要性（PR值）的计算，因为对于上图中的网页A的重要性我们是无法预知的。但是，在原始论文中给出了一种迭代方法求出这个重要性，论文中指出，幂法求矩阵特征值与矩阵的初始值无关。那么，就可以为每个网页随机给一个初始值，然后迭代得到收敛值，并且收敛值与初始值无关。 PageRank求网页i的PR值计算如下： 其中，d为阻尼系数，通常为0.85。 是指向网页 i 的网页集合。 是指网页j中的链接指向的集合， 是指集合中元素的个数。 TextRank在构建图的时候将节点由网页改成了句子，并为节点之间的边引入了权值，其中权值表示两个句子的相似程度。其计算公式如下： 公式中的 为图中节点 和的边 的权重。其他符号与PageRank公式相同。 TextRank算法除了做文本关键词提取，还可以做文本摘要提取，效果不错。但是TextRank的计算复杂度很高，应用不广。 基于主题关键词提取算法主要利用的是主题模型中关于主题的分布的性质进行关键词提取。算法步骤如下： 算法的关键在于主题模型的构建。 主题模型是一种文档生成模型，对于一篇文章，我们的构思思路是先确定几个主题，然后根据主题想好描述主题的词汇，将词汇按照语法规则组成句子，段落，最后生成一篇文章。 主题模型也是基于这个思想，它认为文档是一些主题的混合分布，主题又是词语的概率分布，pLSA模型就是第一个根据这个想法构建的模型。同样地，我们反过来想，我们找到了文档的主题，然后主题中有代表性的词就能表示这篇文档的核心意思，就是文档的关键词。 pLSA模型认为，一篇文档中的每一个词都是通过一定概率选取某个主题，然后再按照一定的概率从主题中选取得到这个词语，这个词语的计算公式为： 一些贝叶斯学派的研究者对于pLSA模型进行了改进，他们认为，文章对应主题的概率以及主题对应词语的概率不是一定的，也服从一定的概率，于是就有了现阶段常用的主题模型--LDA主题模型。 LDA是D.M.Blei在2003年提出的。LDA采用了词袋模型的方法简化了问题的复杂性。在LDA模型中，每一篇文档是一些主题的构成的概率分布，而每一个主题又是很多单词构成的一个概率分布。同时，无论是主题构成的概率分布还是单词构成的概率分布也不是一定的，这些分布也服从Dirichlet 先验分布。 文档的生成模型可以用如下图模型表示： 其中 和 为先验分布的超参数， 为第k个主题下的所有单词的分布， 为文档的主题分布，w为文档的词，z为w所对应的主题 。 图 3 Blei在论文中的图模型 DA挖掘了文本的深层语义即文本的主题，用文本的主题来表示文本的也从一定程度上降低了文本向量的维度，很多人用这种方式对文本做分类，取得了不错的效果。 具体LDA的算法在请 参考 LDA关键词提取算法利用文档的隐含语义信息来提取关键词，但是主题模型提取的关键词比较宽泛，不能很好的反应文档主题。另外，对于LDA模型的时间复杂度较高，需要大量的实践训练。 现阶段，文本的关键词提取在基于文本的搜索、推荐以及数据挖掘领域有着很广泛的应用。同时在实际应用中，因为应用环境的复杂性，对于不同类型的文本，例如长文本和短文本，用同一种文本关键词提取方法得到的效果并相同。因此，在实际应用中针对不同的条件环境所采用的算法会有所不同，没有某一类算法在所有的环境下都有很好的效果。 相对于上文中所提到的算法，一些组合算法在工程上被大量应用以弥补单算法的不足，例如将TF-IDF算法与TextRank算法相结合，或者综合TF-IDF与词性得到关键词等。同时，工程上对于文本的预处理以及文本分词的准确性也有很大的依赖。对于文本的错别字，变形词等信息，需要在预处理阶段予以解决，分词算法的选择，未登录词以及歧义词的识别在一定程度上对于关键词突提取会又很大的影响。 关键词提取是一个看似简单，在实际应用中却十分棘手的任务，从现有的算法的基础上进行工程优化，达观数据在这方面做了很大的努力并且取得了不错的效果。 文本关键词提取算法 本文介绍了三种常用的无监督的关键词提取算法，并介绍了其优缺点。关键词提取在文本挖掘领域具有很广阔的应用，现有的方法也存在一定的问题，我们依然会在关键词提取的问题上继续努力研究，也欢迎大家积极交流。 [1] TextRank算法提取关键词和摘要 http://xiaosheng.me/2017/04/08/article49/ [2] Page L, Brin S, Motwani R,et al. The PageRank citation ranking: Bringing order to the web[R]. StanfordInfoLab, 1999. [3] 刘知远. 基于文档主题结构的关键词抽取方法研究[D]. 北京: 清华大学, 2011. [4] tf-idf， https://zh.wikipedia.org/zh-hans/Tf-idf [5] 一文详解机器领域的LDA主题模型  https://zhuanlan.zhihu.com/p/31470216 [6] Blei D M, Ng A Y, Jordan MI. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003,3(Jan): 993-1022. [7] 赵京胜, 朱巧明, 周国栋, 等. 自动关键词抽取研究综述[J]. 软件学报, 2017,28(9): 2431-2449. 韩伟： 达观数据数据挖掘工程师，负责达观数据文本方面的挖掘与应用。主要参与达观数据标签提取与文本分类系统的构建与实现，对深度学习，NLP数据挖掘领域有浓厚兴趣。 "
174,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738591&idx=4&sn=72b9755457b328b6b04af54508e4ee79&chksm=871acbe1b06d42f70047010483a47ec5c604770cf052fa1955b3a77ca36751a3f80a747fde0b&scene=27,CVPR 2018 | 残差密集网络：利用所有分层特征的图像超分辨率网络,"选自arXiv 图像超分辨率在安防等很多领域有这广泛的应用，而美国东北大学最近提出了一种残差密集网络来从原图生成高分辨率图像。该网络结合残差网络与密集连接网络的特性充分利用原始 LR 图像的所有分层特征，因而能重构出高质量的图像。 单幅图像超分辨率（SISR）旨在于低分辨率（LR）测量的基础上生成视觉良好的高分辨率（HR）图像。SISR 用于各种计算机视觉任务，如安全和监视成像 [38]、医学成像 [22] 和图像生成 [9]。图像超分辨率是一个不适定（ill-posed）逆过程，因为对于任何 LR 输入都存在多种解决方案。为了解决这个逆问题，研究者们已经提出了大量的图像 SR 算法，包括基于插值、基于重建和基于学习的方法 [27, 28, 19, 2, 20, 8, 10, 30]。 图 1. 之前的网络结构（a，b）和我们残差密集块（residual dense block）（c）的比较。其中（a）为 MDSR 中的残差块（residual block）[16]，（b）为 SRDenseNet 中的密集块（dense block）[30]，（c）为我们的残差密集块。 其中，Dong 等人 [2] 首先将一个三层卷积神经网络（CNN）引入到图像 SR 中，与传统方法相比，此方法有了明显的改进。Kim 等人通过使用梯度截断（gradient clipping）、跳过连接（skip connection）或递归监督（recursive-supervision）来降低训练深度网络的难度。通过使用有效的构建模块，图像 SR 的网络变得更深，性能变得更好。Lim 等人使用残差块（图 1（a））构建了一个非常大的有残差缩放（residual scaling）[23] 的网络 EDSR [16] 和一个非常深的网络 MDSR [16]。Tai 等人提出通过记忆块构建 MemNet [25]。随着网络变深，每个卷积层中的特征将具有不同层级的感受野。然而，这些方法忽略了充分利用每个卷积层的信息。尽管提出的记忆块中的门控单元是控制短期记忆 [25] 的，但局部卷积层不能直接访问后续层，所以很难说记忆块充分利用了其内部所有层的信息。 此外，图像中的物体具有不同的大小、视角和高宽比。一个非常深的网络的分层特征将为重构提供更多的线索。然而大多基于深度学习（DL）的方法（如 VDSR [10]、LapSRN [13] 和 EDSR [16]）在重构时忽略了使用分层特征。尽管记忆块 [25] 也使用之前记忆块的信息，但没有从原始 LR 图像是提取多级特征。MemNet 将原始 LR 图像内插至所需大小形成输入。这个预处理的步骤不仅使计算的复杂度平方地增加，而且也丢失了原始 LR 图像的一些细节。Tong 等人为较低增长率（如 16）的图像 SR 引入了密集块（图 1（b））。根据我们的实验（见第 5.2 节），更高的增长率可以进一步提高网络的性能。而在图 1（b）中，很难用密集块来训练更大的网络。 为了解决这些缺点，我们提出了残差密集网络（RDN）（图 2），通过残差密集块（RDB）（图 1（c））来充分利用原始 LR 图像的所有分层特征。对于一个很深的网络来说，直接提取 LR 空间中的每个卷积层的输出很难，可以说是不切实际的。我们使残差密集块（RDB）作为 RDN 的构建模块。RDB 包含密集连通层和带有局部残差学习（LRL）的局部特征融合（LFF）。我们的残差密集块还支持 RDB 间的连续记忆。一个 RDB 的输出可以直接访问下一个 RDB 各层，从而使状态连续传递。RDB 每个卷积层都可以访问所有的后续层，传递需要保留的信息 [7]。将前面的 RDB 与当前 RDB 的所有前面层的状态连接，LFF 通过自适应地保存信息来提取局部密集特征。此外，LFF 通过稳定更大网络的训练来实现极高的增长率。在提取多层局部密集特征后，我们进一步进行全局特征融合（GFF）以全局方式自适应地保留分层特征。如图 2 和图 3 所示，每层都可以直接访问原始的 LR 输入，从而产生隐式的深层监督 [15]。 总得来说，这项工作的主要贡献有三个： 我们提出了一个统一的框架，它通过不同的退化模型（degradation models）使用残差密集网络生成高质量的超分辨率图像，网络充分利用原始低分辨率图像的所有分层特征。 我们提出了残差密集块（RDB），它不仅可以通过连续记忆（CM）机制从前一个 RDB 读取状态，还可以通过局部密集连接充分利用其中的所有层。然后通过局部特征融合（LFF）自适应地保留累积的特征。 我们提出了全局特征融合以自适应地融合 LR 空间中所有 RDB 的分层特征。利用全局残差学习，我们将浅层特征和深层特征结合在一起，从原始 LR 图像中得到全局密集特征。 图 2. 我们提出的残差密集网络（RDN）的结构。 网络架构 如图 2 所示，我们的 RDN 主要包含四部分：浅层特征提取网络（SFENet）、残差密集块（RDBs）、密集特征融合（DFF）以及上采样网络（UPNet）。我们将 ILR 和 ISR 表示为 RDN 的输入和输出，具体来说，我们使用两个 Conv 层来提取浅层特征。 图 3. 残差密集块（RDB）架构。 表 3. BD 和 DN 退化模型的基准结果。 图 7. 使用缩放因子×3 的 BD 退化模型的可视化结果。SR 结果分别是由 Urban100 的图像得到的「img 096」和由 Urban100 得到的「img 099」。 图 8. 使用缩放因子×3 的 DN 退化模型的可视化结果。SR 结果分别是由 B100 的图像得到的「302008」和 Manga109 得到的「LancelotFullThrottle」。 图 9. 缩放因子×4 的实际图像视觉效果。两行分别为图像「chip」和「hatc」的 SR 结果。 论文：Residual Dense Network for Image Super-Resolution 论文链接：https://arxiv.org/abs/1802.08797 一个非常深的卷积神经网络（CNN）最近在图像超分辨率（SR）方面取得了巨大的成功，并提供了分层特征。然而，大多数基于 SR 模型的深层 CNN 并没有充分利用原始低分辨率（LR）图像的分层特征，从而其性能较低。本文中，我们得出了一种新的残差密集网络（RDN）来解决图像超分辨率问题。我们充分利用所有卷积层的分层特征。具体来说，我们提出了残差密集块（RDB），通过密集卷积层来提取充分的局部特征。RDB 还允许将前一个 RDB 的状态直接连接至当前 RDB 的所有层，从而形成连续记忆（CM）机制。然后使用 RDB 中的局部特征融合来自适应地学习来自先前和当前局部特征的更有效特征，并稳定更大网络的训练。在完全获得密集的局部特征后，我们使用全局特征融合整体地联合和自适应地学习全局分层特征。在不同退化模型的基准数据上的大量实验表明，我们的 RDN 相对最先进的方法取得了良好的性能。 "
175,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738568&idx=5&sn=582c500cff5180c82328cc8ec90728c4&chksm=871acbf6b06d42e02f0d98a720a22a709d81c8735888a6e70125330e789f1912ab2cb42917b6&scene=27,机器之心再次入选：「2017 年度学术公众号」TOP 10重磅发布,亲爱的读者们，随着评选结果出炉，机器之心再次入选了科研圈评选的「年度学术公众号」Top 10，在此感谢读者们的支持与认可！如果你喜欢机器之心，请将机器之心置顶，我们会继续每天为大家推送优质的人工智能内容。 
176,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738591&idx=1&sn=68f8ec3e540eb0d7ca600dc7f52955f9&chksm=871acbe1b06d42f72a05a3441bf4f11960ca121545d5608d6166f998ce1dce68e6b5e1832c9c&scene=27,超有趣！手把手教你使用树莓派实现实时人脸检测,"作者：MJRoBot 机器之心编译 本文介绍了如何在树莓派上，使用 OpenCV 和 Python 完成人脸检测项目。该项目不仅描述了识别人脸所需要的具体步骤，同时还提供了很多扩展知识。此外，该项目并不需要读者了解详细的人脸识别理论知识，因此初学者也能轻松跟着步骤实现。 项目所需设备 硬件： 树莓派 3 Model B； 树莓派摄像头模块（PiCam）。 语言和库： OpenCV Python 3 步骤 本文主要讲述如何使用 PiCam 实现实时人脸识别，如下图所示： 本教程使用 OpenCV 完成，一个神奇的「开源计算机视觉库」，并主要关注树莓派（因此，操作系统是树莓派系统）和 Python，但是我也在 Mac 电脑上测试了代码，同样运行很好。OpenCV 具备很强的计算效率，且专门用于实时应用。因此，它非常适合使用摄像头的实时人脸识别。要创建完整的人脸识别项目，我们必须完成3个阶段： 1）人脸检测和数据收集； 2）训练识别器； 3）人脸识别。 如下图所示： 第1步：材料清单 主件： 树莓派 V3：283 RMB（淘宝） 500 万像素 1080p 传感器 OV5647 迷你摄像头模块：83 RMB（淘宝） 第2步：安装OpenCV 3包 我使用的是更新了最新版树莓派系统（Stretch）的树莓派 V3，因此安装 OpenCV 的最佳方式是按照 Adrian Rosebrock 写的教程来进行：《Raspbian Stretch: Install OpenCV 3 + Python on your Raspberry Pi》。经过几次尝试后，我觉得Adrian的教程最好，建议按照该教程一步步来安装。 完成上述教程之后，你应该安装好了 OpenCV 虚拟环境，可用于在树莓派设备上运行本次实验。 我们来到虚拟环境，确认 OpenCV 3 已经正确安装。 Adrian 推荐在每次打开新的终端时都运行命令行「source」，以确保系统变量都得到正确设置。 然后，我们进入虚拟环境： 如果你看到 (cv) 出现在提示符之前，那么你就进入了 cv 虚拟环境： Adrian 希望大家注意 cv Python 虚拟环境是完全独立的，且与 Raspbian Stretch 中包含的默认 Python 版本彼此隔绝。因此，全局站点包目录中的任意 Python 包对于 cv 虚拟环境而言都是不可用的。类似地，cv 站点包中的任意 Python 包对于全局 Python 包安装也都是不可用的。 现在，进入 Python 解释器： 确认你正在运行3.5（或以上）版本。 在解释器内部（将出现>>>），导入 OpenCV 库： 如果没有错误信息，则 OpenCV 已在你的 Python 虚拟环境中正确安装。 你还可以检查已安装的 OpenCV 版本： 将会出现3.3.0（或未来有可能发布更高版本）。 上面的终端截图显示了以上步骤。 在树莓派上安装 OpenCV 之后，我们测试一下，以确认摄像头正常运转。假设你已经在树莓派上安装了 PiCam。 在 IDE 中输入下列 Python 代码： 上面的代码可捕捉PiCam生成的视频流，用BGR颜色和灰色模式展示。 注意：我按照组装方式垂直旋转了摄像头。如果你的情况并非如此，那么注释或删除「flip」命令行。 你还可以从我的 GitHub 下载代码：https://github.com/Mjrovai/OpenCV-Object-Face-Tracking/blob/master/simpleCamTest.py 输入下面的命令行，开始执行： 要完成该程序，你必须在键盘上按 [ESC] 键。在按 [ESC] 键之前，先鼠标点击视频窗口。 上图展示了结果。 想更多地了解 OpenCV，请查看该教程：https://pythonprogramming.net/loading-video-python-opencv-tutorial/ 人脸识别的最基础任务是「人脸检测」。你必须首先「捕捉」人脸（第 1 阶段）才能在未来与捕捉到的新人脸对比时（第 3 阶段）识别它。 最常见的人脸检测方式是使用「Haar 级联分类器」。使用基于 Haar 特征的级联分类器的目标检测是 Paul Viola 和 Michael Jones 2001 年在论文《Rapid Object Detection using a Boosted Cascade of Simple Features》中提出的一种高效目标检测方法。这种机器学习方法基于大量正面、负面图像训练级联函数，然后用于检测其他图像中的对象。这里，我们将用它进行人脸识别。最初，该算法需要大量正类图像（人脸图像）和负类图像（不带人脸的图像）来训练分类器。然后我们需要从中提取特征。好消息是 OpenCV 具备训练器和检测器。如果你想要训练自己的对象分类器，如汽车、飞机等，你可以使用 OpenCV 创建一个。 详情参见：https://docs.opencv.org/3.3.0/dc/d88/tutorial_traincascade.html。 如果不想创建自己的分类器，OpenCV 也包含很多预训练分类器，可用于人脸、眼睛、笑容等的检测。相关的 XML 文件可从该目录下载：https://github.com/Itseez/opencv/tree/master/data/haarcascades。 下面，我们就开始用 OpenCV 创建人脸检测器吧！ 从我的 GitHub 下载文件 faceDetection.py：https://github.com/Mjrovai/OpenCV-Face-Recognition/blob/master/FaceDetection/faceDetection.py。 使用 Python 和 OpenCV 执行人脸检测，上面的几行代码就足够了。注意下面的代码： 这行代码可以加载「分类器」（必须在项目文件夹下面的 Cascades/目录中）。然后，我们在在循环内部调用摄像头，并以 grayscale 模式加载我们的输入视频。现在，我们必须调用分类器函数，向其输入一些非常重要的参数，如比例因子、邻近数和人脸检测的最小尺寸。 gray 表示输入 grayscale 图像。 scaleFactor 表示每个图像缩减的比例大小。 minNeighbors 表示每个备选矩形框具备的邻近数量。数字越大，假正类越少。 minSize 表示人脸识别的最小矩形大小。 该函数将检测图像中的人脸。接下来，我们必须「标记」图像中的人脸，比如，用蓝色矩形。使用下列代码完成这一步： 如果已经标记好人脸，则函数将检测到的人脸的位置返回为一个矩形，左上角 (x,y)，w 表示宽度，h 表示高度 ==> (x,y,w,h)。详见下图。 得到这些位置信息后，我们可以为人脸创建一个「感兴趣区域」（绘制矩形），用 imshow() 函数呈现结果。使用树莓派终端，在你的 Python 环境中运行上面的 Python 脚本： 结果如下： 你也可以加入诸如「眼睛检测」甚至「微笑检测」这样的检测器。在那些用例中，你需要把分类器函数和矩形框内加入原有的面部识别区域中，因为在区域外进行识别没有意义。 注意，在树莓派上，分类方法（HaarCascades）会消耗大量算力，所以在同一代码中使用多个分类器将会显著减慢处理速度。在台式机上运行这些算法则非常容易。 在我的 GitHub上你可以看到另外的例子： faceEyeDetection.py faceSmileDetection.py faceSmileEyeDetection.py 在下图中，你可以看到我们的结果： 要想深入理解面部识别，可以参考这一教程：https://pythonprogramming.net/haar-cascade-face-eye-detection-python-opencv-tutorial/ 我推荐各位读者可以查看以下两个关于人脸识别的教程： 使用 OpenCV 和 Python 从头实现人脸识别：https://www.superdatascience.com/opencv-face-recognition/ 理解人脸识别：https://thecodacus.com/category/opencv/face-recognition/ 现在，我们项目的第一步是创建一个简单的数据集，该数据集将储存每张人脸的 ID 和一组用于人脸检测的灰度图。 因此，以下命令行将为我们的项目创建一个目录，目录名可以如以下为 FacialRecognitionProject 或其它： 在该目录中，除了我们为项目创建的 3 个 Python 脚本外，我们还需要储存人脸分类器。我们可以从 GitHub 中下载：haarcascade_frontalface_default.xml。 下一步需要创建一个子目录「dtatset」，并用它来储存人脸样本： 然后从我的 GitHub 中下载代码 01_face_dataset.py。 上述的代码和人脸识别的代码非常像，我们只是添加了一个「input command」来捕捉用户 ID（整数）。 对于每一个捕捉的帧，我们应该在「dataset」目录中保存为文档： 对于保存上述文件，我们需要导入「os」库，每一个文件的名字都服从以下结构： 例如，对于 face_id = 1 的用户，dataset/ 目录下的第四个样本文件名可能为： 在我的树莓派中，该图像可以打开为： 在我的代码中，我从每一个 ID 捕捉 30 个样本，我们能在最后一个条件语句中修改抽取的样本数。如果我们希望识别新的用户或修改已存在用户的相片，我们就必须以上脚本。 在第二阶段中，我们需要从数据集中抽取所有的用户数据，并训练 OpenCV 识别器，这一过程可由特定的 OpenCV 函数直接完成。这一步将在「trainer/」目录中保存为.yml 文件。 所以，下面开始创建子目录以储存训练数据： 从我的 GitHub 中下载第二个 Python 脚本：02_face_training.py。 确定在 Rpi 中已经安装了 PIL 库，如果没有的话，在终端运行以下命令： 我们将使用 LBPH（LOCAL BINARY PATTERNS HISTOGRAMS）人脸识别器，它由 OpenCV 提供： 函数「getImagesAndLabels (path)」将抽取所有在目录「dataset/」中的照片，并返回 2 个数组：「Ids」和「faces」。通过将这些数组作为输入，我们就可以训练识别器。 在训练过后，文件「trainer.yml」将保存在我们前面定义的 trainer 目录下。此外，我们还在最后使用了 print 函数以确认已经训练的用户面部数量。 这是该项目的最后阶段。这里，我们将通过摄像头捕捉一个新人脸，如果这个人的面孔之前被捕捉和训练过，我们的识别器将会返回其预测的 id 和索引，并展示识别器对于该判断有多大的信心。 让我们从 GitHub 03_face_recognition.py 上下载第三阶段的 python 脚本。 这里我们包含了一个新数组，因此我们将会展示「名称」，而不是编号的 id： 所以，如上所示的列表，Marcelo 的 ID 或索引为 1，Paula 的 ID 等于 2。 下一步，我们将检测一张人脸，正如我们在之前的 haasCascade 分类器中所做的那样。 recognizer.predict () 将把待分析人脸的已捕捉部分作为一个参数，并返回其可能的所有者，指示其 id 以及识别器与这一匹配相关的置信度。 注意，如果匹配是完美的，置信度指数将返回「零」。 最后，如果识别器可以预测人脸，我们将在图像上放置一个文本，带有可能的 id，以及匹配是否正确的概率（概率=100 - 置信度指数）。如果没有，则把「未知」的标签放在人脸上。 下面是这一结果的图片： 在这张图像上，我展示了一些由该项目完成的测试，其中我也使用图像验证识别器是否有效。 我希望该项目能帮助其他人发现更好玩的项目，也希望有助于各位读者实现自己的人脸识别应用。 更多详细的代码请查看GitHub地址：https://github.com/Mjrovai/OpenCV-Face-Recognition 参考阅读： "
177,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738568&idx=4&sn=8bb14990f9e1a44bd6d48f0601470ed6&chksm=871acbf6b06d42e0ee48512f50b6bfe289f8325129b1dfeeef024b55ae160dd0ef69c3858c97&scene=27,学界 |「极简机器学习」，从少量数据中学习精确特征的卷积神经网络,Berkeley Lab 机器学习模型通常依赖于大量训练数据，所以在很多领域中难以施展拳脚。近日，伯克利实验室 CAMERA 的研究人员开发了非常高效的卷积神经网络，可以从有限的训练数据中分析实验科学图像，精确地执行图像分割和图像去噪等，并有望扩展到其它实验研究领域中。 能源部劳伦斯伯克利国家实验室（Berkeley Lab）的数学家们开发了一种针对实验性成像数据的新的机器学习方法。这种新方法不像典型的机器学习方法一样需要数万或数十万张图像用于训练——它可以在使用更少的图像的同时，更快地进行学习。 伯克利实验室的 CAMERA（能源高级数学研究与应用中心，Center for Advanced Mathematics for Energy Research Applications）的 Daniël Pelt 和 James Sethian 开发了一种新算法，他们将这种算法称为「多尺度密集卷积神经网络」（MS-D，Mixed-Scale Dense Convolution Neural Network）。这种新方法与传统方法相比，需要的参数更少，收敛得也更快，而且可以基于相当小的训练集进行「学习」。这种方法已用于从细胞图像中提取生物结构，还打算为多个研究领域提供新的数据分析的计算工具。 随着实验设备可以更快地产生更高分辨率的图像，科学家们可能难以通过人工方式对产生数据进行管理和分析。2014 年，Sethian 在伯克利实验室成立了 CAMERA。CAMERA 是一个集成了许多交叉学科的中心，成立目的在于开发美国能源部科学用户设备办公室的实验探索所需的基础数学方法。CAMERA 是该实验室计算研究部门的一部分。 Sethian 同时也是 UC Berkeley 的数学教授，他认为：「在科学应用中，需要大量人力对实验图像进行注释和标记——可能需要几周才能得到一些批注过的图像。我们的目标是开发一项可以通过很小的数据集进行学习的新技术。」 该算法的相关论文发表于 2017 年 12 月 26 日的美国国家科学院院刊上（见文末）。 Pelt 是荷兰数学与计算科学研究所下属的计算成像小组的成员，他介绍说：「这项突破源于我们意识到，不同图像尺度的特征提取的缩放运算，可以用能处理多个尺度的数学卷积的一层所代替。」 为了得到更广泛的应用，Olivia Jain 和 Simon Mo 领导的伯克利团队建立了门户网站「图像数据标记引擎（SlideCAM，Segmenting Labeled Image Data Engine）」 该算法还可用于理解生物细胞内部结构，这一应用也非常有前景。使用 Pelt 和 Sethian 的 MS-D 方法，仅需要七个细胞的数据，就可确定该细胞的内部结构。 国家 X 射线断层成像中心主任、加利福尼亚大学旧金山医学院的教授 Carolyn Larabell 说：「我们实验室中的主要工作是了解细胞的形态结构是如何影响和控制细胞行为的。我们花费无数小时手动分割细胞，以提取细胞结构并识别细胞间的差异，如识别健康细胞和病变细胞间的差异。新算法可能彻底改变我们对疾病的认知能力，而这种算法也是绘制人类细胞图谱的主要工具，绘制人类细胞图谱是扎克伯格和他的夫人赞助的一项全球合作项目，旨在绘制出健康人体中所有的细胞。」 国家 X 射线断层成像中心位于 ALS（Advanced Light Source，先进光源实验室），伯克利实验室的美国能源部科学用户设备办公室。 从更少数据中获得更多科学 图像无处不在。智能手机和传感器已经产生了大量的图片，这些图片中已经有很多标明了图片内容的相关信息。通过庞大的交叉参考图像的数据库，卷积神经网络和其他机器学习算法已经彻底改变了我们快速识别自然图像的能力（类似于曾经见过和分类过的图像）。 这些方法需要数以百万标记过的图像数据，需要使用超级计算机花费大量的时间进行训练，还需要调整一系列隐藏的内部参数进行「学习」。但是如果没有那么多标记过的图像呢？在许多领域，想要这样的数据集都是一个奢侈的愿望。生物学家记录下细胞图像，手动标记出细胞的边界和结构：对他们而言，为得到一个完整独立的细胞的三维结构而花费好几周是一件稀松平常的事情。材料学家利用断层重建技术对岩石和材料进行对比，手动对不同区域进行标记，辨认出裂缝、断口和空隙。不同但重要的结构间的差异往往都会非常小，而数据中的噪声会将特征掩饰起来，并会使最好的算法以及人类产生混淆。 对于传统的机器学习方法而言，这些珍贵的手工标记的图像远远不够。为了迎接这一挑战，CAMERA 的数学家们用非常有限的数据解决了这一问题。为了用更低的成本获得更高的收益，他们的目标是建立一组高效的数学「算符」，可以大规模减少参数数量。这些数学算符可能会与关键的约束条件结合，以帮助计算机对图像进行识别，例如要求形状和模式在科学上合理。 多尺度密集卷积神经网络 机器学习在图像问题上的应用大多使用的是深层卷积神经网络（DCNNs，deep convolutional neural networks）。在 DCNN 中，输入图和中间图在许多连续的层中进行卷积，使得网络可以学到高度非线性特征。为了在困难的图像处理问题中得到精确的结果，DCNN 通常会依赖一些其他操作，这些操作包括但不限于对图像进行缩放以在不同尺度上提取特征。为了训练更深层更强大的网络，通常会需要添加更多的层类型和连接。最终，DCNN 会使用大量的中间图和可训练的参数（一般会超过一亿）来解决复杂的问题。 相反，新的「MS-D」网络架构避免了这些复杂的问题。它用扩张卷积替代缩放操作以捕捉不同空间范围的特征，在单个层中执行多个尺度的特征提取运算，并将所有的中间图密集地连接起来。新算法只需要很少的中间图和参数就能得到精准的结果，既不需要调整超参数也无需额外的层和连接进行训练。 从低分辨率数据中获得高精确度的结果 另一项挑战在于如何从低分辨率的输入产生高分辨率的输出。任何一个试图将一张小照片放大的人都会发现，随着照片越来越大，图片会变的越来越模糊，因此要完成这一挑战听起来几乎是不可能的。但是用 MS-D 网络处理少量的训练图像确实可以取得一些实际的进展。例如，对纤维增强的微型复合材料的层析重建过程进行降噪。在文献中提到的实验，使用 1024 个 x 射线投影重建的图像得到的图像噪声相对较低。再使用 128 个投影重建，来获得同一个对象的噪声图像。训练的输入是有噪声的图像，在训练中将无噪声的图像作为目标输出。经过训练的网络可以高效地使用有噪声的输入重建高分辨率的输出图。 新的应用 Pelt 和 Sethian 正在将他们的方法应用于许多新的领域，例如对来自同步加速器光源的图像进行快速实时的分析，以及生物领域的重建问题，例如细胞和大脑的映射。 Pelt 认为：「这些新方法都很令人振奋，因为它们让机器学习解决更广泛的成像问题成为可能。通过减少需要的训练图的数量以及增加可处理图像的尺寸，这个新的网络架构可在许多研究领域解决更重要的问题。」 劳伦斯伯克利国家实验室致力于通过推进可持续能源、保护人类健康、创造新材料以及揭示宇宙起源与命运，解决世界面临最紧迫的问题。成立于 1931 年的伯克利实验室已获得 13 项诺贝尔奖。实验室由加利福尼亚大学管理，由美国能源部科学办公室负责。 论文： A mixed-scale dense convolutional neural network for image analysis （多尺度密集卷积神经网络用于图像分析） 论文链接：http://docs.wixstatic.com/ugd/cc66e4_bb1cd44c5f354517bb3f7b8c1db45cc4.pdf 摘要： 在最近的工作中，深度卷积神经网络已成功解决了许多图像处理问题。当下使用的网络架构通常会在标准架构中添加额外的操作和连接，以训练更深层的网络。为了在实际应用中得到更高的准确率，就需要更多的可训练参数。在此，我们介绍的网络架构，是在不同的图像尺度中通过扩张卷积以捕捉特征，并将所有特征图密集连接。新架构可以在使用少量参数和使用一组操作的前提下，提高结果的准确率，使得网络更易在实际中实施、训练和应用，且可适用于不同的问题。我们在几个分割问题中对现有架构和新架构进行了比较，结果表明本文提出的架构可以在使用更少参数、更大程度地避免与训练数据过拟合的情况下，获得准确率更高的结果。  
178,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738568&idx=2&sn=dfc927a14643e471b6beb0ac09b4a96a&chksm=871acbf6b06d42e065b9f27995703258b7301b30351c0e234f7e58cbe83be8496fef652a678a&scene=27,2018全球大学计算机科学与人工智能排名：CMU名列第一,参与：李泽南 计算机科学排名顶级学校排名（CSRankings）旨在帮助人们了解全球各家大学在计算机科学领域体系与师资方面的实力。不同于 US News 和 World Report 的方法（仅仅基于调查），该排名完全基于研究指标，其度量了绝大多数院校教员在计算机科学领域的各大顶会所发布的论文数量。 该项目由麻省大学阿默斯特分校的 Emery Berger 教授发起。由于顶会论文发表难度很高，这种统计排名的方式被认为难以进行造假。目前， 、麻省理工学院（MIT）与斯坦福大学名列全球前三，而  则位列论文发表贡献第一位。 排名网站链接：http://csrankings.org GitHub 项目链接：https://github.com/emeryberger/CSrankings 该排名可以根据院校所属地区、文献发表时间等条件进行指定搜索。在这里，我们根据近十年多来（2007-2018）的论文发表对各大学实力进行了排名，数据截止日期为 2018 年 2 月 19 日： 展开各所大学的教员信息，可以看到 CMU 的邢波教授（Eric P. Xing）、Ruslan Salakhutdinov（苹果机器学习负责人），斯坦福大学的 Christopher D. Manning 教授、李飞飞教授都在表内。其中，在卡耐基梅隆大学的排名里，邢波教授目前高居首位。 如前文介绍，该项目是基于各家院校教员在计算机科学各方向顶会上发布的论文数量进行排名的，如自然语言处理领域目前录入信息的顶会：ACL、EMNLP 和 NAACL；计算机视觉领域的会议包含：CVPR、ECCV 与 ICCV；机器学习 & 数据挖掘会议的论文数据来自 ICML、KDD 与 NIPS；人工智能顶会则参考了 AAAI 与 IJCAI。 在生成排名时，我们可以依据不同条件进行过滤： 目标包含的细分条件： 人工智能 系统 理论 跨学科领域 由于排名方式的特殊性，目前该排名系统或许仍有改进空间。本工具基于 GitHub 项目，所以我们也可以向开发者提出建议，帮助它进行改进。  
179,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738568&idx=3&sn=92a17c297fafa5e5da25e53ad153cb8e&chksm=871acbf6b06d42e07ea521c508216eb7ff975222ecfb31171d7d940e8023f3e1e4956ddde274&scene=27,资源 | 百万级字符：清华大学提出中文自然文本数据集CTW,"arXiv 文字识别一直是图像处理领域中的重要任务。近日，清华大学与腾讯共同推出了中文自然文本数据集（Chinese Text in the Wild，CTW）——一个超大的街景图片中文文本数据集，为训练先进的深度学习模型奠定了基础。目前，该数据集包含 32,285 张图像和 1,018,402 个中文字符，规模远超此前的同类数据集。研究人员表示，未来还将在此数据集之上推出基于业内最先进模型的评测基准。 资源链接：https://ctwdataset.github.io/ 在计算机视觉领域中，自动化的文本检测和识别是一项很重要的任务，它拥有大量的应用方向，如自动驾驶和书籍数码化等。该问题已被广泛地研究过，并按难度等级分成了两个问题：文本图像的文本检测和识别，以及自然图像的文本检测和识别。前者更加简单，已有很多可用的商业工具。然而，自然图像的文本检测和识别仍然是很困难的。例如，一个字符可能在不同的图像中有很不同的外观，包括书写风格、字型、分辨率和照明差异等因素；字符还可能是部分显示的、扭曲的，或者有很复杂的背景，这些因素进一步加大了检测和识别的难度。有时候我们甚至需要处理高类内方差和低类间方差 [2]，如图 1 所示，有三个区别很小的字符，而相同字符的不同势力的外观可能差异很大。 过去几年内，深度学习在很多领域都得到了爆炸式的发展，包括图像分类、语音识别等。拥有数十甚至上百层的深度网络（例如 VGG-19、Google Inception 或 ResNet）都有很不错的建模能力，在多种检测、分类、识别任务中都得到了很有潜力的表现。这些模型需要大量的训练数据。获取海量数据是深度神经网络成功的关键因素。诸如 Image-Net 数据集 [4]、微软 COCO 数据集 [13] 和 ADE20K 数据集 [33]，已成为计算机视觉进步的关键驱动力。 在本文中，清华大学的研究人员提出了一个自然图像的中文文本的大型数据集，称为 Chinese Text in the Wild（CTW）。该数据集包含 32,285 张图像和 1,018,402 个中文字符，规模远超之前的数据集。这些图像源于腾讯街景，从中国的几十个不同城市中捕捉得到，不带任何特定目的的偏好。由于其多样性和复杂性，使得该数据集的收集很困难。它包含了平面文本、凸出文本、城市街景文本、乡镇街景文本、弱照明条件下的文本、远距离文本、部分显示文本等。对于每张图像，数据集中都标注了所有中文字符。对每个中文字符，数据集都标注了其真实字符、边界框和 6 个属性以指出其是否被遮挡、有复杂的背景、被扭曲、3D 凸出、艺术化，和手写体等。 在相关论文《Chinese Text in the Wild》中，清华大学的研究人员以该数据集为基础训练了多种目前业内最先进的深度模型进行字符识别和字符检测。这些模型将作为基线算法为人们提供测试标准。研究人员表示，该数据集、源代码和基线算法将全部公开。新的数据集将极大促进自然图像中中文文本检测和识别算法的发展。 论文：Chinese Text in the Wild 论文链接：https://arxiv.org/abs/1803.00085 摘要： 我们提出了 Chinese Text in the Wild，这是一个街景图像内中文文本的超大型数据集。虽然文本图像的光学字符识别（OCR）已得到充分的研究，并有很多可用的商业工具，但是自然图像中的文本检测和识别仍然是很困难的问题，尤其是对于更复杂的字符集，例如中文文本。训练集的匮乏是很常见的问题，特别是对于需要大量训练数据的深度学习方法而言。 在本文中，我们将提供该新数据集的细节描述，其中包含专家标注的超过 3 万个街景图像的大约 100 万个中文字符。该数据集具有高度多样性，它包含了平面文本、凸出文本、城市街景文本、乡镇街景文本、弱照明条件下的文本、远距离文本、部分显示文本等。对于数据集中的每个字符，这些标注包含其真实字符、边界框及其他 6 个属性。这些属性指出其是否被遮挡、有复杂的背景、是否凸出、是手写体还是打印体等。该数据集的大规模和多样性使得其能适用于多种任务的鲁棒性神经网络训练，特别是检测和识别任务。我们使用多个当前最佳模型得到了基线测试结果，包括用 AlexNet、OverFeat、Google Inception 和 ResNet 执行字符识别，用 YOLOv2 执行字符检测。综合来说，Google Inception 在识别任务上达到了 80.5% top-1 准确率的最佳性能，而 YOLOv2 在检测任务上达到了 71.0% 的 mAP。我们将在网站上公布数据集、源代码和训练后的模型。  "
180,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738568&idx=1&sn=e3492aef7d67cbd20c982c348600d42a&chksm=871acbf6b06d42e0b7ae37438befc2f301ab3c3e5cf616a08acc94ab4e8032ada246c1e81ce4&scene=27,机器学习算法如何调参？这里有一份神经网络学习速率设置指南,每个机器学习的研究者都会面临调参过程的考验，而在调参过程中，学习速率（learning rate）的调整则又是非常重要的一部分。学习速率代表了神经网络中随时间推移，信息累积的速度。在理想情况下，我们会以很大的学习速率开始，逐渐减小速度，直至损失值不再发散。不过，说来容易做来难，本文作者对学习速率的调整思路进行了简要介绍，希望能够对你有所帮助。 在之前的文章里，我已经讲了如何用反向传播和梯度下降来训练神经网络。为了训练神经网络，其中一个需要设置的关键超参数是学习率。提醒一下，为了最小化此网络的损失函数，这个参数缩放了权重更新的幅度。 如果你把学习率设置太低，训练会进展的很慢：因为你在网络的权重上只做了很少的调整。然而，如果你的学习率被设置的太高，它可能在你的损失函数上带来不理想的后果。我已经可视化了这些案例——如果你发现这些图很难理解，我建议你事先参考一下（至少）我此前发布的关于梯度下降的第一部分。 所以，我们怎样找到最优学习速率呢？ 让我们看看 Tesla AI 主管、李飞飞高徒 Andrej Karpathy 怎么说： 完美，我觉得我的工作完成了 好吧，并没有…… 神经网络的损失函数地图（loss landscape）（下图所示）是网络参数值的函数，当在特定数据集上执行推断（预测）时量化与使用特定参数配置相关的「误差」。这个损失地图可能对于很相似的网络架构也看起来很不同。下图来自论文《Visualizing the Loss Landscape of Neural Nets》，其中展示了残差连接可产生更平滑的拓扑结构。 最优学习率取决于你的损失地图的拓扑结构，也就是由你的模型结构和数据集。当你用默认的学习率（由你的深度学习库自动决定）可以提供一个差不多的结果，你也可以通过搜寻最优学习率来提高表现。我希望你在下一部分发现这很简单。 最终，我们希望得到一个学习率，极大地减少网路损失。我们可以在逐步提高每一次小批量（迭代）的学习速率的同时通过做一个简单实验来观察，记录每一次增量之后的损失。这个逐步的增长可以是线性或指数的。 对于太慢的学习速率来说，损失函数可能减小，但是按照非常浅薄的速率减小的。当进入了最优学习率区域，你将会观察到在损失函数上一次非常大的下降。进一步增加学习速率会造成损失函数值「跳来跳去」甚至在最低点附近发散。记住，最好的学习速率搭配着损失函数上最陡的下降，所以我们主要关注分析图的坡度。 你应该为这个实验设置你的学习率界限从而你能看到所有的三个阶段，确保识别最优范围。 另一个大家常用的技巧是学习速率退火（learning rate annealing），推荐大家先从一个比较高的学习速率开始然后慢慢地在训练中降低学习速率。这个方法背后的思想是我们喜欢快速地从初始参数移动到一个参数值「好」的范围，但这之后我们又想要一个学习速率小到我们可以发掘「损失函数上更深且窄的地方」，（来自 Karparthy 的 CS231n 课程笔记：http://cs231n.github.io/neural-networks-3/#annealing-the-learning-rate）。如果你很难想象我刚才所言，回想一下太高的学习速率可以造成参数更新会在最小值和随后的更新间「跳来跳去」，这点子会造成在极小值范围内持续的有噪声的收敛，或者在更极端的例子里可能造成从最小值发散出去。 学习速率退火的最流行方式是「步衰减」（Step Decay），其中学习率经过一定数量的训练 epochs 后下降了一定的百分比。 更常见的，我们可以创建一个学习速率时间表（learning rate schedule)，就是在训练期间根据特定规则来更新学习速率。 周期性学习率 在上述论文中《Cyclical Learning Rates for Training Neural Networks》中，Leslie Smith 提出了一种周期性学习率表，可在两个约束值之间变动。如下图所示，它是一个三角形更新规则，但他也提到如何使用这一规则与固定周期衰减或指数周期衰减相结合。 注意：在本文最后，我将给出实现这一学习率的代码。因此，如果你不关心数学公式的理解，可以跳过该部分。 我们可以将其写为： 其中 x 被定义为 并且 cycle 被计算为 其中η_min 和η_max 定义学习率的界限，iterations 表征已完成的小批量（mini-batches）的数量，stepsize 定义了一个周期长度的一半。据我所知，1−x 一直为正，因此看起来 max 操作并非绝对必要。 为了搞明白这一方程式如何工作，让我们逐步利用可视化构建它。对于下面的视觉效果，三个完整周期的三角形更新以 100 次迭代的步长显示。记住，一次迭代对应于一个小批量的训练。 最重要的是，我们可以在训练期间根据我们已完成的半个周期来确定「进程」。我们用半周期而不是全周期来衡量我们的进程，从而就可以在一个周期内实现对称（后面你会更加清晰认识到这点）。 接下来，我们把半周期进程与在当前周期完成时的半周期数量进行对比。当一个周期开始时，我们有两个半周期要完成；当一个周期结束时，该值达到零。 再接下来，我们将该值加 1，从而把函数移为以 y 轴为中心。现在我们参考半周期点展示一个周期内的进程。 在该点上，我们取绝对值以在每个周期内完成一个三角形。这就是我们分配给 x 的值。 但是，我们希望学习率表从最小值开始，在周期中间增加到最大值，然后再降低到最小值。我们可通过简单计算 1-x 来实现这一点。 通过把学习率范围的一部分添加到最小学习率（也称为基本学习率），现在我们有了一个可以调整学习率的值。 Smith 写到，周期性学习率背后的主要理论假设（与降低学习率相对反）是「增加学习率也许有一个短期的负面影响，但却获得了长期的正面影响」。确实，他的论文包含若干个损失函数演化的实例，与基准固定学习率相比，它们暂时偏离到较高的损失，并最终收敛到较低的损失。 为了直观理解这一短期影响如何带来长期的正面效果，重要的是理解我们收敛最小值的期望特征。最终，我们想要我们的网络以一种泛化到不可见数据的方式从数据中学习。进而，具备良好泛化能力的网络是应该是鲁棒的，参数的小变动并不会太大影响性能。考虑到这一点，尖锐的极小值导致很差的泛化能力也就合理了，正如参数值的小变动会导致巨大的较高损失。通过允许我们的学习率在次数上增加，我们可以「跳出」尖锐的极小值，虽然这会暂时增加损失，但可能最终收敛到更加理想的极小值。 注意：尽管「良好泛化的恰当极小值」已被广泛接受，但也存在很有力的反论（https://arxiv.org/abs/1703.04933）。 此外，增加学习率允许「更快速地穿越鞍点高原」。如下图所见，鞍点上梯度可以非常小。因为参数更新是一个梯度函数，这导致我们优化步骤非常短；在这里增加学习率可以避免在鞍点卡住太久，这很有用。 注意：根据定义，鞍点是一个临界点，其中一些维度观测局部极小值，另一些维度观测局部极大值。由于神经网路存在数千或数百万个参数，在所有维度上观测一个真正的局部极小值不太现实；这就是鞍点出现的意义。当我提到「尖锐的极小值」，实际上我们应该描画一个鞍点，其中极小值维度非常陡峭，极大值维度非常宽广（如下图所示）。 带有热重启的随机梯度下降（SGDR）与周期性方法很相似，其中一个积极的退火表与周期性「再启动」融合到原始的初始学习率之中。 我们可以将其写为 其中η_t 是时间步 t 上的学习率，（在每一个 mini batch 间增长） 和 定义理想学习率的范围，T_current 表征上次再启动之后 epoch 的数量，T_i 定义周期之中 epoch 的数量。让我们试着分解这个等式。 这个退火表依赖于余弦函数，其在-1 和 1 之间变化。 能够取 0 到 1 之间的值，这是我们的余弦函数的输入。余弦函数的相应区域在下图用绿色突出显示。通过添加 1，我们的函数在 0 到 2 之间变化，然后缩小 1/2，现在在 0 到 1 之间变化。因此，我们简单地取极小值学习率，并添加指定学习率范围的一部分。由于这一函数从 1 开始并降为 0，结果是一个从特定范围的极大值开始并衰减为极小值的学习率。一旦我们的周期结束，T_current 重置为 0，我们从极大值学习率再开始循环这一过程。 作者也发现这个学习速率安排表可以适用于： 当训练进行时延长周期 在每一周期之后衰减   和 在每一次重启的时候彻底地提高学习速率，我们可以本质上的退出一个局部低点并且继续探索损失地图。 非常酷的主意：在每一轮循环后截图一下权重，研究员可以通过训练单个模型去建立一个全套模型。这是因为从一个周期到另一个周期，这个网络「沉淀」在不同的局部最优，像在下面图中画的一样。 找寻最优学习速率的和设定一个学习速率安排表都可以简单的用 Keras 的回调函数中应用。 寻找最优学习速率范围 我们可以写一个 Keras 回调函数，就是追踪与一个在确定范围内变化的线性的学习速率相搭配的损失函数。 设置一个学习速率表 步衰减 对于一个简单的步衰减（step decay），我们可以用 LearningRateScheduler 回调。 周期性学习速率 要应用周期性学习速率技巧，我们可以参考这个 repo（https://github.com/bckenstler/CLR），其已在论文中实现了该技术。实际上这个 repo 已在论文附录中被引用。 带有重启的随机梯度下降 要应用这个 SGDR 技巧，我们可以参考：https://github.com/keras-team/keras/pull/3525/files  原文链接：https://www.jeremyjordan.me/nn-learning-rate/ 
181,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738513&idx=3&sn=615db6ca0e298fac8cab585af447de10&chksm=871acb2fb06d4239a74bcb8b4e1b27201b2eeefd76a1df1226dbdf11650d267c657d2138f7bd&scene=27,教程 | 22分钟直冲Kaggle竞赛第二名！一文教你做到,本文介绍了如何使用微软 DVSM、利用迁移学习技术在 20 多分钟时间内达到 Kaggle 猫狗识别竞赛的第二名的性能。 引言 几周前，我写了一篇博客《deep learning and computer vision in the Microsoft Azure cloud》，简要介绍了微软的数据科学虚拟机（DVSM）。本文更偏重实际操作，缘起于 PyImageSearch 读者 Kostas 给我发来的一封邮件： 「你好，Adrian！我对 Kaggle 竞赛（特别是计算机视觉领域的 Kaggle 竞赛）很感兴趣。我在计算机视觉和机器学习/深度学习方面有一些经验，但经验尚不丰富。请问参加这个比赛值得吗？我有没有和其他选手竞争对抗的实力？」 Kostas，这是个好问题——我敢肯定，有类似疑问的不止你一个人。 让我来用一个故事回答你的问题吧： 当我第一次着手编写我的新书《Deep Learning for Computer Vision with Python》时，我的目标是写一本能同时面向新手、有经验的研究人员和从业者的书／自学项目。 那本书从神经网络和机器学习的基础出发，当你根据此书自学到最后时，你就能从零开始搭建最先进的网络模型了。 听起来这是个合乎逻辑的渐进式教育项目，能让你从入门到精通——说得通，对吧？ 但是，这个过程中发生了一件很有意思的事情…… 当你阅读了约 25% 的内容时，你已经可以像深度学习实践者那样，有足够的经验开始解决计算机视觉问题了。那些曾对你来说几乎不可能的问题现在也许变得不那么遥远——而且在某些情况下，你可以利用特定的技术解决这些问题。 其中有个很好的例子是 Kaggle 猫狗识别竞赛： 目标很简单：把输入图像分类为狗或猫。 这对我们来说非常容易——人脑可以轻松分辨出这两种家庭宠物的差异。 但是，对于计算机呢？ 这就没那么简单了。计算机能看到的只是一个 0 和 1 组成的巨大矩阵。 我们如何学习这些图像中的模式，从而分辨猫和狗？ 2014 年这个挑战赛发布时，受到了大家的广泛欢迎。 这对于研究人员和工程师来说是个挑战。它获得了大量的关注，因为这个问题看起来非常容易。此外，谁不喜欢看这些可爱的小动物呢？ 超过 200 支队伍参与了这场挑战，他们尝试了数百种算法及其变体，花费了数千小时的计算时间。 那么今天呢？ 如果使用从预训练的卷积神经网络中提取的特征，我们可以在这个挑战赛中获得第二名的好成绩。 而且，最棒的事情是，如果使用微软的 DSVM（预安装了所有必备的计算机视觉和深度学习库），我们可以在 22 分钟的时间内获得第二名！ 代码地址：http://pyimg.co/5jhwg 让我们启动 Ubuntu DSVM 实例，计时开始！ 通过特征提取进行迁移学习 通常，我们将卷积神经网络视为端到端的图像分类器： 我们向网络输入图像 图像经正向传播通过网络 在网络的末端获得最终的分类概率 但是，并没有「规则」表明我们必须让图像在整个网络中正向传播。相反，我们可以在任意层（如激活层或池化层）终止传播过程，在这一位置提取网络的值，然后使用提取的值作为特征向量。 让我们用 Simonyan 和 Zisserman 提出的 VGG16 架构举个例子： 上图左侧是原始的 VGG16 架构，它可以输出 1000 个 ImageNet 类别标签的概率。 为了将网络变成特征提取器，我们可以在概念上「移除」网络的全连接层，返回最终池化层的输出（上图右侧）——该输出将作为我们的特征向量。 由于在 ImageNet 数据集上训练的 CNN 倾向于学习大量的鉴别滤波器，因此我们通常可以在未经训练的数据集上使用这些预训练的网络——我们把这个过程称为迁移学习。 我们可以将在 ImageNet 数据集上训练的 CNN 的知识进行迁移，将所学的知识编码为特征向量，然后基于这些特征向量训练一个简单的机器学习模型（如 Logistic 回归分类器、线性 SVM 等）。 下载 Kaggle：Dogs vs. Cat 数据集 为了了解整个工作流程，请确保已下载： 我的 Jupyter Notebook：http://pyimg.co/5jhwg Kaggle Dogs vs. Cats 数据集：https://www.kaggle.com/c/dogs-vs-cats 为简洁起见，我们不会将测试集提交给评估服务器。只需下载「train.zip」文件即可。注意：如果你尝试使用「test1.zip」，我们将无法从文件路径中提取类别标签。请不要下载「test1.zip」。 在你下载「train.zip」之后，将其解压，然后你就会得到一个名为「train」的目录，其中有 25000 张关于猫和狗的 JPG 格式图像。 从这里开始，我们可以应用迁移学习了。 DSVM 上的迁移学习 为保证你能跟上进度，请确保你已下载与本文相关的 Jupyter Notebook。 第一个步骤是抓取 Kaggle Dogs vs. Cats 数据集中所有 25000 张图像的路径（见 cell 3）： Dogs vs. Cats 数据集中文件的名称都诸如「cat.153.jpg」或「dog.4375.jpg」——由于类别标签已经写在了文件名中，所以我们可以很容易地提取它们（见 cell 4）。 为了执行特征提取，我们需要一个预训练的网络——ResNet50 是一个不错的选择（见 cell 5）。请注意，我们利用 include_top=False 忽略了全连接层，这让我们能够轻松执行特征提取。 在我们拥有所有的图像路径后，我们需要对它们逐一进行循环，并构建批量，使之通过网络，以执行特征提取。 本节不再赘述整个的循环（请参阅我们的 Jupyter Notebook，其中有完整的文档化代码），仅介绍其中最重要的代码片段： 我们使每个批量的图像在神经网络中传输，然后将最大池化层的输出作为我们的特征。我们将最大池化层的输出压缩成 2048-d 的特征向量。这些特征以数据矩阵的形式堆叠在一起，因此我们可以在这些特征上训练模型。 对于 25000 张图像，整个数据矩阵占用大约 204MB 的 RAM，即使是最小规模的机器也可以轻松管理。 整个特征提取过程使用 Ubuntu DSVM（不需要手动配置或搭建，节省了大量的时间），共耗时 22 分 48 秒。 基于我们提取的特征，我们使用 75% 的数据作为训练集，使用 25% 作为测试集，训练了一个 Logistic 回归分类器（网格搜索适当的参数）： 训练模型仅用时 36s。 所以，我们是如何做到的？ 通过评估 Logistic 回归分类器，我们发现我们的模型在测试集上达到了 98.8896％的准确率： 该准确率足以在 Kaggle Dogs vs. Cat 竞赛中获得第二名： 不过，这种比较并不是完全公平，因为我们没有在 Kaggle 竞赛提供的测试集中评估（而是生成了我们自己的测试集）并将结果提交给评估服务器（因为这不在本入门教程的范围之内），但我认为你应该理解了我要表达的意思。在不到 25 分钟的计算时间内，我们可以使用： 微软的 Ubuntu DSVM 迁移学习/特征提取 建立一个模型，从而在这个挑战赛中达到具有强大竞争力的准确率。 请随意使用本文的代码，将其作为你自己深度学习项目的起点。 下一步 通过本文，你学习了如何应用微软的 DSVM 以及深度学习、卷积神经网络在 Kaggle Dogs vs. Cats 挑战赛中达到第二名的性能。 我们使用了迁移学习（具体来说是特征提取）技术获得了这个结果。 由于 DSVM 配备了所有你在启动、运行本项目所必需的计算机视觉和深度学习库，因此我们： 不仅仅获得了第二名； 同时还打破了计算时长的记录——用时仅 20 多分钟。 结合一些深度学习的知识和预配置的 Ubuntu DSVM 环境，我们可以快速、高效地实现这一结果。 如果你想了解更多关于 DSVM 的知识并建立你的首个实例，请点击以下链接：https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/ 如果你对深度学习的更多细节感兴趣，可以阅读我编写的书和自学教程《Deep Learning for Computer Vision with Python》——我个人已经检验了每个代码示例，保证它在 Ubuntu DSVM 中是开箱即用的。 原文地址：https://blogs.technet.microsoft.com/machinelearning/2018/02/22/22-minutes-to-2nd-place-in-a-kaggle-competition-with-deep-learning-azure/ 
182,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738513&idx=4&sn=2c21ef429dc9d50f042ed33ba366b576&chksm=871acb2fb06d42398804ebf8641e47b2a46254d3c211b86a5f91fe14a9cc4753f6d4f51b6f45&scene=27,资源 | 像「花书」一样排版：Ian Goodfellow「亲授」的高级LaTex教程,机器之心整理 当地时间 3 月 1 号，深度学习知名同名教材《Deep Learning》的第一作者 Ian Goodfellow 发推把这个一行 Python 代码都没有的 GitHub 项目推荐给了大家。他的推特写到，「Yoshua、Aaron（本书另外两位作者）和我发布了《深度学习》一书的 LaTex 模板。如果你想要和我们遵循一样的数学符号约定，或者你想做文章中插入一个符号说明页，这个项目会对你有所帮助。」 GitHub 项目地址：https://github.com/goodfeli/dlbook_notation 项目里包括了定义深度学习教科书风格的各个文件、《深度学习》一书符号说明页的 .tex 文件以及一个两页长的注释（commentary.tex）。 这份注释旨在教会你使用「附属文件」这种高级 LaTex 技巧。Ian 介绍道，「两个附属文件里，math_commands.tex 里面有很多好用的 LaTex 宏，而 notation.tex 定义了一个可以用在任何出版物前面的符号说明页。我们在编写 Goodfellow et al.(2016) 时开发了这些文件，现在发布这些文件供任何人免费使用，以期帮助深度学习社区建立一些标准符号规范。」 拿引用来说，直接引用和间接引用的表示方式就不一样。 间接引用，或者用 Ian 的话说，「支持了句子，但原文并没有出现在句子中」的引用。这种情况下，应该使用 citep 把引用放在句末： 效果： 直接引用，或者说「引用文档的作者或者文档本身是句子中的名词」。这种情况下，应该使用 citet： 效果： 此外，还有引入新概念的时候如何用 newterm 高亮、用 index 指向词汇索引、如何把多个词指向一个索引条目： 效果： 如何用 figref 引用图，如何用 caption 写图注： 效果： 总之，花书里面用到的所有排版技巧，都在这份「男神手把手教你 LaTex 999」教程里得到了详细说明，欢迎各位读（qiang）者（po）小（zheng）伙（huan）伴（zhe）自己去试一试，祝大家在写出和 Goodfellow 男神同样有影响力的文章的同时，也拥有男神同样漂亮的排版。 
183,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738445&idx=4&sn=632374493dd34ae611e67094fd32c221&chksm=871acb73b06d42655b2a2b495bccc5ff7cb1be562447715c92ac00b358f876739d418052e3fb&scene=27,CVPR 2018 | 华中科技大学提出多向文本检测方法：基于角定位与区域分割,"arXiv 在计算机视觉的应用场景里，对图像中的文本进行准确识别是重要而相对困难的任务。来自华中科技大学的研究者们近日提出了一种全新的多项文本检测方法，大幅提高了机器学习的识别准确度。该研究已被即将于 6 月 18 日在美国盐湖城举行的 CVPR 2018 大会接收。 简介 最近，由于现实世界应用（如产品搜索 [4]，图像检索 [19]，以及自动驾驶）需求的增长，从自然场景图像中提取文本信息的研究正变得越来越流行。场景文本检测（Scene text detection）在各种文本读取系统中起着重要的作用 [34, 10, 47, 5, 20, 13, 7, 25]，它的目标是在自然图像中定位出文本。 由于外部因素和内部因素，场景文本检测具有一定的挑战性。外部因素源自环境，例如噪声、模糊和遮挡，它们也是一般目标检测中存在的主要问题。内部因素是由场景文本的属性和变化引起的。与一般目标检测相比，场景文本检测更加复杂，因为：1）场景文本可能以任意方向存在于自然图像中，因此边界框可能是旋转的矩形或者四边形；2）场景文本边界框的长宽比变化比较大；3）因为场景文本的形式可能是字符、单词或者文本行的形式，所以在定位边界的时候算法可能会发生混淆。 在过去几年中，随着一般目标检测和语义分割的快速发展，场景文本检测得到了广泛的研究 [10, 5, 49, 20, 43, 52, 39, 42]，并且在最近取得了明显的进展。基于一般目标检测和语义分割模型，几个精心设计的模型使得文本检测能够更加准确地进行。这些文本检测器可以被划分为两个分支。第一个分支以一般目标检测器（SSD [30]，YOLO [37] 和 DenseBox [18]）为基础，例如 TextBoxes [27]，FCRN [14] 以及 EAST [53] 等，它们直接预测候选的边界框。子二个分支以语义分割为基础，例如 [52] 和 [50]，它们生成分割映射，然后通过后处理生成最终的文本边界框。 与前面的方法不同，来自华中科技大学的研究人员结合了目标检测和语义分割的思想，并将它们以一种可替代的方式进行了应用。新研究的动机主要来源于两方面的观察：1）不管矩形的大小如何、长宽比如何、方向如何，它都可以由角点决定；2）区域分割图可以提供有效的文本位置信息。所以，我们可以首先检测文本的角点（左上角、右上角、右下角和左下角）（如图 1 所示），而不是直接检测文本边界框。此外，我们预测位置敏感分割图（如图 1 所示），而不是像 [52] 和 [50] 中提到的文本/非文本图。最后，我们再通过角点进行采样和分组，以生成候选边界框，并通过分割信息消除不合理的边框。新的方法的处理流程如图 2 所示： 新方法的关键优势如下：1）因为我们是通过对角点进行采样和分组来检测场景文本的，所以新的方法能够处理任意方向的文本；2）因为我们检测的是角点，而不是边界框，所以新的方法可以自然地避免边框比较大的问题；3）因为使用了位置敏感分割，所以无论是字符、单词，还是文本行，我们都能够较好地分割文本实例；4）在新方法中，候选边框的边界是由角点决定的。 研究人员在来自公共基准测试集上的水平文本、定向文本、长定向文本以及多语言文本中验证了该方法的有效性。结果显示新提出的算法在准确率和速度方面均有优势。具体而言，新方法在 ICDAR2015 [22] 上的 F-Measures 分别为 84.3 %、81.5 % 和 72.4 %，这显著优于现有的方法。此外，新方法在效率上也很有竞争力。它每秒可以处理 10.4 张以上的图像 ( 512×512 )。 该研究的主要贡献有四个方面： （1）提出了一种融合目标检测和分割思想的场景文本检测器，这个场景文本检测器可以以端到端的方式进行训练和测试。 ( 2 ) 在位置敏感 ROI 池化 [ 9] 的基础上，提出了一种旋转的位置敏感 ROI 平均池化层，可以处理任意方向的请求。 ( 3 ) 新提出的方法可以同时处理多方向场景文本中的诸多挑战（如旋转、宽高比变化、非常闭合的实例）。 ( 4 ) 新方法在精度和效率上均取得了较好或有竞争力的结果。 网络结构 新方法所用的网络全部是卷积神经网络，它扮演着特征提取器、角检测和位置敏感分割的角色。网络结构如图 3 所示。给定一张图片，网络会生成候选的角点和分割图。 论文：Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation 论文链接：https://arxiv.org/abs/1802.08948 摘要 ： 此前基于深度学习的场景文本检测方法可以被粗略地分为两大类别。第一类将场景文本识别视作一种一般的目标检测问题，这类方法遵循一般目标检测的范式，通过回归文本框来定位场景文本，但是会受到任意方向和较大变化的长宽比的场景文本的困扰。第二类将文本区域进行直接分割，但是大都需要复杂的后处理过程。在这篇论文中，我们提出了一种能将这两类方法的思想进行结合，同时能够避免它们各自弱点的新方法。我们提出了通过定位文本边界框的角点，并在相对位置分割文本区域来检测场景文本的方法。在推理阶段，候选边框通过对角点的采样和分组得到，候选边框进一步通过分割图进行打分，然后使用非极大值抑制（NMS）方法对边框进行抑制。与之前的方法相比，我们的方法能够自然地处理长定向文本，并且不需要复杂的后处理过程。在 ICDAR2013、ICDAR2015、MSRA-TD500、MLT 和 COCO-Text 上的实验证明我们提出的方法能够在准确率和效率方面同时达到更好或者更具竞争力的结果。基于 VGG16，我们的方法在 ICDAR2015 上实现了 84.3% 的 F-measure，在 MSRA-TD500 上达到了 81.5% 的 F-measure。 "
184,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738513&idx=5&sn=16101ee84c36aadae89c4f650e2f5c27&chksm=871acb2fb06d42395493205b4fb22757b777e6c5a08230801c99a3c7f37bd863c02253907910&scene=27,学界 | 从可视化到新模型：纵览深度学习的视觉可解释性,"在本篇论文中，来自 UCLA 的研究人员就目前有关理解神经网络表征和用可解释/分离式表征学习神经网络的研究进行了一次调查。 本文将研究范围圈定到以下六个研究方向： 网络中间层的 CNN 特征可视化。这些方法主要是合成图像，使预训练的 CNN 中的给定神经元的得分最大化，或者用卷积层的 feature maps 反推出输入图。详细内容请看第二节。 CNN 表征的诊断。相关的研究涉及为不同的物体类别诊断 CNN 的特征空间，或揭露卷积层的潜在的表征缺陷。详细内容见第三节。 「模式混合」的分离式表征编码在 CNN 的每个滤波器中。这方面的研究主要用于解卷积层的复合表征以及网络表征可视化。详细内容见第四节。 构建可解释的模型。我们在第五节讨论了可解释的 CNN 模型 [Zhang et al., 2017c]，胶囊网络 capsule network）[Sabour et al., 2017]，可解释的 R-CNNs [Wu et al., 2017]，以及 InfoGAN [Chen et al., 2016]。 通过人机交互进行语义级的由中到尾的学习。CNN 表征的正确的语义解可以进一步实现弱监督下的神经网络的「由中到尾」学习。第七节介绍了一些通过人机交互来学习新模型的方法 [Zhang et al., 2017b] 以及通过有限的人机交互实现积极有效的问答游戏 [Zhang et al., 2017a]。 综上，CNN 表征可视化是探讨网络表征的最直接的方法。网络可视化还为许多诊断 CNN 表征的方法提供了技术基础。预训练的 CNN 的特征的分离式表征以及对可解释的网络表征的学习给最先进的算法带来了更大的挑战。 论文：Visual Interpretability for Deep Learning: a Survey 论文地址： https://arxiv.org/abs/1802.00614 摘要：本篇论文回顾了目前有关理解神经网络表征和用可解释/分离式中间层表征学习神经网络的新兴方向的研究。尽管深度神经网络在不同的任务中取得了不俗的表现，但是它的可解释性一直是深度神经网络的阿克琉斯之踵。当前，深度神经网络获得了很高的鉴别力，同时它也想黑匣子一样难以解释。我们相信良好的模型可解释性或许会帮助研究人员突破深度学习的瓶颈，例如，从很少的注释中学习，通过人机交互进行语义级别的学习，以及 debug 网络的语义表征。在本文中，我们重点关注卷积神经网络（CNN），并重新审视 CNN 表征的可视化，预训练 CNN 表征的诊断方法，预训练 CNN 表征的分离方法，CNN 的分离式表征学习，以及基于模型可解释性的从中到尾的学习。最后，我们将探讨可解释的人工智能的发展趋势。 2 CNN 表征的可视化 将 CNN 滤波器可视化是探索隐藏在神经元内的视觉模式的最直接方式。网络可视化已经有了各种各样的可视化方法。 首先，基于梯度的方法是网络可视化的主流方法 [Zeiler and Fergus, 2014; Mahendran and Vedaldi, 2015; Simonyan et al., 2013; Springenberg et al., 2015]。 其次，上卷积网络 [Dosovitskiy and Brox, 2016] 是另一种典型 CNN 表征可视化技术。 3 CNN 表征的诊断方法 一些方法超出了 CNN 的可视化和 CNN 表示诊断的范畴，以获得对 CNN 中编码的特征的启发式理解。我们将所有相关研究大致分为以下五个方面。 第一个研究方向是从全局角度分析 CNN 特征。 第二个研究方向是提取通过网络直接输出为标签/属性的图像区域，以解释标签/属性的 CNN 表征。 CNN 特征空间中易受影响点的估计也是网络表征诊断的热门方向。 第四个研究方向是基于网络特征空间的分析来细化网络表征。 4 将 CNN 表征解构成说明图和决策树 图 2 ：不同的输入图像获得的滤波器的 Feature maps [Zhang et al., 2018a]。为了可视化 feature map，该方法将 feature map 中激活单元的感受野映射到图像平面。 图 7：语义级 CNN 预测解释的决策树 [Zhang et al., 2018c]。CNN 学习在顶层的卷积层中运用分离式表征进行目标分类，其中每个过滤器代表一个特定的对象部分。决策树以由粗到精的方式编码隐藏在 CNN 全连接层内的各种决策模式。给定一张输入图像，决策树推断出一个解析树（红线）来定量分析 CNN 预测的基本原理，即物体的哪些部分（或滤波器）用于预测以及该部分（或滤波器）对预测有多大贡献。 5 用可解释/分离式表征学习神经网络 6 网络可解释性的评估标准 图 13：与或图在预训练的 CNN 上衍生为语义分支 [Zhang et al., 2017a]。AOG 将特定的 CNN 单元与某些图像区域相关联。红线表示解析图。 7 由中到尾学习的网络可解释性 图 14：QA 过程的图示 [Zhang et al., 2017a]。（上）该方法选择不可解释的对象进行分类。（下）每个目标对象的问题。 "
185,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738445&idx=2&sn=037c3778aea6badf12f2ecfe6d6ead68&chksm=871acb73b06d4265fe42d8e5102c73f292d81789ca17a7d0cf3e025fda04ca30265e5577bc43&scene=27,教程 | 如何理解KL散度的不对称性,众所周知，多被用于量化分布间的差异的 KL 散度是不对称的。今天我们来聊一聊，两个分布的一对 KL 散度之间究竟有什么不同。 为了讨论这个知识点，我们需要掌握（或者暂且当做已知）的先决知识点有： 1 自信息：符合分布 P 的某一事件 x 出现，传达这条信息所需的最少信息长度为自信息，表达为   2 熵：从分布 P 中随机抽选一个事件，传达这条信息所需的最优平均信息长度为香农熵，表达为  3 交叉熵：用分布 P 的最佳信息传递方式来传达分布 Q 中随机抽选的一个事件，所需的平均信息长度为交叉熵，表达为  4 KL 散度：用分布 P 的最佳信息传递方式来传达分布 Q，比用分布 Q 自己的最佳信息传递方式来传达分布 Q，平均多耗费的信息长度为 KL 散度，表达为 D_p(Q) 或 D_KL(Q||P) ，KL 散度衡量了两个分布之间的差异。 注意，如果表达成  D_p(Q)   形式，要传达的信息所属的分布在括号内；如果表达成  D_KL(Q||P)  形式，要传达的信息所属的分布在前。 新增知识点：D_P(Q) 与 D_Q(P)  有什么不一样？ 公式  D_P(Q)  里一共涉及了两个分布： 要传达的信息来自哪个分布，答案是 Q 信息传递的方式由哪个分布决定，答案是 P 由 KL 散度的公式可知，分布 Q 里可能性越大的事件，对  D_P(Q)  影响力越大。如果想让 D_P(Q)  尽量小，就要优先关注分布 Q 里的常见事件（假设为 x），确保它们在分布 P 里不是特别罕见。 因为一旦事件 x 在分布 P 里罕见，意味着在设计分布 P 的信息传递方式时，没有着重优化传递 x 的成本，传达事件 x 所需的成本，log(1/P(x))  会特别大。所以，当这一套传递方式被用于传达分布 Q 的时候，我们会发现，传达常见事件需要的成本特别大，整体成本也就特别大。 类似地，想让 D_P(Q)  特别小，就要优先考虑分布 P 里那些常见的事件们了。这时，分布 Q 里的常见事件，就不再是我们的关注重点。 下面让我们举一个实际的例子，来自《Deep Learning》一书第 3 章。 假设存在一个真实分布 P，由两个高斯分布混合而成，用蓝线表示。 现在，在不知道分布 P 的信息的情况下，我们做出了一个常见的假设：假设数据符合高斯分布。 当我们尝试用一个普通的高斯分布 Q 来近似分布 P，换言之，尝试让 Q 尽量「贴近」P 的时候，可以选择的目标函数有： 选择不同的目标函数，会产生完全不同的 Q。 如果我们选择目标函数 1，结果会像左图一样。在优化过程中，重要的是分布 P 中的*常见事件*，也就是蓝线的两峰，我们要优先确保它们在分布 Q 里不是特别罕见（信息长度不是特别长）。由于分布 P 里有两个峰值区域，分布 Q 无法偏向任何一个峰值，拉锯的结果是，Q 选择了横亘在分布 P 两个峰值中间。 如果我们选择目标函数 2，结果会像右图一样，重要的是分布 P 中的*罕见事件*（信息长度特别长的那些事件），也就是蓝线的谷底，我们优先确保它们在分布 Q 里不是特别常见。左图里那种，分布 Q 横亘在分布 P 两个峰值中间，是我们最不希望发生的、KL 散度格外大的情况。相反，只有一个峰值的分布 Q 最终会选择贴合分布 P 两个峰值区域中的任意一个。 最后，直觉上，因为 D_Q(P)=H_Q(P)-H(P) ，其中多项式的第二项 H(P) 与分布 Q 完全无关，所以这时候，arg min D_Q(P)  等价于 arg min H_Q(P)。 即，优化 KL 散度与优化交叉熵是等价的。但是，反过来的 D_P(Q)=H_P(Q)-H(Q)  就没有这等好事了。 以上，就是，KL 散度如何衡量分布间的差异，以及不对称的 KL 散度在衡量差异的时候会有什么不同了。 欢迎提问，以及拍砖。 
186,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738445&idx=1&sn=b2dbe3aa45c253e37b4a81a0ba3dc4a0&chksm=871acb73b06d426503929c75ff2c8112e04ed8c0acfc80e03d64f36010da255b90e3ea9e9628&scene=27,学习了！谷歌今日上线基于TensorFlow的机器学习速成课程（中文版）,随着机器学习越来越受到公众的关注，很多初学者希望能快速了解机器学习及前沿技术。而今天谷歌上线了基于 TensorFlow 的机器学习速成课程，它包含 40 多项练习、25 节课程以及 15 个小时的紧凑学习内容。谷歌官方描述为机器学习热爱者的自学指南，且课程资料都是中文书写，课程视频都由机器学习技术转述为中文音频。这对于中文读者来说将会有很大的帮助，当然我们也能选择英文语音以更精确地学习内容。此外，据机器之心了解，这曾是谷歌内部培训工程师的课程，有近万名谷歌员工参与并将学到的东西用在产品的优化和增强上。 课程地址：https://developers.google.cn/machine-learning/crash-course/ 按照该课程所述，读者可能需要初级代数知识，如变量与系数、线性方程组和函数曲线等以理解基本的机器学习模型。此外，读者也需要一些 Python 编程经验，但一般只需要最基础的函数定义、列表/字典、循环和条件表达式等。本课程的实现是基于 Python 和 TensorFlow，不过读者在学习前并不需要任何 TensorFlow 知识。 除了前面所述的两个基本要求外，读者可能还需要准备一些基础知识，当然等真正遇到再去查资料也完全没问题。其实准备工作主要分为数学基础、编程基础和函数库三个部分，我们给各位读者提供机器之心的资源文章合集，以便查阅相关问题。 在数学方面，代数相关的变量、系数、线性方程、对数和 Sigmoid 函数有助于读者了解模型最基本的表达，包括怎么定义的推断过程、如何构建的损失函数以及激活函数等。线性代数相关的矩阵和张量等知识有助于读者理解模型在计算过程中到底代表了什么意思，例如矩阵乘法这种仿射变换在神经网络中代表了神经元的线性组合或全连接。概率论与统计学也是有要求的，不过本课程仅仅需要能知道均值、方差等概念就行。对于微积分，我们只需要了解导数、偏导数和链式法则的基本概念就行，虽然最优化方法需要非常多的数学知识才能明确地推导出流行的优化器表达式，但在基础阶段只需要调用工具就行了。 在 Python 编程与常见第三方库等方面，该课程也只有非常少的要求，掌握基本的操作就行。例如 Python 的列表、字典和元组三大数据结构，还有循环和条件等基本表达式。而需要了解的第三方库也是科学计算方面代表，如 NumPy、Pandas 和 Matplotlib 等。以下是 2017 年机器之心发过的教程，它基本上可以为读者提供足够的学习资料。 灵魂追问 | 教程那么多，你……看完了吗？ 简介： 前提条件和准备工作 机器学习概念： 框架处理（15 分钟）机器学习中的监督学习 深入了解机器学习（20 分钟）什么是损失函数，权重和 bias 是什么 降低损失（60 分钟）两种梯度下降，及对学习率的实验 使用 TensorFlow 基本步骤（60 分钟）不能不懂的 TensorFlow 泛化（15 分钟）什么是过拟合，怎样评价一个模型的好坏，把数据集分成测试和训练两部分 训练及测试集（25 分钟）验证把数据集分成两部分的好处 验证（40 分钟）担心过拟合？在测试和训练集外多弄一个验证集 表示法（65 分钟）特征工程，75% 机器学习工程师的时间都在干的事 特征组合（70 分钟）明白什么是特征组合，怎么用 TensorFlow 实现 正则化：简单性（40 分钟）L2 正则化，学习复杂化和普遍化的取舍 逻辑回归（20 分钟）理解逻辑回归，探索损失函数和正则化 分类（90 分钟）评估一个逻辑回归模型的正确性和精度 正则化：稀松性（45 分钟）L2 的其他种类 介绍神经网络（40 分钟）隐藏层，激活函数 训练神经网络（40 分钟）反向传播 多种类神经网络（50 分钟）理解多类分类器问题，Softmax，在 TensorFlow 中实现 Softmax 结果。 嵌入（80 分钟）什么是嵌入，这是干什么的，怎样用好。 工程： 生产 ML 系统（3 分钟）ML 生产中的宽度 静态 vs. 动态训练（7 分钟）静态和动态训练的优缺点 静态 vs. 动态推断（7 分钟）静态和动态推断的优缺点 数据依赖（14 分钟）理解 ML 中的数据依赖 生活中实际的 ML 例子： 预测癌症（5 分钟） 18 世纪文献（5 分钟） 真实世界方针（2 分钟） 结论： 下一步要学习的内容，推荐了 TensorFlow，Google 的课程深度学习，及 Kaggle 比赛等。 练习题： 大部分练习题的数据是用的 California housing data set 。 测试分成三种，编程练习，检查你的理解和 Playground。 课程特点 这一机器学习速成课程最大的特点是它有完整的中文资料、中文语音和字幕以及中文测试题，它为机器学习初学者提供了最实用的的资料。 如下所示，该课程提供的课件非常适合于国内初学者： 如下所示，该课程提供了很多中文练习，包括编程练习和文本理解的选择题，这非常有助于各位读者检验在视频和资料中学习到的知识。 如下展示了机器学习术语，这一部分分成全面的介绍了机器学习中的术语的含义，非常好懂。 最后，该课程还提供了非常多的中文学习资料或技术博客，这些文本资料同样也是扩展读者知识并从原理上学习新技术的重要保证。 
187,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738513&idx=2&sn=6e4c9ba1df1100e22676c52415089dd9&chksm=871acb2fb06d423966cbaa0ce8b36227bbec0d224e2156cd5b5bf2a27e4bccd4ca2615304750&scene=27,业界 | 实时替换视频背景：谷歌展示全新移动端分割技术,选自Google Blog 作者：  Valentin Bazarevsky、Andrei Tkachenka 机器之心编译 为视频中人物实时替换背景的技术能够催生出很多新类型的应用。谷歌最近提出的机器学习视频分割技术首先被应用在了自家的 YouTube app 上，实现了令人惊艳的效果。同时，由于模型被高度压缩，其在 iPhone 7 这样的移动端设备上也可以达到 100+ FPS 的高帧率。 视频分割是一项广泛使用的技术，电影导演和视频内容创作者可以用该技术将场景中的前景从背景中分离出来，并将两者作为两个不同的视觉层。通过修改或替换背景，创作者可以表达特定的情绪、将人放在有趣的位置或强化信息的冲击力。然而，这项技术的执行在传统上是相当耗时的手工过程（例如，对每一帧图像抠图），或者需要利用带绿幕的摄影棚环境以满足实时背景移除。为了让用户能用摄像头实时创造这种效果，谷歌为手机设计了这种实时抠图技术。 今天，通过将该技术整合到 stories，谷歌宣布将给 YouTube app 带来精确、实时、便携的移动视频分割体验。目前仅限于测试版本，stories 是 YouTube 的新轻量视频格式，是特别为 YouTube 创作者而设计的。该新型分割技术不需要专业设备，让创作者能方便地替换和修改背景，从而轻易地提高视频的制作水准。 在 YouTube stories 中实现神经网络视频分割。 谷歌使用机器学习的卷积神经网络来解决语义分割任务，从而实现该技术。特别地，通过满足以下的需求和约束，研究人员设计了适合手机的网络架构和训练流程： 移动端的解决方案必须是轻量级的，并至少达到当前最佳照片分割模型的 10-30 倍的分割速度。对于实时推断，这样的模型需要达到每秒 30 帧的分割速度。 视频模型需要利用时间冗余度（相邻帧看起来相似），和具备时间一致性（相邻帧得到相似的结果）。 高质量的分割结果需要高质量的标注。 数据集 研究人员标注了成千上万张捕捉了广泛类型的前景姿态和背景环境的图像，以为新的机器学习流程提供高质量的数据。这些标注包括前景元素的像素级精确定位，例如头发、眼镜、脖子、皮肤、嘴唇等；而背景标签普遍能达到人类标注质量的 98%（IOU、Intersection-Over-Union）的交叉验证结果。 网络输入 谷歌设计的分割任务是为每个视频的输入帧（三个通道，RGB）计算二进制掩码，以将前景从背景上分割出来。其中，获得计算掩码在帧上的时间一致性是关键。当前的方法是使用 LSTM 或 GRU 来实现，但对于在移动设备上实时应用来说其计算开销太高了。因此，我们首先将前一帧的计算掩码作为先验知识，并作为第四个通道结合当前的 RGB 输入帧，以获得时间一致性，如下图所示： 原本的帧（左）分离为三种色彩通道，并且和之前的掩码（mask）级联在一起（中间）。这就可以用做输入来训练神经网络而预测当前帧的掩码（右）。 训练过程 在视频分割中，我们需要实现帧到帧的时间连续性，同时也需要考虑时间的不连续性，例如突然出现在相机镜头前的人。为了鲁棒地训练模型而解决这些问题，我们需要以多种方式转换每张图片的标注真值，并将其作为前一帧的掩码： 清空前面的掩码（Mask）：训练网络已正确处理第一帧和场景中的新目标，这将模拟某人出现在相机镜头内的场景。 标注真值掩码的仿射变换：根据 Minor 转换训练神经网络以传播和调整前一帧的掩码，而 Major 转换将训练网络以理解不合适的掩码，并丢弃它们。 转换后的图像：谷歌实现了原版图像的薄板样条平滑（thin plate spline smoothing）以加快相机的移动和旋转。 运作中的实时视频分割。 网络架构 通过修正后的输入、输出，研究人员构建了一个标准的沙漏型分割网络架构，并增加了以下改进： 在新方法中，研究人员通过使用有较大步幅（strides=4）的大卷积核以检测高分辨率 RGB 输入帧的目标特征。卷积层有较少的通道数（在 RGB 作为输入的情况下）从而节约了算力，因此使用较大的卷积核也不会有很大的计算成本。 为了提高速度，研究人员通过较大步幅而积极地采用下采样，并结合跳过连接（如 U-Net）以在上采样中恢复低级特征。对于新的分割模型，它相比于不使用跳过连接的模型要提升 5% 的 IOU。 Hourglass 分割网络 w/ skip 连接 为进一步提高速度，谷歌研究人员优化了默认 ResNet 瓶颈。在 ResNet 论文《Deep Residual Learning for Image Recognition》中，作者将网络中间信道压缩四倍（即将 256 信道通过 64 个不同的卷积核压缩为 64 个）。然而，研究人员注意到在更为激进地压缩至 16 或 32 个信道后，质量并没有显著下降。 ResNet 瓶颈和大压缩率 为了细化和提高边缘的准确性，谷歌研究人员为神经网络上层加入了一些 DenseNet 层，其分辨率与 Neual Matting（见论文《Deep Image Matting》）相同。这种技术让模型的整体质量提高了 0.5% IOU，但却显著提高了分割的质量。 这些修改的最终结果是新的神经网络速度很快，并适用于移动端设备。使用高准确率设置时（在验证数据集上达到 94.8% IOU），它在 iPhone 7 上可以达到 100+ FPS，而在 Pixel 2 上可以达到 40+ FPS，在 YouTube stories 中能够提供各种平滑的展示效果。 谷歌下一步的目标是使用 YouTube 中的 stories 来测试新技术的效果。随着新方法的改进和扩展，这种分割技术将会适用于更多场景，谷歌计划在未来将其应用于增强现实服务中。 原文地址：https://research.googleblog.com/2018/03/mobile-real-time-video-segmentation.html 
188,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738397&idx=3&sn=97496ebbfdc0c27c8a1b14dbd00b3a30&chksm=871acaa3b06d43b5a4a728297ae43d625a4b1da448bb1c7b032cd0628172b357a457eb49adb0&scene=27,前沿 | 受AlphaGo启发，AI重建量子系统新方法登上Nature Physics,"Nature等 这是第一次，物理学家证明了机器学习可以利用相对较少的实验测量结果来重建量子系统。这种方法可以让科学家们大大减少探索微观世界所需的时间——相比传统的蛮力方法有指数级的速度提升。此前需要数千年才能完成的重建任务现在只需要几个小时就可完成了。相关论文于 2 月 26 日发表在 Nature Physics 上，机器之心简单编译了该论文，感兴趣的读者参见文末。 「这项研究将会让量子计算机和其他相关量子技术的应用受益，」研究者在论文中写道。 「我们证明了机器智能可以使用非常精巧的方式捕捉量子系统的本质，」论文共同作者，来自纽约 Flatiron Institute 量子物理计算中心的 Giuseppe Carleo 介绍道。「我们现在可以有效地扩展实验的规模了。」 Carleo 是在苏黎世联邦理工学院（ETH Zurich）担任讲师时提出这项研究的，他表示该工作受到了 DeepMind 著名围棋程序 AlphaGo 的启发。后者曾在 2016 年击败了前世界冠军李世石并轰动世界。「AlphaGo 非常强大，」Carleo 说道「这让我们不得不思考它背后的技术能否被用于量子物理的研究中去。」 在微观物理世界中，电子这样的粒子可以以多个不同的能级存在，每个能级有特定的出现概率。每个电子都可以呈现自旋上或自旋下，类似于著名思想实验「薛定谔的猫」中的死或生。在量子领域中，未经观察的系统并不具有其中的任何一种状态（量子态）。取而代之的是，该系统会被认为具备可能出现的任何一种状态。 当被测量后，系统会坍缩为其中一个状态——就像薛定谔的猫在你打开黑箱之后会表现为活着或死去。这种量子机制的诡异特性意味着你无法通过单次实验观测探究整个系统的复杂性。实验者通常只能通过一次次地测量的方法才能最终确定整个系统的状态。 这种方法在包含几个粒子的简单系统很好用，但是「粒子多了事情就复杂了」Carleo 说。当粒子的数量增加时，复杂度突増。如果只考虑每一个电子只有自旋上或自旋下的量子态，一个有 5 个电子的系统将有 32 种可能的量子态。一个有 100 个电子的系统可以有超过 1 百万*千亿*千亿的量子态数目。 粒子纠缠进一步使问题变得复杂。通过量子纠缠，独立的粒子变成了纠缠的而且不能被当成纯粹的分离的实体，即使当物理上它们分开的时候。这个纠缠改变了不同量子态的概率。 所以，传统的办法是无法处理复杂的量子系统的。 滑铁卢大学的 Giacomo Torlai 以及加拿大 Perimeter Institute 的 Carleo 和他的同事通过开发机器学习技术绕过了这些限制。这些研究员把量子系统的实验性测量输入到了一个以人工神经网络为基础的软件工具中。这个软件学习并尝试去模拟这些系统的行为。一旦软件模拟了足够的数据，它就可以重建整个量子系统。 研究员用以不同的量子系统样例为基础的仿真实验性数据集来测试。在这些测试中，这个软件的表现远远超越传统的方法。对于 8 个电子，每一个都是自旋向上或者自旋向下，这个软件只经过 100 次左右的测量就可以重建系统。相比之下，一个传统的靠蛮力的方法需要大约 1 百万次测量才能达到一样程度的正确率。这项新的技术也可以处理更大的系统。研究员称，这项技术可以帮助科学家验证一台量子计算机是正确配置的，以及让任何量子软件按照要求运行。 用紧凑的人工神经网络捕捉复杂量子系统的本质有其他更深远的影响。量子物理计算中心（Center for Computational Quantum Physics）的主任 Andrew Millis 注意到以这项技术为中心，可以继续开发分析交互量子系统的新方法，并可结合其他量子物理启发的机器学习方法。 除了基础研究应用以外，Carleo 说融合机器学习和量子物理的思想可以进一步改良人工智能的通用应用。「我们可以将该方法扩展到其他应用场景里」，他说，「某一天我们也许会有一个自动驾驶车是由量子机制所启发的，谁知道呢。」 论文：Neural-network quantum state tomography 论文地址：https://www.nature.com/articles/s41567-018-0048-5 摘要：随着研究的量子系统变得越来越复杂，其实验实现将变得越加困难，因而需要开发普遍性的理论方法来验证和完整地探索量子资源。量子态断层摄影（QST）技术可从简单的测量中重构完整的量子态，因而是获取对系统的可靠分析的关键技术。然而，QST 的蛮力计算方法需要大量的计算资源，这使其只能应用于小规模系统。我们在本文中展示机器学习如何能处理超过 100 个量子比特的高度纠缠量子态的 QST，并获得相当高的准确率。我们证明了机器学习方法可以用于重构量子多体系统的物理量，而这在传统方法上是非常困难的，涉及海量的计算（例如纠缠熵计算），而我们仅需要简单的实验上可行的测量就可以做到。该方法可用于改善当前和未来的量子设备，如量子计算机、冷原子量子模拟器等。 近期由于数据科学的快速发展，机器学习技术也被研究者用于解决物理学中的基本问题。去年就有论文发表了应用机器学习分析量子多体系统的研究成果，例如物相分类 [11-13]、模拟量子系统 [14] 等。 本文考虑的问题是重构一般性的多体系统的目标波函数 Ψ(x)≡ ⟨ x∣Ψ ⟩，其中 x 是某个参考测量基（例如，自旋 1/2 的σ_z）。我们将用人工神经网络表征下式的多体量子态： 其中网络 p_λ(x) 和 ϕ_µ(x) 分别代表量子态的振幅和相位，Z_λ 是归一化常数。本文中使用的神经网络架构基于受限玻尔兹曼机（RBM）。该架构有二值神经元构成的一个可见层（描述量子比特）和一个隐藏层（和可见层全连接）。RBM 能为多体量子态提供紧凑的变分表征，可以保留 non-trivial 的关联，例如高纠缠度或拓扑特征 [19-24]。具体来说，取 p_λ 作为 RBM 网络（λ是参数），以及另一个独立的 RBM 网络 p_µ（µ是参数）来对相位ϕ_µ=log p_µ(x) 建模。 本文使用的 QST 机器学习方法的执行步骤为：首先，RBM 在由一系列独立的态密度|Ψ(x^|b|)|^2 测量构成的数据集上训练（这些量子态由 n 体量子系统的基矢 {x^|b|} 构成）。这一阶段将优化网络参数（λ,µ）以最大化数据集似然度，使得|Ψ_λ,µ(x^|b|)|^2≃|Ψ(x^|b|)|^2，即让网络表征的量子态逼近真实的量子态。一旦完成训练，Ψ_λ,μ(x) 就可以逼近波函数的振幅和相位，从而重构目标量子态。重构的准确率可以通过增加 RBM 中的隐藏神经元数量 M 而系统地提升。本文的 QST 方法的关键之处在于它只需要使用原始数据（即来自单次测量的多个实验快照），而不需要估计算符的期望值。这种设定意味着可以避免在对算符平均值的评估中必须获得低水平内在高斯噪声的过程。 "
189,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738397&idx=2&sn=293e6b582559a2aa9a75f59f93b3a928&chksm=871acaa3b06d43b546d12fb5ad9bca535c09bf2a7febc51d8b0e7ccc142fc338023f4d0b8a87&scene=27,业界 | 前微软城市计算负责人郑宇出任京东金融首席数据科学家,"2 月 28 日，京东金融正式宣布任命郑宇先生为京东金融副总裁、首席数据科学家，担任城市计算事业部总经理及城市计算研究院院长。   京东金融副总裁、首席数据科学家郑宇 郑宇先生在 2013 年被《MIT 科技评论》评为全球杰出青年创新者（TR35）；2014 年，由于他主导的城市计算具有巨大的商业前景和改变行业格局的潜力，他被《财富》评为中国 40 位 40 岁以下商界精英；2016 年，他获评为美国计算机学会杰出科学家；2017 年在乌镇互联网大会上被评为中国 AI 英雄风云榜十大技术创新人物。 在加盟京东金融之前，郑宇先生是微软亚洲研究院城市计算领域的负责人，他提出了「城市计算」的理念，是城市计算领域的先驱和奠基人。城市计算作为一个新兴学科已经在国际上获得广泛的认可和关注。 过去的近 12 年时间里，郑宇博士发表高质量国际论文百余篇，被引用 16,000 余次，其中多篇论文成为城市计算领域奠基性的论文。郑宇还担任人工智能国际期刊《ACM TIST》的主编，该刊近五年影响因子 10.14，在所有 ACM（美国计算机学会）期刊中排名第一。同时，郑宇还是上海交通大学、香港科技大学和香港理工大学等多所知名大学的讲座教授、客座教授和博士生导师。他还担任大数据领域知名国际会议 ICDE2014 和 CIKM2017 的工业界主席，促进了该领域学界和工业界的融合。郑宇拥有 24 项国际发明专利，他主持研发的 Urban Air 首次利用大数据和人工智能技术来监测和预报细粒度空气质量，该服务覆盖中国 300 多个城市，并被国家环保部采用。2016 年，他主持了城市大数据平台的设计和实施，并成功在中国大数据示范基地贵阳市部署。 京东方面表示，随着郑宇的加入，京东金融城市计算业务正式开始运营，城市计算将利用大数据和人工智能技术服务于政府和大型国有企业，致力于解决城市里的交通、规划、环境、能耗、商业和公共安全等痛点，提高大型企业产能和业务效率。成立四年多来，京东金融一直致力于利用自身技术和数据优势，为金融行业提供数字化的服务，使金融机构降低成本、提高效率的同时增加收入，助力金融机构实现人、货、场的数字化，从而实现无界金融。城市计算事业部的成立，意味着京东金融作为一家科技公司，正在将技术应用领域扩展到金融行业之外的更多领域。 其实，我们可以看到，最近一段时期京东集团和京东金融在全球顶级技术人才引入方面动作频频——去年 9 月，AI 领域权威科学家周伯文出任京东集团副总裁，负责京东 AI 平台与研究部相关业务；10 月，前亚马逊首席科学家薄列峰加盟京东金融，出任 AI 实验室首席科学家；12 月，美国伊利诺伊大学香槟分校（UIUC) 计算机科学学院彭健正式加入京东金融，担任 AI 实验室首席顾问；今年 1 月，加拿大西蒙弗雷泽大学计算科学学院教授裴健出任京东集团副总裁，负责大数据平台与产品研发部。 拓展阅读： "
190,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738397&idx=4&sn=3a84c8c6ae7101bfd6644bbfb848dcb2&chksm=871acaa3b06d43b572c0f1b6e402eb65f637367994e79933b9914e396da7d55d71e183fbeb32&scene=27,资源 | 微软开源MMdnn：实现多个框架之间的模型转换,"GitHub 近日，微软开源 MMdnn，可用于转换、可视化和诊断深度神经网络模型的全面、跨框架解决方案，目前支持 Caffe、Keras、MXNet、CNTK 等框架。 项目地址：https://github.com/Microsoft/MMdnn MMdnn 是一个用于转换、可视化和诊断深度神经网络模型的综合性、跨框架的解决方案。MMdnn 中的「MM」代表模型管理，「dnn」是「deep neural network」（深度神经网络）的缩写。 MMdnn 可将一个框架训练的 DNN 模型转换到其他框架可用。其主要特征包括： 模型文件转换器，转换 DNN 模型使之适合不同框架； 模型代码块生成器，生成适合不同框架的训练或推断代码块； 模型可视化，针对不同框架可视化 DNN 网络架构和参数； 模型兼容性测试（正在进行）。 安装 通过以下命令行获取稳定版的 MMdnn： 或者通过以下命令尝试最新版本： 模型转换 业界和学界存在大量现有框架，适合开发者和研究者来设计模型，每个框架具备自己的网络结构定义和模型保存格式。框架之间的差距阻碍了模型的交互操作。 我们提供一个模型转换器，帮助开发者通过中间表征格式转换模型，以适合不同框架。 支持框架 每个支持的框架都有详细的 README 文档，它们可以在以下conversion件夹找到。 地址：https://github.com/Microsoft/MMdnn/tree/master/mmdnn/conversion Caffe Keras MXNet TensorFlow（实验阶段，强烈建议先阅读 README） Microsoft Cognitive Toolkit (CNTK)  PyTorch CoreML（实验阶段） 测试模型 我们在部分 ImageNet 模型上对当前支持的框架间模型转换功能进行了测试。 正在测试的框架： PyTorch CNTK Caffe2 ONNX 正在测试的模型： RNN 图像风格迁移 目标检测 模型可视化 你可以使用 MMdnn 模型可视化工具（http://vis.mmdnn.com/），提交自己的 IR json 文件进行模型可视化。为了运行下面的命令行，你需要使用喜欢的包管理器安装 requests、Keras、TensorFlow。 使用 Keras inception_v3 模型作为示例。 1. 下载预训练模型： 2. 将预训练模型文件转换成中间表征格式： 3. 打开 MMdnn 模型可视化工具地址（http://mmdnn.eastasia.cloudapp.azure.com:8080/），选择文件 keras_inception_v3.json。 社区支持 本项目仍在继续开发与探索，它需要各位读者完善中间表征与支持的框架。因此，该项目的作者表示他非常希望有开发者能提供新的运算或扩展。 中间表征：中间表征在 protobuf 二进制文件中储存网络架构，在 NumPynative 格式中储存预训练权重。此外，目前 IR 权重数据使用的是 NHWC 格式。中间表征的细节请查看 ops.txt 和 graph.proto 文件。 框架：我们正在扩展到其它框架版本和可视化工具，例如 Caffe2、PyTorch 和 CoreML 等。此外，本项目也在积极开发 RNN 相关的操作方法。 以下是该项目实现框架转换的基本案例，其中包括官方的教程和用户提供的各种案例，机器之心简要介绍了官方 Keras 到 CNTK 的转换教程。 官方教程： Keras ""inception_v3"" to CNTK 用户案例： MXNet ""resnet 152 11k"" to PyTorch MXNet ""resnext"" to Keras Tensorflow ""resnet 101"" to PyTorch Tensorflow ""mnist mlp model"" to CNTK Tensorflow ""Inception_v3"" to MXNet Caffe ""AlexNet"" to Tensorflow Caffe ""inception_v4"" to Tensorflow Caffe ""VGG16_SOD"" to Tensorflow Caffe ""Squeezenet v1.1"" to CNTK Keras「inception_v3」模型到 CNTK 的转换 1. 安装 Keras 和 CNTK 2. 准备 Keras 模型。以下示例将首先下载预训练模型，然后使用简单的模型抽取器从 Keras 应用中获取模型，抽取器将抽取 Keras 模型架构和权重。 架构文件 imagenet_inception_v3.json 和权重文件 imagenet_inception_v3.h5 会下载至当前工作目录。 3. 将预训练模型文件转换为中间表征 以上的命令会将 imagenet_inception_v3.json 作为神经网络架构的描述文件，imagenet_inception_v3.h5 作为预训练权重。然后计算出中间表征文件 converted.json 用于可视化，计算出 converted.proto 和 converted.npy 以进一步转换为其它框架。 4. 转换 IR 文件为 CNTK 模型 你将得到文件 converted_cntk.py，包括构建 Inception V3 网络的原始 CNTK 代码。 经过这三步，你已经将预训练 Keras Inception_v3 模型转换成 CNTK 网络文件 converted_cntk.py 和权重文件 converted.npy。你可以用这两个文件调整训练或推断。 5. 转存原始 CNTK 模型 CNTK 可直接加载文件 cntk_inception_v3.dnn。 "
191,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738397&idx=1&sn=661842f3b590ecec0006353962b8292c&chksm=871acaa3b06d43b5d238585773579b769f5ba132dc0886aa94f8f3cfeb9012c6064d2eb87ec2&scene=27,机器学习+区块链：算法商店Algorithmia推出DanKu，用以太坊合约交易ML模型,"Algorithmia 近日，谷歌投资的「算法商店」公司 Algorithmia 借助区块链技术，推出了一种去信任的机器学习合约，可在公共区块链比如以太坊（Ethereum）上评估和购买机器学习模型，进而创建了一个去中心化和去信任的市场，从而使得机器学习从业者有机会直接把其技能转化为现实收益，同时也允许参与者或组织从全世界征求机器学习模型。 机器学习算法的发展速度令人惊奇，但不一定在社区之中越发普及。这正是 Algorithmia 推出 DanKu 的原因所在，一个在公共区块链比如以太坊（Ethereum）上评估和购买机器学习模型的基于区块链的新协议。DanKu 可以使每个人获取高质量、客观衡量的机器学习模型。Algorithmia 相信算法的广泛获取和部署是未来 AI 走向平衡的基石，DanKu 是走向这一愿景的其中一步。 DanKu 协议利用了基于智能合约的区块链技术。合约允许所有人向可提供最佳训练的机器学习模型的所有人发布一个数据集、评估函数和货币奖励。参与者训练深度神经网络建模数据，并把其已训练的网络提交至区块链。区块链执行这些神经网络模型以评估这些提交，并保证支付给到最佳的模型。 为了交易机器学习模型，合约创建了一个去中心化和去信任的市场，从而使得机器学习从业者有机会直接把其技能转化为现实收益，同时也允许参与者或组织从全世界征求机器学习模型。这将激励更优机器学习模型的创建，使得 AI 在公司和软件智能体中更普及。拥有数据集的任何人，包括软件智能体，皆可创建合约。 Algorithmia 同时也发布了首个针对机器学习问题的 DanKu 竞赛，更多信息，详见本文最后一节。 背景 2018 来临之际，人工智能和区块链继续霸占着科技新闻头条。2017 早期，Algorithmia 询问自己能否把两项技术融合一起来解决机器学习问题。在所有的想法中，我们发现我们不是第一组人想把区块链和机器学习技术结合的人。 在所有为各种问题提供的区块链+机器学习的答案中，我们立刻注意到了想法的多样性。一个非常好的例子是 OpenMined，它允许你在你不能获取的数据上训练你的模型。 把区块链和 AI 放在一起像是在吸引眼球，我们决定在早期只展示不宣传，从而使我们聚焦在一个具体的问题定义上。 DanKu 诞生背后的思想：通过去信任机器学习合约，实现在以太坊上交易机器学习模型。 它是如何工作的？ 可以将 DanKu 协议分成四个步骤来描述： 1. Bob 创建了一个 DanKu 新合约。他提交了一个数据集、一个评估标准和合约的奖励金额。 2. 机器学习从业者 Alice 下载了 Bob 提交的数据集，并独立地工作来训练一个机器学习模型。在成功地训练好模型之后，Alice 提交了她的模型到合约（即区块链）上。其他类似 Alice 的参与者也可以提交他们的模型。 3. 在提交阶段结束之后，Bob 给出测试数据集。测试数据集将用于评估提交的模型。 4. 在未来某个时间，区块链将评估提交的模型，并为获胜的模型支付奖金。如果没有模型能满足标准，奖金将退还给 Bob。 看！Bob 和其他参与者刚刚以去信任的方式交易了机器学习模型。这个合约也在区块链上运行了一个完全可用的机器学习模型！酷不酷！ 注意：为了确保竞争的公平和信任，还需要执行更多的步骤，详情请参考白皮书：https://algorithmia.com/static/research/MLBlockchain/Machine_Learning_Models_on_the_Ethereum_Blockchain.pdf 在区块链上跑第一个神经网络 为了演示，我们决定写一个 DanKu 合约来实现一个神经网络。Solidity 是以太坊合约用的编程语言，它不是用通常的方式来设计机器学习模型的。它没有一个数学库，甚至没有浮点数。以太坊也在跑代码时有计算花费高的问题。这是软件工程的噩梦。 最初，我们以简单的东西开始：一个线性模型，用没有隐藏层的神经网络开始做。在这之后，我们允许上传者来指定任何简单的网络架构，如果没有计算开销太高的问题就应该可行。 我们先在本地开发所有事情，直到准备好在实际的区块链上测试。我们在以太坊 Ropsten 测试网上开始测试，之后将在以太坊区块链上测试，确保它能真正的工作。 在发布不久，就有 2.2 万台机器在以太坊上跑了第一个神经网络。那些在挖矿人看起来像机器代码的东西实际上就是神经网络。 DanKu 如何改变游戏规则 看到第一个 DanKu 合约出现是一件很令人激动的事情。但正如大多数新技术一样，DanKu 也会造成某些规则的瓦解。由于 DanKu 协议不需要信用机制，它排除了用中间人协助交换机器学习模型的需要。这种游戏规则的改变将进一步为多数人降低进入机器学习领域的门槛，有可能让开源机器模型的数量小幅增加。 这种协议还有可能用于为癌症研究等项目众筹资金。大学和研究团队可以为某些开放性问题如蛋白质折叠等创建协议。任何人都能直接为该项目贡献资金。这将吸引更多的机器学习/生物信息学从业者参与进来，从而让这些问题更有希望得到解决。该协议可以确保奖金将直接授予解决问题的人或团队。为医疗研究贡献资金的方式将变得完全不同。 DanKu 合约也有可能为 GPU 挖矿人的套利提供机会。如果存在利润的话，GPU 挖矿田/池将可能转换为机器学习训练过程。这些池可能由数据科学家管理，他们将尝试解决这些机器学习问题。然后，合约奖金将在数据科学家和 GPU 提供者之间瓜分。 其它有趣的应用将在金融领域。如果提交的模型能得到具体的金融方案，那么创建 DanKu 合约并将其金融化将变得更简单。由于评估这些预测模型的价值将变得更加容易，也可以为 DanKu 合约确定合适的价格。 DanKu 还给 AI 系统创造了自我提升的机会。当 AI 系统遇到新的数据时，它可以通过合约将需求释放出去，以自动化和无缝连接的方式提升自身。由于加密支付和交易对任何人都是开放的，DanKu 对加密货币的使用也使其成为自我提升的很有用的方式。其实，它就是另一种 API 端点。 下一步 目前，Algorithmia 还没有给出 DanKu 的明确发展路线图，但该公司认为这一项目仍有很多提升空间。以太坊的改进、合约的进一步优化以及协议设计提升都可以让 DanKu 合约不断提升。随着改进的深入，DanKu 将在未来支持更多的机器学习模型。 首次公开 DanKu 合约竞赛 随着协议的宣布，第一个公开的 DanKu 合约也已创建完成： 第一个竞赛以 2016 年美国总统选举在每个郡的投票数据为数据集。每个郡县有 3 个数据点：经度、纬度以及被选上的候选人。 举个例子，数据点可以是这样：[047606200, 122332100, 0]。 其中前两个数字代表西雅图的经纬度， 第三个数字，0 代表民主党候选人，1 代表共和党候选人。 本次竞赛中有 500 个数据点，其中数据集的 80% 内容用于训练，20% 用于验证。为了保证竞赛的公平性，在合约中这 500 个数据点是随机选择的。 合约的参与者需要训练一个简单的前向神经网络，他们可以根据神经网络层数、神经元与偏差的结构来定义网络。在训练之后，合约参与者需要提交他们的网络定义、权重和偏差。 DanKu 合约在 block 5121944（Feb 19th, 2018）上初始化。评估审查会寻找至少 50% 正确率的模型，胜者会得到 5 个以太币（ETH）。 本次竞赛有一份参赛指南：https://github.com/algorithmiaio/danku/tree/master/competition Algorithmia 表示，在竞赛结束后，获胜模型将会被部署到 Algorithmia 市场上。 "
192,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738356&idx=5&sn=fc21272132ab339b9690bf875584d0b7&chksm=871acacab06d43dcb94a1a53706ca998ca46259f8ab77f9f72d47ad073856a35c4c0be8e5c68&scene=27,学界 | CVPR 2018接收论文公布，上海交通大学6篇论文简介,不久之前，CVPR 2018 论文接收列表公布。据机器之心了解，上海交通大学电子系人工智能实验室倪冰冰教授课题组有 6 篇论文入选，本文对这几篇论文做了简介，更多详细内容可通过论文网盘链接下载查看。 CVPR 2018 论文接收列表：http://cvpr2018.thecvf.com/files/cvpr_2018_final_accept_list.txt Paper 1：《Fine-grained Video Captioning for Sports Narrative》 细粒度视频描述——体育视频自动解说 网盘链接：https://pan.baidu.com/s/1miUzoCC 视频描述方向的研究在近段时间取得了较大的进展，但是一直都停留在粗略的视频内容讲述上，没有做到对于人物交互关系和动作细节的细致描述，而这些恰恰都是体育视频中非常重要的部分。在这篇 CVPR 工作中，作者提出了一个全新的细粒度视频描述课题，做出了一个对应的体育视频细粒度描述数据集，并用一个完善的视频描述网络解决了该问题，实现了国际首次人工智能体育视频自动解说。 在文章中，作者通过一个时域-空域定位子网络来进行动作片段的分割和球员角色的定位；通过一个引入骨骼信息的细粒度动作识别子网络来精确地识别球员在高速运动中做出的细小动作；再通过一个群体交互子网络来构建球员间的交互关系。通过以上三个子网络捕获充足全面的视频特征，从而输出准确的细粒度视频描述语句。 此外，文章中提出的细粒度体育视频自动解说数据集（FSN dataset）也将会在不久后公开供科研使用，以促进细粒度视频描述领域的技术发展。   Paper 2：《Structure Preserving Video Prediction》 面向保持结构一致性的视频预测模型 网盘链接：https://pan.baidu.com/s/1kWUb4c3 像素级的视频生成一直是计算机视觉领域的热点问题。过去的方法一直试图解决所预测的视频中存在的运动模糊问题。这个问题一方面由随时间增加所带来的累计误差引起，另一方面由于像素级的视频生成的解空间非常巨大。这里将像素级的视频生成一直存在的两个问题总结如下： 静态结构预测损失 这个问题来源于预测具有固定结构的场景，城市景观中的交通标志、树木等任务。这些静态结构的运动常常是由相机运动引起的。现有方法的预测结构大多不能保持原始对象结构，例如物体的边缘结构信息。 动态结构预测损失 虽然最近的一些工作可以预测一般粗粒度运动。但是一般不能精确预测精细的局部运动，如人体关节运动。 论文提出的视频预测模型图示。该架构使用论文提出的多频带分析和时变卷积核技术，能够更大限度的利用输入信息和更灵活的应对视频内容的动态变化，使得更精确地预测像素级的视频内容成为可能。 在这篇文章中，作者提出基于多频带分析和时变卷积核的视频预测模型来解决上述两个问题。一方面作者将输入视频分解为多个频率分量分别进行处理，以求从原始视频中获取尽可能多的物体静态结构信息，称之为多频带分析；另一方面作者利用输入视频帧来生成最终预测模型中的卷积核，以求能够更灵活的应对动态结构预测任务，称之为时变卷集核。两个方法分别较好地解决了上述两个问题，显著提高了视频预测精度。     Paper 3：《Multiple Granularity Group Interaction Prediction》 基于多粒度的群体交互预测 网盘链接：https://pan.baidu.com/s/1i6HovGh 多粒度群体交互预测框架。首先我们把骨架序列处理成两个不同粒度的信息，分别代表整体轨迹运动和肢体细节运动。基于 seq2seq 预测结构，我们设计了同粒度间信息交互子网络来考虑群体之间的相互影响，以及不同粒度间的信息交互子网络来促进两个粒度上信息的交互，更准确地预测结果。最后整合预测出的两个粒度信息，展示群体交互预测结果。 当前大多数人体活动分析（识别或预测）的工作仅关注于单个粒度，例如在粗粒度层次对整体运动进行建模（轨迹预测）；或者在细粒度层次对肢体细节运动进行建模（骨骼动作预测）。相反，在这项工作中，作者提出了多粒度群体交互预测网络，能够同时考虑两个粒度上的信息（整体轨迹和细节动作）。首先对于每个人的骨架运动序列，作者把它处理成能够分别代表轨迹运动和肢体运动的两个粒度上的运动序列。对于每个粒度上的信息，基于 seq2seq 网络结构，作者设计了同粒度间的交互网络，在预测每个人这个粒度上的运动信息时能够考虑其他人的运动信息。同时，基于双向 LSTM，作者设计了不同粒度间的信息交互网络，来促进每个人的不同粒度上信息的交互，更准确地预测未来轨迹和细节动作。最后作者把两个粒度上的信息整合在一起，多景观式地展示预测的群体交互。此方法在 SBU 和 Choi's New Dataset 数据集上都取得了目前最佳效果。   Paper 4：《pose transferrable person re-identification》 姿态可迁移行人再识别 网盘链接：https://pan.baidu.com/s/1nwFetDZ 行人再识别旨在解决跨时空匹配行人的问题，其在智能安防领域有极大的应用价值。由于行人姿态、外观、光照、遮挡等因素的影响，行人再识别仍然是一项极具挑战性的任务。为了解决行人姿态变化丰富导致模型难以在有限训练集下获得良好性能的问题，作者在这篇 CVPR 的工作中提出了一个姿态可迁移的行人再识别模型。 假设给定大小两个数据集，分别称为 A 和 B。其中数据集 A 中覆盖的行人姿态少，而 B 中的样本包含丰富的姿态。作者提出将数据集 A 中的图片样本与数据集 B 中样本的骨架进行配对，并通过 Skeleton-to-Image 模型生成与 A 中样本共享身份信息且与 B 中样本共享姿态的新样本集 C。为了提高生成样本的质量，作者提出了一个与对抗生成网络中的判别器平行的向导模块。向导模块是一个预训练好的行人再识别模型，用于指导生成器生成包含更丰富身份信息、更适应行人再识别任务的样本。在得到了生成样本集 C 后，作者将其与数据集 A 混合并通过平滑机制分配样本权重后训练模型。实验结果表明此方法能够与其他高性能方法 (包括特征学习、度量学习类方法) 结合并进一步提升它们的性能。   Paper 5：《Crowd Counting via Adversarial Cross-Scale Consistency Pursuit》 基于对抗跨尺度一致性追求的人群计数方法 网盘链接：https://pan.baidu.com/s/1mjPpKqG 作者提出了一个新的人群计数的网络结构 Adversarial Cross-Scale Consistency Pursuit Network，在四个公开的人群计数数据集上刷新了目前国际最佳的计数精度。人群计数任务是一个极具挑战的任务，原因在于其存在场景变化跨度大、目标空间尺度变化大、人群之间存在严重遮挡等困难。现有的方法存在以下两个缺陷：一、由不同大小的卷积子构成的多通道卷积网络融合得到的图像多尺度特征，再经传统的欧式损失（L1/L2）回归用来计数的人群密度图会导致密度图模糊，同时由于在网络中使用池化层，大大降低了密度图的分辨率，给最终的计数带来误差；二、输入一张图像计算得出的人数与将此图像分割成 4 份分别输入得到的总人数存在差异，此即为跨尺度统计不一致带来的误差。 针对以上两点，作者提出了基于生成对抗网络的跨尺度结构模型，其中对抗损失的引入使得生成的密度图更加尖锐，U-net 结构的生成网络保证了密度图的高分辨率，同时跨尺度一致性正则子约束了图像间的跨尺度误差。因此，该提出的模型最终能生成质量好分辨率高的人群密度图，从而获得更高的人群计数精度。   Paper 6：《Scale-Transferrable Object Detection》 基于尺度变换模块的物体检测器 网盘链接：https://pan.baidu.com/s/1i6Yjvpz 作者构建了一个类似于 SSD 的一阶段物体检测器，称之为 STDN（尺度转移检测网络）。与 SSD 物体检测方法相比主要有两点不同，一是基础网络使用的是 DenseNet，二是作者使用了一个尺度转移模块来获得不同分辨率的特征图，这些特征图被用来做物体检测。这个尺度转移模块由 mean pooling 层和像素重排层构成。Mean pooling 层用来获得低分辨率的特征图；像素重排层通过压缩特征图的通道数来扩大特征图的分辨率，没有额外的计算开销。尺度转移模块可以直接嵌入到 DenseNet 网络中，不需要在 DenseNet 网络之后添加额外的层就能获得多尺度的特征图。而且像素重排层可以有效地压缩 DenseNet 网络特征图的通道数，从而减少之后卷积层的参数数量。作者在 pascal voc07 和 coco 数据集上取得了不错的检测性能，对构建开销较小的物体检测器具有一定的启发意义。 注：上海交通大学电子系人工智能实验室由倪冰冰教授、徐奕教授领衔，杨小康教授、张文军教授指导  
193,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738356&idx=2&sn=381fdd93dc1858580143ee2dff9cf304&chksm=871acacab06d43dcee1d41ff6a8c7b00c1709928a2c1c4f3c252611f6d079b13cdb4339e1fd4&scene=27,业界 | OpenAI发布8个仿真机器人环境和HER实现：可用于训练实体机器人模型,"OpenAI 本文发布八个仿真机器人环境和 Hindsight Experience Replay 的基线实现，这是过去一年的成果总结作者已用这些环境来训练实体机器人用到的模型，并同时发布乐一系列的机器人研究需求。 这次的发布包括了四个用到了 Fetch 研究平台（Fetch research platform）的环境和四个用到了 ShadowHand 机器人的平台。这些平台中包含的改造任务比 Gym 上现有的 MuJoCo 持续控制环境要难一点，所有的 MuJoCo 任务都可以简单地被最近发布的算法比如 PPO 解决。此外，我们新发布的环境使用真实的机器人模型，并需要智能体解决实际问题。 该项目发布了 8 个 Gym 机器人环境，使用的是 MuJoCo 物理模拟器。这些环境包括： Gym 是 OpenAI 发布的用于开发和比较强化学习算法的工具包。它可以教智能体很多事情，比如行走、跑动甚至玩乒乓球等。 Fetch ShadowHand （左图）HandManipulateEgg-v0：ShadowHand 必须去操纵一个鸡蛋直到它到达了指定的地点和姿势。 （右图）HandManipulatePen-v0：ShadowHand 必须去操纵一支笔直到它到达了指定的地点和姿势。 所有新任务都有「目标」这一概念，比如幻灯片任务中冰球的期望位置或手块操纵任务中块的期望方向。默认情况下如果期望目标未实现，所有环境使用-1 的稀疏奖励，如果目标达成则使用 0。这与一系列旧的 Gym 连续控制问题中使用的形状奖励形成了鲜明对比，比如带有形状奖励的 Walker2d-v2。 我们也为每个环境引入了带有紧密奖励的变体。但是，我们相信稀疏奖励在机器人应用中更为现实，并鼓励每个人使用稀疏奖励变体。 除却这些新的机器人环境，我们也给出了 Hindsight Experience Replay（HER）的代码，它是一个可从失败中汲取教训的强化学习算法。我们的结果表明 HER 通过仅有的稀疏奖励可从绝大多数新机器人问题中习得成功的策略。下面我们也展示了一些未来研究的潜在方向，可以进一步提升 HER 在这些任务上的表现。 要理解 HER，我们需要先看一看 FetchSlide 的内容，一个我们要学习去滑动在桌子上的冰球然后打击目标的任务。我们第一次尝试非常可能失败。除非我们非常走运，后面几次同样也会失败。一般强化学习算法不会从这样的经验学习什么，因为他们的奖励（reward）是固定值（在这个案例中是-1），这样的奖励不包含学习信号，从而算法不会去学习。 HER 形式化的关键是人在直觉上的行动： 即使我们在任何特定的目标上还没有成功，但至少实现了一个不同的方法。所以为什么我们不假设使用我们开始时希望实现的目标，来替代我们最初原始设定的目标？按这样做的话，强化学习算法从它达到一定目标时就能得到一个学习信号；即使它不是我们原本想要达到的。如果我们重复这个过程，我们终将学到怎样去达成任意的目标，包括哪些我们非常想要达到的目标。 这个方法使我们可以学习怎样去在桌子上滑动一个冰球，即便我们的奖励是稀疏的，并且我们可能永远不会在早期击到目标。我们叫这个技术 Hindsight Experience Replay，因为它会在这一集结束之后选择目标的重放经验（一种在策略之外的强化学习算法像 DQN 或 DDPG）。HER 可以被任何策略之外的强化学习算法包含在内（举例，HER 可以被 DDPG 包括，我们写作「DDPG+HER」)。 我们发现 HER 在基于目标的环境和稀疏奖励中表现得极其出色。我们在新的任务中对比了 DDPG+HER 和原版 DDPG，该对比中的所有环境分别包含稀疏和密集型奖励两种版本。 HandManipulateBlockRotateXYZ-v0 中四个不同配置下的中位测试成功率（曲线）和四分位距（阴影区域）。数据在训练期间进行绘制，并在每一种配置上使用五个随机 Seed 求均值。 带有稀疏奖励的 DDPG+HER 明显优于其它所有的配置，并且只有稀疏奖励能在这个挑战性的任务中学习到成功的策略。有趣的是，DDPG + HER 在密集奖励的情况下也能够学习，但表现并不好。此外，原版 DDPG 在两种情况下都没有较好的表现。我们发现这种趋势在大多数环境中都是正确的，读者可以在技术报告论文中查看详情。 虽然 HER 是很有前途的方式，它能用像我们在本文提出的机器人环境那样的稀疏奖励来学习基于目标的复杂任务，但它仍有很大的提升空间。与我们最近发布的 Requests for Research 2.0 相似，我们对如何具体提升 HER 有一些思考与探索。 自动 Hindsight 目标创建：我们现有有一个硬编码的策略来选择我们希望替换的 Hindsight 目标。如果这个策略可以被学习替代，那么将会很有意思。 无偏 HER：目标置换以无原则的方式改变经验的分布。这种偏差在理论上会导致不稳定性，尽管我们在实践中并没有发现这种情况。不过，通过重要性采样，我们可以推导出 HER 的无偏版本。 HER+HRL：将 HER 与最近在层级强化学习（HRL）中的新观点结合起来可能会很有意思。它不仅能将 HER 用与目标，同时还能通过高层次的策略应用到动作生成，因此我们可以假定高层级要求实现原始目标 B。 更丰富的价值函数：扩展最近的研究并在额外的输入上调整值函数，如折扣因子或有效的阈值等。 更快的信息传播：大多数离策略深度强化学习算法使用目标网络来稳定训练。然而由于变化需要时间来传播，这将会影响训练的速度。此外，我们也在实验中注意到它经常是决定 DDPG + HER 学习速度的最重要因素。 在策略 HER：目前，HER 只能被用于离策略算法，因为我们替代了目标，使得经验变得极其离策略。但是，当前最优的算法比如 PPO 展示了非常吸引人的稳定属性。调查 HER 能否与这样的在策略算法相结合会很有趣，比如通过重要性采样。在这一方向上已经有一些初步成果。 高频动作强化学习：当前的强化学习算法对采取动作的频率非常敏感，这就是为什么跳帧技术经常用于 Atari。在连续控制领域，随着动作采取的频率趋向无穷大，表现趋向于零。这是由于两个原因：不一致的探索（exploration），和需要更多次的 Bootstrap 来及时传播关于回报的信息。 把 HER 与强化学习的最新进展相结合。最近有大量的研究提升了强化学习的不同方面。比如，HER 能够与 Prioritized Experience Replay、distributional RL、entropy-regularized RL 相结合。 引入「目标」的理念需要对现有的 Gym API 做若干反向兼容的更改： 所有基于目标的环境使用 gym.spaces.Dict 观察空间。环境被期望包含一个智能体试图达成的期望目标（desired_goal），已经被达成的目标，以及实际观察（比如机器人的状态）。 我们展示了环境的奖励函数，并允许再计算带有已更改目标的奖励。这允许 HER 风格的算法替代目标。 下面是一个示例，它与其中一个基于目标的新环境交互，并执行目标替换： def policy (observation, desired_goal) 新的基于目标的环境可以被用于与 Gym 兼容的强化学习算法，比如 Baselines。用 gym.wrappers.FlattenDictWrapper 来向量化基于字典的观察空间为一个数组： 原文链接：https://blog.openai.com/ingredients-for-robotics-research/ "
194,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738356&idx=4&sn=226dde020b35d5edb3d63531b233ce43&chksm=871acacab06d43dcda60cb62b9e97ee3de853bd6e69c436cd6abe56395ea2b5a355b37d02ed4&scene=27,资源 | DeepPavlov：一个训练对话系统和聊天机器人的开源库,"本文介绍了一个构建端到端对话系统和训练聊天机器人的开源项目 DeepPavlov，该开源库的构建基于 TensorFlow 和 Keras，并旨在推动 NLP 和对话系统的研究，提升复杂对话系统的实现和评估效果。机器之心简要介绍了该项目和基本技术，希望实现对话机器人的读者可进一步阅读原项目。 项目地址：https://github.com/deepmipt/DeepPavlov 这是一个开源的对话 AI 库，建立在 TensorFlow 和 Keras 上，其用途是： NLP 和对话系统研究； 复杂对话系统的实现和评估。 我们的目标是为研究者提供： 用于实现和测试他们自己的对话模型并随后将模型共享的框架； 一系列预定义的 NLP 模型/对话系统组件（机器学习/深度学习/规则系统）和流程模板； 对话模型的基准测试环境和对相关数据的系统性评估。 并为 AI 应用开发者提供： 建立对话软件的框架； 将应用与对应基础建设（通讯、技术支持软件等）相集成的工具。 格位填充组件（Slot filling component）：基于命名实体识别（NER）神经网络和模糊 Levenshtein 搜索，以从文本中提取归一化的格位值（slot values）。NER 网络组件根据论文《Application of a Hybrid Bi-LSTM-CRF model to the task of Russian Named Entity Recognition》重新生成了架构，由《Neural Architectures for Named Entity Recognition》中的 LSTM+CRF 架构所启发。 专用分类组件：基于论文《Convolutional Neural Networks for Sentence Classification》中的 shallow-and-wide CNN 架构。该模型允许语句的多标签分类。 自动拼写和校正组件：基于论文《An Improved Error Model for Noisy Channel Spelling Correction》，并使用基于统计学的误差模型、一个静态词典和一个 ARPA 语言模型以校正拼写错误。 目标导向的对话机器人：基于论文《Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning》中的 Hybrid Code Networks 架构。它允许在目标导向任务的对话中预测回应。该模型是相当可定制的：嵌入、格位填充器和专用分类器可以根据需要使用或者不用。 为俄语预训练的嵌入：在联合俄语 Wikipedia 和 Lenta.ru 语料库词向量上进行预训练得到的词嵌入。 用 Telegram 部署目标导向的对话机器人和格位填充（slot-filling）的视频 demo： 用 Telegram 接口运行目标导向的对话机器人： 用控制台接口运行目标导向的对话机器人： 用 Telegram 接口运行格位填充模型： 用控制台接口运行格位填充模型： 原则 这个库遵循以下原则设计： 将端到端学习架构作为长期目标； 目前采用混合的机器学习/深度学习/规则系统的架构； 模块化的对话系统架构； 基于组件的软件工程，最大化复用性； 易于扩展和基准测试； 为单个 NLP 任务提供多个组件，通过数据驱动选择合适的组件。 目标架构 我们的库的目标架构： DeepPavlov 建立在机器学习库（TensorFlow、Keras）之上。可以用其它外部的库建立基础组件。 关键概念 Agent（智能体）：对话智能体用自然语言（文本）和用户进行交流。 Skill（技能）：用于满足用户需求的交互单元。通常可以通过展示信息或完成任务（例如，通过 FAQ 回答问题等）；然而，根据经验，某些任务的成功会被定义成连续的进展（例如，闲聊）。 Components（组件）：基础功能模块： Rule-based Components（基于规则的组件）—无法训练； Machine Learning Components（机器学习组件）—仅能独立训练； Deep Learning Components（深度学习组件）—可以独立地训练，也能以端对端的方式结合到工作链中。 Switcher（转换器）：智能体排序和选择向用户展示的最终应答的机制。 Components Chainer（组件链接器）：从各种组件（Rule-based/ml/dl）构建智能体/组件管道的工具，允许以整体的形式训练和推理管道。 项目模块 配置 NLP 的流程配置为 JSON 文件，它包含四个元素： 配置文件中每一个类都有一个 name 参数，它是注册的代码名。通过重复它的__init__() 方法参数，我们可以定义其它任何参数。__init__() 参数的默认值在类的实例初始化中被配置值覆盖。 数据集读取器 DatasetReader 类能读取数据并返回特定的格式。一个具体的 DatasetReader 类应该从基本的 deeppavlov.data.dataset_reader.DatasetReader 类继承，并注册为代码名： class DSTC2DatasetReader (DatasetReader) 数据集 Dataset 类构成我们所需的数据集（「训练」、「验证」和「测试」）和批量数据。一个具体的 Dataset 类应该注册并可以从 deeppavlov.data.dataset_reader.Dataset 类继承。 deeppavlov.data.dataset_reader.Dataset 类不是抽象类，它同样可以像 Dataset 类那样使用。 词汇 Vocab 是一个可训练的类，它能构建和序列化词汇。Vocab 能索引任何数据，它能索引 X（特征）和 y（回答）类型的数据。一个具体的 Vocab 类应该注册并可以从 deeppavlov.data.vocab.DefaultVocabulary 类继承。 deeppavlov.data.vocab.DefaultVocabulary 并不是一个抽象的类，它同样可以像 Vocab 类那样使用。 模型 Model 是制定训练、推断过程和生成特征的主要类。如果模型需要其它模型生成特征，那么就需要将其传递到构造函数和配置文件中。所有的模型可根据需要嵌套，例如 deeppavlov.skills.go_bot.go_bot.GoalOrientedBot 主要由 11 个独立的 Model 类构建，其中有三个为神经网络： 所有模型都应该注册并从 deeppavlov.core.models.inferable.Inferable 或 Inferable 和 deeppavlov.core.models.trainable.Trainable 接口继承。从 Trainable 继承的模型可以继续训练，从 Inferable 接口继承的模型只能执行推断。通常，Inferable 模型是基于规则的模型或从第三方库导入的预训练模型。 训练 所有从 deeppavlov.core.models.trainable.Trainable 接口继承的模型都可训练，训练过程在 train() 方法中有详细描述。 class MyModel (Inferable, Trainable) def train (*args, **kwargs) 所有在实验中可以改变的训练参数（如 Epoch 数、批量大小、容忍度、学习率个优化器等）都应该传递到模型的构造函数__init__()，且__init__() 中的默认参数值将会被 JSON 配置值覆盖。要改变这些值，我们不需重写代码，只需要修改配置文件就行。 训练过程由 train_now 属性控制。如果 train_now 为真，表示模型正在执行训练。在使用 Vocab 时，这个参数十分有用，因为可以在单个模型中训练一些词汇，而另一些词汇只会在流程中的其它模型上执行推断。JASON 配置文件中的训练参数以设置成： 推断 所有从 deeppavlov.core.models.inferable.Inferable 接口继承的模型都能执行推断。infer() 方法应返回模型可执行的操作，例如分词器应该返回符号、命名实体识别器应该返回识别的实体等。此外，infer() 中应该定义特定格式的返回数据。 推断由 deeppavlov.core.commands.train.infer_model_from_config（）函数触发，并不需要单独的 JSON 进行推断，且 train_now 参数在推断中也会被忽略。 "
195,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738356&idx=1&sn=5ee1548fd879add0fcd7bb6f1a3b30e4&chksm=871acacab06d43dc121433eb4173c4c3411b2c980cbf595b93d341e2c97ef5202754e3e13f5f&scene=27,使用Faster R-CNN、ResNet诊断皮肤病，深度学习再次超越人类专家,"IEEE 由于在特征识别任务上具有优势，医疗图像诊断一直是人工智能技术应用的重要方向。近日，韩国研究人员应用深度学习算法在皮肤病诊断上击败了 42 位皮肤科专家，其研究发表在了 Nature 系列期刊的《Investigative of Dermatology》上。据介绍，该工作使用了 Faster R-CNN、ResNet 等计算机视觉算法。目前，研究人员已将该方法制成安卓 APP 供人们使用（Google play 下载链接见文中）。 人工智能目前在与专业医生的能力对比上还罕有胜迹。但深度神经网络方法最近已经可以在灰指甲这一真菌疾病的诊断上击败 42 名皮肤科专家了——这种疾病每年困扰着 3500 万美国人。 人工智能在医疗领域的这一巨大成功很大程度上得归功于韩国研究者提出的包含了 50,000 张手指甲与脚趾甲图片组成的庞大数据集。它可以用于训练深度神经网络识别灰指甲——一种可使指甲变色和变脆的常见真菌感染——为深度学习模型带来超越医学专家的强大优势。 「这项研究首次展示了 AI 可以超越人类专家，」韩国首尔第一皮肤病医院的皮肤病医生和临床医生 Seung Seog Han 说。「目前为止，在很多研究中，AI 在糖尿病视网膜病、皮肤癌的诊断和肺部 X 射线解读的表现都达到了和人类专家相近的程度。」 过去的测试包括「AI versus doctors」，AI 在皮肤病诊断中的表现大致上和人类专家持平。但在这项研究中，42 个人类专家仅有 1 个略微在三个试验之一的特定测试场景中超过了深度神经网络。该研究发表在 2018 年 1 月 19 日的网络杂志 PLOS ONE 上。 尤其是，不同于简单案例，深度神经网络在极度困难的案例上表现得比皮肤病专家好得多，Han 介绍道。除了 Han 以外，该团队的主要研究者还包括韩国翰林大学的皮肤病学教授 Gyeong Hun Park，以及韩国蔚山大学的皮肤病学教授 Sung Eun Chang。 Han 作为医生的日常工作包括治疗多种类型的皮肤疾病。但他也学习了几种计算机编程语言例如 C++和 Python，并持续了好几年。当他看到 AlphaGo 击败顶级人类围棋选手李世石的新闻之后，对深度学习产生了兴趣。 深度学习算法通常能解决在大数据中检测模式的专业性任务，而人类难以把握大数据的规律。在这个案例中，韩国的研究者发现可以用微软研究院开发的深度学习算法帮助医生从数字照片中识别可能的灰指甲感染病例。 但所有的深度学习模型都需要大量的数据来训练 AI 识别相关的模式。收集和灰指甲感染相关的有用照片是一项巨大的挑战，因为通常这类照片并没有标准的格式。很多照片都从不同的角度拍摄，并会同时展示健康的指甲和被感染的指甲。此外，由于深度学习算法的技术局限性，所有的照片都需要转换成 224×224 像素，这使得很多照片变得无法识别。 Han 和他的同事们训练了一个基于 Faster R-CNN 的目标检测算法模型来识别和裁剪照片，从而使照片仅包括感染的趾甲和指甲，然后将照片放大，以适用于深度神经网络的训练。大多数的照片来自 MedicalPhoto，这是一个皮肤病临床照片管理程序，由 Han 在 2007 年开发。 然而，Han 不得不收集由 Faster R-CNN 裁剪得到的 10 万张照片，进行人工读取并对每张照片标记两次，以确保训练数据的准确性：不准确或不充分的趾甲/指甲照片被剔除。这项工作花费了他大约 550 个小时，总共超过了 70 天，即使他坚持每天工作数小时，并以 10 秒每张的速度处理照片。 该数据集帮助训练了用于识别病症的卷积神经网络——微软的 ResNet-152 和牛津大学的 VGG-19 模型，以执行识别指甲真菌感染可能病例的工作。这种深度学习方法表现优于 42 位皮肤科专家组成的小组——其中包括 16 名教授、18 名临床医生和 8 名住院医师。 研究者还展示了在额外测试中，深度学习方法通常优于最好的五位皮肤科专家。另外，研究者发现 AI 的诊断评估比一般医生、医学生、护士和非医务人员要好。 研究团队已经放出了一个 demo 做演示： 试用网址：http://nail.medicalphoto.org 安卓 APP：https://play.google.com/store/apps/details?id=com.phonegap.onychomycosis_en 通过用网站和 APP 收集数据，研究人员希望发现当 AI 用于医疗实践时潜在的问题。 Han 和其同事也在皮肤癌等其他皮肤病上测试了深度学习。相关论文在 2 月 8 日发表在了在线期刊《Journal of Investigative Dermatology》上（见文末）。 该研究表明，人工智能能够在极为依赖临床摄影的远距离医疗（telemedicine）中极为有帮助，例如诊断灰指甲等。然而，目前仍然需要人类皮肤科医生使用病人的一般病史、足臭等大量因素来确诊。极少数医生对只基于图片做诊断感到合适。 Han 和他的同事认为自己的研究对普通从业者非常有帮助，他们经常见到病人抱怨皮肤和指甲病状。Han 说，「AI 诊断要比普通临床诊断更为准确，我认为它对普通从业者确定甲癣的治疗方向有所帮助。」 论文：Classification of the clinical images for benign and malignant cutaneous tumors using a deep learning algorithm 摘要： 我们测试使用深度学习算法对 12 种皮肤病的临床照片进行分类，包括基底细胞癌、鳞状细胞癌、上皮内癌、光化性角化病、脂溢性角化病、恶性黑色素瘤、黑素细胞痣、雀斑样痣、化脓性肉芽肿、血管瘤、皮肤纤维瘤、疣。使用来自 Asan 数据集、MED-NODE 数据集和 atlas site images 中的训练集图像（共 20826 张）对卷积神经网络（Microsoft ResNet-152 模型）进行调整。然后使用 Asan、Hallym 和 Edinburgh 数据集的测试集图像验证训练后的模型。使用 Asan 数据集进行验证时，基地细胞癌、鳞状细胞癌、上皮内癌、黑色素瘤诊断的曲线下面积（AUC）分别是 0.96 ± 0.01、0.83 ± 0.01、0.82 ± 0.02、0.96 ± 0.00。使用 Edinburgh 数据集进行验证时，对应疾病的曲线下面积分别是 0.90 ± 0.01、0.91 ± 0.01、0.83 ± 0.01、0.88 ± 0.01。使用 Hallym 数据集进行验证时，基底细胞癌诊断的敏感度是 87.1% ± 6.0%。使用 480 张 Asan 和 Edinburgh 图像接受测试的算法性能可与 16 位皮肤科医生媲美。为了提高 CNN 的性能，我们还应该收集涉及年龄范围更大、种族更广泛的图像。 原文链接：https://spectrum.ieee.org/the-human-os/robotics/artificial-intelligence/ai-beats-dermatologists-in-diagnosing-nail-fungus "
196,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738301&idx=2&sn=c9b67f7f5e1ea2ce05a7697bae64c3e5&chksm=871aca03b06d4315f715e64cc1b95c99a30d57a24b900b94bda567fca5973f0bbeb496e4ec1a&scene=27,专访 | 周明：如果用一个词形容NLP圈的2017，我选「想象」,过去一年，从技术向产业，有哪些值得记住的人和事？未来一年，AI 场景化落地还有哪些可能性？ 8 位 AI 行业的局内人，向我们讲了讲他们的故事和看法。 1998 年 11 月 5 日，北京知春路希格玛大厦，微软中国研究院（后更名为微软亚洲研究院（MSRA））成立了。毫无意外地，从当年比尔盖茨在总部建立微软研究院到 MSRA，自然语言都是最早确定的小组之一。 「得语言者得天下」，微软全球执行副总裁沈向洋说过。其中的考量很简单，语言难，相比于以语音、图像为代表的感知智能层，以自然语言处理技术为代表的认知智能层是当下人工智能发展的主要难题。 语言也意味着一切。「如果 NLP 取得突破的话，那么知识获取就会突破，推理就会突破，解题、回答问题、预测等能力都会取得突破。」微软亚洲研究院副院长周明说。他认为，语言一旦突破，会带动认知智能的突破，带动整个人工智能的突破。 1999 年，清华大学任教的周明，在李开复邀请下加入微软亚洲研究院负责自然语言计算组。过去十八年，他带领团队解决语言相关的人工智能基础问题，自然语言理解业务已经覆盖中文，日文，英文三种语言，在分词、句法分析、语义分析、机器翻译、情感分析、问答、理解、文摘、搜索引擎、聊天对话等技术点都有所布局。 这些研究成果逐渐应用于微软 Office、必应搜索、Windows 等产品，团队还参与研发包括微软输入法、英库词典（必应词典）、中英翻译、微软中国文化系列（微软对联、微软字谜、微软绝句）等重要产品和项目。微软小冰（中国）、Rinna（日本）、Zo（美国）等聊天机器人系统开发的背后，同样也有他们的身影。 在这次专访中，周明向机器之能讲述了这一年 NLP 圈的起伏变化。他认为，如果要用一个词来形容 NLP 圈的 2017，那这个词应该是「想象」。 以下为对话原文，机器之能做了不改变原意的整理。   这张图能很好的展示微软 NLP 的布局，我们有产业、产品和研究三个层面的布局。 产品布局主要是通过必应（Bing）搜索体现的，搜索是很考验自然语言处理能力的一项应用，因为它涉及到对问题、文档进行理解。理解能力加强后，搜索的准确度就会有所加强。另一方面有了理解能力，用户在搜索框提一个问题，系统自然就能返回一个答案，不需要像以前一样，需要用户从若干个页面链接中自己寻找。此外，搜索业务还能够与问答系统进行整合，通过对话和回答为用户提供信息服务。 产品层面除必应团队外，我们还有微软商业智能团队，他们和必应团队一样，着重解决如何将自然语言技术做好、落地、实现很好的用户体验这个问题。 研究层面微软在总部和 MSRA 都有布局，主要从事图中 NLP 基础技术和核心技术两个栏目下的这些技术点的研究。比如偏基础研究的团队会研究知识如何表达（Embedding），如何做语义分析这类问题；偏应用技术的团队会解决机器翻译、问答理解、信息检索、推荐、聊天对话等问题。 产品团队和技术团队合作，就是 NLP+，解决搜索引擎的关键技术、客服、商业智能、语音助手等等问题。比如我们的智能助手微软小娜（Cortana），聊天机器人小冰等等。在底层我们会和用户画像、大数据、云计算、机器学习、知识图谱等相应团队合作，我们将他们的成果用在自然语言分析和推理上。 所以说我们每一个团队在做技术的时候都不是独立的，都可以得到周围相关团队的支持，我们是按全局一盘棋的设想来布局的。 自从建院以来，MSRA 一直强调要做有用的研究，所以我们和产品组有很好的合作，我们会互相了解对方的需求。我们研究院知道公司的产品战略，知道他们需要用到怎样的技术，对于有些还不存在的技术，我们会想要超前一些，把他们能用到的技术提前开发出来。另外产品部门也知道我们的需求，比如我们需要数据，产品组就会提供很多数据来给我们使用。 一旦我们研究院把某些技术突破了，产品组就能很快地将技术融入到他们的产品之中。因为我们双方已经很默契了，所以基本上从我们的技术成型，到他们上线，「一夜之间」就能完成。像上个月我们参与 SQuAD 比赛的 R-NET 就已经在微软的产品中得到使用。当然，有的时候产品组需要重新编写代码，因为产品需要有安全性、速度等等的考量，还要跟已有的系统很好地配合。 这一年的进步可以总结成四个方面，神经机器翻译、聊天机器人、阅读理解、创作。 首先神经机器翻译，这里有几个例子。红色字体是原来做的不好的地方，现在有了神经机器翻译，能够做的非常好，句子非常流畅，用词非常准确，和人的标准答案相仿，甚至有的地方你会感觉到比人类翻译的还要好。 神经机器翻译发展的前提是有大规模的双语对照语料，只要数据够大，就能达到这样的效果。所以在科技新闻领域，因为语料很多，现在翻译效果已经能够达到人类的水平，但是在一些其他语料较少的领域，比如科学论文，翻译水平还是不行。 去年微软和华为合作，在 Mate 10 手机中嵌入微软的神经网络机器翻译，可以算得上是在终端运行神经网络机器翻译的第一例。在手机上运行神经网络，由于运算速度、存储能力的限制，微软机器翻译产品团队对网络做了不少针对性的优化。 聊天机器人方面，我们和微软（亚洲）互联网工程院合作，从 2014 年起推动微软小冰，后来有了日本、美国、印度、印度尼西亚一共五个版本，可以说进展还是很快的。 现在小冰各个产品的用户总和已经达到两亿多，平均交互轮数是 23 轮，我认为这很了不起。小冰的技术落地后，小冰团队最近又做了一个音箱，让她以实体的形式走进家庭。微软小冰之外我们还有智能助手微软小娜（Cortana），能够完成面向任务的对话交互。 阅读理解方面我们刚刚在 SQuAD 数据集上的 EM 分值上超越了人类，这是一个突破，带动了一些产品的进展，例如搜索引擎、客服等。 创作的话我们探索了一些对对联、写诗、谱曲等等。 这四个方面代表了我们在 NLP 领域的一些进步。在这张图中我用金字塔形来表示这四个技术之间的关系，难度是逐级上升的。 最下面神经网络机器翻译现在效果已经非常好了，因为它是 Single-turn，一句进一句出。聊天机器人就要难一些，是 Multi-turn，对以前聊的东西也要有记忆。机器阅读理解除了当前的句子，上下文，还要融入世界知识，会更难一些。最后创作，最开始我们都不了解要怎么建模，因为创作是感性的东西，比如写诗、写词，强调灵感和文采，要为这些东西建模其实是很难的。目前我们在创作方面只是能利用简单的技术进行模仿，离人类真正的创作水平还很远。 再往上其实还有更难的，比如解决一个问题、做决策、做预测，都是和自然语言相关的，是认知智能的一部分，现在我们还没有做很多。 有很多，举个例子，我们做的对联。微软没有一个产品能用到它，必应、Office、Azure 都用不上，但是我们认为对联是人工智能的一个重要标志，因为它代表着创作，创作是当前人工智能还需要摸索的一个领域。 我认为做脑洞大开的创新有两层意义，一是促使你去思考和解决人类常见的问题，二是开脑洞产生的技术可以旁征博引，比如我们做了对联，这里面的一些技术可以放到微软的机器翻译里面，提高机器翻译的准确率。实际上科学之间是相通的。 这个要从技术本身讲起了。因为图像、语音技术都是 Single-turn 的技术，就是一个输入一个输出。而自然语言是 Multi-turn，做完一次输入输出后，要把结果作为下轮输入的一部分继续输出。最典型的例子就是多轮对话，系统需要结合之前的对话生成内容；还有就是机器阅读理解，需要考虑上下文；写诗，一句诗是一个 Single-turn，那写个绝句就是做四次 Single-turn，而每一次都要将之前的结果考虑进去。 这有什么意义呢？我认为可以理解为两个层面。首先，从 NLP 技术本身来说，有很多原来不敢想象的应用现在可以落地了，比如自动客服、神经网络机器翻译，原来不敢做，现在可以往前走一点了。第二层我认为，因为 Multi-turn 是其他智能领域还做不好的技术，一旦这一技术有所突破，再加上世界知识的普遍使用，从技术上会反哺图像识别和语音识别等。他们现在是 Single-turn，将来有可能是 Multi-turn；他们现在没用到背景知识，将来有可能可以结合背景知识做一些事情。 所以说 NLP 对其他智能，对整个人工智能领域都会有推进作用，这也就是为什么沈博士（指沈向洋）说「得语言者得天下」的原因。 自四年前语言学相关议题大规模引入神经网络以来，效果相较于统计时代有了很大的提升。除了下图展示的这些基本的问题之外，这些年我们着重解决的问题还包括如何用小数据训练出与大数据相仿的结果、如何用单语数据帮助双语数据进行翻译水平的提升（因为有的场景下双语语料很少，却有大量的单语数据）、强化学习在聊天和机器翻译中的应用等。 具体到这一年的话，在神经网络机器翻译任务上我们看到了更多单语数据的加入。比如我们研究院刘铁岩博士领导下的对偶学习的研究对神经网络机器翻译的影响，就用大规模单语数据，提高了神经机器翻译的水平。 在对话方面，我们已经能够利用用户的当前输入和上下文以及用户画像进行个性化建模，这里面涉及到很多模型理论上的研究和设计，比如怎样做用户画像，怎样对上下文信息进行编码，怎样通过注意力模型将最重要的信息捕捉到，以及怎样生成上下文相关的、用户个性化的、有主题知识的、不空洞回复，也表现出了从 Single-turn 到 Multi-turn 的过渡。 阅读理解是去年一个非常强的热点，也就是说我们的技术在上图的金字塔中已经走到了阅读理解这一步，利用端到端的训练，引入背景知识来解决阅读理解问题。 再往上，把自然语言技术延伸到其他领域，例如音乐、创作，去年我们做了一些很好的尝试，也取得了一些进展。 我觉得是「想象」。曾经一些我们认为不能解决的任务，比如阅读理解，或者一些不应该我们 NLP 解决的任务，比如音乐，由于有了更好的工具，更大的数据，我们才发现这些东西是能够解决的，是能够和 NLP 相结合的，这在之前是不能想象的事情。这种想象是伴随着行动产生的，同时又能引领我们走向新的道路。没有技术的进步，很多事情我们想都不敢去想。 我们还是从这四个方面说，首先神经网络机器翻译，未来一年，在典型的场景和领域下，比如新闻，达到人类的水平是可以期待的。然后在对话领域，客服会有较大的进展，由于问答技术、聊天技术、阅读理解技术的提升，客服的效率将会被大大提高，但不会完全替代人类客服。 在创作领域，很多新闻可以由机器人来完成。还有一些过去很多不敢尝试的，例如歌词、音乐的创作，我认为明年会有一些好玩的新东西出来，至于说能产生多大的影响，怎样落地，现在还不可知。但是作为一种社会现象，我认为在创作领域，明年还是可以期待一下的。 如果是站在很工程的角度，就是数据不够。和机器翻译等任务相比，写歌词、谱曲之类的数据要少很多，所以我们要想办法获得更多的数据。第二就是灵感不高，目前神经网络所做的创作都是源于已有数据的，所以只能是追随者，现在机器写出的歌会让人有一种似曾相识的感觉，没有灵感上的迸发。这是大数据的特点，训练数据中谁的歌多，就像谁多一些，然而真正有才华的艺术家是很少的，所以要想让机器有非凡的才华，还是有很长的路要走。 但是现在机器对创作者可以起到辅助的作用，对于艺术家来说，可以起到提示作用。比如写着写着没词了，机器一提词，可能就会有新的灵感冒出来。对于普通人来说，机器其实是降低了普通人的创作门槛。 首先我认为多模态的融合会带来很大的机遇。就比如说图像和文字的结合，现在「读图」这件事更多的是图像领域的科研人员在做，NLP 领域的人很少涉足，NLP 研究人员一般是你读出来什么，表示成自然语言的形式，我再进行后续的处理。但我认为，做 NLP 的人需要了解信号是怎样输入进来的，这很重要。就自然语言来讲，在信息的感知层面，现在除传统的键盘输入外，还有语音输入、图像输入，这和我们人类观察世界的方式是一致的。人类在接受外界信号时，实际上在脑海中是形成了一系列的自然语言的表述的，从这个表述出发，我们才会想去创作。所以说，信号是如何进来的是非常重要的。 图像和语言充分衔接后，就会产生非常大的机会。比如跨媒介交流、基于图文的多媒体问答对话、搜索（直接用图片搜索或者图文混合信息搜索）。其实我们人是不怎么区分图片、文字、声音，因为到脑海中都会变成表述。这样类比到神经网络上，我们也可以对不同类型的信息源同等对待，同等建模，得到一个融合的信息，再基于这种信息进行编码解码，然后再生成其他媒介的东西，或者混合媒介的东西。 这个方向有所突破的话，对机器人领域的发展会非常有帮助。它感知对方，了解对方是什么样的人，说过什么话，机器人得到对方的一个统一的印象，就可以做出自然的表情和反应。现在机器人的多模态、人机交互是做不好的，未来可以期待一下。 我觉得有两类人才，一个是系统实现型人才，这些人能够在了解现有的技术方法理论之后，快速解决问题。我们需要特别特别多这样的人才，现在中国这样的人才并不多。第二类人才是拔尖人才和领军人才，他们知道现在技术的发展水平和状况，并且能够预测未来的一些领域的发展，提出一些新的理念、理论体系，还能够亲自身体力行，带领团队让技术落地。 从微软亚洲研究院的角度，我们希望两者兼顾，因为第一，我们是工业界的研究院，我们要做一些有用的研究来快速帮助提升微软产品的智能水平，并释放一些通用的技术给社会或第三方机构。第二，作为一个研究院，我们也有使命将最先进的方法、思想、理念释放出来，帮助整个人类社会。 我们鼓励跨领域的研究，将不同背景的人凑在一起来解决一个问题。比如在微软的对联、写词、谱曲等技术的研究过程中，我们会学习到其他学科的一些方法。有一些想法在人家学科是常识，没什么了不起的，我们学会了之后再反哺到我们的技术中，就很有优势了，两边学科都懂，那你建的模型就比别人好，思路也开阔。我们希望未来的创新是基于这种跨领域的、交叉学科的。 任何公司都有人才流动的问题，有人来有人走，这其实挺正常的。宏观上来看，对社会是一个正面的促进，只有人才流动，新兴学科才能够发展起来。人才都集中在一起，对新的机会视而不见，这也是不科学的。我们微软亚洲研究院被称作黄埔军校，「校」是什么意思，有学生来，有学生毕业，这才叫学校。学生毕业之后还在黄埔军校，那就不是军校，那是黄埔军营。 所以对于我们来说， 学校价值的体现就在于我们的学生强，人脉广。从我个人角度而言，也不是说一定要让我的员工留下来天天做自然语言，这对他们来说不一定是最好的事情。他们应该去做别的方面，甚至图形图像、大数据，都可以做。或者到学校去当教授，培养更多的人才，让他们尽最大的努力对社会产生贡献。 整个微软亚洲研究院的人对此也是有同感的，我们的使命就是把优秀的人才培养出来，让一般的人才变优秀，优秀的人才变杰出，然后再去引领更多的人，把整个社会带动起来，这也是过去 20 年来微软亚洲研究院所秉承的理念。我们看到很多人才从这里走出去，实现了自己的人生价值，对社会产生了很大的贡献，我们乐见此事的发生。 我们和中文信息学会以及中国计算机学会合作举办了很多次暑期学校，每一期能够培养 200 到 300 名研究生和博士生，目前已经十几年了。我们还有实习生计划，自然语言方向已经有 450 多名实习生从我们这里得到过培训，他们现在在中国的各个地方、各个公司，现在很多都是领军人物了。另外，我们还培养了 20 名左右的博士生和 30 名左右的博士后，这些人除了少数留在微软，很多也都在其它公司或者学校工作，有一些人已经担任院长和博导。 但与此同时，我们也反对恶意的人才挖角。有些公司为了人才不择手段，甚至连人带技术一起挖过去，我们认为这是不道德的，一方面触犯了法律，另一方面也是对人才的不尊重。因为有的时候，公司挖这个人才过去，一夜之间有了一个新技术，那这个人才对他来说就没有太多利用价值了，这样的公司不是抱着培养人才的目的去的，更不会说让这个人才培养更多的人，而是一种急功近利型的、短期的行为，我们对此强烈反对。 另外我认为，如果我们的社会是一个金钱至上、薪水至上的社会，对中国赶超世界先进水平会产生很大的阻碍。多挣一些钱，社会就是先进社会吗？我认为是反的，如果一个社会道德体系良好，大家和谐共同发展，就算短期没有经济上的显著表现，长期也一定会胜出。 现在很多在校生，已经没有了我们当年那种为国家奋斗终身的意志，哪里钱多去哪，以后的路以后再说。我认为这样一代一代传下来的话，我们整个行业、整个社会未来堪忧。微软作为一个负责任的公司，我们对社会是有使命感的，我们需要对员工、客户、股东负责，更需要对社会负责。 我认为现在美国第一，中国第二。这里有几个指标，首先在世界上最著名的自然语言领域的学术大会 ACL 上，美国无论在投稿率还是录取率，都排名第一位，中国在过去五年一直排名第二位，后面是一些其他国家，比如英国、德国、日本、韩国等。但是中国的论文数量比其他几个国家加起来都多，这说明了中美两国遥遥领先的位置。 中美之间大概有每年 20 篇左右的论文数量差距，实际上只要有一个「涨停点」，中国就有可能跟美国并驾齐驱，甚至超越，这是指日可待的。 首先，政府有明确的纲要进行鼓励，不论是投资还是政策都在鼓励人工智能的发展。然后中国有强烈的需求，我们有 7 亿多网民，电子商务、搜索、办公系统、云服务，这些需求中国在社会上都是排名第一的。再然后，有了这么强的需求，就有很多的数据，也有很清晰的商业模型，就又会有越来越多的投资加入，越来越多的公司成立。另外，大学的研究力量也在一点点增强。 中国有一个清晰的蓝图，又有很强的执行力，本身又有这么强的需求。在过去几十年的发展中，包括网络、云、人才等基础设施都齐备，所以我觉得我们是一个万事俱备的状态。同时，也没有人来掣肘，不像其他国家可能会有人有不同的意见，我们基本上是全国上下都在迎接人工智能时代的到来。所以我相信，中国在人工智能领域能够实现弯道超车，如愿以偿地达到世界顶级水平。 但与此同时我们也看到了一些顾虑。比如第一，我们照比美国来讲，顶尖人才的数量还是要少很多，我们通才比较多，比如编程人员、做系统的人员我们有很多，但是能够提出先进理念思想、引领潮流、引领世界的人才相对来讲还很少。 另外我们的博士生，追随的意愿太强，很多人的目标就在于写几篇文章，能毕业就行。我更希望我们的博士生引领的意愿更强，可能一开始还没有能力引领，只能追随，但慢慢的，应该有自信心来引领这个世界。必须有强大的信心驱使着我们，我们才能通过技术难关达到世界发展的顶点，这其中志向起到很大的作用。 虽然我们期待在学术理论上引领世界，但是回到现实，目前工业产品都是数据驱动的，也就是说谁有数据，谁就能第一时间建模，第一时间得到用户反馈，然后快速迭代自己的系统来满足用户的需求，所以说实际上，谁掌握场景、数据，谁就掌握了入门主动权。 目前各个行业都要数字化、智能化。但咱们国家很多工业体系连数字化都没有完全达到，在数据的采集、整理、建模方面还没有达到很好的智能状态，这里有很多研究以及创业的机会。就是说先不管理论突破，就算是能将现有的模型巧妙的用在各个领域，提升行业效率，对社会都是极大的贡献。 比如交通、医疗、教育、司法、金融，这些行业背后有一堆数据，但这些数据要么没有被及时存起来，要么存起来却也找不到，要么就是没把它建模，没有使它的数据潜能发挥出来。所以这些领域只要把数据做好，加上人工智能的一些能力，就会极大的提高整个社会生产力的潜能，这个非常重要。需要工业界的人和人工智能界的人共同努力，工业界要找到需求，人工智能界要将系统做成可实现、可无缝对接、可跟踪的东西，不要总是高高在上。 所以从这个意义上来说，场景加数据就是决胜之道，其他都是随之可以沓来。 有说法认为，人工智能会拉大数字鸿沟，强者愈强，最终造成垄断，你是否赞同？  我认为，先进国家、先进公司抱着盈利和实现自己社会使命感的目的，一定会将一些技术普惠到民间。但是同时，有些大公司也会希望构建自己的技术壁垒。但我个人认为，国家政策应该通过建立普惠制度极力消除这种鸿沟和壁垒。 现在很多技术没有生态环境发展不起来，那么为了要有这个生态环境，你就要把你的一些东西释放出来，让所有人受益，让大家在你的基础上发展新技术，整个生态环境才能起来。所以一个公司一家独霸，什么都做得好，不借鉴其他公司的任何东西，然后站在世界的制高点上，完全封闭，不开源，谁也不给用，直接垄断，这种情况很难出现。 现在的形势就是开源，你中有我，我中有你，社区互相贡献，互相促进，共同发展。当然，不排除有些国家经济水平还没有达到能够跟上人工智能浪潮的程度，比如人员配置不足，基础设施建设欠缺，那这些国家可能会在人工智能竞争中落在下方。 我的看法还是很正面的。第一，由于人工智能的进步，很多原来做不了的事情现在能够实现了，最典型的是智慧城市。以前你用一万个人、一亿个人也解决不了智慧城市的问题，它涉及到基础设施、信号灯调配、车辆识别、调度与管理，没有人工智能，这件事想都不敢去想。 智慧城市让整个社会效率提高了，那么人们就能有更多的时间去做更多的事情，比如以前你在车上堵一个小时才能到，现在半个小时就到了，那你就能做很多新的事情，或者把原先的工作做得更好。这说明了什么，人工智能释放了人类更多的潜力，节省了更多的时间，创造了新的机会。 第二，人工智能对现有职业有一定的冲击，比如翻译、新闻稿，类似这种任务人工智能能够很好地完成。但其实仔细想想，人工智能是将人类的需求充分的挖掘出来了。就像翻译，以前人们出国，很少有能带得起翻译的，现在有了语音翻译这件事情，普通人也能有翻译了。 还有写稿，很多稿子都要求时效性，以前雇多少人也不一定能很好地满足时效性，现在让人工智能先很快地生成一个稿子，编辑人员检查一下就能发稿了。教学方面也是一样，人工智能出题、改卷、减少了老师的工作负担，使教育更加公平、个性化。所以说人工智能对现有的人员没有进行冲击，反而提高了现有人员的工作效率和能力。 当然，有些情况下，比如客服，由于效率的提高，原来需要一百人，现在需要五十人。但我认为，现在的客服大多数都是年轻人，他们整天回答枯燥的问题，还要笑脸相迎，承受很多压力，人工智能能把这部分人解放出来，让他们有自己的爱好，发展新的职业道路，也是正面的。 所以整体来讲，人工智能是会起到辅助人类，让社会变得更美好的作用。 8位AI行业局内人讲述对过去、对未来的看法 点击下方，阅读更多                 
197,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738445&idx=3&sn=56f289da3bd388ca4169f4d0f724201e&chksm=871acb73b06d4265057ea9b164fc9da304ac766f879fc2bf4fea7a4d79cd85abe13ee76ab0ca&scene=27,入门 | 奇异值分解简介：从原理到基础机器学习应用,machinelearningmastery 矩阵分解在机器学习应用中的重要性无需多言。本文对适用范围很广的奇异值分解方法进行了介绍，并通过代码演示说明了其工作方式、计算方法及其常见的几种基础应用。 矩阵分解也叫矩阵因子分解，涉及到用给定矩阵的组成元素描述该矩阵。 奇异值分解（SVD）可能是最著名和使用最广泛的矩阵分解方法。所有矩阵都有一种 SVD 方法，这使得其比特征分解（eigendecomposition）等其它方法更加稳定。因此，这种方法在很多应用中都有应用，包括压缩、去噪、数据压缩。 在这份教程中，你将了解用于将矩阵分解成其组成元素的奇异值分解方法。 在完成本教程后，你将了解： 奇异值分解是什么以及涉及什么 如何计算 SVD 以及如何根据 SVD 元素重建矩形和方形矩阵 如何使用 SVD 计算伪逆和执行降维   那就开始吧！ 教程概览 本教程分为 5 部分，依次为： 1. 奇异值分解 2. 计算奇异值分解 3. 根据 SVD 重建矩阵 4. 用于伪逆的 SVD 5. 用于降维的 SVD 奇异值分解 奇异值分解（SVD）是一种用于将矩阵归约成其组成部分的矩阵分解方法，以使后面的某些矩阵计算更简单。 为了说明简单，我们将关注用于实数值矩阵的 SVD，而会忽略复数矩阵的情况。 其中 A 是我们希望分解的 n×m 的实矩阵，U 是一个 m×m 矩阵，Sigma（通常用大写的希腊字母 ∑表示）是一个 m×n 的对角矩阵，V^T 是一个 n×n 矩阵的转置，其中 T 是上标。 奇异值分解是线性代数的一个亮点。 ——《Introduction to Linear Algebra》第五版，2016 年，第 371 页   Sigma 矩阵中的对角值被称为原始矩阵 A 的奇异值。U 矩阵的列被称为 A 的左奇异向量，V 的列被称为 A 的右奇异向量。   SVD 是通过迭代式的数值方法计算的。我不会详细深入这些方法的细节。每一个矩形矩阵都有一个奇异值分解，尽管所得到的矩阵可能包含复数值以及浮点算术的局限性可能会导致某些矩阵无法简单利落地完成分解。   奇异值分解（SVD）提供了另一种将矩阵分解成奇异向量和奇异值的方式。SVD 让我们可以发现某些与特征分解同种类型的信息。但是，SVD 有更广的适用性。 ——《Deep Learning》，2016 年，第 44-45   SVD 在矩阵求逆等其它矩阵运算的计算有广泛的应用，但也可用作机器学习中的数据归约方法。SVD 也可用在最小二乘线性回归、图像压缩和数据去噪中。   奇异值分解（SVD）在统计学、机器学习和计算机科学领域有很多应用。将 SVD 应用于矩阵就像是使用 X 射线进行透视…… ——《No Bullshit Guide To Linear Algebra》，2017 年，第 297 页 计算奇异值分解   SVD 可以通过调用 svd() 函数进行计算。   该函数在处理矩阵后会返回 U、Sigma 和 V^T 元素。Sigma 对角矩阵是按奇异值向量的形式返回的。V 矩阵是以转置后的形式返回的，比如 V.T.   下面的示例定义了一个 3×2 矩阵并计算了奇异值分解。 运行这个示例，首先会显示定义的 3×2 矩阵，然后会显示分解计算得到的 3×3 的 U 矩阵、2 个元素的 Sigma 向量和 2×3 的 V^T 矩阵元素。 根据 SVD 重建矩阵   原始矩阵可以根据 U、Sigma 和 V^T 元素重建出来。   svd() 返回的 U、s 和 V 元素不能直接相乘。   s 向量必须使用 diag() 函数转换成对角矩阵。默认情况下，这个函数将创建一个相对于原来矩阵的 m×m 的方形矩阵。这是有问题的，因为该矩阵的尺寸并不符合矩阵乘法的规则，即一个矩阵的列数必须等于后一个矩阵的行数。   在创建了方形的 Sigma 对角矩阵之后，各个矩阵的大小与我们分解的原始 n×m 矩阵是相关的，如下：   而事实上我们需要： 我们可以通过创建一个全是 0 值的 m×n 的新 Sigma 矩阵（比如：更多行）并使用通过 diag() 计算得到的方形对角矩阵来填充矩阵的前 n×n 部分。 运行这个示例，首先会显示原始矩阵，然后会显示根据 SVD 元素重建的矩阵。 上面使用 Sigma 对角矩阵的复杂之处仅存在于 m 和 n 不相等的情况中。当重建一个方形矩阵时，其对角矩阵可以直接使用，如下。 运行这个示例会显示原来的 3×3 矩阵和根据 SVD 元素直接重建的版本。 用于伪逆的 SVD   伪逆（pseudoinverse）是将方形矩阵的矩阵求逆泛化应用到行数和列数不相等的矩形矩阵上。 这也被称为广义逆（Generalized Inverse）或摩尔-彭若斯逆（Moore-Penrose Inverse）， 得名于两位独立发现该方法的研究者。   矩阵求逆不是为非方形矩阵定义的。[...] 当 A 的列数大于行数时，那么使用伪逆求解线性方程是众多解决方案中的一种。 ——《Deep Learning》，2016 年，第 46 页   伪逆表示为 A^+，其中 A 是被求逆的矩阵，+ 是上标。   伪逆是使用 A 的奇异值分解计算的： 或者，没有点符号： 其中 A^+ 是 A 的伪逆，D^+ 是对角矩阵 Sigma 的伪逆，U^T 是 U 的转置。   我们可以根据 SVD 运算得到 U 和 V。 根据 Sigma 创建一个对角矩阵，计算 Sigma 中每个非零元素的倒数，然后如果原始矩阵是矩形的就取其转置，就可以计算得到 D^+。 伪逆提供了一种求解线性回归方程的方法，尤其是当行数多于列数时，而这也是很常见的情况。   NumPy 提供了函数 pinv() 来计算矩形矩阵的伪逆。   下面的示例定义了一个 4×2 的矩阵并计算了其伪逆。 运行这个示例，首先显示定义的矩阵，然后显示计算出的伪逆。 我们可以通过 SVD 采用人工的方式计算伪逆，并将结果与 pinv() 函数的结果进行比较。   首先我们必须计算 SVD。然后我们必须计算 s 数组中每个值的倒数。然后将这个 s 数组转换成一个对角矩阵，它额外增加了一行 0 以使其变成矩形形式。最后，我们可以根据这些元素计算伪逆。   具体实现方式为： 下面列出了完整的示例。 运行这个示例，首先显示定义的矩形矩阵，然后显示其伪逆，结果与上面 pinv() 函数的结果一致。 用于降维的 SVD   SVD 的一大常见应用是降维。   具有大量特征的数据（比如特征数（列数）多于观察数（行数））也许可以被归约成与所涉预测问题最相关的更小特征子集。   其结果是一个秩更低的矩阵，据说接近原始矩阵。   为了做到这一点，我们可以在原来的数据上执行一次 SVD 操作并选择 Sigma 中前 k 个最大的奇异值。这些列可以从 Sigma 中选择得到，行可以从 V^T 中选择得到。 然后可以重建原始向量 A 的近似 B。 在自然语言处理中，这种方法可以被用在文档中词出现情况或词频的矩阵上，并被称为隐含语义分析（Latent Semantic Analysis）或隐含语义索引（Latent Semantic Indexing）。   在实践中，我们可以保留和使用被称为 T 的描述性数据子集。这是矩阵的密集总结或投射。 此外，这种变换既可以在原来的矩阵 A 上计算和应用，也可以在其它类似的矩阵上计算和应用。 下面的示例是使用 SVD 的数据归约。   首先定义一个 3×10 的矩阵，其列数多于行数。然后计算 SVD 并且只选取其前两个特征。这些元素再重新结合起来，得到原始矩阵的准确再现。最后计算转换的方式有两种。 运行这个示例，首先显示定义的矩阵，然后是重建的近似矩阵，然后是原始矩阵的两个同样的变换结果。 scikit-learn 提供了直接实现这种功能的 TruncatedSVD 类。   TruncatedSVD 的创建必须指定所需的特征数或所要选择的成分数，比如 2。一旦创建完成，你就可以通过调用 fit() 函数来拟合该变换（比如：计算 V^Tk），然后再通过调用 transform() 函数将其应用于原始矩阵。结果得到上面被称为 T 的 A 的变换。   下面的示例演示了 TruncatedSVD 类。 运行这个示例，首先显示定义的矩阵，然后是该矩阵变换后的版本。   可以看到，结果得到的值与上面人工计算的结果一致，但某些值的符号不一样。由于所涉及的计算的性质以及所用的基础库和方法的差异，可以预见在符号方面会存在一些不稳定性。只要对该变换进行训练以便复用，这种不稳定性在实践中应该不成问题。 
198,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738356&idx=3&sn=603baf72534e6d29dfdee91d00208528&chksm=871acacab06d43dc192a5e75e4eb38472428711ab45999aed3f2f6b502e10bee19d8873233cb&scene=27,业界 | 哪家GPU云提供商最合适？也许这份评测能给你答案,"RARE Technologies Shiva Manne 做深度学习开发和实验既可以选择自己搭建硬件平台（参阅《 》），也可以向 GPU 提供商购买使用服务。本文介绍了 RARE Technologies 的 Shiva Manne 对几个主要 GPU 平台的评测结果，希望能为想要选择最适合自己的平台的企业或开发者提供帮助。 我们最近发表了使用 word2vec 的大规模机器学习基准评测文章，参阅：https://goo.gl/gndD16。该文章在成本、易用性、稳定性、可扩展性和性能等实用性方面对几个流行的硬件提供商和机器学习框架进行了比较。因为那次基准评测只关注了 CPU，所以我们又在 GPU 上进行了一次类似的机器学习基准评测。 这次基准评测囊括了这些硬件平台：亚马逊网络服务 AWS EC2、Google Cloud Engine（GCE）、IBM Softlayer、Hetzner、Paperspace 和 LeaderGPU。 因为很多现代机器学习任务都要使用 GPU，所以理解不同 GPU 提供商的成本和性能之间权衡是至关重要的。 我要感谢所有这些提供商，感谢他们在我的测试期间慷慨提供的基准分数和出色支持。我将在下面详细讨论和比较所有这些平台，但其中每个平台都有自己的相对优势和短板，而且「GPU 即服务（GPUaaS）」市场本身也非常激动人心和活跃。 （备注：微软 Azure 是唯一一家完全没有任何回应的主要供应商，实际上我们从他们的官方支持频道上也没有得到任何回应。） 这个硬件提供商列表包含提供虚拟实例（AWS、GCE）、裸机基础设施（Softlayer）和专用服务器（Hetzner）的公司，也有相对较新的专注于提供 GPUaaS 的玩家（LeaderGPU、Paperspace）。我们根据每个平台上实例的价格而将 GPU 分为了两类——「预算型」和「高端型」（见表 1）。我们的目标是看高端实例要那么多钱是否值得。 任务 我们 RARE Technologies 常常需要解决自然语言处理（NLP）问题，所以我为本基准评测任务设置了一个情绪分类任务。我们要训练双向 LSTM 来执行一个推文的二元分类任务。算法的选择则不是非常重要的；对于本基准评测，我唯一真正的要求是算法应该是计算密集型的。为了确保 GPU 利用率最大化，我使用了 Keras 的 CuDNN 支持的快速 LSTM 实现——CuDNNLSTM。 CuDNNLSTM 地址：https://keras.io/layers/recurrent/#cudnnlstm 数据集 我们使用了 Twitter 情绪分析数据集，其中包含 1,578,627 条已分类的推文；对于每一行，如果情绪是正面的就标注 1，如果是负面的则标注 0。模型使用 90% 的数据（经过混洗）训练 4 epoch，另外 10% 的留存数据用于模型评估。 Twitter 情绪分析数据集：http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/ Docker 为了再现性，我创建了一个英伟达 Docker 镜像，其中包含重复运行该基准评测任务所需的所有依赖和数据。这个 Dockerfile 以及所有所需的代码都可在这个 GitHub 库找到：https://github.com/RaRe-Technologies/benchmark_GPU_platforms 我们完整公布了设置方式和代码，这样不仅任何人都可以重现这些结果，而且你也可以换上你自己的硬件平台或其它算法选择，从而进行你自己的基准评测。 在我之前的文章中，我曾根据自己的经验推荐使用 AWS、Softlayer 和 GCE。在 LeaderGPU 和 Paperspace 上订购一个实例是相当简单的，无需任何复杂设置。与 AWS 和 GCE 所需的几秒准备时间相比，Paperspace 和 LeaderGPU 所需的准备时间稍微长一点（几分钟）。 LeaderGPU、亚马逊、Paperspace 提供了免费可用的深度学习机器镜像，其中预装了英伟达驱动和 Python 开发环境；它们还提供了免费的英伟达 Docker——基本上就包含了立即开始实验所需的一切。尽管这能让工作轻松很多（尤其是对于那些只是想开始实验机器学习模型的初学者而言），但我选择用老方式从头开始设置一切（对 LeaderGPU 除外），以便评估为满足个体需求而定制实例的难度。在这个过程中，我遇到了在所有平台上都很常见的一些问题，比如英伟达驱动与已安装的 gcc 版本不兼容，或 GPU 使用量在安装驱动之后就达到了 100%，但又没有证据表明运行着什么进程。还有意外的情况——在 Paperspace 低端实例（P6000）上运行我的 Docker 时出现了一个错误。这个问题的原因是 Docker 上的 TensorFlow 是使用 CPU 优化（MSSE、MAVX、MFMA）从源编译的，而 Paperspace 实例不支持这些 CPU 优化。不使用这些优化再运行 Docker 就行了。 就稳定性而言，我没在这些平台上遇到任何问题。 不出所料，专用服务器是控制成本的最佳选择。这是因为 Hetzner 是按月收费的，换算成每小时的价格就非常低。当然，这个数字是均摊之后的，但只要你有足够多的任务保证服务器足够繁忙，那么就能保证成本低廉。在虚拟实例提供商中，显而易见的赢家是 Paperspace。对于低端 GPU，在 Paperspace 上训练一个模型的成本只有在 AWS 上的一半。在高端 GPU 方面 Paperspace 也有类似的优势。 下图是将表 1 相应部分总结成的图表： AWS 和 GCE 在高端和低端 GPU 上的成本优势各有不同。在低端 GPU 方面 GCE 比 AWS 便宜很多，而在高端 GPU 方面 GCE 则比 AWS 稍贵一点。这说明昂贵的 AWS GPU 的额外成本可能是值得的，能够提供物有所值的价值。 IBM Softlayer 和 LeaderGPU 看起来很贵，这主要是由于他们的多 GPU 实例使用率不足。这个基准评测是使用 Keras 框架执行的，而 Keras 框架的多 GPU 实现的效率非常低，有时候甚至还比不上在同一台机器上运行的单个 GPU。但 IBM Softlayer 和 LeaderGPU 这两个平台都不提供单个 GPU 实例。在 Softlayer 上运行的基准评测通过 Keras 的 multi_gpu_model 函数使用了所有可用的 GPU，而在 LeaderGPU 上运行的基准评测只使用了可用 GPU 中的一个。这就导致出现了资源利用不足所造成的额外成本。另外，LeaderGPU 分别以和 GTX 1080 与 Tesla P100 一样的价格提供了更强大的 GPU——GTX 1080 Ti 和 Tesla V100。在这些服务器上运行肯定能降低整体成本。考虑到这些因素，在该图中的 LeaderGPU 的低端成本实际上还是相当划算的。尤其是当你计划使用能更好地利用多 GPU 的非 Keras 框架时。 另外似乎还有另一个普遍趋势——更便宜的 GPU 的性价比优于更昂贵的 GPU；这说明训练时间的减少不能抵消总体成本的增长。 关于使用 Keras 训练多 GPU 模型的备注 学术界和行业很多人都非常喜欢使用 Keras 等高级 API 来开发深度学习模型。因为这是接受度最高和开发活动最活跃的深度学习框架之一，用户期望无需额外的处理就能切换成多 GPU 模型。但实际情况肯定不是如此，如下图给出的证据。多 GPU 的加速效果是相当难以预料的——在「双 GTX 1080」服务器上多 GPU 训练有明显的加速，而在「双 P100」服务器上多 GPU 的训练速度甚至比单 GPU 还慢。在我调查这个成本问题时，我也在 GitHub 上看到了其它一些对此的博客和问题讨论。 图 2：使用 Keras 在多 GPU 和单个 GPU（这些机器的其它方面完全一样）上训练所用的训练时间。 对于健全性测试（sanity testing），我们在训练结束时检测了最终的模型准确度。在表 1 中可以看到，没有显著的差异表明底层的硬件/平台对训练质量有影响，所以该基准评测的设置是正确的。 GPU 价格经常变化，但目前来看，AWS 以 0.9 美元/小时的起步价提供 K80 GPU（p2 实例），并且按使用秒数计费，而更强大和性能更高的 Tesla V100 GPU（p3 实例）的起步价为 3.06 美元/小时。其它服务还包括数据迁移、弹性 IP 地址和 EBS 优化实例，这些需要额外的成本。GCE 是一个经济实惠的选择，可以分别以 0.45 美元/小时和 1.46 美元/小时的起步价提供 Tesla K80 和 P100。这些都是按秒收费的，并且还提供了激励方案，会根据使用情况提供折扣。尽管与 AWS 不同，它们需要附加到一个 CPU 实例上（0.0475 美元/小时的 n1-standard-1）。 在低成本方面，Paperspace 和 GCE 在专用 GPU 费率上差不多，从 0.4 美元/小时的 Quadro M4000 到 2.3 美元/小时的 Tesla V100。除了通常的按小时付费，它们也有按月收费的模式（5 美元/月），其中包含存储和维护费用。Paperspace 按毫秒收费，额外付费可以购买附加服务。Hetzner 只提供了一种按月收费的使用 GTX 1080 的专用服务器，另外还需要一次额外的安装费用。 IBM Softlayer 是市场上按月和按小时提供裸机 GPU 服务器的少数几家平台之一。它提供 3 种 GPU 服务器（包含 Tesla M60 和 K80），起步价 2.8 美元/小时。这些服务器有静态配置，意味着相比于其它云提供商，其定制可能性很有限。Softlayer 的按小时收费方式更加糟糕，事实上在短期运行的任务上成本更高。 LeaderGPU 是一个相对较新的玩家，提供使用各种 GPU（P100、V100、GTX 1080、GTX 1080Ti）的专用服务器。用户可以按月、按小时或按分钟（按秒计费）选择付费方案。服务器最小的有 2 GPU，一直到 8 GPU，价格从 0.02 欧元/分到 0.08 欧元/分。 Spot/Preemptive 实例 某些平台为它们空闲的计算能力提供了很大的折扣（50%-90%）（AWS 的 spot 实例和 GCE 的 preemptive 实例），但它们可能会出人意料地终止服务。因为无法保证该实例什么时候才会再次上线，所以训练时间很难预测。对于可以应付这种中断的应用而言，这可能没什么问题，但很多任务（比如实例时间有限的项目）在这种情况下没什么好处（尤其是考虑到人力时间的浪费时）。在 preemptive/spot 实例上运行任务需要额外的代码才能很好地处理实例的中断和重启（检查点/将数据存储到永久磁盘等）。另外，价格波动（就 AWS 的情况）会导致成本随基准评测运行时的计算能力供需情况而发生变化。需要多次运行才能得到平均成本。鉴于我完成本基准评测的时间有限，所以我没有做 spot/preemptive 实例的评测。 Paperspace 在性能和成本上似乎领先一步，对于只是想实验深度学习技术的偶尔使用一次/不经常使用的用户而言尤其如此；另一份评测报告也有类似的结果，参阅：https://goo.gl/rL02rs 专用服务器（比如 LeaderGPU 提供的那种）和裸机服务器（比如 Hetzner）很适合考虑长期使用这些资源的重度用户。注意，因为它们在服务器定制方面的灵活性较小，所以要确保你的任务是 CPU/GPU 高度密集型的，这样才能受益于定价。 不要忽视 Paperspace 和 LeaderGPU 这样更新的提供商，因为它们可以帮助减少大量成本。因为相关的惯性和平台切换成本，企业可能并不愿意更换提供商，但这些更小的平台确实值得考虑。 对于想要实现与提供商的其它服务集成整合（人工智能集成——亚马逊的 Rekognition、谷歌的 Cloud AI）的人来说，AWS 和 GCE 可能是非常棒的选择。 除非你计划运行需要数天时间才能完成的任务，否则坚持使用低端单 GPU 实例才是最好的选择（参阅：http://minimaxir.com/2017/11/benchmark-gpus）。 高端 GPU 速度快得多，但投资回报率（ROI）实际上更差。只有当更短的训练时间（更低的研发周期延迟）比更高的硬件成本更重要时，你才应该选择高端 GPU。 原文链接：https://rare-technologies.com/machine-learning-benchmarks-hardware-providers-gpu-part-2/ "
199,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738301&idx=4&sn=eb0b6f606ed290100446616fb2a84e7e&chksm=871aca03b06d431527134a7e86d1fa2b109870ace1022c739f399549395ecc88ecbe1d000d5d&scene=27,学界 | 通过扭曲空间来执行数据分类：基于向量场的新型神经网络架构,"最近，向量场被用于分析生成对抗网络（GAN）优化问题，并在对 GAN 局限性的洞察和理解，以及扩展方法上取得了相当不错的结果。本论文提出了一种新的架构，将向量场作为激活函数而获得强大的非线性属性。以二值交叉熵作为损失函数，作者通过随机梯度下降方法优化向量场，并在小数据集上取得了不错的效果。 通过将向量场的概念应用到神经网络，可以在其中发现大量已建立的数学和物理概念、抽象和可视化分析方法。例如，本研究利用了欧拉的求解常微分方程的方法 [11] 实现将数据点作为粒子随向量场流动的过程。 本文利用三个二维非线性可分数据集完成计算实验，并使用了由简单高斯核函数生成的向量场。在不同的初始化超参数下，损失函数一致地随 epoch 的增加而减少。此外，作者也进一步分析了实验结果。 论文：Vector F ield Based Neural Networks 论文地址：https://arxiv.org/abs/1802.08235 本文提出了一种新的神经网络架构，它结合向量场中丰富的数学和物理思想，并将向量场作为隐藏层对数据进行非线性变换。其中，数据点被当成粒子，遵循向量场定义的方向而流动，直观地表征了分类过程中数据点的变换。该架构将数据点跟随向量场的流线从初始分布移向新的分布，其最终目标是将不同类别的数据点分离。本文通过梯度下降学习该向量场，解决了优化问题。 2 向量场神经网路 N 维空间中的向量场是一个平滑函数 K：R^n → R^n，对应的常微分方程（ODE）： 其中 X ∈ R^n，ODE 的解曲线 X(t) 被称为向量场 K 的流线。给定在时间 t_0 上位置为 X(t_0) = X_0 的粒子，其物理解释是每一个向量 K(X) 表示作用于给定空间位置中粒子的速度，流线表示粒子沿着路径 X(t) 传播时所完成的位移。在时间 t_N > t_0 时，粒子将处于位置 X(t_N)。 给定由一些参数θ定义的向量场族 K(X, θ)，作者提出了一种在向量场族中搜索最佳向量场以变换输入空间中所有点 X_0 的方法。此外，在变换空间中的点 X(t_N) 间，不同类别的点可以线性分离。直观上，向量场表征了使得数据线性可分的变换。 作者使用了欧拉的方法 [11] 以利用 X_N 逼近 ODE 的解 X(t_N)，其中可离散化为 X_i ≈ X(t_0 + ih)，K(X, θ) 可作为我们迭代更新的向量场： 其中 h 是步长，N 是迭代数，因此 t_N = t_0 + Nh 是超参数，θ 表示向量场的参数。对于欧拉方法，当 h → 0，K(θ, X) 的流线可以精确计算。 下图 1 展示了由向量场层级架构变换的输入数据，它还提出了旨在线性分离数据的最优化向量场。注意架构的最后一层为线性分离器，它可以通过 Logistic 函数实现。 4 结果和讨论 本文使用了两个 scikit-learn 机器学习数据集 [12]（moons 和 circle）和一个正弦数据集（由作者创建）。 在图 4 中，可以看到初始的边界层在变换后的空间中转换成了一个超平面。虽然该算法通过弯曲空间和将圆的中心提取到外部而获得了很好的分类结果，它还生成了初始空间不同点的重叠。 一种缓解出现变换空间的数据点重叠的方法是利用正则化，正则化将作为阻尼器，将初始空间中的粒子移动平滑化，以防止初始空间的不同点在变换后的空间中重叠。 "
200,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738301&idx=3&sn=da7a9cc1de33c896277b74037566bb3f&chksm=871aca03b06d4315fdc5d797b687bc600a3cce747d4ae6001a7e88687af6441df772e4772316&scene=27,报告 | 普华永道发布2018 AI预测报告：塑造商业策略的8个洞察,不久之前，普华永道发布报告《2018 AI predictions：8 insights to shape business strategy》，对 2018 年的 AI 趋势进行了预测，并介绍其对商业、政府和社会的影响。机器之心对该报告的重点内容进行了编译，全文详见报告地址。 报告地址：https://www.pwc.com/us/en/advisory-services/assets/ai-predictions-2018-report.pdf 人工智能非常复杂，且发展迅速。AI 在一些领域做了很多，在另一些领域做得较少，这是任何人十年前都无法预测的。今天，任何人都几乎不可能预测未来 5 到 10 年人工智能将会给大家呈现什么。但这不代表我们不可以大胆地预测下一年或者写一个十年人工智能将会带来什么。 本报告的目标是不同的： 为未来 12 个月的 AI 趋势进行了预测，并介绍其对商业、政府和社会的影响。我们对进行短期预测很自信，因为这些初期趋势早已在进行了，只不过还没有获得应用的关注而已。 我们做出了 8 个预测。这些预测不仅仅基于人工智能远见者和计算机科学家，也基于普华永道保险、咨询、税务服务的领导从困惑于如何将 AI 应用到自己公司、帮助员工适应 AI 社会的客户处所观察到的现象。 每个人都经常看到这样的新闻标题： 机器人和人工智能将会摧毁我们的工作。但我们没有从另一方面看。反之，我们都看到了一个更复杂的景象：随着 AI 的到来，工作市场在逐步扩大，如果准备充足，这将是正面的影响。新的工作将会补偿失去的工作。人还是会在工作，而且他们在人工智能的帮助下将会工作得更有效率。 启示 大众对人工智能的接受将会很快实现 「人工智能会摧毁大量工作」将会是个假警报，人们会更乐意地接受工作场景和社会中出现的人工智能。我们也许更少地听到机器人在接替我们的工作，而是更多的机器使我们的工作变得简单了。这种转变将导致人工智能被接受的速度比一些机构预测的更快。 机构的重组即将开始 这将会是一个漫长的过程，但一些高瞻远瞩的机构已经在破除将数据分成独立仓库和把员工分成独立单元的做法。一些机构也已经开始大量地组织人工智能和其他数字技术的培训。这些培训不只是教授新的技能，它将教授一种新的着重于与同事和人工智能合作的思维。 很多出版物描述的人工智能驱动的未来看起来非常神奇：从不会发生事故或发生交通拥堵的自动驾驶车队、在毫秒内就能诊断疾病的机器人医生以及优化人力和货物流动的智能基础设施等。所有的这些都会来到我们的身边，但不会在 2018 年就实现。 启示 商业问题将为 AI 打开大门 领导者并不需要完全采用 AI，但在他们寻求商业需求的最佳解决方案时，AI 将发挥越来越重要的作用。你认为商业组织是否希望有自动化记账、一般会计和预算以及其它更多合规职能？是否希望部分实现采购、物流和客户服务等过程的自动化？人工智能很可能成为解决方案的一部分，不论使用者是否察觉到了它。 需要新的投资回报率（ROI）度量方法 有时候衡量 AI 价值最好的方法就是使用与其它商业投资相同的度量标准：度量收入的增长或者其他开销的降低。但是 AI 最强大的优势通常是间接的，因此商业组织可能会希望探索其它的 ROI 度量方法。自动化全职等效人数（FTE）能够捕捉 AI 如何从平凡的工作中释放人类劳动力。其它的指标还可以展示 AI 如何能改善人类的决策制定与预测。 许多公司管理人没有看到他们投资大数据的收益。然而业务和技术高管认为他们可以利用这些数据做更多的事情，这其中存在着沟通隔阂。但问题是学习曲线陡峭、工具仍不成熟，且面临着相当大的结构组织方面的挑战。 与此同时，商业组织当前还是可以利用新工具和技术进步的优势，包括： 挖掘结构化不足的数据的简单方法，包括用于文本索引和分类的自然语言处理 包含更多 AI 成分的企业应用套件 新兴的 lake-as-a-service 数据平台 利用不同类型数据的公共云平台 自动化的机器学习和数据管理 启示 成功将引领成功 那些已经在一项应用中成功执行数据管理的企业将会在下一个计划中领先一步。他们将开发最佳实践以有效利用数据资源，并跨组织边界进行工作。 第三方数据提供商将蓬勃发展 商业组织准备内部数据而支持人工智能和其它创新是无可替代的，但是可以有补充：供应商越来越多地提供公开数据源，并将其组织成数据池以为 AI 的应用做准备。 更多的合成数据将到来 随着数据变得更加有价值，合成数据和其它精简或增强数据学习技术将加速进步。例如，我们可能并不需要一整支自动驾驶车队在道路上生成的数据，而只需要少数车辆加上复杂的数学运算就能满足模型的训练。 随着 AI 技术扩散到越来越多的具体领域，数据科学家和 AI 专家通常缺乏的知识和技能将变得越来越重要。 目前在计算机科学家之间已经展开了投标战，但要使 AI 技术获得成功，目前的顶尖 AI 人才资源还不够。机构需要引入能与 AI 专家合作的领域专家。他们并非一定是程序员，但他们必须理解数据科学和数据可视化的基础，以及 AI 的工作（思维）方式。 随着 AI 走出计算机实验室并进入日常工作流程，这些领域专家甚至将比计算机科学家更重要。很多领域专家将需要适当地学习新技能。 启示 更快地提升技能意味着更快的 AI 部署 希望依靠 AI 优势发展的企业不该仅仅在最聪明的计算机科学家上投资。如果他们希望让 AI 技术更快地迭代和更高效地运行，他们应该为功能专家提供对 AI 技术的理解能力。更大型的机构应该分清轻重缓急，即确定 AI 技术更可能首先出现失败的地方，并从这里开始优化。 通过提升技能可以获得新的学习方法 机构需要提升他们大量员工的技能，即学习数据科学的基础和开发 AI 应用的思维方式。这项任务影响重大，企业必须寻找评估高潜能学习者的技能的方法，并给予他们个性化的学习路径，从而能快速地提升自我。 AI 在哪项工作中已经显露超越人类的能力？黑客。比如，机器学习可以轻松使得恶意攻击者追踪你在社交媒体上的行为，然后为你定制钓鱼推特或电子邮件。人类黑客无法这么快、这么好地完成这项工作。 AI 越发展，其对网络攻击的影响就越大。先进技术，如机器学习、深度学习、神经网络，使计算机找到和破译模式。它们还能找到和利用漏洞。 智能恶意软件和勒索软件随着扩展不断学习，机器智能辅助的全球网络攻击，先进的数据分析师来定制攻击，很不幸，这些可能很快就冲着你的组织而来。AI 本身，如果不好好保护，也会生成新的漏洞。比如，恶意使用者可以向算法的训练数据集中添加有偏数据。 启示 不要拿鸡蛋去碰石头，要选择合适的武器 从企业层面来看，很多公司可能会选择降低在 AI 方面的发展或利用速度，但是网络安全不能停滞不前：攻击者将使用 AI，因此防御者也必须使用 AI。如果一个公司的 IT 部门或网络安全提供商没有使用 AI，那么它必须立即思考 AI 在安全方面的长短期应用。使用案例包括分布式拒绝服务攻击（DDOS）模式识别、优先处理升级和调查日志警报，以及基于风险的认证。 网络安全可能加速企业对 AI 的接受度 谨慎看待和使用 AI 的公司为了网络安全也将使用 AI，网络防御将是很多公司使用 AI 的第一次尝试。进一步刺激公司接受 AI 的地方在于数据需求：AI 在公司内部可获取数据的规模越大，其防御网络威胁的能力越强。一些公司已经构建基于云的预置「威胁湖」，其利用了 AI 的能力。 AI 黑客可能增加公共恐惧 很多人对 AI 感到焦虑不安，更多人担心网络安全。很可能今年 AI 登上头条时，不是因为它帮助了人类，而可能是它帮助了一次大型黑客行为。更好的网络安全可以缓解这种风险。除了为 AI 能力开发特征集，网络安全性增强还要求企业增强数据和计算平台，支持高级分析师拥有优先权限进行监控、对象级变化管理、源代码 review 和扩展的网络安全控制。 AI 自主武器会成为连环杀手吗？被命令减少空气污染的 AI 系统会决定最合乎逻辑的方式是消灭人类吗？这样的恐惧可能对惊悚片有利，但是这种危险可以管理。 这里介绍一下很多 AI 支持者不愿意提及的秘密：AI 并没有那么智能，至少目前是这样。尽管 AI 在模式和图像识别、自动化执行复杂任务、帮助人类决策方面越来越强大，但它仍然只能用于特定任务，且不具备像人那样的智能。 现实风险 如果 AI 总是可控的，那么它未必总是可理解的，很多 AI 算法超出了人类的理解。很多 AI 供应商没有揭露其程序保护知识产权的原理。在这两种情况中，AI 作出了决策，但其终端用户并不清楚它如何作出的决策。这就是「黑箱」，我们无法看到它的内部是什么。 启示 很多黑箱将被打开 我们期待企业能够面对终端用户和监管机构不断增长的对于部署可解释、透明和可证明的 AI 技术的压力。这可能需要 AI 供应商共享一些秘密，也可能需要深度学习用户和其他先进 AI 科学家开发新技术，来解释之前不可理解的 AI。 企业面临权衡 大部分 AI 可以解释，但是成本高昂。和其他任何流程一样，如果每一步都必须归档和解释，则流程变慢，成本增长。但是打开黑箱将降低某些风险，帮助建立利益相关者的信任。 企业需要 AI 可解释性决策的框架 可解释性、透明度和可证明性不是绝对的，它们依据于一个标准而存在。一个评估商业、性能、监管和信誉的框架可以帮助做出最佳决策，使每个 AI 使用案例能够按照该标准进行。使用 AI 帮助制定生死攸关的医疗决策的医疗公司和确定潜在目标的私募股权基金的需求是不同的。 人工智能将出现巨大增长：根据我们的研究，人工智能产业规模在 2030 年将达到 15.7 万亿。人工智能这张大饼已经超过了任何公司，很多国家在研究出台新战略从而获取最大利益。 美国一开始就很强势，2016 年接连发布三份报告，其中列出了一个让美国变成人工智能强国的计划，以促进经济和国家安全。 建议包括增加联邦经费、制度调整、创建分享公共数据集和环境、对标准和标杆的定义、劳动力发展，及使用人工智能增强网络安全和军事力量。 但自 2017 年川普执政以来，（美国）政府已经放弃了这项计划，政府在消减人工智能研究经费。 但最近通过的税费改革可以使人工智能在美国蓬勃发展。降低企业税政策、海外回迁现金政策以及允许增长海外 100% 的投资非常可能鼓励人工智能和其他领域的科技发展。最近行政机构注重的放松管制政策可在很多方面帮助人工智能发展，比如在无人机和无人驾驶车领域。 启示 中国的投资将会唤醒西方 如果中国开始制造领先的 AI 应用，西方将会回应。不管是「（前苏联）人造卫星时刻」，还是逐渐认识到其在失去主导地位，西方政策制定者可能感受到了改变规则并为 AI 提供资金的压力。 将会出现更多的国家和地区策略 更多的国家将会发布影响公司的 AI 策略。不难看到欧洲已经发布一般数据保护条例（GDPR）保护个人数据，并制定了在该区域促进 AI 的政策。 协作也会到来 AI 的国家之争不会停止。但是我们确实希望联合国、世界经济论坛和其他多边组织提供更多的机会，推动不同国家在 AI 研究领域的国际合作。 新技术经常会带来不知是否合理的新恐惧，并且影响超出阴谋论者的范围之外。据 2017 年 PwC 调查，77% 的公司 CEO 认为 AI 和自动化将进一步影响并瓦解他们的业务运作方式。如果我们询问政府官员，答案将会相似。领导者们将很快不得不面对有关 AI 的棘手问题。他们可能是社区组织，担忧不公的选民，也可能是担心可靠性的客户，或者是关心风险管理、ROI 和品牌的董事会。 在上述所有情况中，利益相关者想要知道组织和机构正负责任地使用 AI，从而完善了业务，强化了整个社会。我们相信，将负责任地使用 AI 作为准则才是压力的来源。 启示 负责任 AI 的新结构 当面临设计、构建和部署值得信任和激励的 AI 系统时，许多公司会建立团队和流程来寻找数据和模型之中的偏见，密切监控恶意行为者可能「欺骗」算法的方式。人工智能治理委员会也可能适用于许多企业。 公私合作以及政府与公民的合作 负责任地使用 AI 的最好方式之一是推进公、私部门机构之间的合作，尤其是当涉及到 AI 的社会影响之时。同样，当越来越多的政府探索如何使用 AI 来有效分配服务，公民也就逐渐参与到这个过程之中。比如，在英国，RSA（鼓励艺术、制造和商业的皇家学会）正在进行一系列有关在刑事司法和民主辩论中使用 AI 及其伦理学讨论的公民陪审团。 加强负责任创新的自律组织 由于监管机构可能着急赶上，且自我监管有其局限，自律组织（SRO）可能会带领负责任的 AI。SRO 根据特定原则把 AI 用户聚集在一起，然后监督和规范合规性，按需征收罚款，并将违规行为提交给监管机构。这是一个已在其他行业使用的模式，可能也同样适用于 AI 及其他技术。 
201,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738301&idx=5&sn=ffa2bd12e4562f4775ce2afde278b000&chksm=871aca03b06d43153cded111c08e84dad5e8f2fbd40332d60e8b54e61391a2c1826aafffbc6f&scene=27,机器之心新年全球招聘：开发、记者、分析师与各类实习生,机器之心是全球化的人工智能信息服务平台 ，一直为人工智能从业者提供高质量知识及商业信息，主营业务包括媒体、报告、数据库产品、活动和产品服务等。不久前， ，投资方包括星路资本、人民网和 TalkingData 等，这是机器之心的第三轮融资，此前还获得联想之星、今日头条、科大讯飞和第四范式等机构的天使轮融资的Pre-A轮融资。 机器之心将持续生产和运营高质量内容，并正在开发全新的信息产品，我们需要全球范围内更多的开发、内容、分析和运营人员加入。 工作职责： 负责栏目整体内容建设的执行与效果分析，系列专题的选题策划、组织推进，并配合平台达成内容团队的考核目标。 岗位要求： 拥有足够的信息敏感性、一定程度的产业洞察力、认识和解读行业命题的逻辑框架； 接受过专业的采写训练，具备专题策划能力； 在科技、商业类媒体工作 3 年以上，有管理记者的经验、熟悉人工智能行业是加分项。 工作职责： 负责常规内容生产，挖掘和研究国内外人工智能领域优秀创业公司，并撰写相应的人物、公司和产业报道。 岗位要求： 对人工智能前沿技术及其产业应用有浓厚求知欲、好奇心； 具备基本商业常识，逻辑性强，能够多角度思考热点话题； 有良好的职业态度与团队协作精神，接受过专业采写训练，在科技、商业媒体工作 2 年以上。 工作职责： 编译、校对英文文章。 协助编辑撰写技术、产品、公司和行业相关文章。 岗位要求： 良好英语翻译能力； 对前沿技术感兴趣，有热情。 工作地点：北京、武汉 
202,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738252&idx=4&sn=536833cc427ad6b1515e9ed5d09769f1&chksm=871aca32b06d43249b8ac21271cc73e6f95d539dd3401bb6a4836c9519d5a1919725743fbf14&scene=27,CVPR 2018 | 美国东北大学提出MoNet，使用紧密池化缓解特征高维问题,"近日，来自美国东北大学和美国信息科学研究所的研究者联合发布论文《MoNet: Moments Embedding Network》，提出 MoNet 网络，使用新型子矩阵平方根层，在双线性池化之前执行矩阵归一化，结合紧凑池化在不损害性能的前提下大幅降低维度，其性能优于 G^2DeNet。目前该论文已被 CVPR 2018 接收。 将图像的局部表示嵌入成既具有代表性、又不受轻微噪声影响的特征，是很多计算机视觉任务中的重要一步。在深度卷积神经网络（CNN）成功之前，研究人员使用手动的连续独立步骤解决该问题。典型包括 HOG、SIFT、协方差描述子、VLAD、Fisher 向量和双线性池化。尽管 CNN 是端到端地训练的，但是它们可以被看作两部分：卷积层负责特征提取步骤，后面的全连接层是编码步骤。现在已有多项研究探索用卷积嵌入方法替换全连接层，无论训练采用两段式还是端到端方式。 双线性 CNN 由 Lin et al. 首次提出，旨在池化不同空间位置的二阶统计信息。双线性池化已被证明在多项任务中有用，包括细粒度图像分类、大规模图像识别、分割、视觉问答、人脸识别和艺术风格重建。Wang et al. 提出，使用高斯嵌入层纳入一阶信息。实践证明，归一化方法对这些 CNN 的性能也很重要。研究者提出了两种归一化方法用于双线性池化矩阵：对于 其中 表示局部特征，一方面，由于 M 是正定对称矩阵（SPD），Ionescu et al. 提出使用矩阵对数（matrix-logarithm）来将 SPD 矩阵从黎曼流行映射到欧氏空间，即 （ ） 另一方面，Wang et al. 提出矩阵方幂（matrix-power）方法，将 M 非线性地扩展到 。两项研究中，矩阵方幂的性能和数值稳定性都优于矩阵对数。此外，Li et al. 对矩阵方幂归一化在解决通用大规模图像识别问题上的优秀性能提供了理论支持。因此，本论文提出将矩阵方幂正则化整合进 MoNet 架构中。 上述特征编码的一个重要缺陷是编码后特征的维度极高。由于张量相乘，最后的特征维度是 ，其中 C 是最后一个卷积层的特征通道数。即使在 C 相对较低的情况下，如 VGG16 中，C = 512，最后特征的维度也超过 260K。该问题可通过随机投影（random projection）、张量速写（tensor sketching）和低秩属性来缓解。但是，由于矩阵方幂归一化层应用在池化矩阵 M 上，因此很难结合矩阵归一化和紧凑池化来同时达到更好的性能和更低的最后特征维度。 本论文使用同质填充局部特征（homogeneous padded local feature）的张量积重写了 G^2DeNet 的方程，使之对齐 BCNN 架构，以使高斯嵌入操作和双线性池化解耦合。本论文没有特别关注双线性池化矩阵 M，而是推导出子矩阵平方根层，对（非）同质局部特征上直接执行矩阵方幂归一化。在新型子矩阵平方根层的帮助下，研究者利用紧凑池化逼近张量积，同时使维度更低。 本论文的贡献有以下三方面： 利用实证矩矩阵（moment matrix）结合 G^2DeNet 和双线性池化 CNN，并将高斯嵌入与双线性池化解耦合。 提出新型子矩阵平方根层，在双线性池化层之前直接对特征执行归一化处理，从而利用紧凑池化降低表示的维度。 利用矩阵反向传播推导出子矩阵平方根层的梯度，这样 MoNet 架构可以进行协同优化。 MoNet MoNet 网络的架构概述如上述图 1 所示。在本节中，我们将详述每个模块的设计。 对于输入图像 I，ReLU X 之后最后一个卷积层的输出由整个空间位置 i = 1, 2, . . . , n 上的局部特征 x_i 组成。接着，我们将其映射到齐次坐标，方法是添加额外的值为 1 的维度，并把所有元素除以 。之后，应用一个适当的子矩阵平方根归一化。最后，使用一个紧密双线性池化层池化整个空间位置中所有 n 个特征，并在最后的全连接层之前进行逐元素的平方根正则化和 归一化。 论文：MoNet: Moments Embedding Network 论文链接：https://arxiv.org/abs/1802.07303 近期双线性池化作为一种特征编码层被提出，可在深度网络的卷积层之后使用，提升在多个视觉任务中的表现。与传统的全局平均池化层或全连接层相比，双线性池化以平移不变式的形式收集二阶信息。但是，这一池化层家族的一个严重弊端是其维度爆炸。为解决这一问题，已探索了紧密的近似池化方法。另外，最近成果表明，通过矩阵归一化来调整不稳定的较高阶信息可获得显著的性能提升。然而，紧密池化与矩阵归一化的结合至今未被探索。 在本论文中，我们通过实证矩矩阵结合了双线性池化层与全局高斯嵌入层。此外，我们提出一个全新的子矩阵平方根层，借助此层，可以直接归一化卷积层的输出，并通过现成的紧密池化方法来缓解维度问题。我们在三个广泛使用的细粒度分类数据集上进行了实验，实验表明，我们提出的 MoNet 架构相比 G^2DeNet 架构有着更好的表现。与紧密池化技术结合使用时，本方法可以用维度数降低了 96% 的编码特征获得可比的表现。 "
203,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738252&idx=2&sn=445d2b0f571514e9e7574ed17cde7a93&chksm=871aca32b06d4324856666d3b4eb2e83b0292d22a71bc67ef4a9f49729017ac5f71cfd911064&scene=27,专访 | 这位「计算」的信徒，要用机器智能塑造城市的未来,过去一年，从技术向产业，有哪些值得记住的人和事？未来一年，AI 场景化落地还有哪些可能性？ 8 位 AI 行业局内人，向我们讲了讲他们的故事和看法。 搜索王坚，几个关键词出现频率很高——不懂技术、大忽悠、疯子、先知…… 很多人在讨论他，从当年那些饱受争议的细节贴片里「揭秘」这位「阿里云先生」的功过是非。 连他自己也曾在给全体员工颁发阿里云「飞天奖」时说，「坚持就是伟大」，颇为悲壮。 这话说来贴切，熬过最艰难的两年时间，拥有机会窗口的阿里云开始进入快车道。在阿里云正式对外开放的两年后，阿里巴巴招股书里提到，2013 财年云计算收入超过 6.5 亿元人民币，截至 2013 年底，已经有 98 万客户使用阿里云计算平台。同年，阿里云开启全球化步伐。到了 2017 年，营收数字突破 100 亿元。 阿里云也一直踩在互联网行业的机会点上。云计算让阿里巴巴在技术布局上有了坚硬基础，两年前开始的「城市大脑」计划则进了首批国家人工智能开放创新平台名单手握官方「船票」。 城市大脑已经和杭州、苏州、吉隆坡、上海等城市合作起来，它被认为是阿里云产业 AI 革命的动力，王坚将它类比登月计划。他也不止一次说明自己对这件事的着迷和重视： 「城市可能是人类有史以来发明的最大智能硬件，这么复杂的智能问题放在我们面前时，就很难用传统意义上的人工智能来描述了。」 这背后实际上是计算的重要性持续凸显，无论是互联网还是谈人工智能，越来越需要利用高效计算在云和端之间对资源进行合理分配。对王坚而言，所有的「先知」、「预言」，都来源于他对「计算（computing）」本身的坚定相信。 我们在约见的云栖科技博悟馆里，看到了名为《2050 年的杭州》的装置展示：一朵「云」下面是 2050 年杭州的城市面貌。这是王坚对机器智能塑造未来城市的积极想象，「水、电、道路，所有的公共资源只需现在的十分之一，建筑不像现在这样越长越高，而是越来越矮。」 我们问到如果你来回答「王坚是一个什么样的人」，会想说点什么？ 王坚提到了——乐观和在任何环境变化中都能不断找到新乐趣。 「计算」，始终是一切新乐趣的支点。 以下为机器之能对王坚的专访实录，编辑做了不改变原意的删减。 很多讨论大部分围绕你对阿里云的预判与坚持，也有人给你贴先知的标签，很好奇你是怎么做决策的？ 有几个因素交织在一起的，撇开我个人因素，大家对阿里做云计算也有很多不同看法，甚至认为阿里不是做这种事情的公司。也有对中国有不同看法的因素，10 年前中国一家公司突然要做一个技术平台，并把技术平台做成商业，对大家来说，都是挑战。当时中国第一次开所谓的大规模云计算大会，唱主角的都是传统 IT 公司。阿里云那时候都还没有太多收入，中国随便找一家软件公司的收入都比阿里云大。所以我也觉得大家有这样那样的看法，是很容易想象和接受的事情。 至于是不是先知，是另外一件事情，我知道计算的重要性，这一点确实比别人更坚决一点。至今对我来说，没有 GPU、CPU 的区分，归根到底是计算对社会进步的重要性。 过去我们会讲计算机的重要性，把屏幕、键盘和鼠标这些都包括进来。但慢慢地你就发现这里面核心是在计算上，现在开始强调计算的重要性，这是认知上巨大的进步。看我的《在线》一书就会知道，我对云计算有自己的理解，我是坚信计算对未来发展的重要性。 回过头看，你觉得有哪些你认为的对或错的决定？你有为做一个决定而为难的时候吗？   其实是没有，我觉得很大的因素是直觉决定的。对我来讲，难的不是有了几个不同的选择，如何做决定，难的是坚持了一些直觉。困难不是做决定，而是有一个方向之后，不要被周围嘈杂的声音带偏了方向。 举个最简单的例子，对我来讲计算这件事情是几乎没有改变过的，这不见得是我的一个决定，却是非常重要的直觉和判断。当时确实有人认为做网盘就是云计算，可能今天很多人还是这么认为。做网盘可能是当时最容易看到所谓变现和商业场景的应用，公司内部也有这样的声音。从需求角度也没错，当时的确很多业务需要图片存储能力，但这些对我来说就是会带偏方向的「噪音」。 2008 年，我们刚开始做的时候，今天大多数流行的开源平台都没有出生，也没有特别可以借鉴的，所以要不要做我们称为「飞天」的自己的技术平台？这是非常大的决定。今天很难有人体会那个决定有多大，实事求是说，这个决定可以把一家公司搞垮。这也是我觉得责任重大的地方。现在看来，这个决定是对的，变成了我们的核心竞争力。今天都说自己有云计算平台，但我可以自豪地说我们有「飞天」。 类似的事情有很多，说一件阿里云发展历史上最有意思的一件事情，阿里云的网站是在公司成立两年以后上线的，一般互联网公司都是公司成立的同时就有网站了。阿里云也可能是中国互联网圈里唯一一家成立时没有网站的公司。 有一次我说大家认为我们是皮包公司，因为你是互联网公司，那你怎么连网站都没有？其实大家不明白我们的网站就是我们的产品，我们用了两年时间开发了最初的阿里云产品。这也是为什么阿里云成立时没有上线网站，因为它是特殊的。我们因此跳过了传统 IT 公司对云计算产品的认识，从这个角度来说，这也是一个非常困难的决定。 你刚刚说，最难的是坚持，那么很多决定也是你自己在跟自己较劲的过程吧。 其实我也没有那么痛苦去做一个决定，我甚至都没有说排个方案出来，应该是直觉吧。但是坚持按照直觉做下去，这还是有它的难度。   云计算技术平台这样一件大事，可能很多人的想象是，你要先说服马总或董事会，你是如何促成这些人做出一个明确的表态？ 一个真正有意思的事情不可能是大家讨论出来的，大家讨论的是怎么帮助你，支持你，但做不做不是大家讨论出来的，这是我长期以来的观点。我在阿里没有定过 KPI，也没有写过商业计划书，结果一个朋友跟我说，这是因为你的业务在公司不重要（笑），所以这件事大家也是仁者见仁智者见智。但是不管别人怎么看，我能看到马总对这件事的坚定，所以我觉得任何有关创新的决定并不是一个逻辑上可以推理证明的事情。 所以那时候你们两个人对做技术布局是有默契的，可以这么说吗？ 这个啊……一总结这个味道反而不太容易表达出来。我觉得马总是对技术有向往的人，这样讲更加确切一点。他对技术的向往甚至超过很多做技术的人，很多做技术的人可能觉得它只是技术而已。向往可能是最好的状态，甚至比理想都要好，理想还要想着去实现，多少还带有一定程度的功利，我是这么看的。所以我觉得「向往」是非常了不起的事情。我有时候说，做技术的人也是太现实了，就是这个原因。 现在阿里云的营收状况、国内市场占比情况都不错，你觉得需要有危机感吗，或者现在有吗？ 从大的方面来看，我最早做阿里云的时候就说我们要做通用计算，从计算需求角度来说，我们今天还只是开始的开始的开始。从大型机、小型机到个人电脑、手机，计算装置形态的变化很快，可能一、二十年就变一次。但是对计算需求的不断增长是很长久的。实际上，云计算就是满足人民日益增长的对计算的需求。计算的需求会长时间的存在，并且高速增长，可以用 100 年这个尺度来讨论。把今天所有的计算市场份额加起来，也可能只是 20 年以后的千分之一的市场。 这点跟一般的日常消费不一样，人再厉害，不能一天吃 100 斤龙虾，总能看到上限。但大家对计算的需求是没有上限的。人工智能出来以后，带出新的计算需求。由比特币带来的挖矿搞得显卡供不应求。先不说比特币本身有没有道理，但这一现象反映的也是计算上的需求。所以，从行业来说我没有危机感，但是从能力角度来说是一个很大的危机感，你有没有能力满不断增长的计算需求，这是对技术和商业能力的综合考验。 你之前说过云计算国际化，其实是倒逼一个企业具备全球服务能力，而不是说今天把这个东西卖到哪个市场上这种传统企业的逻辑。那我想听听你说一下阿里云的全球服务能力，主要体现在哪些方面？   为什么要有全球服务能力？不是你搞了一个办事处，你就是全球的了，是因为你的客户是全球跑的，你的服务能不能到那里。过去你卖一个产品可以说我不卖，但今天你说了不算数，因为你的客户在那里。我觉得这是非常大的变化，云计算也是倒逼我们有这么一次变化。 阿里云做的很多事情就说明了这个问题，一，比如要做云计算服务，当然要有全球的基础设施。在各大洲建数据中心，也是因为需要全球的基础设施。没有金刚钻不要揽瓷器活，是吧？二，产品要能满足，今后我们的产品在不同的国家都是同步上线的，这是非常大的改变，我们要从这些细节上慢慢体现出来。 阿里云是从什么时候开始真正重视人工智能的？ 就我个人而言，读大学的时候就接触人工智能了。实际上要用今天的 AI 标准的话，我觉得阿里云成立的第一天我就开始做这件事了。阿里云以前也做过输入法，做输入法的目的不是为了做输入法，是为了做自然语言处理。如果从这个角度，用我们今天比较流行的话说，我们第一天就是 AI 公司。 但这么说没有意义，如果讲真正意义上对智能技术领域关注，是 2014 年成立 iDST（Institute of Data Science & Technologies），这对我来说是蛮有标志性的，最早招的几个人包括金榕、漆远都是公司在这方面的骨干。iDST 这个名字也反映了我们的思考，以前我读大学时的 AI 实际上是探讨逻辑跟智能的关系，现在我们关心数据跟智能技术的关系。 也是这段时间的工作，让我开始理解，所有机器能做的事情，都不应该人去做的，机器会把人解放出来。所以我现在会很强调机器智能这个概念。   现在看来，阿里向外输出 AI 能力的主要渠道在云平台上面，那如何理解 AI 跟云的关系？ 三个词我觉得要分开，一个是云，一个是云计算，另一个叫 AI 也好，或者叫 MI（Machine Intelligence）也好。阿里云本身就是一个巨大的计算平台，它是云计算的基础，在阿里云上的应用可以叫做云。阿里云自身做最基本的东西。我一直觉得对阿里云而言，计算重要，云是不重要，阿里云计算就是要让客户的云无所不在。为什么说以前大家觉得网盘就是云计算，根据我的理解，网盘的应用实际上就是云计算上的应用而已，是一个很重要的云计算应用，但是它不是基础的云计算服务。 那么，到今天为止，云应用跟传统应用之间的差别是什么，是机器智能或者说 AI 慢慢变成了这些云应用的很重要的不可分割的一部分。今天 AI 那么热是有道理的，就是慢慢地，各种基于云的业务里面都少不了 AI 这个成分，就是这么一个关系。AI 创造了新的计算需求。 传统上，你没有办法离开一个机器来讨论一个应用，软件它最后还是要找台机器，找不到机器这个软件你就不知道放哪里，你说 Offic 软件如果没有台机器，你哪里找到 Office？今天，你没有办法离开云计算谈 AI，这个关系是一样的。 对阿里来说，AI 的研究和应用重点体现在哪些方面，边界会是什么？ 我无法提供一个简单的答案，比如百度讲 All in AI，腾讯讲 AI in all，那阿里应该说句什么话才能匹配？我觉得还要从不同的角度来看待这个事情。 回到阿里最早的初心，我觉得阿里对 AI 最重要的体现就是将数据作为了一个起点。同时阿里是有自己的体系的，也达成了重要的共识。比如机器智能，从这个角度来看，阿里没有 AI 的研究，只有 MI 的研究。 我经常举一个例子，我一个朋友，他说：我本来是做视觉的，现在有人告诉我你是做 AI 的，他直接傻掉了。以前自然语言处理是一个专业，但今天很多人说你不是做自然语言处理，你是做 AI 的。就相当于，你做了一台 PC，但有人告诉你，你是在做一台 AI 机器，照这么下去的话，明明是电动推子给你理的发，大家却说是 AI 在剃头。所以，我觉得阿里在坚持做一个很重要的事情，就是将数据作为起点，在做机器的智能。 另外，我们坚定地相信智能技术是重要的，我认为，智能技术比计算更高一层，至少从两方面可以反映出来。 第一，在今天，智能技术非常重要，跟其他任何技术相比，都不在一个层次上，智能技术是它们的提升。像阿里做电商，以后也要基于智能技术，从这个角度说，我们不但要 all in AI，也要 AI in all，对我们来说，这是自然而然的事情。 第二，AI 真正创新的未来是什么？大多数人觉得中国的强项是在 AI 上的应用，我不同意这个观点。我认为，中国长远的价值一定不在 AI 上的应用，这是为什么我要做城市大脑的原因。城市大脑破了一个东西，它可以看到过去传统意义上的 AI 根本没有触达到的问题，我觉得这是它的价值。 城市大脑对我们重要的地方在于，我们看到了一个新的问题，这个问题是来挑战、或者推动整个智能技术的发展。这个时候，谁离问题近，谁就在创造新的东西。如果今天的方法可以解决城市大脑想要解决的问题，那我今天也不会花时间做这件事情，所以我觉得在阿里至少这两方面是我们可以看得到的。当时科技部要把城市大脑列进去，我还有点犹豫，但是今天看来是件好事。 为什么犹豫？ 我当时犹豫，是因为如果大家觉得城市大脑只是 AI 的一个大应用，那就亏大了，因为过两年大家都不谈 AI 了（笑）。但是现在命名为「城市大脑国家人工智能开放创新平台」，我觉得挺好的，城市大脑的确为人工智能提供了重要的开放创新平台，智能技术也确实是城市大脑非常重要的一部分。   世界上最遥远的距离是摄像头与红绿灯，它们在同一根杆上，但从来没有通过数据被连接过。 宣布城市大脑项目的时候，其实也不是完全准备好的一个状态，是吗？ 对，肯定是没有准备好的，某种意义上说，AI 到今天都没有准备好。因为这相当于一个新的东西。 你说的是说技术没有准备好，还是解决方案？   所有的东西都没有准备好。从理论到实践，到大家心理的准备，都没有准备好。我已经很难想象什么叫没有准备好，只能说就是没有准备好。 所以这又是一个凭着直觉的决定？ 对，这不是一个靠讨论讨论就能做出的决定。 但是你最早得跟杭州政府谈拢啊。 是没谈就拢了。要谈的事情都是谈不拢的，谈得拢的事情都是不要谈的。 如果你认真去分析当时的线索，起因是跟交通有关。我的直觉表明，交通不是现在的方法可以解决的，现在任何名词的堆积都解决不了交通问题，包括 AI 这个词。所以我认为，应该有一个新的机制来做这件事情，但机制是什么，当时没有完全想清楚，我只是用了一个词——「城市数据大脑」。为什么把数据放在里面，是因为数据是其中最关键的部分，那为什么缩写就写成「城市大脑」，因为城市大脑就是一个新的机制。没有城市大脑，数据失去了物质基础，没有数据，城市大脑便没有生命力。但这个东西到底是什么？到今天为止，我都还在定义城市大脑，甚至我自己想成立研究所专门研究它。 尽管我最早提出城市大脑，但是第一次决定做城市大脑的是杭州市，这非常了不起。阿里云牵头组织相关企业帮助杭州市在做这件事，这也非常了不起，因为那时需要有企业愿意帮一个城市做这样的创新。 但当时有一些「噪音」，比如能否叫城市决策支持系统这样的名字，我当时坚持不能叫这样的名字。  大部分人都认为城市大脑是解决交通问题，但这是有差别的，因为解决交通问题是大家都可以接受的，但叫城市大脑是对现有城市问题的重新思考，大家是否接受是要担风险的。城市大脑就是机器智能中的阿波罗登月计划，能把一大批想象不到的技术带出来，生命力会很强。 杭州后来成立了一个领导小组，就用了「杭州城市数据大脑」这个名字。这个词第一次出现在正式的文件中时，在学术界都没有出现过。我觉得这是一个历史性的里程碑。 如果说做这样的决定也是依靠直觉，那直觉是什么时候有的？   我觉得直觉还是要有积累的。为了做云计算，我跑过中国多少城市？没有几个省市是我没有跑过的，一年内，宁夏去五六次，贵州跑五六次，这样的人也不多。我不是去玩，而是去了解每个城市如何做信息化建设的。没有这些积累，也没有可能有这个直觉。 我为什么反对叫决策支持系统，为什么说城市大脑不是传统的信息化建设，甚至为什么我不说打通数据孤岛。直到今天，大家讲大数据应用，第一条还是打通数据孤岛，现在看来这种说法都有些胡扯。所有人都在说，打通数据孤岛后，做信息化建设。但我认为这是想把手指头和脚趾头连在一起，让手指头和脚趾头协调工作，这是不可能的。手指头和脚趾头能协调工作是因为靠大脑，它不是靠绑在一起。不要认为这是一个简单的道理，但今天我们几乎所有想做的事情，都是想把手指头和脚趾头接在一起，很多 AI 还想把手指头跟脚趾头做得更聪明，那更加做不到一起。 做了这么长时间的智慧城市，你会发现这是没有大脑的智慧城市。所有做的事情，还是想把手指头跟脚趾头连在一起。这一点我体会很深，当你跑过所有的地方你就知道有这个问题。 这些协调一定要靠机制，比如，智力的分配，多少智力分配在手上，多少智力分配在脑上，要有一个自然的机制。城市基础设施的建设，智力的分配是有一套方法的。所以城市大脑实际上是在改变这样的事情。   在这些城市的调研，最核心的具体痛点有哪些？ 世界上最遥远的距离是摄像头与红绿灯，它们在同一根杆上，但从来没有通过数据被连接过，摄像头看到的东西没有变成红绿灯的行动。就像手指头与脚趾头，没有一个大脑，它们是连不起来的。我觉得，做 IT 技术的不要把落后生产力带给别人，有再高明的技术，把手指头跟脚指头连在一起，那也是落后的生产力。但今天几乎所有传统信息化建设都在干这样的事情。 实际上，中国社会已经基本完成了一个数字化过程，或者计算机化的过程，打下了很好的物质基础，但还谈不上协调。这个协调不是靠手指头和脚趾头，而是靠大脑，这是我们真正面临的深层次的挑战。如果只要 AI，但没有城市大脑这样的架构，那所有的 AI 也就只能帮助怎么把脚指头跟手指头连在一起。 这个项目对团队比较大的考验是什么？因为最近在海外马来西亚城市大脑的项目，说要培养 300 位数据科学家，可能还会产生相关联的公司有 1000 多家左右，我觉得这个听起来是特别重的事情。   我觉得，慢慢发展下去这个数字一定会更大。但具体到场景，是不是这个数字，我也不好说，因为我不具体负责。但有一点可以肯定，会有越来越多的人围绕城市大脑做事。在阿里，有一个部门在做城市大脑，所以这不只是一个项目。而跟我们合作的公司里，也开始有城市大脑部门，大家接受度这么高，超乎我的想象。 做城市大脑这样类型的事情，对于这些以前埋头做研究的人来说，是不是还是一个比较艰难的调整过程？他们可能需要做一些做事方式上的转变，比如现在要泡在公安局或者厂房。   对，我觉得是一个很大的挑战。他们至少要完成三级跳。比如做视频的，对于他们来说，他们就是做视频处理的，只不过用了跟 AI 比较像的方法论，这是他们的起点，也是他们的阵地。这是第一跳。 但这个社会需要新的机器智能的引擎，所以他要从视频的处理变成引擎，这点非常重要。有了这个引擎，才能把视频处理这些技术变成动力，就像电跟马达的关系。这是第二跳。 引擎再上一层才是真正的城市大脑，这是第三跳。 我的理解，对于大部分 iDST 的研究人员来说，本来都还停留在视频处理的阵地，但正在被城市大脑的巨大场景带着完成这三级跳。 另外，能不能提出好的问题是一个最大的挑战。今天大部分的人工智能问题，也许是一辈子都解决不了，也有可能人类永远解决不了的，从这个角度来说，是很难的。但从另外的角度来看，也是很浅薄的，例如凭借常识提出希望机器能听懂人的话、认出人的脸这样的问题。 但城市大脑，离人的经验太远，想提出问题都极其需要专业知识。这对他们来说，也是挑战和困难的问题。对于交警来讲，反而更知道解决交通这个问题的难度，虽然他不知道你如何解决这个问题，但是他理解这个事情，他知道难的地方。所以我觉得大部分的问题是提了一个难解决的问题，但不见得是一个难提出来的问题。为什么阿波罗登月计划了不起？其实是因为它突然会让大家提很多你坐在地上提不出来的问题。   华先胜老师好像在城市大脑上付出的时间会多一些是吗？   对，他其实是从淘宝转到 iDST，但他不是最早进入城市大脑的。最早进入城市大脑的都是阿里云原来做智能交通的那批同学，因为一说交通自然而然就是互联网+信号灯之类的。但他们很痛苦，痛苦到很多人都不愿意见我。我也很郁闷，因为他们没有触及到城市大脑真正的问题，这是一个新的事情。城市大脑涉及到视频，从这个角度来看，华先胜是最容易切入的，他现在对城市大脑的认识深刻度也是足够的。这个变化我觉得还是蛮大的。 其实我都没有时间去想这两年的变化，这不是一般的变化。这两年对城市大脑认识的飞跃，一定会超过了这两年大家对 AI 认识的飞跃。 你之前一直说城市大脑不仅仅是一个人工智能应用，类比登月计划，这个巨大的场景是可以倒逼出很多的技术进步和创新，那目前来说它做了哪些重要的创新？   我们一点点来分析。第一，城市大脑涉及规模化的视频。说得极端一点，以前我们的视频（技术）只需要处理一个车牌号，大家都知道是用来罚款的，但突然发现，通过视频还可以知道车有没有刮蹭，这是一个巨大的进步。因为车牌号是明确的可以描述的场景，但刮蹭很难描述。除了刮蹭之外，我们还可以从视频里知道这个地方堵没堵，这就从一个确定的问题变成一个非常不确定的问题，更是一个巨大的进步。 第二，原来做车牌识别，只需要一点计算能力，但你要去识别有没有剐蹭，计算能力可能就要增加 1000 倍。1000 倍是什么概念，如果你今天做一件事情需要一年，你会好好计划。但如果这件事情要多花你 1000 倍的时间，你有生之年都干不完，可能就压根儿不会开始了。 你再往下想，以前大家说不清楚大数据是什么，但你突然发现城市多了一个新的东西，那就是数据资源。当认识到视频是城市重要的数据资源的时候，你会发现视频就不仅仅是视频了。 从本身的视频处理技术的发展，到计算能力的提升，再从大数据到数据资源，最后城市大脑就变成了城市的基础设施，管理着所有的其他基础设施。 从这几个层面来看，每个层面都值得做很深的研究。比如视频处理，本身对机器学习或者窄义的人工智能就是挑战。1000 倍的计算量，对计算能力也是个挑战。大家本来讲的是大数据，但你突然发现你手里有个东西叫数据资源，这是巨大的进步。 再往下想，你会觉得这是非常大的变化，越到上面去看，越会影响所有的事情。设想一下，没有巨大的计算能力，你不可能把这个场景找出来，再往上看，数据资源远远不只是视频资源。然后你会理解，城市为什么会是人类最了不起的发明，会是最大的智能硬件。 所以从这个角度说，跟以前相比，人工智能跳出非常大的一步，这就是为什么人工智能值得热一热的地方。从识别圆锥体到下棋，再到解决真实的问题，比如识别车牌，这些都是进步，但这些还都是局部问题。到了城市大脑，你就会发现有一个那么大的对象-城市，要整体去理解它，我觉得这是巨大的飞跃。 如果一定要做个比喻的话，我觉得城市大脑给机器智能或者人工智能带来最大的影响，就像阿波罗登月计划。在登月计划以前，不管你在地球上如何折腾，都不需要摆脱地心引力。但你要登月，就需要有一个东西来摆脱地心引力，这是一次质的变化。   做城市大脑会是一个什么量级的投入？以前看它应该是阿里云孵化出来的一个项目，但是现在听起来它是一个挺大的事儿。   根据我的理解，以后世界上主要的计算都是因为这个而完成的，不只是阿里云，所有云计算服务的对象也都是它。可以说，城市大脑用掉的计算量，会是世界计算消耗量的最主要部分。所以我觉得非常值得从各个角度去研究。另外，尽管这不仅是一个人工智能的应用，但它是借着人工智能的热度做起来的，所以要感谢人工智能。 听说也有越来越多的公司成立城市大脑这个部门，所以从这个角度讲这也是超出我的想象的。但回过头来讲最大的挑战是什么？我觉得最大的挑战就是我今天已经没有办法具体到一个或几个技术上，我觉得这件事情到今天为止最大的挑战是，能不能真的把它变成城市的基础设施。智慧城市有一个最大的问题，到今天为止它还是一个项目，那项目这件事情建完就建完了。但是像地铁就不是个项目，它就是会持续下去的，直到这个基础设施被淘汰。 对阿里云和社会来说，这都是最大的挑战。用他们搞城市规划的来讲，城市大脑是一个城市的另外一个图层。就像我们下面有地下管网的下水道，完了铺电缆，铺水管网，那么城市大脑就是在上面的一层。而这一层跟其他还不一样的地方，这一层跟所有层都有关系，它本质是在优化城市所有公共资源的使用。所以我觉得城市大脑无论是对阿里云，还是对社会，最大的挑战就在于能不能变成基础设施，能不能真的做到这个终极目标。   或者能这么理解吗，这是阿里云下一个重要阶段？ 至少我是这么认为的，不只是阿里云，亚马逊也是这样。其实今天的云计算，在我的理解，过去不在云上完成的东西给搬到云上来了，是计算效率极大的提高，很了不起。但并没有创造出新的计算需求，而城市大脑创造了新的计算需求，这个需求可能是原来的 1000 倍，它代表了质的变化。 这个过程中，你体验过哪些艰难时刻？   有人说城市大脑跟当时做阿里云很像，其实也谈不上艰难，就是 99% 的人都不相信这个事情，但只要有一个人相信，你就去做好了，不用在乎还有 99 个人是不是相信。为什么说我的运气呢，是因为不管你信不信我们都干下去了。 而中间具体事情（坎）就太多了，在某一个时刻只要有一个人没扛住，那可能整个事情都扛不住了，这种情况出现的太多了。   所以就是因为你一直在扛着，这个事情一直能继续。   不是说我，我是说有一个人，可能中间干活的任何一个人，他稍微出点岔子，这件事情可能就没有了，这个人可能是阿里云的一个员工，也可能是一个交警，真的是有很多这样的地方。我们运气好的地方，真的像一个接力棒一样，一棒一棒接下去了，这个还真是蛮运气的。 但是你想，接力赛跑，你想交棒，那就会有掉棒的，特别是快速奔跑的时候，所以要保证一棒棒交下去。所以但凡有意义的事情，早期的人都很痛苦，恨不得脱离，到今天为止很多在做的也觉得这是个苦海无边的事情。反正我觉得大部分今天对 AI 热衷的人碰到城市大脑都会苦海无边，很想跑掉。   现在经手城市大脑项目的主要部门，是独立项目团队还是调用阿里云的人？ 城市大脑是阿里的一个重要业务，iDST 是阿里非常重要的技术部门。城市大脑不是阿里一家可以完成的，是不同企业在整个产业上的创新，同时还要协调城市各个部门。更重要的是，城市大脑是杭州市的城市大脑，不是阿里的，阿里只是帮杭州市做城市大脑而已。 城市大脑最核心的地方，是像电一样，电网建好很重要，但最后电在里面跑更重要。所以杭州城市大脑，从第一天就能够跑杭州市的数据，可能这是最了不起的事情。 就像修路一样，车如何在路上跑起来，可能比路更重要。因为没有车的话，就会变成鬼城。所以没有城市的数据，这个大脑也就是鬼城。 所以我们很运气的地方在于，杭州市没有一开始将它变成鬼城，从这个角度说，杭州做了巨大的创新，创新的幅度不小于我们最初提出城市大脑。这也是杭州市成立数据资源局了不起的地方，今天大部分企业都没有这个概念，甚至做 AI 的人都没有建立数据资源的概念。 城市大脑里面哪些事情你们要做，哪些是要放弃或者说回避，或者说引入更多的合作伙伴？边界会有吗？ 我不知道，因为，第一，这件事情是没有被完全定义好的。就像你修一条路，建一个城市，要不要沥青厂，谁来供应沥青，铺沥青的机器谁来提供，分工也是需要很长时间的。 提出这个问题的人，大概都觉得城市大脑是一个成熟的行业，但实际上不是，每位从事相关研究的人都要为此付出很大的代价。恰恰这样，我们才有机会。但回过头来看，有一件事情是明确的，那就是阿里云在这其中最核心的、最基本的、也是不可替代的，就是为城市大脑提供计算资源，这是我可以讲清楚的地方。 那同样也可以讲清楚的地方，为什么叫城市大脑？就是这条路一定属于这个城市的，就像修路，不能因为哪个包工队修了这条路，最后这个路是包工队的，这个逻辑是不对的。同样，阿里云为什么能做这件事情，是因为云计算的平台是最重要的基础，离开了云计算是没有办法谈这件事情的，至于在这上面衍生了什么，我觉得这是慢慢发展的过程。   我们对 AI 的东西，某种意义上是高估的，但对 AI 的潜力是低估的。   你每天典型的工作状态是怎么样的，比较关心的问题都是什么？   我觉得其实还是很简单的，其实我还是很运气的，还可以天天碰到你觉得很有意思的事情，包括计划今年 5 月在云栖小镇举办 2050 大会这样的东西。 所以你说我今天什么状态，我觉得蛮有意思的。因为我从大学毕业到现在，还能碰到这么多有意思的事情，是极其幸运的。 如果说工作状态，严格意义上，我觉得很运气的地方，是我从一个具体的 KPI 里面跳出来了，但还有机会和想法去做很多具体的事情。而且我可以考虑细节问题，所以才能看到别人没有机会碰到的问题。 城市大脑是个基础设施，实际上这是一个非常基本的思考，只是大家不会做那么基本的思考，愿意把它当成是一个所谓的层次高的事情，就把自己搞坏掉了。 所以我为什么总拿下水道举例子，为什么喜欢用 Fundamental 这个词，Fundamental 可以理解成最基础的东西，也可以理解成层次很高的东西，但是它仍然是很基础的东西。   相比互联网和移动互联网，AI 的价值和外延有很大区别吗？ 我觉得还是有本质差别的。到现在为止，我们对 AI 的东西，某种意义上是高估的，但对 AI 的潜力是低估的，就跟软件一样。 我觉得 AI 跟互联网的关系，是软件跟硬件的关系，软件的重要性怎么强调都不为过，但是没有硬件，就相当于灵魂没有安放之处。 所以大家今天讲 AI 的重要性，甚至有人说互联网时代过去了，这是很危险的说法。没有互联网，今天的 AI 是没有藏身之处的，灵魂是没有安身之处的。 它们之间关系比软件、硬件还更加进一步的地方是什么？硬件相对比较成熟一点，软件创造的可能性要大过硬件，这是为什么软件的想象力要大一点的地方。 不管什么网，对我来说都是互联网。但实际上你提到的互联网、移动互联网，甚至物联网，这种不同的叫法本身就说明互联网还不成熟，有这么多所谓的网出来，就表明这台电脑今天是不成熟的，远不像 PC 的架构那么清楚。 所以 AI 的灵魂安放在哪里，今天还是模糊的。我说分布式智能就是这个原因，很难说这个东西在哪里了。今天所谓的 AI，就是彻头彻尾的一个分布式智能，就是说你很难说智能就在手机上。就像人的智能，其实也不完全是在大脑里。 之前听到过一个观点，说人工智能带来的改变可能是立体的，它可以渗透到各个经济利益体中。 人工智能得益于计算通过互联网到处都是，所以人工智能是不可能到达计算不到达的地方。就像水到不了的地方没有生命，这句话我觉得要旗帜鲜明地说出来，至少知道边界在哪里。   今年无人不谈 AI，公司融资额都相当惊人。但具体到每一个公司感觉大多数仍然面目模糊。这样的热度还能维持多久？你怎么判断？ 我觉得，大家高估了现有人工智能框架（以深度学习为代表）的影响力，但是大家低估了计算这些最基本的东西对我们的推动作用。不过，人工智能为什么这么热，是有道理的。你想有一个技术框架，既可以解决视觉的问题，也可以解决自然语言处理的问题，这是非常了不起的。 我今天看到的一个挑战是，有些事情曾经很重要，但大家都知道这个时代过去了。 我经常会举一个例子，比如语音识别，语音识别大家都知道，识别率也很高，比人的识别率都高。可是我们人没有觉得你理解错了好像有什么奇怪的地方对不对？所以我有时候会开玩笑，我说语音识别识出来是机器智能，你不能说它错了，只是机器这么认为而已。 回过头来看，这说明什么问题呢，我们要问问我们要解决的问题是不是对的，对这个社会发展最大的价值是不是在这个地方？再回过头来，我们把语音识别搞成了 100% 让人满意的时候，如果我们的城市已经完蛋了，大家都活不下来了，那为什么不能用这些资源来解决我们今天城市更迫切的问题？如何用最少的资源推动社会的发展，是需要大家去思考的。 8位AI行业局内人讲述对过去、对未来的看法 点击下方，阅读更多                 
204,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738252&idx=3&sn=33fd30b535d6192651705d1d10511387&chksm=871aca32b06d432424d813b6c417763cb7a27676704ae8d3305c29c815671d650d9d71bff899&scene=27,教程 | 通过Python实现马尔科夫链蒙特卡罗方法的入门级应用,通过把马尔科夫链蒙特卡罗（MCMC）应用于一个具体问题，本文介绍了 Python 中 MCMC 的入门级应用。机器之心对本文进行了编译介绍。 GitHub 地址：https://github.com/WillKoehrsen/ai-projects/blob/master/bayesian/bayesian_inference.ipynb 过去几月中，我总是反复遇到同一个数据科学术语：马尔科夫链蒙特卡罗（Markov Chain Monte Carlo/MCMC）。每当我在实验室、博客、文章中听到这个概念，我常常点头赞同，觉得它很酷，但实际上并没有一个清晰的认知。有几次我尝试着学习 MCMC 和贝叶斯推理，我每次从阅读书籍开始，结果却很快放弃。我感到很恼怒，于是决定转向一种学习任何新技能的最佳方法：将它应用于一个具体问题。 使用我的睡眠数据（我一直打算对此一探究竟）和一本实际应用的书（Bayesian Methods for Hackers），我终于通过一个实际问题学习了马尔科夫链蒙特卡罗。像往常一样，比起阅读抽象的概念，将这些技术应用到具体问题中能让学习变得更简单、更愉快。本文介绍了 Python 中的马尔科夫链蒙特卡罗的入门级应用，正是它教会了我使用这个强大的建模分析工具。 我鼓励大家参阅 GitHub，并将其用于自己的数据当中。本文将重点介绍它的应用和结果，所以会产生很多高层次的话题。如果你在阅读后想了解更多，可以阅读文中提供的链接。 我的 Garmin Vivosmart 手表可以根据心率和运动情况追踪我的睡眠和起床状况。它并非 100％准确，不过真实数据从不完美，我们仍然可以借助正确的模型从噪声数据中提取有用的知识！ 本项目的目标是借助睡眠数据创建一个模型，通过把睡眠看作时间函数，而确定睡眠的后验概率。由于时间是连续变量，确定整个后验分布非常棘手。因此我们转而使用一些可实现近似分布的方法，比如马尔可夫链蒙特卡罗（MCMC）。 选择一个概率分布 在开始使用 MCMC 之前，我们需要确定一个合适的函数来对睡眠的后验概率分布进行建模。一个简单的方法是直观检查这些数据。对于我的睡眠的时间函数的观察如下图所示。 上图中，每个数据点都用点表示，点的强度显示在特定时间的观测数量。我的手表只能记录我入睡的那一分钟，所以为了扩大数据量，我在精确时间的两边增加以分钟为单位的数据点。举例而言，如果我的手表显示我在晚上 10:05 入睡，那么 10:05 之前的每一分钟都被表示为 0（清醒），10:05 之后的每一分钟都被表示为 1（睡着）。这将大约 60 个夜晚的观测数据扩展到了 11340 个数据点。 我们可以发现，我一般在晚上 10 点之后入睡。但是我们想创建一个模型，以概率的形式捕捉从清醒到入睡的过渡过程。我们可以在模型中使用一个简单的阶跃函数，它在一个精确的时间从唤醒（0）过渡成入睡（1），但是这无法表现数据的不确定性。我不可能在每天晚上的同一时间睡觉，因此需要一个能模拟过渡过程的函数对这一渐进过程进行建模，显示变化特性。给定上述数据的情况下，我们的最佳选择是在 0 和 1 的边界之间平滑过渡的 logistic 函数。以下是睡眠概率作为时间函数的 logistic 方程： 其中，β 和 α 是我们在 MCMC 过程中必须学习的模型参数。具有不同参数的 logsitic 函数图像如下所示。 logsitic 函数很适合本案例中的数据，因为入睡的可能性会逐渐转变，此函数能捕捉睡眠模式之中的变化情况。我们希望能够在函数中插入时间 t，获得睡眠概率（其值在 0 和 1 之间）。我们最终得到的不是在晚上 10:00 入睡与否的直接答案，而是一个概率。为了建立这个模型，我们使用这些数据，通过 MCMC 寻找最佳的 α 和 β 参数。 马尔科夫链蒙特卡罗 马尔可夫链蒙特卡罗指从概率分布中抽样以构建最大可能分布的一类方法。我们不能直接构建 logistic 分布，所以，与之相反，我们为函数的参数（α 和 β）生成了上千个值——被称为样本——从而创造分布的近似值。MCMC 背后的思想是，当我们生成更多的样本时，我们的近似值越来越接近实际的真实分布。 马尔科夫链蒙特卡罗方法分为两部分。蒙特卡罗指的是使用重复随机样本获得数值解的一般性技术。蒙特卡罗可以被视为进行了若干次实验，其中每次都对模型中的变量进行改变并观察其响应。通过选择随机数，我们可以探索大部分参数空间，即变量可能值的范围。下图显示了我们的问题使用正常先验后的参数空间。 显然，我们无法一一尝试图像中的每一个点。但是通过对较高概率区域（红色区域）进行随机抽样，我们可以为问题建立最可能的模型。 马尔科夫链 马尔科夫链是一个随机过程，其中次态仅依赖于当前状态（在此语境中，一个状态指的是参数的一次赋值）。马尔科夫链没有记忆性，因为只有当前状态对下一状态起作用，而与到达当前状态的方式无关。如果这种说法还是有些难以理解，我们可以用日常现象中的天气来举例。如果我们想预测明日天气，我们可以仅通过今日天气来得到一个合理的估计。如果今天下雪了，我们可以查看下雪次日天气分布的历史数据，估算明天天气的概率。马尔科夫链的概念在于，我们无需了解整个历史过程就能预测下一状态，这个近似在许多现实情况中就能很好地工作。 综合马尔科夫链和蒙特卡罗的思想，马尔科夫链蒙特卡罗是一种基于当前值重复绘制某一分布参数随机值的方法。每个值的样本都是随机的，但是值的选择受限于当前状态和假定的参数先验分布。MCMC 可以被认为是一种随机游走，在这个过程中逐渐收敛到真实分布。 为了绘制 α 和 β 的随机值，我们需要假设这些值的先验分布。由于我们对参数没有任何提前的假设，我们可以使用正态分布。正态分布也称高斯分布，它由均值和方差定义，分别显示数据的位置以及扩散情况。下图是具有不同均值和方差的几种正态分布： 我们所使用的 MCMC 算法被称为 Metropolis Hastings。为了将我们观察的数据与模型联系起来，每绘制一组随机值，算法会根据数据对其进行评估。如果随机值与数据不一致（这里稍微进行了一些简化），这些值将被拒绝，模型保持当前状态。反之，如果随机值与数据一致，这些值将会分配给参数并成为当前状态。该过程将持续进行指定的步骤数目，模型的准确率也随着步骤数量的增加而改善。 综合而言，马尔科夫链蒙特卡罗在我们的问题当中基本步骤如下： 为 logistic 函数选择一组初始参数 α 和 β。 根据当前状态，把新的随机值分配给 α 和 β。 检查新的随机值是否与观察结果一致。如果不一致，拒绝这些随机值并返回前一个状态。如果一致，则接受这些值，将其作为新的当前状态。 对指定的迭代次数重复执行步骤 2 和 3。 该算法将返回它为 α 和 β 生成的所有值。然后，我们可以使用这些值的平均值作为 logistc 函数中 α 和 β 的最终可能值。MCMC 无法返回「真实」值，它给出的是分布的近似值。给定数据的情况下，最终输出的睡眠概率模型将是具有 α 和 β 均值的 logistic 函数。 Python 实现 上述细节在我脑海中徘徊已久，最后终于在 Python 中进行了实现！亲眼看到第一手的结果比读取别人的描述有帮助得多。要在 Python 中实现 MCMC，我们需要使用 PyMC3 贝叶斯推理库。它将大部分细节进行了抽象，从而让我们能不迷失在理论中，并建立我们的模型。 下面的代码创建的模型带有参数 α 和 β、概率 p 和观察结果 observed。step 变量指的是特定的算法，sleep_trace 则保存了模型生成的所有参数值。 （请在 notebook 中查阅完整代码） 为了了解运行此代码时发生的情况，我们可以查看模型运行过程中生成的 α 和 β 的所有值。 它们被称为轨迹图。我们可以看到，每个状态都与之前的状态有关（马尔科夫链），但是这些值波动显著（蒙特卡罗采样）。 在 MCMC 中，通常高达 90% 的轨迹会被抛弃。该算法无法立即收敛到真正的分布，且初始值往往并不准确。后期的参数值通常更好，这意味着它们是适用于建立模型的参数。我们使用了 10000 个样本并丢弃了前 50%，但是一个行业的应用可能会使用数十万甚至上百万个样本。 给定足够多的迭代次数，MCMC 将收敛于真实值。但是，对收敛进行评估可能比较困难。对此我将不在本文讨论（一个方法是测量轨迹的自相关），但是，如果我们想要结果最准确，这是一个重要的考虑因素。PyMC3 建立了评估模型好坏的函数，其中包括轨迹图和自相关图。 睡眠模型 最终建立并运行模型之后，是时候使用结果了。我们将最后 5000 个 α 和 β 样本的平均值作为参数最可能的值，这就让我们能够创建一条曲线，建模睡眠后验概率： 该模型能很好地反映数据的结果。此外，它捕捉了我睡眠模式当中的固有变化。该模型给出的不是一个简单的是非答案，而是一个概率。例如，我们可以通过该模型找到在给定时间我睡着的概率，并能找到睡眠概率经过 50% 的时间： 尽管我每天都试图在 10 点上床睡觉，但这显然不是大多数下的实际情况。我们可以发现，我上床的平均时间是晚上 10:14 左右。 在数据给定的情况下，这些值是最有可能的估计值。然而，因为模型本身是近似的，所以存在与这些概率相关的不确定性。为了表示这种不确定性，我们可以使用所有的 α 和 β 样本（而不是它们的平均值）来预测某一给定时间的睡眠概率，然后据此绘制直方图。 这些结果更好地反映了 MCMC 模型真正做了什么。MCMC 找到的不是一个简单的答案，而是可能值的样本。贝叶斯推理在现实世界中起到了重要作用，是因为它从概率的角度表示预测结果。我们可以说，问题会有一个可能性最大的答案，但是更加准确的回应是任何预测都存在一系列的可能值。 唤醒模型 我可以使用描述早晨醒来时间的数据建立一个类似的模型。我定了一个闹钟，努力在早晨 6:00 起床。但是可以看到，我并非每日都是如此。下图展现了我从入睡到醒来过渡过程的最终模型以及观察数据。 通过查询模型，我们可以找出在给定时间我睡着的概率以及最有可能醒来的时间。 看起来我得处理一下我的闹钟了！ 睡眠时长 出于好奇心和练习目的，我最终想创造的是关于我睡眠时长的模型。首先，我们需要找到一个函数来模拟数据的分布。我猜想结果应该会是正态分布的形式，但是我们只有通过检查数据才能得到最终结果。 正态分布确实可行，但这无法捕捉右侧的偏离点（即我睡眠时间非常长的情况）。我们可以用两个独立的正态分布来表示两个模型，但是，我想使用偏正态分布。偏正态分布有三个参数：均值、方差、偏斜度 α。以上三个参数都需要通过 MCMC 来学习。下面的代码建立了上述模型，并进行了 Metropolis Hastings 抽样。 现在，我们可以使用这三个参数的均值来构建最有可能的分布。以下是根据数据观察得到的最终偏正态分布。 看起来拟合得不错！我们可以通过查询模型，找到我至少可以获得一定睡眠时长的概率，同时也能找到最可能的睡眠时长： 我对这个结果并不是完全满意，但是对一个研究生而言，这样的结果已经不错啦。 小结 这个项目的顺利完成再次展示了解决问题的重要性，而且我们最好选择解决真实存在的应用问题（https://towardsdatascience.com/how-to-master-new-skills-656d42d0e09c）。在使用马尔科夫链蒙特卡罗构建贝叶斯推理的端对端实现过程中，我学习了许多基础知识，而且非常享受这个过程。我不仅更加了解我的习惯（以及我需要改进的方面），而且终于弄明白了 MCMC 和贝叶斯推理到底是什么。在数据科学领域，我们要不断地给自己的库存知识增加新的工具，而最有效的学习方法就是找到一个问题并着手开始解决它！  原文链接：https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98 
205,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738301&idx=1&sn=d30f1f85b202c32f5b33444bec71f3c7&chksm=871aca03b06d4315bf7fb51e512d30de1da95ab056eee34e30e7e6a9a933dcad644028ea523c&scene=27,从基础概念到实现，小白如何快速入门PyTorch,"analyticsvidhya PyTorch 是一个有潜力能改变深度学习实现面貌的 Python 库，它的使用非常灵活与轻松。在本文中，我们将以更实用的方式探索 PyTorch，包括基础知识和案例研究等。此外，本文还将比较使用 NumPy 和 PyTorch 从头构建神经网络的方式，以了解它们在实现中的相似之处。 PyTorch 的构建者表明，PyTorch 的哲学是解决当务之急，也就是说即时构建和运行我们的计算图。这恰好适合 Python 的编程方法，因为我们不需等待整个代码都被写入才能知道是否起作用。我们很容易运行部分代码，并实时检查它。 PyTorch 是一个基于 Python 的库，旨在为深度学习提供一个灵活的开发平台。PyTorch 的工作流程非常接近于 Python 的科学计算库 NumPy。那么为什么我们需要使用 PyTorch 构建深度学习模型？以下作者根据实际经验提供了三个理由： 便于使用的 API：它的使用如同 Python 那样简单。 支持 Python：正如上文所述，PyTorch 可以平滑地与 Python 数据科学栈相结合。它与 NumPy 一样简单，甚至我们都感觉不出它们的区别。 动态计算图：PyTorch 不再采用特定的函数预定义计算图，而是提供构建动态计算图的框架，甚至我们可以在运行时修正它们。这种动态框架在我们不知道所构建的神经网络需要多少内存时非常有用。 其它一些使用 PyTorch 的优点还有多 GPU 支持、自定义数据加载器和极简的预处理过程等。自从它在 2016 年 1 月份发布以来，许多研究者将其采用为标准的实现库，因为它构建新颖的、极其复杂的计算图同样非常简单。即使这样，PyTorch 被主流数据科学家和研究员接收还是花了很长时间，因为它目前仍然是新的项目，且还有很多地方需要构建与完善。 在讨论 PyTorch 的各个组件前，我们需要了解它的工作流。PyTorch 使用一种称之为 imperative / eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为「define-by-run」方法。 更多介绍请查看：http://pytorch.org/about/ 安装 PyTorch 非常简单，我们可以按照自己的系统跟随官方文档的步骤轻松完成。例如以下选择在 Linux、Python 3.5 和 CUDA 9.1 的环境下安装 PyTorch： 我们在基础部分主要需要了解的 PyTorch 元素有 PyTorch 张量、数学运算、自动求导模块、最优化模块和神经网络模块。下面本文会依次对这些模块进行简要的介绍： PyTorch 张量 正如 PyTorch 文档所说，如果我们熟悉 NumPy 的多维数组，那么 Torch 张量的很多操作我们能轻易地掌握。PyTorch 提供了 CPU 张量和 GPU 张量，并且极大地加速了计算的速度。 从张量的构建与运行就能体会到 PyTorch 相比 TensorFLow 需要声明张量、初始化张量要简洁地多。以下语句将随机初始化一个 5×3 的二维张量，因为 PyTorch 是一种动态图，所以它声明和真实赋值是同时进行的。 若我们希望随机初始化的张量服从某些分布，那么我们可以直接对张量对象使用一些方法。如下初始化的张量将服从均匀分布： 在 PyTorch 中，torch.Tensor 是一种多维矩阵，其中每个元素都是一个单一的数据类型，且该构造函数默认的为 torch.FloatTensor。以下是具体张量的类型： 除了直接定义维度，一般我们还可以从 Python 列表或 NumPy 数组中创建张量。而且根据 Python 列表和元组等数据结构的习惯，我们可以使用相似的索引方式进行取值或赋值等。以下通过 Python 列表创建一个 Torch 张量，并通过索引赋值： 若 x 为我们定义的 5×3 Torch 张量，且初始化数值服从-1 到 1 的均匀分布，那么我们可以执行很多基础的数学运算。以下执行了一个简单的矩阵间对应元素乘积。 PyTorch 同样支持广播（Broadcasting）操作，一般它会隐式地把一个数组的异常维度调整到与另一个算子相匹配的维度以实现维度兼容。为了定义两个形状是否是可兼容的，PyTorch 会从最后开始往前逐个比较它们的维度大小。在这个过程中，如果两者的对应维度相同，或者其一（或者全是）等于 1，则继续进行比较，直到最前面的维度。若不满足这两个条件，程序就会报错。如下展示了 PyTorch 的广播操作： 正如 PyTorch 在官网上所说，PyTorch 是一个张量和动态神经网络 Python 库，它有着极其强大的 GPU 加速性能。我们一般可以直接定义 GPU 张量，也可以由 CPU 张量转化为 GPU 张量。如下，我们定义了两个 GPU 张量，并对这两个张量执行矩阵乘法。当然，我们也可以如下所示将 CPU 张量转换为 GPU 张量。 数学运算 如 NumPy 一样，高效地实现数学函数对于科学计算库至关重要。PyTorch 提供了一个简单的接口，并支持 200 多种数学运算，以下是 PyTorch 实现简单加运算的过程： 这种运算与 Python 非常像，我们可以在定义的 PyTorch 张量上执行多种矩阵运算。例如我们可以转置二维张量： AutoGrad 模块 TensorFlow、Caffe 和 CNTK 等大多数框架都是使用的静态计算图，开发者必须建立或定义一个神经网络，并重复使用相同的结构来执行模型训练。改变网络的模式就意味着我们必须从头开始设计并定义相关的模块。 但 PyTorch 使用的技术为自动微分（automatic differentiation）。在这种机制下，系统会有一个 Recorder 来记录我们执行的运算，然后再反向计算对应的梯度。这种技术在构建神经网络的过程中十分强大，因为我们可以通过计算前向传播过程中参数的微分来节省时间。 从概念上来说，Autograd 会维护一个图并记录对变量执行的所有运算。这会产生一个有向无环图，其中叶结点为输入向量，根结点为输出向量。通过从根结点到叶结点追踪图的路径，我们可以轻易地使用链式法则自动计算梯度。 在内部，Autograd 将这个图表征为 Function 对象的图，并且可以应用 apply() 计算评估图的结果。在计算前向传播中，当 Autograd 在执行请求的计算时，它还会同时构建一个表征梯度计算的图，且每个 Variable 的 .grad_fn 属性就是这个图的输入单元。在前向传播完成后，我们可以在后向传播中根据这个动态图来计算梯度。 以下展示了通过 backward() 和 torch.autograd.grad 计算梯度的方法，其中 torch.eq() 评估表达式是不是相等，即 x.grad 的计算结果是不是等于 2x。 以下展示了对 y 求导的结果，即 dz/dy。从上面 z 的定义可知结果应该是 3，那么以下展示了该计算过程： 前面是使用 backward() 求解变量的梯度，后面我们也可以使用 torch.autograd.grad 计算梯度。如下所示，我们使用另外一种方式求解同一个函数的梯度。 最优化模块 torch.optim 是实现神经网络中多种优化算法的模块，它目前已经支持大多数一般的方法，所以我们不需要从头构建优化算法。以下展示了使用 Adam 优化器的基本代码： 神经网络模块 PyTorch AutoGrad 使得计算图的定义和梯度的计算十分简单，但原版的 AutoGrad 可能对定义复杂的神经网络显得太底层，因此我们需要神经网络模块帮助简化工作。该 nn 包定义了一组函数，我们可以将其视为有一些可训练权重的神经网络层级。我们也可以将该神经网络模块视为类似于 Keras 的 PyTorch 组件。 我们一般可以使用 torch.nn 包构建神经网络，下面提供了一些 API 的表达及意义： 线性层- nn.Linear、nn.Bilinear 卷积层 - nn.Conv1d、nn.Conv2d、nn.Conv3d、nn.ConvTranspose2d 非线性激活函数- nn.Sigmoid、nn.Tanh、nn.ReLU、nn.LeakyReLU 池化层 - nn.MaxPool1d、nn.AveragePool2d 循环网络 - nn.LSTM、nn.GRU 归一化 - nn.BatchNorm2d Dropout - nn.Dropout、nn.Dropout2d 嵌入 - nn.Embedding 损失函数 - nn.MSELoss、nn.CrossEntropyLoss、nn.NLLLoss 以上就是 PyTorch 的基本组件，我们可以使用它们快速构建神经网络。当然以上只是简单的概念介绍，每一个模块都有非常多的函数与方法，读者可详细查阅 PyTorch 文档了解更多。 在这一部分中，我们分别使用 NumPy 和 PyTorch 构建简单的神经网络以实现二元分类问题，本文的后面会对这一部分的代码进行解释。 def sigmoid (x) def derivatives_sigmoid (x) 现在，我们会发现使用 PyTorch 实现相同的网络会非常简单。以下的代码同样也用粗体表示出它与 NumPy 的不同之处： def sigmoid (x) def derivatives_sigmoid (x) 在一份基准脚本中，它展示出 PyTorch 在训练长短期记忆（LSTM）网络上比其它主要框架的表现都要好，因为它运行一个 Epoch 有最少的中位数时间。 PyTorch 中的数据加载 API 经过了优良的设计，接口是针对特定数据集、采样器和数据加载器而构建的。对比于 TensorFlow 的数据加载工具（readers, queues 等），我发现 PyTorch 的数据加载模块更易于使用。同时它们还能无缝对接神经网络构建模块，所以我们不需要第三方高级库。 然而，我并不推荐使用使用 PyTorch 部署模型，因为 PyTorch 仍然不是那么成熟。正如 PyTorch 开发者所说：「我们经常看到用户首先创建一个 PyTorch 模型来测试是否可行，然后当需要部署模型到生产中时，他们会转化为 Caffe 2 等其他框架，并将其部署到移动端或其它平台。」 前面我们已经了解了PyTorch的基本组成元素与特性，下面我们会通过线性回归与手写字体识别两个具体的案例探讨如何使用 PyTorch 构建高效地模型。 PyTorch 线性回归 定义数据： 定义模型，在 PyTorch 中，我们可以使用高级 API 来定义相关的模型或层级。如下定义了「torch.nn.Linear(1, 1)」，即一个输入变量和一个输出变量。 class Model (torch.nn.Module) def __init__ (self) def forward (self, x) 构建损失函数和优化器，构建损失函数也可以直接使用「torch.nn.MSELoss(size_average=False)」调用均方根误差函数。优化器可以使用「torch.optim.SGD()」提到用随机梯度下降，其中我们需要提供优化的目标和学习率等参数。 训练模型，执行前向传播计算损失函数，并优化参数： 用 PyTorch 解决图像识别问题 为了进一步熟悉 PyTorch，我们将使用它解决 Analytics Vidhya 的深度学习实践问题：识别手写数字。我们的问题是给定一张 28 x 28 的图像，利用模型识别其所代表的手写数字。 所以首先我们需要下载训练集与测试集，数据集包含了一个压缩文件以储存所有的图像。其中 train.csv 和 test.csv 分别储存了训练和测试图像，且图像的格式为 png。下面我们将一步步构建简单的神经网络以实现手写数字识别功能。 第 0 步：准备工作 a）导入必要的函数库 b）设置随机的 Seed，因此我们能控制模型产生的随机数基本不变（伪随机数）。 c）设置工作目录的路径。 第 1 步：加载与预处理数据 a）现在读取 CSV 格式的数据集，并获取文件名与对应的标注。 b）接下来可以打印准备好的图片。 c）对于更简单的数据操作，我们可以储存所有的图像作为 NumPy 数组。 d）因为这个是一个典型的机器学习问题，所以我们可以创建验证集以监控模型的运行情况。下面我们以 7:3 的比例分割训练集与验证集。 第 2 步：构建模型 a）下面是模型的主体，我们定义的神经网络共有三层，即输入层、隐藏层和输出层。输入层和输出层的神经元数量是固定的，即 28 x 28 和 10 x 1，它们分别代表了输入图像的像素和类别。我们在隐藏层采用了 50 个神经元，并采用 Adam 作为最优化算法。 b）以下将开始训练模型。 def preproc (unclean_batch_x) def batch_creator (batch_size) 训练准确度为： 0.8779008746355685 测试准确度为： 0.867482993197279 这些分数非常令人满意，因为我们只是用简单的神经网络训练了 5 个 Epoch。以上，本文介绍了简单的 PyTorch 入门概念，并利用简单的案例熟悉 PyTorch 的使用。读者可以继续阅读 PyTorch 的文档以了解更多信息。 原文链接：https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/ "
206,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738513&idx=1&sn=ba0ff59b1dfbf139cc33c4a6b5b65a48&chksm=871acb2fb06d42391e3b107c35f615fd55e1fb5075f4a5f9d1560cdea178a51fd419e39018ca&scene=27,NumPy能力大评估：这里有70道测试题,"选自Machine Learning Plus 作者：Selva Prabhakaran 本 NumPy 测试题旨在为大家提供参考，让大家可以使用 NumPy 的更多功能。问题共分为四个等级，L1 最简单，难度依次增加。机器之心对该测试题进行了编译介绍，希望能对大家有所帮助。每个问题之后附有代码答案，参见原文。 原文链接：https://www.machinelearningplus.com/101-numpy-exercises-python/ 如果你想先回顾一下 NumPy 的知识，推荐阅读： NumPy 基础：https://www.machinelearningplus.com/numpy-tutorial-part1-array-python-examples NumPy 高级教程：https://www.machinelearningplus.com/numpy-tutorial-python-part2 1. 将 NumPy 导入为 np，并查看版本 难度：L1 问题：将 NumPy 导入为 np，并输出版本号。 2. 如何创建 1 维数组？ 难度：L1 问题：创建数字从 0 到 9 的 1 维数组。 期望输出： #> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 3. 如何创建 boolean 数组？ 难度：L1 问题：创建所有 True 的 3×3 NumPy 数组。 4. 如何从 1 维数组中提取满足给定条件的项？ 难度：L1 问题：从 arr 中提取所有奇数。 输入： arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])` 期望输出： #> array([1, 3, 5, 7, 9]) 5. 如何将 NumPy 数组中满足给定条件的项替换成另一个数值？ 难度：L1 问题：将 arr 中的所有奇数替换成 -1。 输入： arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 期望输出： 6. 如何在不影响原始数组的前提下替换满足给定条件的项？ 难度：L2 问题：将 arr 中所有奇数替换成 -1，且不改变 arr。 输入： arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 期望输出： #> array([ 0, -1, 2, -1, 4, -1, 6, -1, 8, -1]) r #> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 7. 如何重塑（reshape）数组？ 难度：L1 问题：将 1 维数组转换成 2 维数组（两行）。 输入： np.arange(10) #> array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 期望输出 #> array([[0, 1, 2, 3, 4], #>        [5, 6, 7, 8, 9]]) 8. 如何垂直堆叠两个数组？ 难度：L2 问题：垂直堆叠数组 a 和 b。 输入： a = np.arange(10).reshape(2,-1) b = np.repeat(1, 10).reshape(2,-1) 期望输出： #> array([[0, 1, 2, 3, 4], #>        [5, 6, 7, 8, 9], #>        [1, 1, 1, 1, 1], #>        [1, 1, 1, 1, 1]]) 9. 如何水平堆叠两个数组？ 难度：L2 问题：水平堆叠数组 a 和 b。 输入： a = np.arange(10).reshape(2,-1) b = np.repeat(1, 10).reshape(2,-1) 期望输出： #> array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1], #>        [5, 6, 7, 8, 9, 1, 1, 1, 1, 1]]) 10. 在不使用硬编码的前提下，如何在 NumPy 中生成自定义序列？ 难度：L2 问题：在不使用硬编码的前提下创建以下模式。仅使用 NumPy 函数和以下输入数组 a。 输入： a = np.array([1,2,3])` 期望输出： #> array([1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]) 11. 如何获得两个 Python NumPy 数组中共同的项？ 难度：L2 问题：获取数组 a 和 b 中的共同项。 输入： a = np.array([1,2,3,2,3,4,3,4,5,6]) b = np.array([7,2,10,2,7,4,9,4,9,8]) 期望输出： array([2, 4]) 12. 如何从一个数组中移除与另一个数组重复的项？ 难度：L2 问题：从数组 a 中移除出现在数组 b 中的所有项。 输入： a = np.array([1,2,3,4,5]) b = np.array([5,6,7,8,9]) 期望输出： array([1,2,3,4]) 13. 如何获取两个数组匹配元素的位置？ 难度：L2 问题：获取数组 a 和 b 中匹配元素的位置。 输入： a = np.array([1,2,3,2,3,4,3,4,5,6]) b = np.array([7,2,10,2,7,4,9,4,9,8]) 期望输出： #> (array([1, 3, 5, 7]),) 14. 如何从 NumPy 数组中提取给定范围内的所有数字？ 难度：L2 问题：从数组 a 中提取 5 和 10 之间的所有项。 输入： a = np.arange(15) 期望输出： (array([ 5, 6, 7, 8, 9, 10]),) 15. 如何创建一个 Python 函数以对 NumPy 数组执行元素级的操作？ 难度：L2 问题：转换函数 maxx，使其从只能对比标量而变为对比两个数组。 输入： def maxx(x, y):     """"""Get the maximum of two items""""""     if x >= y:         return x     else:         return y maxx(1, 5) #> 5 期望输出： a = np.array([5, 7, 9, 8, 6, 4, 5]) b = np.array([6, 3, 4, 8, 9, 7, 1]) pair_max(a, b) #> array([ 6.,  7.,  9.,  8.,  9.,  7.,  5.]) 16. 如何在 2d NumPy 数组中交换两个列？ 难度：L2 问题：在数组 arr 中交换列 1 和列 2。 arr = np.arange(9).reshape(3,3) arr 17. 如何在 2d NumPy 数组中交换两个行？ 难度：L2 问题：在数组 arr 中交换行 1 和行 2。 arr = np.arange(9).reshape(3,3) arr 18. 如何反转 2D 数组的所有行？ 难度：L2 问题：反转 2D 数组 arr 中的所有行。 # Input arr = np.arange(9).reshape(3,3) 19. 如何反转 2D 数组的所有列？ 难度：L2 问题：反转 2D 数组 arr 中的所有列。 # Input arr = np.arange(9).reshape(3,3) 20. 如何创建一个包含 5 和 10 之间随机浮点的 2 维数组？ 难度：L2 问题：创建一个形态为 5×3 的 2 维数组，包含 5 和 10 之间的随机十进制小数。 21. 如何在 Python NumPy 数组中仅输出小数点后三位的数字？ 难度：L1 问题：输出或显示 NumPy 数组 rand_arr 中小数点后三位的数字。 输入： rand_arr = np.random.random((5,3)) 22. 如何通过禁用科学计数法（如 1e10）打印 NumPy 数组？ 难度：L1 问题：通过禁用科学计数法（如 1e10）打印 NumPy 数组 rand_arr。 输入： # Create the random array np.random.seed(100) rand_arr = np.random.random([3,3])/1e3 rand_arr #> array([[  5.434049e-04,   2.783694e-04,   4.245176e-04], #>        [  8.447761e-04,   4.718856e-06,   1.215691e-04], #>        [  6.707491e-04,   8.258528e-04,   1.367066e-04]]) 期望输出： #> array([[ 0.000543,  0.000278,  0.000425], #>        [ 0.000845,  0.000005,  0.000122], #>        [ 0.000671,  0.000826,  0.000137]]) 23. 如何限制 NumPy 数组输出中项的数目？ 难度：L1 问题：将 Python NumPy 数组 a 输出的项的数目限制在最多 6 个元素。 输入： a = np.arange(15) #> array([  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 期望输出： #> array([ 0, 1, 2, ..., 12, 13, 14]) 24. 如何在不截断数组的前提下打印出完整的 NumPy 数组？ 难度：L1 问题：在不截断数组的前提下打印出完整的 NumPy 数组 a。 输入： np.set_printoptions(threshold=6) a = np.arange(15) a #> array([ 0, 1, 2, ..., 12, 13, 14]) 期望输出： a #> array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 25. 如何向 Python NumPy 导入包含数字和文本的数据集，同时保持文本不变？ 难度：L2 问题：导入 iris 数据集，保持文本不变。 26. 如何从 1 维元组数组中提取特定的列？ 难度：L2 问题：从前一个问题导入的 1 维 iris 中提取文本列 species。 输入： url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_1d = np.genfromtxt(url, delimiter=',', dtype=None) 27. 如何将 1 维元组数组转换成 2 维 NumPy 数组？ 难度：L2 问题：忽略 species 文本字段，将 1 维 iris 转换成 2 维数组 iris_2d。 输入： url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_1d = np.genfromtxt(url, delimiter=',', dtype=None) 28. 如何计算 NumPy 数组的平均值、中位数和标准差？ 难度：L1 问题：找出 iris sepallength（第一列）的平均值、中位数和标准差。 url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') 29. 如何归一化数组，使值的范围在 0 和 1 之间？ 难度：L2 问题：创建 iris sepallength 的归一化格式，使其值在 0 到 1 之间。 输入： url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' sepallength = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0]) 30. 如何计算 softmax 分数？ 难度：L3 问题：计算 sepallength 的 softmax 分数。 url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' sepallength = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0]) 31. 如何找到 NumPy 数组的百分数？ 难度：L1 问题：找出 iris sepallength（第一列）的第 5 个和第 95 个百分数。 url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' sepallength = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0]) 32. 如何在数组的随机位置插入值？ 难度：L2 问题：在 iris_2d 数据集中的 20 个随机位置插入 np.nan 值。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='object') 33. 如何在 NumPy 数组中找出缺失值的位置？ 难度：L2 问题：在 iris_2d 的 sepallength（第一列）中找出缺失值的数目和位置。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='float') iris_2d[np.random.randint(150, size=20), np.random.randint(4, size=20)] = np.nan 34. 如何基于两个或以上条件过滤 NumPy 数组？ 难度：L3 问题：过滤 iris_2d 中满足 petallength（第三列）> 1.5 和 sepallength（第一列）< 5.0 的行。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0,1,2,3]) 35. 如何在 NumPy 数组中删除包含缺失值的行？ 难度：L3 问题：选择 iris_2d 中不包含 nan 值的行。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0,1,2,3]) 36. 如何找出 NumPy 数组中两列之间的关联性？ 难度：L2 问题：找出 iris_2d 中 SepalLength（第一列）和 PetalLength（第三列）之间的关联性。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0,1,2,3]) 37. 如何确定给定数组是否有空值？ 难度：L2 问题：确定 iris_2d 是否有缺失值。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0,1,2,3]) 38. 如何在 NumPy 数组中将所有缺失值替换成 0？ 难度：L2 问题：在 NumPy 数组中将所有 nan 替换成 0。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0,1,2,3]) iris_2d[np.random.randint(150, size=20), np.random.randint(4, size=20)] = np.nan 39. 如何在 NumPy 数组中找出唯一值的数量？ 难度：L2 问题：在 iris 的 species 列中找出唯一值及其数量。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 40. 如何将一个数值转换为一个类别（文本）数组？ 难度：L2 问题：将 iris_2d 的 petallength（第三列）转换以构建一个文本数组，按如下规则进行转换： Less than 3 –> ‘small’ 3-5 –> 'medium' '>=5 –> 'large' # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 41. 如何基于 NumPy 数组现有列创建一个新的列？ 难度：L2 问题：为 iris_2d 中的 volume 列创建一个新的列，volume 指 (pi x petallength x sepal_length^2)/3。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris_2d = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 42. 如何在 NumPy 中执行概率采样？ 难度：L3 问题：随机采样 iris 数据集中的 species 列，使得 setose 的数量是 versicolor 和 virginica 数量的两倍。 # Import iris keeping the text column intact url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') 43. 如何在多维数组中找到一维的第二最大值？ 难度：L2 问题：在 species setosa 的 petallength 列中找到第二最大值。 # Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 44. 如何用给定列将 2 维数组排序？ 难度：L2 问题：基于 sepallength 列将 iris 数据集排序。 url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 45. 如何在 NumPy 数组中找到最频繁出现的值？ 难度：L1 问题：在 iris 数据集中找到 petallength（第三列）中最频繁出现的值。 url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 46. 如何找到第一个大于给定值的数的位置？ 难度：L2  问题：在 iris 数据集的 petalwidth（第四列）中找到第一个值大于 1.0 的数的位置。 # Input: url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') 47. 如何将数组中所有大于给定值的数替换为给定的 cutoff 值？ 难度：L2 问题：对于数组 a，将所有大于 30 的值替换为 30，将所有小于 10 的值替换为 10。 输入： np.random.seed(100) np.random.uniform(1,50, 20) 48. 如何在 NumPy 数组中找到 top-n 数值的位置？ 难度：L2 问题：在给定数组 a 中找到 top-5 最大值的位置。 np.random.seed(100) a = np.random.uniform(1,50, 20) 49. 如何逐行计算数组中所有值的数量？ 难度：L4 问题：逐行计算唯一值的数量。 输入： np.random.seed(100) arr = np.random.randint(1,11,size=(6, 10)) arr > array([[ 9,  9,  4,  8,  8,  1,  5,  3,  6,  3], >        [ 3,  3,  2,  1,  9,  5,  1, 10,  7,  3], >        [ 5,  2,  6,  4,  5,  5,  4,  8,  2,  2], >        [ 8,  8,  1,  3, 10, 10,  4,  3,  6,  9], >        [ 2,  1,  8,  7,  3,  1,  9,  3,  6,  2], >        [ 9,  2,  6,  5,  3,  9,  4,  6,  1, 10]]) 期望输出： > [[1, 0, 2, 1, 1, 1, 0, 2, 2, 0], > [2, 1, 3, 0, 1, 0, 1, 0, 1, 1], > [0, 3, 0, 2, 3, 1, 0, 1, 0, 0], > [1, 0, 2, 1, 0, 1, 0, 2, 1, 2], > [2, 2, 2, 0, 0, 1, 1, 1, 1, 0], > [1, 1, 1, 1, 1, 2, 0, 0, 2, 1]] 输出包含 10 个列，表示从 1 到 10 的数字。这些数值分别代表每一行的计数数量。例如，Cell(0,2) 中有值 2，这意味着，数字 3 在第一行出现了两次。 50. 如何将 array_of_arrays 转换为平面 1 维数组？ 难度：L2 问题：将 array_of_arrays 转换为平面线性 1 维数组。 # Input: arr1 = np.arange(3) arr2 = np.arange(3,7) arr3 = np.arange(7,10) array_of_arrays = np.array([arr1, arr2, arr3]) array_of_arrays#> array([array([0, 1, 2]), array([3, 4, 5, 6]), array([7, 8, 9])], dtype=object) 期望输出： #> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 51. 如何为 NumPy 数组生成 one-hot 编码？ 难度：L4 问题：计算 one-hot 编码。 输入： np.random.seed(101)  arr = np.random.randint(1,4, size=6) arr #> array([2, 3, 2, 2, 2, 1]) 输出： #> array([[ 0.,  1.,  0.], #>        [ 0.,  0.,  1.], #>        [ 0.,  1.,  0.], #>        [ 0.,  1.,  0.], #>        [ 0.,  1.,  0.], #>        [ 1.,  0.,  0.]]) 52. 如何创建由类别变量分组确定的一维数值？ 难度：L3 问题：创建由类别变量分组的行数。使用以下来自 iris species 的样本作为输入。 输入： url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' species = np.genfromtxt(url, delimiter=',', dtype='str', usecols=4) species_small = np.sort(np.random.choice(species, size=20)) species_small #> array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', #>        'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', #>        'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', #>        'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', #>        'Iris-virginica', 'Iris-virginica', 'Iris-virginica', #>        'Iris-virginica', 'Iris-virginica', 'Iris-virginica'], #>       dtype='<U15') 期望输出： #> [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 7] 53. 如何基于给定的类别变量创建分组 id？ 难度：L4 问题：基于给定的类别变量创建分组 id。使用以下来自 iris species 的样本作为输入。 输入： url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' species = np.genfromtxt(url, delimiter=',', dtype='str', usecols=4) species_small = np.sort(np.random.choice(species, size=20)) species_small #> array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', #>        'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', #>        'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', #>        'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', #>        'Iris-virginica', 'Iris-virginica', 'Iris-virginica', #>        'Iris-virginica', 'Iris-virginica', 'Iris-virginica'], #>       dtype='<U15') 期望输出： #> [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2] 54. 如何使用 NumPy 对数组中的项进行排序？ 难度：L2 问题：为给定的数值数组 a 创建排序。 输入： np.random.seed(10) a = np.random.randint(20, size=10)print(a)#> [ 9 4 15 0 17 16 17 8 9 0] 期望输出： [4 2 6 0 8 7 9 3 5 1] 55. 如何使用 NumPy 对多维数组中的项进行排序？ 难度：L3 问题：给出一个数值数组 a，创建一个形态相同的排序数组。 输入： np.random.seed(10) a = np.random.randint(20, size=[2,5])print(a)#> [[ 9 4 15 0 17]#> [16 17 8 9 0]] 期望输出： #> [[4 2 6 0 8] #> [7 9 3 5 1]] 56. 如何在 2 维 NumPy 数组中找到每一行的最大值？ 难度：L2 问题：在给定数组中找到每一行的最大值。 np.random.seed(100) a = np.random.randint(1,10, [5,3]) a #> array([[9, 9, 4], #>        [8, 8, 1], #>        [5, 3, 6], #>        [3, 3, 3], #>        [2, 1, 9]]) 57. 如何计算 2 维 NumPy 数组每一行的 min-by-max？ 难度：L3 问题：给定一个 2 维 NumPy 数组，计算每一行的 min-by-max。 np.random.seed(100) a = np.random.randint(1,10, [5,3]) a #> array([[9, 9, 4], #>        [8, 8, 1], #>        [5, 3, 6], #>        [3, 3, 3], #>        [2, 1, 9]]) 58. 如何在 NumPy 数组中找到重复条目？ 难度：L3 问题：在给定的 NumPy 数组中找到重复条目（从第二次出现开始），并将其标记为 True。第一次出现的条目需要标记为 False。 # Input np.random.seed(100) a = np.random.randint(0, 5, 10) print('Array: ', a) #> Array: [0 0 3 0 2 4 2 2 2 2] 期望输出： #> [False True False True False False True True True True] 59. 如何找到 NumPy 的分组平均值？ 难度：L3 问题：在 2 维 NumPy 数组的类别列中找到数值的平均值。 输入： url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='object') names = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth', 'species') 期望解： #> [[b'Iris-setosa', 3.418], #> [b'Iris-versicolor', 2.770], #> [b'Iris-virginica', 2.974]] 60. 如何将 PIL 图像转换成 NumPy 数组？ 难度：L3 问题：从以下 URL 中导入图像，并将其转换成 NumPy 数组。 URL = 'https://upload.wikimedia.org/wikipedia/commons/8/8b/Denali_Mt_McKinley.jpg' 61. 如何删除 NumPy 数组中所有的缺失值？ 难度：L2 问题：从 1 维 NumPy 数组中删除所有的 nan 值。 输入： np.array([1,2,3,np.nan,5,6,7,np.nan]) 期望输出： array([ 1., 2., 3., 5., 6., 7.]) 62. 如何计算两个数组之间的欧几里得距离？ 难度：L3 问题：计算两个数组 a 和 b 之间的欧几里得距离。 输入： a = np.array([1,2,3,4,5]) b = np.array([4,5,6,7,8]) 63. 如何在一个 1 维数组中找到所有的局部极大值（peak）？ 难度：L4 问题：在 1 维数组 a 中找到所有的 peak，peak 指一个数字比两侧的数字都大。 输入： a = np.array([1, 3, 7, 1, 2, 6, 0, 1]) 期望输出： #> array([2, 5]) 64. 如何从 2 维数组中减去 1 维数组，从 2 维数组的每一行分别减去 1 维数组的每一项？ 难度：L2 问题：从 2 维数组 a_2d 中减去 1 维数组 b_1d，即从 a_2d 的每一行分别减去 b_1d 的每一项。 输入： a_2d = np.array([[3,3,3],[4,4,4],[5,5,5]]) b_1d = np.array([1,1,1] 期望输出： #> [[2 2 2] #> [2 2 2] #> [2 2 2]] 65. 如何在数组中找出某个项的第 n 个重复索引？ 难度：L2 问题：找到数组 x 中数字 1 的第 5 个重复索引。 x = np.array([1, 2, 1, 1, 3, 4, 3, 1, 1, 2, 1, 1, 2]) 66. 如何将 NumPy 的 datetime64 对象（object）转换为 datetime 的 datetime 对象？ 难度：L2 问题：将 NumPy 的 datetime64 对象（object）转换为 datetime 的 datetime 对象。 # Input: a numpy datetime64 object dt64 = np.datetime64('2018-02-25 22:10:10') 67. 如何计算 NumPy 数组的移动平均数？ 难度：L3 问题：给定 1 维数组，计算 window size 为 3 的移动平均数。 输入： np.random.seed(100) Z = np.random.randint(10, size=10) 68. 给定起始数字、length 和步长，如何创建一个 NumPy 数组序列？ 难度：L2 问题：从 5 开始，创建一个 length 为 10 的 NumPy 数组，相邻数字的差是 3。 69. 如何在不规则 NumPy 日期序列中填充缺失日期？ 难度：L3 问题：给定一个非连续日期序列的数组，通过填充缺失的日期，使其变成连续的日期序列。 输入： # Input dates = np.arange(np.datetime64('2018-02-01'), np.datetime64('2018-02-25'), 2) print(dates) #> ['2018-02-01' '2018-02-03' '2018-02-05' '2018-02-07' '2018-02-09' #>  '2018-02-11' '2018-02-13' '2018-02-15' '2018-02-17' '2018-02-19' #>  '2018-02-21' '2018-02-23'] 70. 如何基于给定的 1 维数组创建 strides？ 难度：L4 问题：给定 1 维数组 arr，使用 strides 生成一个 2 维矩阵，其中 window length 等于 4，strides 等于 2，例如 [[0,1,2,3], [2,3,4,5], [4,5,6,7]..]。 输入： arr = np.arange(15)  arr #> array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 期望输出： #> [[ 0 1 2 3] #> [ 2 3 4 5] #> [ 4 5 6 7] #> [ 6 7 8 9] #> [ 8 9 10 11] #> [10 11 12 13]] "
207,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738224&idx=5&sn=c71dfef8714621aa8d6ff40eaf3de12f&chksm=871aca4eb06d4358b665e4b8b21d55fcc40611ea6c50e33f3448f68b5c2fbc6b529b5ac5fd9f&scene=27,AAAI 2018 | 腾讯AI Lab现场陈述论文：训练L1稀疏模型的象限性消极下降算法,"王倪剑桥 腾讯 AI Lab 共有 12 篇论文入选在美国新奥尔良举行的国际人工智能领域顶级学术会议 AAAI 2018。腾讯技术工程官方号独家编译了论文《训练 L1 稀疏模型的象限性消极下降算法》(Training L1-Regularized Models with Orthant-Wise Passive Descent Algorithms)，该论文被 AAAI 2018 录用为现场陈述论文 (Oral Presentation)，由腾讯 AI Lab 独立完成，王倪剑桥为论文唯一作者。 论文地址：https://arxiv.org/abs/1704.07987   中文概要 L1 范数正则模型是一种常用的高维数据的分析方法。对于现代大规模互联网数据上的该模型，研究其优化算法可以提高其收敛速度，进而在有限时间内显著其模型准确率，或者降低对服务器资源的依赖。经典的随机梯度下降 (SGD) 虽然可以适用于神经网络等多种模型，但对于 L1 范数不可导性并不适用。 在本文中，我们提出了一种新的随机优化方法，随机象限性消极下降算法 (OPDA)。本算法的出发点是 L1 范数函数在任何一个象限内是连续可导的，因此，模型的参数在每一次更新之后被投影到上一次迭代时所在的象限。我们使用随机方差缩减梯度 (SVRG) 方法产生梯度方向，并结合伪牛顿法 (Quasi-Newton) 来使用损失函数的二阶信息。我们提出了一种新的象限投影方法，使得该算法收敛于一个更加稀疏的最优点。我们在理论上证明了，在强凸和光滑的损失函数上，该算法可以线性收敛。在 RCV1 等典型稀疏数据集上，我们测试了不同参数下 L1/L2 范数约束 Logistic 回归下该算法性能，其结果显著超越了已有的线性收敛算法 Proximal-SVRG，并且在卷积神经网络 (CNN) 的实验上超越 Proximal-SGD 等算法，证明了该算法在凸函数和非凸函数上均有很好的表现。 大家好，我是王倪剑桥，来自腾讯 AI Lab。   引言： 学习稀疏表征一直都是一个非常重要的数据分析任务。比如说，在生物学领域，为了对单个个体进行基因分析，常常涉及到数百万个基因，然而其中真正对某任务，比如帮助预测某种稀有癌症，有用的基因片段并不多。在金融序列预测和网络广告等领域，也有很多数据数量甚至比数据维度还小的情况。这本身是一个病态 (ill-condition) 的问题，然而如果对解有一个稀疏先验的话，问题则是可解的。通信领域的压缩感知中的核心部分也是如何高效求解稀疏模型。目前，这些方法包括 Lasso (Robert Tibshirani), Dantzig Selector (Emmanuel Candes, Terrence Tao), OMP (Joel A. Tropp, Tong Zhang), FoBa (Tong Zhang) 等。 首先，我们给一个直观的例子，为什么 L1 范数正则项（绝对值的和）适用于求解稀疏模型。下图中，蓝色区域是约束内的可行解区域，左边是 L1 范数球（norm ball），右边是 L2 范数球，红色圈是均方损失函数（average-of-square-loss）的等高线。这些球和等高线之间的交点是模型的解。我们可以看到 L1 正则化模型的解接近 Y 轴，这意味着其 X 维元素更接近于 0。这是一个简单的例子，可以看出来 L1 正则项比 L2 正则项更适合于学习稀疏模型。 我们可以把大部分问题统一为最小化一个正则化函数 P(x)= F(x)+R(x)，其中 F(x) 是 N 个损失函数的平均，其中每个都依赖于一个数据样本，R 是 L1 正则项。我们还假设每个损失函数都满足二次可导 (twice differentiable)、强凸 (strongly-convex) 和光滑 (smooth)。由于该 L1 范数是不可导的（在零点），目前最通用的优化方法是近端方法 (proximal method)，通过迭代式地采取梯度下降步骤，然后在当前点上优化一个 proximal 问题。 相关文献介绍： 我们的主要参考方法是象限性伪牛顿法 (OWL-QN: Galen Andrew, Jianfeng Gao)，这种方法基于 L-BFGS (Jorge Nocedal)，一种最常用的伪牛顿法。该方法 OWL-QN 将更新后的参数限制在一个特定的象限内，因为在每个单个象限中，其绝对值函数实际上是可微分的。OWL-QN 的一个关键创新点与在零点的次梯度 (subgradient) 有关。这里 L1 正则项 R(x) 的次梯度既可以是正λ，也可以是负 λ，那么如何选择次梯度会影响收敛速度。以下面大括号内第三个分支为例：我们研究的是当前点 X 的第 i 维 X_i，和梯度 V_i。如果 X_i 等于 0 且 X_i 加上 λ 为负，那么该次梯度就设为 V_i 加上正 λ，因为在减去这个次梯度之后，X_i 将会是一个正值，那么 R(x) 的次梯度将仍然为正 λ，这会使 L1 范数的次梯度在一次迭代中保持一致。 为了处理大规模互联网数据，研究者提出了很多用于加速训练过程的优化方法，比如随机梯度下降法 (SGD)。但是，SGD 通常需要衰减步长才能收敛，而且它的收敛率是次线性的 (sublinear)。近期出现了 SVRG (Rie Johnson, Tong Zhang) 和 SAGA (Aaron Defazio, Francis Bach) 等一些随机方差缩减方法，可以无需降低步长就能收敛，而且可以在光滑和强凸的模型上实现线性收敛率 (linear convergence rate)。在 SGD 方法中，第 k 步的下降方向 v_k 是在该数据集的一个随机子集 S_k 上进行评估的。在 SVRG 方法中，我们需要周期性地在一些参考点（比如 tilde-X）上计算一个准确梯度。这个准确梯度构成了 SVRG 的 v_k 的第三项，然后我们必须通过在 tilde-X 上减去一个随机梯度（这是在同一子集 S_k 上计算的）来平衡它的期望。 受 SVRG 和 OWL-QN 的启发，我们开发了一种针对 L1 正则化模型的随机优化方法。在第一步时，我们计算损失函数 F(x) 的 SVRG，然后我们使用来自 OWL-QN 的思想来计算一个参考方向，尽量使下降方向以在一次迭代后维持同一个象限。而我们的实际下降方向 V_k 是用这里的第三个等式计算的。 现在我们可以计算解前进的方向了。我们可以通过计算当前点上的 Hessian 矩阵或估计近似的 Hessian 矩阵（伪牛顿法）来使用该损失函数的二阶信息，从而加速其收敛。然后我们可以通过在当前点附近的 Taylor 二次展开来计算最优下降方向 D_k。如果我们使用 L-BFGS，我们可以绕开耗时的矩阵求逆运算，只通过快速的矩阵向量乘法就可以做到。我们也可以直接将 V_k 分配给 D_k，这样就是一个典型的一阶方法。在这个步骤之后，方向 D_k 的象限必须与参考方向保持一致，这意味着：如果 D_k 的某些维度与参考方向的符号不同，它们就必须被分配为 0，我们将这个对齐后的版本记为 P_k。 除了对齐参考，上述计算梯度并没有涉及到 L1 正则化 R(x) 的偏导数，因为我们要避免对随机梯度的引入额外方差。为了使解是稀疏的，我们提出了一种全新的对齐算子来激励零元素。这个算子作用在 X,Y 两个元素上，如果 X 和 Y 符号不同或 X 的绝对值小于一个阈值，就强制 X 为零。通过这个对齐算子，每次在我们完成了之前的计算之后，我们就检查下一个点是否与当前点在同一个象限，如果不在，下一个点的某些维度就会被强制为零。在这一步之后，显然 X_k 的更多维度应该为零，而不是绝对值很小的非零值。 收敛性分析：在这篇论文中，我们证明在平滑性和强凸性的假设下，我们的方法将以一个线性速率收敛。 为了进行可视化，我们在这张图中绘制了在一个简单的二维合成函数的优化轨迹。我们的方法 OPDA 用红线标识，作为基准的 Proximal-Gradient Descent 算法用蓝线标识。在迭代了同样多次数之后，我们看到 OPDA 用更快的速度收敛到了等高线中更低的区域。 在凸函数上的实验：我们还使用 L2/L1 regularized Logistic 回归上进行了一些实验。在这一部分，我们将我们的方法与一个线性收敛算法 Prox-SVRG (Lin Xiao, Tong Zhang) 进行了比较。在这张图中，Y 轴表示解的次优性 (suboptimality)，即当前目标函数与最小值之间的间隔；X 轴表示完整扫过数据集的次数。我们发现使用不同的步长以及 L2 和 L1 正则化系数时，OPDA 的速度稳定地超越了 Prox-SVRG 算法。   在深度学习上的实验：我们使用 L1 正则化稀疏卷积神经网络 (sparse-CNN) 进行了实验，以表明我们的方法在非凸函数的有效性。下图中红线表示我们的方法，蓝线表示近端 SVRG。我们测试了不同规模的 L1 正则化。我们看到 OPDA 收敛速度比近 Prox-SGD 更快。而且在 L1 正则项更强的情况下，这种差异更大。   总结： 我们提出的 OPDA 算法可以适用于快速优化 L1 正则的稀疏模型。实际上，由于我们的方法的象限性本质，下降方向和更新后的点的许多维度都会在对齐过程中被强制变为零，这会使等效步长变小很多；但是，在同等步长条件下，我们方法的速度仍然远远快于近端方法。这证明我们提出的对齐算子确实能将方向调校得更好，使其整体框架更为有效，而且在每次迭代中所需额外运算量接近可以忽略不计。    "
208,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738224&idx=3&sn=1225106cca5d691a70df805ff51dcb69&chksm=871aca4eb06d435825ff0aeccc24d1f1fc7bf0677ea3d91842438dbbfd385d96543350b130d6&scene=27,前沿 | 剧本自动生成电影：杜克大学提出AI视频生成新方法,Science 用写好的剧本（文字）自动生成电影或许是很多剧作家的梦想，人工智能技术最近让这一梦想距离现实更近了一步。杜克大学 Yitong Li 等人提出了一种结合变分自编码器（VAE）与生成对抗网络（GAN）的算法，可为一小段文本生成相应短视频，该研究已在 AAAI 2018 大会上进行了展示。 编剧们最近又有新的理由拒绝高成本和重要电影厂牌的众多资源了，这得归功于一个新的人工智能算法，它可以消化很短的剧本然后产生一段视频。尽管这些新的电影距离奥斯卡获奖的程度还相去甚远，但我们已经可以想象一种类似的技术将在未来的某一天在娱乐圈以外的地方找到用处，比如帮助目击证人重现一个撞车或者犯罪现场。 人工智能在识别及标注图片这一领域已经显得驾轻就熟。所谓的「生成」算法走了另一条路，用标签（或者脑部扫描）来生产新的图片。少部分的研究甚至可以用一帧电影画面来预测之后一系列的画面帧。但是把这些结合在一起，从文字创建一个图片然后让它动起来从而变成一个实际的电影，此前还从来没人做到过。 「目前为止据我而知，这是第一个有好结果的文本-转-视频的工作。它们不是完美的，但至少它们看起来已经像真的电影了。」Tinne Tuytelaars 说道，他是一位在比利时 Katholieke Universiteit Leuven 的计算机科学家，她已经做过两个视频预测的研究。「这真的做的很棒」。 这个新的算法是机器学习的一种形式，这意味着它需要训练。尤其是，这是一个神经网络，或者是一系列像老式大脑神经元一样的小计算元素形成的层。在训练中，软件评估了它每次尝试的表现，然后反馈在几百万个网络连接里循环来改善之后的计算。 这个网络的工作方式为两个阶段「以模仿人类创造艺术的方式」，研究员说。第一阶段是用文本生成一个视频的「主旨」，基本上是一个模糊的背景图片加上一团模糊的标注，标记主要动作发生的地方。第二阶段用「主旨」和文本生产一个短视频。在训练中，第二个网络表现为「鉴别器」。它观察新生成的视频，例如，在一个海上帆船视频的旁边写道「在大海上航行」，鉴别器会被训练来找出符合叙述内容的图像。随着鉴别器的性能越来越好，它会变得更加苛刻，它的反馈也为生成器网络设置了一个新的更高的标准。 研究员在十种场景中训练了这个算法，包括「在草地上打高尔夫球」，和「在海上玩风筝冲浪」，算法在这个场景下表现的比较粗糙，呈现 VHS 录像的颗粒感画面。一种简单的分类算法可以在 6 种选项里猜对大约 50% 的几率。（但总把风筝冲浪和航行弄混）。更多的，这个网络可以为荒唐的场景生产视频，比如「在雪上航行」和「在游泳池上打高尔夫」，该研究团队本月在路易斯安纳州新奥尔良的 AAAI 2018 大会上对这项研究进行了报告。 「他们的方法非常有意思，融合了两个阶段」，Hamed Pirsiavash 说，他是马里兰大学的一位计算机科学家，此前也完成过视频预测工作。「这是个超级困难的工作。所以我也非常高兴能看到这些人做出了这么好的成果。」 现在，算法完成的视频只有 32 帧大约 1 秒大小像邮票一样，64×64 像素的尺寸。更大的分辨率会降低正确率，杜克大学的计算机科学家 Yitong Li 表示，他也是这篇文章的第一作者。因为人们经常在图像里被扭曲，他希望在未来使用人体骨骼模型来提高动作的效果。 Tuytelaars 也在好莱坞以外的其他领域看到了新方法应用的方向，视频生成技术也导向更好的视频压缩技术，可以只存简介而不存视频。它也可以从其他机器学习算法中生成训练数据。举例，真实的视频短片可能帮助自动驾驶车为不常见的危险情况而准备。深度理解了视觉世界的程序可以从审查到监控中筛选出有用的应用。「新技术可以帮助自动驾驶车预测一个摩托车将开向哪里，或者训练家庭服务机器人打开冰箱，」Pirsiavash 说道。 目前看来，让 AI 生成好莱坞大片还不现实，但同时，我们终于知道 「 在草地上玩风筝冲浪 」 是什么样子了。 论文：Video Generation from Text 论文链接：http://www.aaai.org/GuideBook2018/16152-72279-GB.pdf 摘要： 从已有生成模型中用文本生成视频是一个困难的技术挑战。我们训练了一个有条件的提取动态和静态信息的生成模型来处理这个问题。这种思路在混合框架下被证明可行的，我们的模型应用了变分自编码器（VAE）和生成对抗网络（GAN）。动态特征，被叫做「主旨」，常被用来画出语境的背景颜色和物体构造结构。动态特征可用于将输入文本转换为图片过滤器。为了得到大量的模型训练数据，我们在公开的在线视频之上发展了一个方法来自动创建对应的文字--视频语料库。实验结果表明，我们提出的架构可以生成具有一定拟真度的多种平滑短视频，文本中的信息可以正确地在视频中显现。这种方法比直接使用文本转图片再生成视频的基线模型在表现上好很多。我们通过视觉观察和用于评估 GAN 生成图片的评分来对生成视频的效果进行了评估。  原文链接：http://www.sciencemag.org/news/2018/02/new-algorithm-can-create-movies-just-few-snippets-text 
209,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738224&idx=2&sn=f2292f63e8486014ccaa196185a0b1cf&chksm=871aca4eb06d4358f97d4335a1879b45ae2508f74d0b66bdcb73b88a51fa2bf18902596ba1ba&scene=27,深度 | 让机器思考与互相理解：DeepMind提出机器心智理论神经网络ToMnet,"AI 不仅需要判断，也需要具备思考问题的能力。真正的人工智能应该和人类一样，可以理解自己以及周围智能体的心理状态，这些状态包括情绪、新年、意图、欲望、假装与知识等。DeepMind 近日提出的「机器心智理论」神经网络 ToMnet 让计算机拥有了这种能力，这或许是我们在人工智能技术上的一次重要进步。 对于现在所有的深度学习和深度强化学习方法而言，我们有一个担忧的问题：从某些方面来说，我们对这些系统的理解非常有限。神经网络经常被说成是难以理解、难以解释的黑箱子。即使我们对其权重有完整的解释，还是很难掌控它们到底利用了什么模式，也很难掌控哪里会出问题。随着智能体进入人类世界，要求理解这些系统的声音变得越来越大。 让我们先停下来，思考另一个问题：「理解」另一个智能体到底意味着什么？作为人类，我们每天都在面临这项挑战，我们与他人交流合作，但几乎无法触及这些人的内隐特征、内隐状态和计算过程。但我们还是用卓越的熟练度来行使职责。我们可以预测陌生人将来的行为，并且推断他们对世界的观点；我们规划与其他人的互动，并且建立高效的交流。 能够「理解」其他智能体的一个显著特征是极少甚至不引用智能体真正的基础构架。我们没有试图去预测其他神经的活动，推断他们的前额皮质的连通性，或者计划与一个非常详细的其他人的海马体图的预测的交互。从认知心理学延伸出的一个突出的讨论是我们的社会推理（social reasoning）并不是依赖于其他媒介的高层次的模型（Gopnik & Wellman, 1992）。这些模型用了一些没法解释表面现象背后原理的抽象概念；相反，我们表示了其他人的精神状态，比如他们的欲望、信仰和意图。这个能力一般被解释成我们的心智理论（Premack & Woodruff, 1978）。在一些案例中我们也让自己的意识来模仿其他人（比如 Gordon, 1986; Gallese & Goldman, 1998），我们对其智能体的终极理解并不是从把我们的模型与真理一一对应从而测量出来的，而是从这些模型会对比如说预测或者计划这种任务付出多少来决定（Dennett, 1991）。 在这篇文章里，来自 DeepMind 的研究人员受到了人类心智理论的启发，试图建立一个模拟其他介质的模型。我们把这个描述成机器心智理论。我们的目标不是去确保生成一个介质行为与内含转化的算法的模型。与之相反，我们专注于一个观察者怎样能自动的学习怎样利用有限数据模拟其他介质的模型（Botvinick et al., 2017）。正是这一点让 DeepMind 的新研究有别于前人方向，也就是那些依赖于用其他介质手工制造的模型作为有噪声的对比对象。--比如用反向阻耗（Ng et al., 2000; Abbeel & Ng, 2004），贝叶斯推理（Lucas et al., 2014; Evans et al., 2016），贝叶斯心智理论（Baker et al., 2011; Jara-Ettinger et al., 2016; Baker et al., 2017）或者博弈论（Camerer et al., 2004; Yoshida et al., 2008; Camerer, 2010; Lanctot et al., 2017）。与以上研究不同，我们学习了代理模型，然后学习了怎样从通过元学习从头推理它们。 建立一个丰富的，灵活的而且高效绩的机器心智理论也许是一个大挑战。我们没有试着去在这里解决所有问题。一个主要的信息是当这些问题被正确表述的时候，很多初始的，建立机器心智理论（ToM）的挑战可以被分解成简单的学习问题。我们这里的工作是找出这些简单的构想方式。 DeepMind 称，新研究有很多潜在的应用方向。学习其他的丰富的模型可以提高多智能体任务中的决策，尤其是基于模型的规划和想象所需要的（Hassabis et al., 2013; Hula et al., 2015; Oliehoek & Amato, 2016）。这样的模型对于数据校准（HadfieldMenell et al., 2016）和灵活合作（Nowak, 2006; Kleiman-Weiner et al., 2016; Barrett et al., 2017; Kris Cao）是很重要的，并且很有可能是未来机器决策中的道德的重要组成部分（Churchland, 1996）。它们也是对通讯和教育学非常有用的（Dragan et al., 2013; Fisac et al., 2017; Milli et al., 2017），也非常可能在人机交互领域扮演重要角色。探索这些能力之下的条件也可以阐明人类能力的起源（Carey, 2009）。最后，这样的模型也会是人类理解人造智能体的重要介质。 最后，我们被理解人造智能体这一目标所激励。这里我们有一个新奇的方法：除了从结构上改编智能体来把它们的内部状态以人类理解的形式暴露出来，我们追寻制造可以降低行为空间维度并且能以更易懂的形式表现的中间系统。从这个角度，追求机器心智理论（ToM）是建造缺失的机器与人的期望之间的交互界面（Cohen et al., 1981）。 DeepMind 的新方法 DeepMind 认为，构建心智理论的挑战本质上在于元学习问题（Schmidhuber et al., 1996; Thrun & Pratt, 1998; Hochreiter et al., 2001; Vilalta & Drissi, 2002）。在测试时，我们希望遇到一个以前没见过的智能体，并且它们已经对自身的行为方式有一个强大且丰富的先验知识。此外，在我们看到该智能体在现实中的行动时，我们希望能收集它们的隐藏特性和精神状态数据（构成后验知识），这有助于我们预测它们未来的行为。 为此，我们定制了元学习任务。我们构建了一个观察者，它在每一个 episode 都能访问一组新型智能体的行为轨迹，观察者的目标是预测智能体未来的行为。在训练过程中，观察者应该从有限数据中快速形成新智能体的的预测。这种新智能体的「学习如何学习」通常可以称为元学习。通过这个过程，观察者还应该学习智能体行为的有效先验知识，这些知识隐含地捕捉了训练空间中智能体间的共同点。 DeepMind 引入了两个概念来描述该观察者网络及其功能角色。我们区分两个一般心智理论，即网络的预学习权重与特定智能体心智理论。在网络的预学习权重中，它包含了训练集中所有智能体一般行为的预测。而在特定智能体心智理论中，从单个智能体在测试时的察形成「agent embedding」，它包含了使得智能体的特性和精神状态区别于其它智能体的内容。这些对应于智能体行为的先验知识和后验知识。 本论文的结构是一系列在机器心智理论网络（我们称之为 ToMnet）的实验，它们的复杂度是递增的。这些实验展示了 ToMnet 背后的思想和能力，并展示了它学习其他智能体丰富模型的能力，其中包含了人类心智理论的典型特征，如对错误信念的认识等。 本论文中的一些实验直接受到 Baker 及其同事在贝叶斯心智理论研究成果的启发，例如经典的 food-truck 实验（Baker et al., 2011; 2017）。由于该项工作的目标不同，我们并没有试图直接复制这些实验。特别的，我们并不是立即利用计算项搜索人类判断的解释，而是强调机器学习、可扩展性和自主性。我们将在未来的工作中解释人类的判断。我们的实验应该推广至许多先前实验的构造以适应我们的目标。 本研究的主要贡献包括： 章节 3.1 中，我们展示了对简单、随机的智能体而言，ToMnet 能学习逼近贝叶斯优化的层级推理到智能体的特性； 章节 3.2 中，我们展示了 ToMnet 学习推理算法智能体的目标（有效完成了 few-shot 逆强化学习），以及它们如何平衡成本与奖励。 章节 3.3 中，我们展示了 ToMnet 学习表示不同类型的深度强化学习智能体，掌握深度强化学习智能体不同变体的关键元素，并组成这些智能体的抽象嵌入（abstract embeddings）。我们也展示了 ToMnet 能发现行为空间新的抽象。 章节 3.4 中，我们展示了当在 POMDP 中活动的深度强化学习智能体上训练 ToMnet 时，它暗中学习到这些智能体能持有该世界的错误信念，这也是人类心智理论的核心。 章节 3.5 中，我们展示了能够训练 ToMnet 来预测智能体的信念状态，明确揭开智能体的错误信念。我们也展示了 ToMnet 能单独从行为中推断什么智能体具有观看的能力，以及因此它们倾向于相信什么。 character net 从 POMDP 集合中解析智能体过去的轨迹，从而形成 character 嵌入 e_char。心理状态网络解析当前片段的智能体轨迹，形成心理状态嵌入 e_mental。然后，这些嵌入被输入至预测网络，可用于查询当前状态。预测网络输出对智能体未来行为的预测，如下一步动作概率 π hat、特定对象被消耗的概率 c hat 和预测后继者表示 SR hat（Dayan, 1993）。 （a）示例智能体之前的轨迹。彩色方块代表四个对象。红色箭头表示智能体的位置和动作。（b）查询示例：来自新 MDP 的状态。黑点代表智能体位置。（c）基于（a）中对之前轨迹的观测，针对（b）中的查询 MDP，ToMnet 对智能体下一个动作的预测（上方）和对该片段结束时对象是否被消耗的预测（下方）。（d）ToMnet 使用折扣因子 γ = 0.9，对（b）中查询的后继者表示（successor representation，SR）的预测。黑色部分表示更高的期望折扣状态占用。 （a）ToMnet 的三个组件分别对应三个行为预测目标，图中表示简单 ToMnet 与没有 character net 或没有 mental net 的网络的对比。长条越长越好；具备 character net 和 mental net 的网络最好。（b）查询 POMDP 状态在时间 t = 0 时，ToMnet 对智能体未来状态占用的预测（左），如图 4d 所示。星星表示子目标。右边的三幅图根据每个亚种的示例智能体在 N_past = 5 past POMDPs 上的行为输出（示例智能体通常指粉色对象）。ToMnet 事先并不了解每个智能体属于哪个亚种，但是可以根据智能体之前的行为推断出来。 论文：Machine Theory of Mind 论文链接：https://arxiv.org/abs/1802.07740 摘要： 心智理论（ToM; Premack & Woodruff, 1978）广义上指个体有能力理解他人的心理状态，包括期望、信念和意图。我们提出对机器进行训练，使之也具备这项能力。我们设计了一种心智理论神经网络 ToMnet，它使用元学习通过观察智能体的行为而对它们进行建模。通过该过程，该网络得到一个对智能体行为具备强大先验知识的模型，同时能够利用少量行为观测对智能体特征和心理状态进行更丰富的预测。我们将 ToMnet 应用于在 gridworld 环境中采取动作的智能体，结果表明该网络学会对来自不同群体的智能体进行建模，包括随机、规则系统和深度强化学习智能体等，该网络通过了经典的 ToM 任务，如""Sally-Anne""测试，即意识到他人持有的错误观念。我们认为该系统（自动学习如何对出现在其世界中的其他智能体进行建模）是开发多智能体 AI 系统的重要步骤，可以帮助构建人机交互的中介技术，推进可解释性 AI 的发展。  "
210,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738224&idx=1&sn=dd3a9bc5b71cdc23bf92fd8816fd68f0&chksm=871aca4eb06d43585821885b7a769b7d2f9b07fbdabbfc4852c77355102af645fcff53c382c0&scene=27,既能欺骗机器，也能迷惑人类！Goodfellow等人提出新一代对抗样本,"arXiv 机器学习模型易受对抗样本的影响，导致识别错误，那么人类呢？在本文中，谷歌大脑 Ian Goodfellow 等人通过最新技术创建了首个欺骗人类的对抗样本，其实现过程是把已知参数和架构的计算机视觉模型的对抗样本迁移至未获取参数和架构的其他模型，并通过修改模型更好地匹配人类视觉系统的初始处理。 机器学习模型很容易受到对抗样本的愚弄：输入经过对抗样本优化后导致模型输出错误的分类（Szegedy et al., 2013; Biggio et al., 2013）。在计算机视觉领域中，对抗样本通常是对数据集中的样本图像进行微小扰动形成的图像。很多构建对抗样本的流行算法依赖模型架构和参数对输入进行梯度优化。但由于无法获取大脑的「架构和参数」，这些方法无法针对人类构建对抗样本。 一个有趣现象是对抗样本通常可从一个模型迁移到另一个，这使得攻击未获取架构和参数的模型成为可能。这自然就提出了一个问题，即对抗样本是否可以欺骗人类。人类有很多认知偏差和视觉错觉，但这些通常不算是自然图像的微小扰动，目前也无法通过机器学习损失函数来优化生成。因此，目前该领域对此的观点是迁移性对抗样本没有影响人类视觉感知，尽管研究者并未进行彻底的实证研究。 研究者对上述问题进行了严密研究，为机器学习和神经科学互相学习创造了机会。神经科学通常为机器学习提供存在证明——我们研究对象识别算法之前，先假设有可能构建成功，因为人脑能够识别对象。详情可查看 Hassabis et al. (2017) 关于神经科学对人工智能影响的论述。如果我们知道人脑能够抵御某类对抗样本，那么这就为机器学习安全性的类似机制提供了存在证明。如果我们知道人脑会被对抗样本愚弄，那么机器学习安全性研究的重心或许应该从设计对对抗样本具备鲁棒性的模型转移到设计安全系统但包含非鲁棒性的机器学习组件。类似地，如果针对计算机视觉开发的对抗样本对人脑也有影响，那么这有助于更好地理解人脑功能。 （a）对抗样本图像典型示例（Goodfellow et al.，2014）。该对抗利用几何变换攻击愚弄模型的能力适度且有限，可以愚弄的模型不包括生成该对抗图像的模型。（b）该对抗攻击使猫图像被标注为计算机，但是对几何变换具备鲁棒性（Athalye，2017）。与 a 中的攻击不同，该图像包含对人类来说语义上更像计算机的语义特征。（c）使图像被标注为面包机的对抗 patch，可引起多个视角的误分类（Brown et al.，2017）。与 b 类似，该 patch 包括人类看来更像面包机的特征。（d）本论文中，研究者发现当对抗样本图像用于愚弄多个模型，而不是不同视角的同样模型时会有类似的影响。此处图像对应一系列把猫识别为狗的对抗攻击。上：从左到右，攻击针对的是越来越大型的模型集群（右侧是原始图像）。每张图像上方是两个测试模型的类别预测结果。随着攻击目标模型的数量越来越多，生成的图像对人类来说越来越像狗。下：攻击针对 10 个模型时，不断变化的攻击程度。即使在 eps = 8 时，该图像对人类来说也更像狗。 该研究调查了能够在多个计算机视觉模型之间强烈迁移的对抗样本对人类视觉感知的影响力。利用三个关键点来测试对抗样本是否会对人类视觉系统产生可观测的影响： 首先，研究者使用近期的黑箱对抗样本构建技术为未获取模型架构或参数的目标模型创建对抗样本。第二，研究者调整机器学习模型来模仿人类的初始视觉处理，使对抗样本更有可能从模型迁移至人类观察者。第三，研究者在时限性环境中评估人类观察者的分类结果，以使对抗样本对人类感知的细微影响也能被检测到。 换言之，人类可以在分类任务上达到接近完美的准确率，性能的微小改变可能不会对应到准确率的可观变化。图像呈现时间足够简短的情况下，人类甚至无法对干净图像实现完美的准确率，性能的微小改变会导致准确率方面更加可观的变化。此外，短时间的图像呈现限制了大脑利用循环和自上而下处理路径的时间（Potter et al., 2014），被认为是使大脑内部的处理过程更接近前馈人工神经网络。 研究者发现可在多个计算机视觉模型之间迁移的对抗样本能够成功地影响人类观察者的感知，从而发现了一种可同时适用于计算机视觉模型和人脑的新型假象（illusion）。 Goodfellow et al. (2017) 将对抗样本定义为「机器学习模型的输入，一个有意设计出并导致模型犯错的攻击者」。在视觉目标识别中，对抗样本通常是向自然图像中添加微小扰动后的图像，可以破坏机器学习分类器的预测。图 2a 就是一个典型示例：通过向熊猫图像添加微小扰动，使模型将其误分类为长臂猿。扰动通常很小，难以觉察（即它无法保存为 8 比特的标准 png 文件，因为扰动比像素动态范围的 1/255 还要小）。该扰动并非噪声，它依赖于根据神经网络参数仔细选择的结构，但是即使扩大到可感知的程度，人类观察者也不会识别出任何有意义的结构。注意：对抗样本也存在于恶意软件检测等领域中（Grosse et al., 2017），但是本论文主要关注图像分类任务。 对抗样本定义的两个方面对本研究尤其重要： 1. 对抗样本旨在引发错误。 它们的设计目的并非背离人类的判断。如果对抗样本与人类输出相悖，则不可能存在针对人类的对抗样本。一些任务存在客观正确的答案，比如预测输入数字是否为素数。研究者希望模型获得正确答案，而不是人类给出的答案（何况时间有限的情况下人类可能也无法很好地判断数字是否为素数）。定义什么构成视觉对象识别的错误很有难度，因为图像添加扰动之后可能不再对应于真实物理场景的照片，并且定义图像的真实对象类别在哲学上是困难的，因为真实物体的图像并不是真实物体。该研究假设当输出标签不同于人类为干净图像（即对抗样本的起始点）提供的标签时，则对抗图像被错误分类。研究者制造小的对抗扰动，并且假设这些微小扰动不足以改变真类。 2. 对抗样本并非不可感知。 如果是这样，则从定义上来看就不可能制造出针对人类的对抗样本，因为改变人类的分类也就意味着改变人类的感知内容。此外，在很多领域中，做出不可感知的改变都是不可能的（如自然语言处理，即使改变一个字符也是可感知的）。计算机视觉算法经常被人类无法感知的对抗样本愚弄，但这并不是一般定义的一部分（可参见图 2b、c）。 研究者构建了 k 个 CNN 模型的集成 (k = 10)，这些模型在 ImageNet 上进行训练。每个模型是以下架构的实例：Inception V3、Inception V4、Inception ResNet V2、ResNet V2 50、ResNet V2 101、ResNet V2 152 (Szegedy et al., 2015; 2016; He et al., 2016)。为更好地匹配人类视觉系统的初始处理，研究者预先为每个模型输入添加一个视网膜层，它整合了一些由人眼执行的变换。在该层中，研究者执行图像的 eccentricity-dependent 模糊化，以近似人类受试者的视觉皮质通过视网膜晶格接收的输入。模型细节详见附录 B。研究者使用来自 Van Essen＆Anderson（1995）（基于猕猴视觉系统）的 eccentricity-dependent 空间分辨率测量以及观察者和屏幕的已知几何角度，来确定每个图像位置的空间模糊程度，从而把 CNN 限制为人类视觉系统也可获取的信息。该层是完全可微的，在运行对抗攻击时允许梯度通过网络进行反向传播。 论文：Adversarial Examples that Fool both Human and Computer Vision  论文链接：https://arxiv.org/abs/1802.08195 摘要： 机器学习模型易受对抗样本的影响：图像的微小改变即可导致计算机视觉模型犯错，比如把校车识别为鸵鸟。但是，人类能否避免同样的错误依然未知。这里，我们利用最新技术创建了首个欺骗人类的对抗样本，这些技术将已知参数和架构的计算机视觉模型的对抗样本迁移至未获取参数和架构的其他模型，并通过修改模型更好地匹配人类视觉系统的初始处理。我们发现，在计算机视觉模型之间进行有效迁移的对抗样本对时限性环境下的人类观察者的分类结果产生影响。  "
211,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738224&idx=4&sn=8285699a68ccafbf358e6c623e245eda&chksm=871aca4eb06d43580fdfb6242c1ef5af8ab5f8b0255285b0258c245f76bf5c4fe5835505beef&scene=27,入门 | 强化学习的基本概念与代码实现,"DeepLearning4j 从 AlphaGo 到自动驾驶汽车，我们能在很多最先进的人工智能应用中找到强化学习的身影。这种技术是如何从零开始慢慢学会完成任务，成长为「超越人类水平」的专家的？本文将会进行一番简要介绍。 神经网络造就了最近我们在计算机视觉、机器翻译和时间序列预测等领域上的突破—同时它也可以和强化学习算法结合来创建一些惊人的成果，例如 AlphaGo（参阅： ）。 强化学习指的是面向目标的算法，这种算法学习如何在一些具体的步骤中达到一个目标或者最大化；例如，最大化一个游戏中通过一些行动而获得的得分。它们可以从一个空白状态开始，然后在合适的条件下达到超越人类水平的性能。就像被糖果和体罚刺激的小孩子一样，当它们做出错误的预测时，这些算法会受到惩罚，当它们做出正确的预测时，它们会得到奖励—这便是强化的意义所在。 结合深度学习的强化算法可以在围棋和 Atari 游戏中打败人类冠军。尽管这听起来还不具有足够的说服力，但是这已经远远优于它们之前的成就了，而且目前最先进的进步是很迅速的。 两个强化学习的算法 Deep-Q learning 和 A3C 已经在 Deeplearning4j 库上实现了，现在，它已经可以玩《毁灭战士（Doom）》了。 强化学习解决了对即刻行动和与之相关的延迟响应之间的关联的问题。就像人类一样，强化学习算法必须等待一会，看看它们决策的结果如何。它们运行在延迟响应的环境之中，在这种环境中理解哪些行动在多个步骤后导致哪些结果是很困难的。 我们可以期望强化学习算法在更加模糊的现实环境中表现得更好，它可以在真实环境下从任意多个可能的行动中进行选择，而不是从有限个电子游戏动作选项中进行选择。也就是说，随着时间的推移，我们希望它们（强化学习算法）能够在现实世界中有着实现目标的价值。 强化学习入门（https://docs.skymind.ai/docs?__hstc=3042607.e3fc0b81c1643174a38ec061d10e5084.1517447567935.1517447567935.1517447567935.1&__hssc=3042607.1.1517447567935&__hsfp=3282609078） 强化学习定义 我们可以通过了解智能体、环境、状态、行动以及奖励等概念来理解强化学习，我们会在一下的内容里解释这些概念。大写字母表示事物的集合，小写字母代表事物的实例；例如，A 是所有可能存在的行动的集合，而 a 是这个集合中包含的一个实例。 智能体（Agent）：可以采取行动的智能个体；例如，可以完成投递的无人机，或者在视频游戏中朝目标行动的超级马里奥。强化学习算法就是一个智能体。而在现实生活中，那个智能体就是你。 行动（Action）：A 是智能体可以采取的行动的集合。一个行动（action）几乎是一目了然的，但是应该注意的是智能体是在从可能的行动列表中进行选择。在电子游戏中，这个行动列表可能包括向右奔跑或者向左奔跑，向高出处跳或者向低处跳，下蹲或者站住不动。在股市中，这个行动列表可能包括买入，卖出或者持有任何有价证券或者它们的变体。在处理空中飞行的无人机时，行动选项包含三维空间中的很多速度和加速度。 环境（Environment）：指的就是智能体行走于其中的世界。这个环境将智能体当前的状态和行动作为输入，输出是智能体的奖励和下一步的状态。如果你是一个智能体，那么你所处的环境就是能够处理行动和决定你一系列行动的结果的物理规律和社会规则。 状态（State，S）：一个状态就是智能体所处的具体即时状态；也就是说，一个具体的地方和时刻，这是一个具体的即时配置，它能够将智能体和其他重要的失事物关联起来，例如工具、敌人和或者奖励。它是由环境返回的当前形势。你是否曾在错误的时间出现在错误的地点？那无疑就是一个状态了。 奖励（Reward，R）：奖励是我们衡量某个智能体的行动成败的反馈。例如，在视频游戏中，当马里奥碰到金币的时候，它就会赢得分数。面对任何既定的状态，智能体要以行动的形式向环境输出，然后环境会返回这个智能体的一个新状态（这个新状态会受到基于之前状态的行动的影响）和奖励（如果有任何奖励的话）。奖励可能是即时的，也可能是迟滞的。它们可以有效地评估该智能体的行动。 策略（policy，π）：policy 是智能体基于当前的状态做出下一步行动所用的策略。 价值（value，V）：期望的具有折扣的长期收益，而不是短期回报 R。我们定义 Vπ(s) 为当前状态为 s 时基于策略π的长期回报。 Q 价值或者行动价值（Q）：Q 价值（Q-Value）和上述的价值类似，不同的是它还使用了另一个参数--当前的行动 a。Vπ(s) 指的是基于当前状态 s，行动 a 和策略π，得到的长期回报。 所以，环境就是能够将当前状态下采取的动作转换成下一个状态和奖励的函数；智能体是将新的状态和奖励转换成下一个行动的函数。我们可以知悉智能体的函数，但是我们无法知悉环境的函数。环境是一个我们只能看到输入输出的黑盒子。强化学习相当于智能体在尝试逼近这个环境的函数，这样我们就能够向黑盒子环境发送最大化奖励的行动了。 在上图的反馈回路中，每一个代表时间步骤的下标 t 和 t+1 都指的是一个不同的状态：在 t 时刻和 t+1 时刻的状态。与其他诸如监督学习和非监督学习形式不同—强化学习仅可以被认为是一系列先后发生的状态-行动（state-action）对。 强化学习通过行动产生的结果来判断行动。它是面向目标的，它的目标是习得能够让智能体达到目标的一些行动序列。这里有一些例子： 在电子游戏中，这个目标是以最高的分数完成游戏，所以游戏过程中每一次得到的额外分数都会影响智能体随后的行动；也就是说，智能体可能学会：为了最大化它的得分，他应该射击战舰，触碰硬币或者躲避流星。 在现实世界中，一个机器人的目标可能是从 A 点移动到 B 点，机器人从 A 点向 B 点移动的每一英寸都算作得分。 可以通过对输入的解释将强化学习与监督学习和非监督学习区分开来。我们可以通过描述它们学习的「东西」来说明它们的不同之处。 无监督学习：那东西就是这个样子的。（无监督学习算法学到了没有名字的事物之间的相似性，通过进一步的扩展，它们可以通过识别不寻常或者不相似的实例来发现相反或者执行异常检测） 监督学习：那个东西是一块「双层吉士汉堡」。（标签，联系名字和面孔……）这些监督学习算法学到了数据实体实例和它们的标签之间的关联；也就是说，监督学习算法需要有一个有标签的数据集。那些标签被用来「监督」和矫正算法，因为算法在预测标签的时候可能会做出错误的猜测。 强化学习：吃了这个东西，因为它味道蛮不错，而且可以让你活得更久。（基于短期和回报和长期回报的奖励，就相当于你摄入的卡路里或者你生存的时间一样。）强化学习可以被看做是在一个具有稀疏反馈的环境中的监督学习。 强化学习的域选择 可以将一个自动强化学习的智能体想象为一个盲人，这个盲人智能依靠耳朵和手中的白手杖来尝试在这个世界中导航。智能体有一些允许它们感知所处环境的小窗，但是那些小窗甚至是最不适合它们感知周遭环境的最不适合的方式。 事实上，决定你的智能体的输入和反馈类型是一个需要解决的复杂问题。这就是所谓的域选择问题。学习玩电子游戏的算法可以忽略这个问题，因为它们的环境是人为设定的，而且是受到严格限制的。因此，电子游戏提供了无菌的实验室环境，可以在里面测试强化学习的想法。域选择需要人为决定，通常是基于需要解决的问题的知识或理论来进行的；例如，在无人车的算法中输入域的选择可能包括雷达传感器、相机以及 GPS 数据的信息。 状态-动作对（state-action pair）& 复杂的奖励概率分布 强化学习算法的目标是习得针对任意给定状态的最佳行动，这意味着行动必须被排序，并逐个赋值。由于那些行动都是依赖于状态的，所以我们实际上测量的是状态-行动对（state-action pairs）的价值；也就是说，您在某个状态下采取的行动，也就是你在某地方所做的某件事情。这里有几个例子，可以描述一下一个行动的价值和意义取决于智能体在采取这个行动时所面对的状态。 如果这里的行动指的是和某人结婚，那么您在 18 岁的时候和一位 35 岁的结婚可能会与您在 90 岁的时候与一位 35 岁的结婚大有不同，这两个结果可能会有着不同的动机，而且会进一步导致不同的结果。 如果这里的行动时大喊一声「Fire」，那么在一个人群密集的影院和在一众持枪者旁边大喊这句话则有不同的意义。如果不了解具体的语境，我们就不能预测行动会导致的结果。 我们用上述的 Q 函数将状态-行动对映射到我们希望它能够产生的价值上。Q 函数将智能体的状态和行动作为输入，将它们映射到可能的奖励上。 强化学习是通过一系列状态-行动对来运行智能体的过程，观察状态-行动对所导致的结果，调整 Q 函数的预测，直到它能够准确地预测出智能体应该采取的最佳行动。这种预测被称作策略。 强化学习是一种尝试，它对于大量的状态-行动对以及与之关联的奖励的复杂概率分布进行建模。这是强化学习与马尔科夫决策过程（https://deeplearning4j.org/markovchainmontecarlo）配合使用的一个原因，马尔科夫决策过程是一个从复杂的分布中进行采样，以推断它的属性的一种方法。这和启发 Stan Ulam 来发明蒙特卡罗方法的问题是很相似的；即在纸牌游戏中通过给定的手牌尝试推断获胜的机会。 任何统计方法，其本质上都是无知的。有些现象（例如生物学、政治学或者与棋类游戏有关的现象）的巨大复杂性使得从最初原则去推断是不可能的。唯一的方法就是通过统计去研究它们，从表面去衡量事件，并尝试建立它们之间的关联，即便我们不懂得它们相关联的机制。就像深度神经网络一样，强化学习就是这样的方法，依靠采样来从数据中抽取信息。 强化学习是迭代的。在大多数有趣的应用中，它起始的时候都并不明白当前的状态-行动对会产生怎样的奖励。强化学习算法通过在一次又一次的状态中运行以学到这些关联，就像运动员或者音乐家在一次又一次的状态迭代中提升他们的水平一样。 机器学习与时间之间的关系 也许你会认为强化学习算法与实践的关系与人类有所不同。我们可以在相同的状态下采取不同的行动运行算法，直至我们可以可以推断哪个行动是状态对应的最佳行动。事实上，我们给算法设定了它们自己的土拨鼠日（http://www.imdb.com/title/tt0107048/0），它们从一个蠢蛋开始，然后慢慢获得智慧。 由于人类从来不会经历电影之外的那种土拨鼠日，所以强化学习有可能比人类学到更多、更好。你可能会说，与人类相比，这些强化学习算法的真正优势并不在于它们的固有本质，而在于它们能够并行地存在于很多芯片上的能力，然后夜以继日不知疲倦地进行训练，因此能够学到更多。一个在围棋游戏上训练的算法，例如 AlphaGo，它能够玩的游戏比任何人类有望在 100 个有生之年玩得还要多。 深度神经网络和深度强化学习 神经网络适合用在什么地方呢？神经网络是能够学会映射状态-行动对和奖励的智能体。就像所有的神经网络一样，它们使用参数来逼近与输入输出相关的函数，它们的学习通过沿着错误降低的方向迭代地调整参数或者权重构成。 在强化学习中，卷积网络可以被用来识别智能体的状态；例如，马里奥所在的屏幕，或者无人机前面的地形。也就是说，它们起到了典型的图像识别的作用。 但是卷积网络在强化学习中能够得到比在监督学习中更多的解释。在监督学习中，网络给一副图片赋予一个标签；也就是说，它将名称映射到像素上。 事实上，卷积网络会根据概率对最适合图片的标签进行排序。给定一张驴子的图片时，卷积网络可能会以 80% 的可能性将其判断为驴子，以 50% 的概率将其判断为马，以 30% 的概率将其判断为狗。 在强化学习中，给定代表一个状态的图片，卷积网络可以给出一个在这个状态下可以采取的行动的排序；例如，它可能预测运行向右跑的动作会得 5 分，跳跃的动作会得 7 分，向左跑会得 0 分。 给期望的奖励赋予价值之后，Q 函数就会简单地选择具有最高的 Q 价值的状态-行动对。 在强化学习的起始阶段，神经网络的参数可能会被随机初始化。利用从环境中获得的反馈，神经网络可以使用期望奖励和实际奖励之间的差距来调整网络参数，以提升状态-行动对的解释性。 这种反馈回路与监督学习中的误差反向传播类似。然而，监督学习开始的时候就已经含有神经网络尝试预测的真实标签。它的目标就是去创建能够映射不同的图片与对应的名字的模型。 强化学习依靠环境来为算法提供与每个新行动对应的标量数字。环境返回的奖励可以使变化的、延迟的或者受已知变量影响的，这会给反馈回路引入噪声。 这会得到关于 Q 函数的更完整的表达，它不仅仅考虑由一个行动产生的即时奖励，而且还能够将奖励顺序地延迟到几个时间步长的深度。 就像人类一样，Q 函数也是递归的。就像调用湿体函数 human() 一样，human() 函数自身又包含另一个 human() 函数，我们是里面的所有结果，给一个给定的状态-行动对调用 Q 函数，需要我们调用一个嵌套的 Q 函数来预测下一个状态的价值，它反过来又要依赖之后的状态的 Q 函数，以此类推。 代码 RL4J 的例子在这里可以获得（https://github.com/deeplearning4j/dl4j-examples/tree/master/rl4j-examples）。  class A3CALE public static HistoryProcessor Configuration ALE_HP new HistoryProcessor Configuration (                      4 ,       //History length                      84 ,      //resize width                      110 ,     //resize height                      84 ,      //crop width                      84 ,      //crop height                      0 ,       //cropping x offset                      0 ,       //cropping y offset                      4         //skip mod  (one frame is picked every x             ) ;     public static A3CDiscrete.A3CConfiguration ALE_A3C =             new A3CDiscrete.A3CConfiguration (                      123 ,            //Random seed                      10000 ,          //Max step By epoch                      8000000 ,        //Max step                      8 ,              //Number of threads                      32 ,             //t_max                      500 ,            //num step noop warmup                      0.1 ,            //reward scaling                      0.99 ,           //gamma                      10.0             //td-error clipping             ) ;     public static final ActorCriticFactoryCompGraphStdConv.Configuration ALE_NET_A3C =             new ActorCriticFactoryCompGraphStdConv.Configuration (                      0.00025 , //learning rate                      0.000 ,   //l2 regularization                     null, null, false             ) ;     public static void main (String[] args)  throws IOException {         //record the training data in rl4j-data in a new folder         DataManager manager = new DataManager (true) ;         //setup the emulation environment through ALE, you will need a ROM file         ALEMDP mdp = null;         try {             mdp = new ALEMDP ( ""pong.bin"" ) ;         } catch  (UnsatisfiedLinkError e)  {             System.out.println ( ""To run this example, uncomment the \""ale-platform\"" dependency in the pom.xml file."" ) ;         }         //setup the training         A3CDiscreteConv<ALEMDP.GameScreen> a3c = new A3CDiscreteConv (mdp, ALE_NET_A3C, ALE_HP, ALE_A3C, manager) ;         //start the training         a3c.train () ;         //save the model at the end         a3c.getPolicy () .save ( ""ale-a3c.model"" ) ;         //close the ALE env         mdp.close () ;     } } "
212,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738252&idx=1&sn=cb9db5c81098576d71100dbcf54e288f&chksm=871aca32b06d43249588b5ec2182efd1314c352d57973c55ef05adbe4fee07cbc1e3452dfac2&scene=27,从数学到实现，全面回顾高斯过程中的函数最优化,"高斯过程可以被认为是一种机器学习算法，它利用点与点之间同质性的度量作为核函数，以从输入的训练数据预测未知点的值。本文从理论推导和实现详细地介绍了高斯过程，并在后面提供了用它来近似求未知函数最优解的方法。 我们回顾了高斯过程（GP）拟合数据所需的数学和代码，最后得出一个常用应用的 demo——通过高斯过程搜索法快速实现函数最小化。下面的动图演示了这种方法的动态过程，其中红色的点是从红色曲线采样的样本。使用这些样本，我们试图利用 GP 尽快找到曲线的最小值。 附录包含（i）高斯回归后验推导； (ii)SKLearn 的 GP 实现；(iii) GP 分类器的快速回顾。 高斯过程（GP）是处理以下一般问题的一个工具：函数 f(x) 在 n 个点采样，并得到一组有噪声 [1] 的函数度量值 {f(xi)=y_i ± σ_i,i=1,…,n}。那么若给定这些可用的样本，且 f hat 为一些候选函数，我们是否就能估计出 f =f hat 的概率? 为了逐步明确上述问题，我们首先应用贝叶斯法则， 上式左边的数值是所求概率的简写，即给定样本函数值 {y} 的条件下 f=f hat 的概率。在上式的右边，分子中的第一项需要我们对测量过程中的误差来源做一些假设，分子中的第二项是先验概率，在这里我们必须采用最合理的假设。例如，我们将在下面看到先验概率有效地决定了 f 函数在给定平滑度的概率。 在 GP 方法中，右式中两个分子都服从多元正态/高斯分布。模型可以选择高斯的具体参数来达到良好的拟合度，但特征的正态族假设对于解决数学问题是必不可少的。采用这种方法，我们可以通过分析写出后验概率，然后用在一些应用和计算中。例如，我们使用这种方法来获得文中最开始处图片上的曲线，即通过拟合 GP 后验概率中随机抽样而得到曲线估计，在两个收缩点处被固定为相等的测量值。后验样本对于可视化和求蒙特卡洛的平均值都很有用。 在本文中，我们做的工作有： （i）回顾计算上述后验概率所需的数学运算； （ii）讨论数值评估，并使用 GP 来拟合一些实例数据； （iii）回顾拟合的 GP 如何快速最小化成本函数，例如机器学习中的交叉验证分。 附录包括高斯过程回归推导，SKLearn 的 GP 实现和 GP 分类器的快速回顾。 我们的 GitHub 提供了简单的高斯过程示例：https://github.com/EFavDB/gaussian_processes 注意：为了理解这篇文章中的数学细节，应该熟悉多元正态分布。但如果主要对应用感兴趣，可以忽略这些细节。 为了计算 (1) 式左边的值，我们要先计算右边的值。因为分母不依赖 f hat，我们只需要考虑分子中的项。这意味着分母必须是所有候选函数共有的归一化因子。在本节中，我们先将分子两项的估计公式写出来，然后考虑后验概率。 我们要做的第一个假设是，假如实际函数是 f hat，那么我们的测量值 y 关于 f hat 是独立的并且服从高斯分布。这个假设意味着方程 (1) 右边的第一项是： 上式中的 y_i 是我们样本点的实际测量值，σ_i 方是它们的方差不确定度。 第二个假设是，假设先验概率 p（f hat）的公式。我们将注意力集中在一组数据点点 {x_i : i=1,…,N} 上，其中前 n 个点是已被抽样的点，剩下的（N-n）个点是其他位置的测试点，即我们用来估计 f 函数联合统计数据的点。接下来，简单地假设这些点服从 f 的多元正态分布，由协方差矩阵Σ来控制，由此得到 这里，我们简写 f_i≡f（x_i）。请注意，我们已经默认上面正态分布的均值是零。这是为了简便起见：如果非零均值合适，那么可以与均值相加分析，或者从原始 f 中减去非零均值使新的分布均值为零。 Σ的特殊形式是在 GP 建模时最需要建模者观察力和独创性的地方。对研究主题非常熟悉的研究者可以构建非常好且非常复杂的先验概率，而这种先验通常是各项求和的形式，其中每一项都在所讨论问题的数据基础上加入了一些物理相关的贡献。在这篇文章中，我们假设一个简单的公式， 注意，这个假设下，如果 x_i 和 x_j 很接近，那么指数近似等于 1。这确保了附近的点高度相关，从而使所有高概率函数变得平滑。当两测试点远离时，式 (4) 中的衰减速率由长度参数 l 控制。如果 l 很大（小），曲线将在一个很长（短）的距离上平滑。我们将在下一节中说明这些问题，并在下下节中解释如何从已有的样本数据中推断合适的长度参数。 现在，如果我们把式 (2) 和式 (3) 代入式 (1)，将得到后验概率 p(f1|{y}) 的表达式。这是一个指数函数，它的独立变量是 f_i 函数中的二次项。也可以说，与前验概率一样，后验概率服从多变量正态分布。通过简单计算，就可以得到这个分布均值和协方差的明确表达式：使用块（符号），其中 0 对应于采样点，1 对应于测试点，测试点的边缘分布是 y 是测量值的向量，长度为 n, 方程 (5) 是高斯过程回归的一个主要结果——有了这个结果，我们就可以评估后验概率了。注意，在采样值 y 中所有点的均值是线性的，并且在测量值附近每个点处的方差减小。如果你有兴趣仔细推导这个结果，可以参考我们的附录，在那里有两个推导。但是，在接下来的正文中，我们仅简单地探讨这个公式的应用。 在本节中，我们将介绍式 (5) 的两个典型应用：（i）在测试点 x 处评估后验分布的均值和标准差，（ii）从后验概率中直接采样函数 f_hat。前者可以获得 f 函数在所有位置的置信区间，而后者可以用来实现可视化和从后验概率中获得一般的蒙特卡洛平均值。这两个概念都在这篇文章的标题图片中进行了说明：图片中，我们将 GP 拟合一个已有两个测量点的一维函数。蓝色阴影区域表示每个位置函数值的一个σ置信区间，彩色曲线是后验样本。 我们的 SimpleGP fitter 类的代码可以在 GitHub 上找到。我们将在下文中解释他是如何工作的，但如果对细节感兴趣应该仔细查阅代码。 下面的代码对我们的 SimpleGP 类进行了初始化，定义了一些样本位置、样本值和不确定性，然后评估了一组测试点后验概率的均值和标准差。简而言之，这个过程如下：通过拟合评估了出现在式（5）中的逆矩阵 ，并保留结果供以后使用，这可以避免在每个测试点中重新评估这个逆矩阵。接下来，通过调用区间，针对每个测试点再次评估式 (5)。 # Initialize fitter -- set covariance parameters WIDTH_SCALE = 1.0 LENGTH_SCALE = 1.0 model = SimpleGP(WIDTH_SCALE, LENGTH_SCALE, noise=0)   # Insert observed sample data here, fit  sample_x = [-0.5, 2.5] sample_y = [.5, 0] sample_s = [0.01, 0.25] model.fit(sample_x, sample_y, sample_s)   # Get the mean and std at each point in x_test test_x = np.arange(-5, 5, .05) means, stds = model.interval(test_x) 在以上代码块中，WIDTH_SCALE 和 LENGTH_SCALE 用来指定协方差矩阵式（4）。前者对应于等式中的σ，后者对应于 l。增加 WIDTH_SCALE 对应于未知函数大小不确定度的增加，增加 LENGTH_SCALE 对应于增加我们期望的函数平滑程度。下图说明这些问题：这里，通过设置 WIDTH_SCALE = LENGTH_SCALE = 1 获得蓝色区间，通过设置 WIDTH_SCALE = 0.5 和 LENGTH_SCALE = 2 获得橙色区间。结果是橙色相对蓝色后验估计更加紧密平滑。在这两幅图中，实曲线表示后验分布均值，竖线表示一个σ置信区间。 为了从后验概率中采样实际函数，我们将再次简单地评估式 (5) 中的均值和协方差矩阵，这次是对我们所求采样函数的多个测试点进行。一旦我们有了这些测试点后验概率的均值和协方差矩阵，我们可以使用多元正态采样的外部库从 (5) 中抽取样本——为此，我们使用了 python 中的 numpy。下面代码的最后一步执行这些步骤。 # Insert observed sample data here.  sample_x = [-1.5, -0.5, 0.7, 1.4, 2.5, 3.0] sample_y = [1, 2, 2, .5, 0, 0.5] sample_s = [0.01, 0.25, 0.5, 0.01, 0.3, 0.01]   # Initialize fitter -- set covariance parameters WIDTH_SCALE = 1.0 LENGTH_SCALE = 1.0 model = SimpleGP(WIDTH_SCALE, LENGTH_SCALE, noise=0) model.fit(sample_x, sample_y, sample_s)   # Get the mean and std at each point in test_x test_x = np.arange(-5, 5, .05) means, stds = model.interval(test_x)   # Sample here SAMPLES = 10 samples = model.sample(test_x, SAMPLES) 注意在第 2-4 行中，我们添加了一些附加的函数样本点（为了好玩）。最后的区间和后验样本如下图所示。注意到在采样点附近，后验结果非常理想。然而，在图的左侧，一旦我们移动的距离≥1（即协方差矩阵 (4) 中的长度参数），后验就趋近于先验。 之前，我们证明了我们的协方差的长度参数显着地影响后验概率的区间形状以及其中的样本。适当设置这些参数是使用 GP 的一个普遍难点。在这里，我们描述两种方法，可以巧妙地设置超参数，并给出一些采样数据。 交叉验证是设置超参数的标准方法。这需要将可用的样本数据分为训练集和验证集。训练集通过一系列超参数进行 GP 拟合，然后在已有的验证集上评估模型的准确性。然后，通过选择不同的超参数重复这个过程，选择可以使验证集表现最优的一组。 通常情况下，人们倾向于将 GP 应用于评估样本有较高成本的情况。这意味着人们通常在只有少数样本可用的情况下使用 GP。这种情况下，随着训练点数量的增加，最优超参数可以快速变化，意味着通过交叉验证得到的最优选择可能远不如训练一个完整样本集得到的最优集合 [3]。 设置超参数的另一种常见方法是使边缘似然最大化。这就是说，我们试图最大化已关察样本的可能性，因而在可用超参数的基础上进行优化。具体来说，就是通过对未知的 f hat 进行积分来评估边缘似然 [4]。 我们可以像附录中评估后验分布那样直接进行积分。但更快的方法是注意到 f 积分后，y 值服从如下的正态分布 其中σ^2 * I_00 在式（6）中定义，由此得出， 上面两项是矛盾的：第二项通过找出使指数最大的协方差矩阵来减小，最大化指数使数据过度拟合。然而，第一项与第二项相反，第一项是高斯积分的归一化因子，它随着衰减长度变短和对角线偏差降低而变大，相当于抑制复杂度的正则项。 实际中，为了使式 (10) 最大，通常利用梯度的解析表达式和梯度下降法，这是 SKLearn 采取的方法。模型的一个优点是能够优化 GP 的超参数。然而，式（10）不一定是凸的，通常存在多个局部最小值。为了获得一个好的最小值，可以尝试在一些优秀的初始点上进行初始化。或者可以在随机点重复初始化梯度下降，最后选择最优解。 现在我们将介绍 GP 的一个常用的应用：快速地搜索函数最小值。在这个问题中，我们可以迭代获得函数的噪声样本，从而尽快识别函数的全局最小值。梯度下降可以应用于这种情况，但是如果函数不具备凸性，通常需要重复采样。为了减少所需的步骤/样本的数量，可以尝试应用更一般的探索式策略，即平衡「优化当前已知最小值的目标」与「寻找可能更小的新局部最小值的目标」。GP 后验为开发这样的策略的提供了一个天然的起点。 GP 搜索法的想法是在 GP 后验的基础上获得一个得分函数。这个得分函数用来对搜索给定点的信息进行编码，它可以对探索（explore）和利用（exploit）形成一种权衡。一旦每个点都进行评分，那么具有最大（或最小，最合适的）分数的点将会被采样。然后迭代重复该过程直到找到一个符合要求的解为止。我们将在下面讨论四种可能的选择，并给出一个例子。 GLCB 在每点的评分方式为 这里，μ和σ是函数在 x 处的均值和标准差的 GP 后验估计值，κ是控制参数。请注意，第一项μ（x）鼓励利用最可靠的局部最小值，并在它的周围执行搜索。类似地，第二项κσ鼓励在当前 GP 最不确定真实函数值的点上进行探索。 如果目前为止所看到的最小值是 y，则可以利用该点处的真实函数值小于 y 的概率来给每个点评分。也就是说，我们可以写为 上式常见的变形叫做预期改进，定义为 这个得分函数倾向于鼓励更多地去探索而不是改善概率，因为它更重视不确定性。 要得到的最终得分函数是问题中最小值的概率。获得这个分数的一个方法是进行多次后验采样。对于每个样本，首先标记它的全局最小值，然后采取多数投票方法来决定接下来的样本。 本文最开始处的动图展示了一个实际的 GP 搜索，使用 skopt[5] 在 python 中执行。左边的红色曲线是正在寻找全局最小值的（隐藏）曲线 f。红点是目前已经获得的样本，绿色阴影曲线是每个点的 GP 后验置信区间，该置信区间会随着更多样本的获得而逐渐改进。右边是通过在 GP 后验基础上分析得到的每点的预期改进（EI）得分函数——该例中用于指导搜索的得分函数。该过程用五个随机样本进行初始化，然后进行引导搜索。请注意，随着过程的演变，前几个样本集中利用已知的局部最小值。但经过几次迭代后，继续对这些位置采样的收益减小，探索中间点的需求占了上风——中间点是发现的实际全局最小值的点。 在这篇文章中，我们概括了大部分 GP 的数学运算：得到后验概率所需的数学，如何进行后验采样，最后讨论了如何实际应用后验概率。 总的来说，GP 代表了一个可以拟合任何函数的强大工具。实际中，使用这个工具的挑战主要在于合适超参数的选择，寻找合适的参数经常被困在局部最小，使拟合失效。不过，如果选择得当，GP 的应用可以提供一些有价值的性能提升。 附录中讨论了关于 GP 的其他话题。如果对更多的细节感兴趣，我们可以推荐 Rasmussen 和 Williams 的免费在线文本 [6]。 本附录中，我们提出后验推导（5）的两种方法。 先平方，结合式（2）和式（3），简单地计算得出 这里，(1/σ^2) * I 在式（6）中被定义，但在样本集外的所有行中都为零。为了得到式（5），我们必须统一为正文中逆矩阵的分块结构。 首先，我们可以得出 这里我们使用了分块表示法。为了计算上述的逆矩阵，我们将利用分块矩阵求逆公式， 矩阵（A2）中块 C = 0、D = 1，这大大简化了上述过程。代入后得到 利用这个结果和（A1），我们可以得到测试集的平均值 其中第二行的分子分母同时乘了 (1/σ^2) * I_00 的逆。类似地，测试集的协方差由（A3）的右下块给出。得到， 由（A4）和（A5）得出式（5）。 在第二种方法中，我们考虑一组测试点 f_1 和一组观测样本 f_0 的联合分布，我们再次假设密度函数的均值为零。那么两者的联合概率密度是 现在，我们利用结果 式（5）的得出需要利用上述两个表达式。主要的困难在于平方，类似于之前的推导，这可以通过分块矩阵求逆公式来完成。 SKLearn 提供了包含 GaussianProcessRegressor 的类。使得可以在任何维度上进行拟合和采样——即它比我们的最小类更一般，因为它可以在多维上拟合特征向量。另外，SKLearn 类的拟合方法尝试为一组给定的数据找到一组最佳的超参数。如上所述，这是通过边缘似然的最大化来完成的。这里，我们提供一些关于这个类的基本注释和可以用来定义（3）中协方差矩阵Σ的内核函数，以及一段说明调用的简单代码。 径向基函数（RBF）：这是默认值，相当于式（4）。RBF 由一个尺度参数 l 表征，多维情况下，可以是一个允许各向异性相关长度的向量。 White kernel：The White Kernel 用于噪声估计——文档建议用于估计全局噪声水平，但不是逐点。 Matern：这是一个广义的指数衰减，其中指数是分离距离的幂律。特殊限制包括 RBF 和绝对距离指数衰减。 有理二次方程：（1+（d / l）2）α。 Exp-Sine-Squared：它允许模拟周期性函数。类似于 RBF，但其距离是实际距离的正弦。存在周期性参数和「方差」——高斯抑制（Gaussian suppression）的尺度。 点积核函数：格式为 1 +xi⋅xj。它不是稳定的，就是说如果加入一个常量的平移，结果就会改变。如果把 N（0,1）的先验值放在系数上，将得到线性回归分析的结果。 核函数作为对象：可以支持核函数之间的二进制操作以创建更复杂的核函数，例如加法、乘法和指数（后者只是将初始核函数提升为幂）。你可以通过一些辅助函数来访问核函数中的所有参数，例如 kernel.get_params().kernel.hyperparameters 是所有超参数的列表。 n_restarts_optimizer：重新拟合的次数，用于探索多个局部最小值，默认值是零。 alpha：这个可选参数允许每个测量都传递不确定性。 normalize_y：用来表示我们正在寻找的 y 的平均值不一定是零。 下面的代码进行了一次简单拟合，结果是本文最开始展示的图片。 from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C from sklearn.gaussian_process import GaussianProcessRegressor import numpy as np   # Build a model kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (0.5, 2)) gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)   # Some data xobs = np.array([[1], [1.5], [-3]]) yobs = np.array([3, 0, 1])   # Fit the model to the data (optimize hyper parameters) gp.fit(xobs, yobs)   # Plot points and predictions x_set = np.arange(-6, 6, 0.1) x_set = np.array([[i] for i in x_set]) means, sigmas = gp.predict(x_set, return_std=True)   plt.figure(figsize=(8, 5)) plt.errorbar(x_set, means, yerr=sigmas, alpha=0.5) plt.plot(x_set, means, 'g', linewidth=4) colors = ['g', 'r', 'b', 'k'] for c in colors:  y_set = gp.sample_y(x_set, random_state=np.random.randint(1000))  plt.plot(x_set, y_set, c + '--', alpha=0.5) 关于 sklearn 实现的更多细节可以在这里找到：http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html 这里，我们说明通常 GP 如何被用来拟合二进制类型数据，响应变量 y 可以取 0 或 1 的数据。GP 分类器的数学运算不像 GP 回归那样清楚。因为 0/1 响应不是高斯分布的，意味着后验概率也不是。为了利用该程序，可以通过拉普拉斯（Laplace）近似正常地对后验概率近似。 首先写这样一个公式，表示在 x 处看到一个给定的 y 值的概率。具体如下， 这个公式是 logistic 回归的一个正常非线性泛化。此外，f 的先验概率再次得到等式（3）。使用此式和（A8），我们可以得到 f 的后验概率 利用这个公式，可以很容易地从近似后验中获得置信区间和样本，类似于回归。 "
213,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738167&idx=4&sn=844dae727da0f3f764d33f117da5bfbb&chksm=871ac989b06d409f907c89955708ff4b7fcfdc628b6e00797da734db88df94616e814145cb70&scene=27,ICLR 2018 | 斯坦福大学论文通过对抗训练实现可保证的分布式鲁棒性,"选自ICLR 神经网络容易受到对抗样本的影响，研究者们提出了许多启发式的攻击和防御机制。本文主要从分布式鲁棒优化的角度出发，从而保证了对抗输入扰动下神经网络的性能。 试想经典的监督学习问题，我们最小化期望损失函数 EP0 [ℓ(θ;Z)](θ∈Θ)，其中 Z〜P0 是空间 Z 上的分布，ℓ 是损失函数。在许多系统中，鲁棒性对于变化的数据生成分布 P0 是可取的，不管它们来自协变量变化、潜在定义域变化 [2]，还是对抗攻击（adversarial attack）[22,28]。随着深度网络在现代以性能至上的系统中变得普遍（例如自动驾驶车的感知、肿瘤的自动检测），模型失败会日益导致危及生命的情况发生；在这些系统中，部署那些我们无法证实鲁棒性的模型是不负责任的。 然而，最近的研究表明，神经网络容易受到对抗样本的影响；看似不可察觉的数据扰动可能导致模型的错误，例如输出错误分类 [22,28,34,37]。随后，许多研究者提出了对抗攻击和防御机制 [44,38,39,40,48,13,31,23]。虽然这些工作为对抗训练提供了初步基础，但是不能保证所提出的白箱（white-box）攻击是否能找到最有对抗性的扰动，以及是否存在这些防御一定能够成功阻止的一类攻击。另一方面，使用 SMT 求解器对深度网络的验证提供了鲁棒性的正式保证 [26,27,24]，但通常是 NP-hard；即使在小型网络上，这种方法也需要高昂的计算费用。 我们从分布式鲁棒优化的角度出发，提供一种对抗性训练过程，并在计算和统计性能上提供可证明的保证。我们假设数据生成分布 P0 附近的分布类别为 P，并考虑问题最小化 sup P∈P EP [ℓ(θ;Z)]（θ∈Θ）。 P 的选择影响鲁棒性保证和可计算性；我们发现具有高效计算性松弛（relaxation）的鲁棒性集合 P，即使在损失 ℓ 是非凸的情况下也适用。我们提供了一个对抗训练过程，对于平滑的 ℓ，享有类似于非鲁棒方法的收敛性保证，即使对于最坏情况下的整体损失函数 supP∈P EP [ℓ(θ;Z)]，也可以证明性能。在 Tensorflow 的一个简单实现中，我们的方法实现经验风险最小化（ERM, empirical risk minimization）所需时间是随机梯度方法的 5-10 倍，与其它对抗训练过程的运行时间相匹敌 [22,28,31]。 我们表明，我们的方法通过学习防御训练集中的对抗性干扰来获得泛化能力，使我们训练出的模型能够防止对测试集的攻击。 我们简要概述我们的方法。令 c：Z×Z→R +∪{∞}，其中 c(z,z0) 是攻击扰乱 z0 到 z 的成本函数（我们通常使用 。我们考虑在 Wasserstein 距离 Wc(·,·) 下的分布 P0 的鲁棒性区域  （正式定义参见第 2 节）。对于深度网络和其它复杂模型， 这个问题（1）的表达式是在 ρ 取任意值时是难以解决的。因此，我们考虑该表达式在固定惩罚参数 γ≥0 时的拉格朗日松弛，即 （关于这些等式的严格陈述，请参阅命题 1）。在这里，我们已经用鲁棒性的替代函数 φγ(θ;Z) 代替了通常的损失函数 ℓ(θ;Z)。这个替代函数（2b）允许数据 z 的对抗扰动，由惩罚 γ 调整。我们通常用经验分布 Pbn 代替惩罚问题（2）中的分布 P0 来解决问题，因为 P0 是未知的（我们在下面把这称为惩罚问题）。 惩罚问题 （2）的关键特征是稳健水平的鲁棒性——特别是针对不可察觉的对抗扰动的防御——可以在基本上平滑损失函数 ℓ 没有计算/统计成本下实现。特别的是，对于足够大的惩罚 γ（在对偶、足够小的鲁棒 ρ 下），鲁棒替代函数（2b）z 7→ℓ(θ; z)−γc(z, z0) 是严格下凸（凹函数）的，因此优化更加容易（如果 ℓ(θ, z) 在 z 中是平滑）。因此，应用于问题（2）的随机梯度方法与非鲁棒方法（ERM）具有相似的收敛保证。在第 3 部分中，我们为任意 ρ 提供了鲁棒性保证；我们给出了在最坏情况下可以高效计算的基于数据的损失函数上限 supP：Wc(P,P0)≤ρ EP [ℓ(θ;Z)]。即，我们主要的对抗训练过程输出的最坏情况下的性能保证不比它差。当 ρ=ρbn 时，我们的约束是紧的，这对于经验目标而言是鲁棒的。这些结果表明，使用平滑激活函数的网络比使用 ReLU 的网络更具有优势。我们在第 4 部分通过实验验证了我们的结果，并且表明，即使对于非平滑的损失函数，我们也可以在各种对抗攻击情况下达到最先进网络的性能。 鲁棒优化和对抗训练对于某些不确定集合 U，标准的鲁棒优化方法可以将以形式为 supu∈U ℓ(θ; z+u) 的损失函数最小化 [3,42,51]。不幸的是，这种方法是棘手的，除了特殊结构的损失函数，如线性和简单的凸函数的组成 [3,51,52]。尽管如此，这种鲁棒方法构成了对抗训练近期取得进展的基础 [46,22,39,13,31]，它启发式地提供了在随机优化过程中扰动数据的方法。 其中一种启发式算法使用局部线性化损失函数（以「快速梯度符号法」[22] 提出，p=∞）： 一种对抗训练方式基于这些扰动损失函数进行训练 [22,28]，同时其它许多函数基于迭代的变量 [39,48,13,31]。Madry 等人 [31] 观察到这些过程试图优化目标  ，这是惩罚问题（2）的限制版本。这种鲁棒性的概念通常是难以处理的：内含的上确界在 u 中通常是非凹的，因此不清楚使用这些技术的拟合模型是否收敛，并且可能存在这些技术无法发现的最坏情况的扰动。事实上，当深度网络使用 ReLU 激活函数时，发现最坏情况扰动是 NP-hard 的，这意味着快速迭代启发式算法是困难的（参见附录 B 中的引理 2）。平滑可以在标准深度结构中用指数线性单位（ELU's）[16] 获得，这使我们能够以低计算成本找到拉格朗日最坏情况扰动。 分布鲁棒优化为了说明当前的工作，我们回顾了一些关于鲁棒性和学习的重要工作。在鲁棒目标中选择 P（1）会影响我们希望考虑的不确定性集合的丰富性以及最终优化问题的易处理性。之前的分布鲁棒性方法已经考虑了 P 的有限维参数，例如矩的约束集、支持度（support）或方向偏差 [14,17,21]，以及概率测量的非参数距离，如 f-散度 [4,5,32,29,18,35] 和 Wasserstein 距离 [8,19,45]。与 f-散度（例如χ2- 或 Kullback-Leibler 散度）相反（当分布 P0 的支持度是固定时有效），围绕 P0 的 Wasserstein 球包含一组分布 Q，它们具有不同支持度并且（在某种意义上）保证对未知数据的鲁棒性。 许多作者研究了易处理类型的不确定集合 P 和损失函数 ℓ。例如，Ben-Tal 等人 [4] 与 Namkoong 和 Duchi[36] 使用 f-散度球的凸优化方法。对于由 Wasserstein 球形成的最坏情况的 P 域，Esfahani 和 Kuhn[19]、Shafieezadeh-Abadeh 等人 [45] 与 Blanchet 等人 [8] 展示了如何将鞍点（saddle-point）问题（1）转换为正则化的 ERM 问题，但这只适用于有限类凸损失函数 ℓ 和成本函数 c。在这项工作中，我们处理更大的一类损失函数和成本函数，并为拉格朗日松弛鞍点问题提供直接解决方法（1）。 图 1. 合成数据的实验结果。训练数据用蓝色和红色表示。ERM、FGM 和 WRM 的分类边界分别以黄色、紫色和绿色表示。左边为边界与训练数据一起的图示，右边为分开的真实类边界的图示。 图 2. 用合成数据（a）和 MNIST（b）进行实验的鲁棒性保证（11）（蓝色）和样本外（out-of-sample/测试）最差情况下的性能（红色）之间的经验性比较。在（11）中省略了统计误差项 ǫn(t)。垂直虚线表示在训练集 ρbn（θWRM）上达到的鲁棒性水平。 图 3. 对 MNIST 数据集的 PGM 攻击。（a）和（b）分别显示了对于 PGM 攻击在欧氏距离和∞范数下的测试错误分类错误与对抗扰动水平 ǫadv。（a）中的垂直虚线表示用于训练 PGM、FGM 和 IFGM 模型的扰动水平以及估计的半径 pρbn（θWRM）。对于 MNIST，C2=9.21 和 C∞=1.00。 图 4. 损失函数表面的稳定性。在（a）中，我们显示了给定 γadv 的扰动分布 ρbtest 的平均距离，这是对于决策表面的输入的局部稳定性指标。（a）中的垂直虚线表示我们用于训练 WRM 的 γ。在（b）中，我们将最小的 WRM 扰动（最大 γadv）可视化，以使模型对数据点进行错误分类。更多的例子见附录 A.2。 表 1. 1000 次实验后的 Episode 长度（平均数 ± 标准差） 图 5. 训练中的 Episode 长度。纵坐标 Episode 长度的环境上限为 400 步长。 论文链接：https://arxiv.org/abs/1710.10571 摘要： 神经网络容易受到对抗样本的影响，研究者们提出了许多启发式的攻击和防御机制。我们主要从分布式鲁棒优化的角度出发，它保证了对抗输入扰动下的性能。通过对在 Wasserstein 球中的潜在数据分布采用拉格朗日惩罚形式的扰动，我们提供了一种用最坏情况下的训练数据扰动来增加模型参数更新的训练过程。对于平滑损失函数，相对于经验风险最小化，我们的过程证明地实现了稳健水平的鲁棒性，并伴有很小的计算/统计成本。此外，我们的统计保证使我们能够有效证明整体损失函数的鲁棒性。对于不可察觉的扰动，我们的方法达到或优于启发式方法的性能。 "
214,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738167&idx=3&sn=dc26c3d273833192550290327d2c6220&chksm=871ac989b06d409f89bd2a1ea99ed415a4118482805f05e692bef968a65301fc596957606fde&scene=27,深度 | 像玩乐高一样拆解Faster R-CNN：详解目标检测的实现过程,"选自tryolabs Matt Simon 本文详细解释了 Faster R-CNN 的网络架构和工作流，一步步带领读者理解目标检测的工作原理，作者本人也提供了 Luminoth 实现，供大家参考。 Luminoth 实现：https://github.com/tryolabs/luminoth/tree/master/luminoth/models/fasterrcnn 在阅读本文之前，若想了解 R-CNN 网络家族的发展，可以参看机器之心的文章： 深度 | 用于图像分割的卷积神经网络：从R-CNN到Mark R-CNN 去年，我们决定深入了解 Faster R-CNN，阅读原始论文以及其中引用到的其他论文，现在我们对其工作方式和实现方法有了清晰的理解。 我们最终在 Luminoth 中实现了 Faster R-CNN，Luminoth 是基于 TensorFlow 的计算机视觉工具包，易于训练和监控，支持多种不同的模型。到目前为止，Luminoth 已经吸引了很大的关注，我们在 ODSC Europe 和 ODSC West 的论坛中也介绍过这个项目。 （ODSC，Open Data Science Conference，专注于开源数据科学的会议）。 基于开发 Luminoth 的工作和过去的报告，我们认为把所有实现 Faster RCNN 的细节和相关链接整合到一篇博客中是一个不错的点子，这对未来其他对此领域感兴趣的人会很有意义。 Faster R-CNN 最早在 2015 年的 NIPS 发布。其在发布后经历了几次修改，这在之后博文中会有讨论。Faster-RCNN 是 RCNN 系列论文的第三次迭代，这一系列论文的一作和联合作者是 Ross Girshick。 这一切始于 2014 年的一篇论文「Rich feature hierarchies for accurate object detection and semantic segmentation」(R-CNN)，其使用了称为 Selective Search 的算法用来提取感兴趣候选区域，并用一个标准的卷积神经网络 (CNN) 去分类和调整这些区域。Fast R-CNN 从 R-CNN 演变优化而来，Fast R-CNN 发布于 2015 年上半年，其中一种称为感兴趣区域池化的技术，使得网络可以共享计算结果，从而让模型提速。这一系列算法最终被优化为 Faster R-CNN，这是第一个完全可微分的模型。 Faster R-CNN 的框架由几个模块部件组成，所以其框架有些复杂。我们将从高层次的概述开始，之后会介绍不同组成部分的具体细节。 从一张图片开始，我们将会得到： 一个边框列表 每个边框会被分配一个标签 每对标签和边框所对应的概率 完整的 Faster R-CNN 框架 输入的图片以长×宽×高的张量形式表征，之后会被馈送入预训练好的卷积神经网络，在中间层得到特征图。使用该特征图作为特征提取器并用于下一流程。 上述方法在迁移学习中经常使用，尤其在为小数据集训练分类器时，其通常取用了在另一个较大数据集训练好的权重。我们在下一章节会深入了解这个部分。接着，我们会使用到区域建议网络（Region Proposal Network，RPN）。使用 CNN 计算得到的特征，去寻找到预设好数量的可能包含目标的区域 (边框)。 使用深度学习进行目标检测最大的困难可能是生成一个长度可变的边框列表。使用深度神经网络建模时，模型最后一部分通常是一个固定尺寸的张量输出（除了循环神经网络）。例如，在图片分类中，输出是 (N,) 形状的张量，N 是类别的数量，其中在第 i 个位置标量含有该图片属于类别 i 的概率。 RPN 中长度可变列表的问题可以使用锚点解决：使用固定尺寸的参考边框在原始图片上一致地定位。不是直接探测目标在哪，而是把问题分两个方面建模，对每个锚点，我们考虑： 这个锚点包含相关目标吗？ 如何调整锚点以更好的拟合到相关目标？ 可能会有点困扰，但是没关系，下面会深入了解。 在取得一系列的相关目标和其在原始图片上的位置后，目标探测问题就可以相对直观地解决了。使用 CNN 提取到的特征和相关目标的边框，我们在相关目标的特征图上使用感兴趣区域池化 (RoI Pooling)，并将与目标相关的特征信息存入一个新的张量。之后的流程与 R-CNN 模型一致，利用这些信息： 对边框内的内容分类（或者舍弃它，并用「背景」标记边框内容） 调整边框的坐标（使之更好地包含目标） 显然，这样做会遗失掉部分信息，但这正是 Faster-RCNN 如何进行目标探测的基本思想。下一步，我们会仔细讨论框架、损失函数以及训练过程中各个组件的具体细节。 之前提到过，Faster R-CNN 第一步要使用在图片分类任务 (例如，ImageNet) 上预训练好的卷积神经网络，使用该网络得到的中间层特征的输出。这对有深度学习背景的人来说很简单，但是理解如何使用和为什么这样做才是关键，同时，可视化中间层的特征输出也很重要。没有一致的意见表明哪个网络框架是最好的。原始的 Faster R-CNN 使用的是在 ImageNet 上预训练的 ZF 和 VGG，但之后出现了很多不同的网络，且不同网络的参数数量变化很大。例如，MobileNet，以速度优先的一个小型的高效框架，大约有 330 万个参数，而 ResNet-152(152 层)，曾经的 ImageNet 图片分类竞赛优胜者，大约有 6000 万个参数。最新的网络结构如 DenseNet，可以在提高准确度的同时缩减参数数量。 VGG 在讨论网络结构孰优孰劣之前，让我们先以 VGG-16 为例来尝试理解 Faster-RCNN 是如何工作的。 VGG 网络结构 VGG，其名字来自于在 ImageNet ILSVRC 2014 竞赛中使用此网络的小组组名，首次发布于论文」Very Deep Convolutional Networks for Large-Scale Image Recognition」, 作者是 Karen Simonyan 和 Andrew Zisserman。以今天的标准来看这个网络谈不上深度，但是在发布之际 VGG16 比当时常用的网络要多一倍的层数，其推动了「深度 → 更强大性能 → 更好结果」的浪潮（只要训练是可行的）。 当使用 VGG 进行分类任务时，其输入是 224×224×3 的张量 (表示一个 224×224 像素大小的 RGB 图片)。在分类任务中输入图片的尺寸是固定的，因为网络最后一部分的全连接层需要固定长度的输入。在接入全连接层前，通常要将最后一层卷积的输出展开成一维张量。 因为要使用卷积网络中间层的输出，所以输入图片的尺寸不再有限制。至少，在这个模块中不再是问题，因为只有卷积层参与计算。让我们深入了解一下底层的细节，看看具体要使用哪一层卷积网络的输出。Faster R-CNN 论文中没有具体指定使用哪一层；但是在官方的实现中可以观察到，作者使用的是 conv5/conv5_1 这一层 (caffe 代码)。 每一层卷积网络都在前一层的信息基础上提取更加抽象的特征。第一层通常学习到简单的边缘，第二层寻找目标边缘的模式，以激活后续卷积网络中更加复杂的形状。最终，我们得到一个在空间维度上比原始图片小很多，但表征更深的卷积特征图。特征图的长和宽会随着卷积层间的池化而缩小，深度会随着卷积层滤波器的数量而增加。 从图片到卷积特征图 卷积特征图将图片的所有信息编码到深度的维度上，同时保留着原始图片上目标物体的相对位置信息。例如，如果图片左上角有一个红色矩形，经过卷积层的激活，那么红色矩形的位置信息仍然保留在卷积特征图的左上角。 VGG vs ResNet 如今，ResNet 已经取代大多数 VGG 网络作为提取特征的基础框架。Faster-RCNN 的三位联合作者 (Kaiming He, Shaoqing Ren 和 Jian Sun) 也是论文「Deep Residual Learning for Image (https://arxiv.org/abs/1512.03385) Recognition」的作者，这篇论文最初介绍了 ResNets 这一框架。 ResNet 对比 VGG 的优势在于它是一个更深层、大型的网络，因此有更大的容量去学习所需要的信息。这些结论在图片分类任务中可行，在目标探测的问题中也应该同样有效。 ResNet 在使用残差连接和批归一化的方法后更加易于训练，这些方法在 VGG 发布的时候还没有出现。 现在，我们将使用处理过后的特征图并建议目标区域，也就是用于分类任务的感兴趣区域。之前提到过锚点是解决长度可变问题的一种方法，现在将详细介绍。 我们的目标是寻找图片中的边框。这些边框是不同尺寸、不同比例的矩形。设想我们在解决问题前已知图片中有两个目标。那么首先想到的应该是训练一个网络，这个网络可以返回 8 个值：包含（xmin, ymin, xmax, ymax）的两个元组，每个元组都用于定义一个目标的边框坐标。这个方法有着根本问题，例如，图片可能是不同尺寸和比例的，因此训练一个可以直接准确预测原始坐标的模型是很复杂的。另一个问题是无效预测：当预测（xmin,xmax）和（ymin,ymax）时，应该强制设定 xmin 要小于 xmax，ymin 要小于 ymax。 另一种更加简单的方法是去预测参考边框的偏移量。使用参考边框（xcenter, ycenter, width, height），学习预测偏移量（Δxcenter,Δycenter,Δwidth,Δheight），因此我们只得到一些小数值的预测结果并挪动参考变量就可以达到更好的拟合结果。 锚点是用固定的边框置于不同尺寸和比例的图片上，并且在之后目标位置的预测中用作参考边框。 我们在处理的卷积特征图的尺寸分别是 convwidth×convheight×convdepth，因此在卷积图的 convwidth×convheight 上每一个点都生成一组锚点。很重要的一点是即使我们是在特征图上生成的锚点，这些锚点最终是要映射回原始图片的尺寸。 因为我们只用到了卷积和池化层，所以特征图的最终维度与原始图片是呈比例的。数学上，如果图片的尺寸是 w×h，那么特征图最终会缩小到尺寸为 w/r 和 h/r，其中 r 是次级采样率。如果我们在特征图上每个空间位置上都定义一个锚点，那么最终图片的锚点会相隔 r 个像素，在 VGG 中，r=16。 原始图片的锚点中心 为了选择一组合适锚点，我们通常定义一组固定尺寸 (例如，64px、128px、256px，此处为边框大小) 和比例 (例如，0.5、1、1.5，此处为边框长宽比) 的边框，使用这些变量的所有可能组合得到候选边框 (这个例子中有 1 个锚点和 9 个边框)。 左侧：锚点、中心：特征图空间单一锚点在原图中的表达，右侧：所有锚点在原图中的表达 RPN 采用卷积特征图并在图像上生成建议。 像我们之前提到的那样，RPN 接受所有的参考框（锚点）并为目标输出一套好的建议。它通过为每个锚点提供两个不同的输出来完成。 第一个输出是锚点作为目标的概率。如果你愿意，可以叫做「目标性得分」。注意，RPN 不关心目标的类别，只在意它实际上是不是一个目标（而不是背景）。我们将用这个目标性得分来过滤掉不好的预测，为第二阶段做准备。第二个输出是边框回归，用于调整锚点以更好的拟合其预测的目标。 RPN 是用完全卷积的方式高效实现的，用基础网络返回的卷积特征图作为输入。首先，我们使用一个有 512 个通道和 3x3 卷积核大小的卷积层，然后我们有两个使用 1x1 卷积核的并行卷积层，其通道数量取决于每个点的锚点数量。 RPN 架构的卷积实现，其中 k 是锚点的数量。 对于分类层，我们对每个锚点输出两个预测值：它是背景（不是目标）的分数，和它是前景（实际的目标）的分数。 对于回归或边框调整层，我们输出四个预测值：Δxcenter、Δycenter、Δwidth、Δheight，我们将会把这些值用到锚点中来得到最终的建议。 使用最终的建议坐标和它们的目标性得分，然后可以得到一套很好的对于目标的建议。 训练、目标和损失函数 RPN 执行两种不同类型的预测：二进制分类和边框回归调整。为了训练，我们把所有的锚点分成两类。一类是「前景」，它与真实目标重叠并且其 IoU（Intersection of Union）值大于 0.5；另一类是「背景」，它不与任何真实目标重叠或与真实目标的 IoU 值 小于 0.1。 然后，我们对这些锚点随机采样，构成大小为 256 的 mini batch——维持前景锚点和背景锚点之间的平衡比例。 RPN 用所有以 mini batch 筛选出来的锚点和二进制交叉熵（binary cross entropy）来计算分类损失。然后它只用那些标记为前景的 mini batch 锚点来计算回归损失。为了计算回归的目标，我们使用前景锚点和最接近的真实目标，并计算将锚点转化为目标所需的正确 Δ。 论文中建议使用 Smooth L1 loss 来计算回归误差，而不是用简单的 L1 或 L2 loss。Smooth L1 基本上就是 L1，但是当 L1 的误差足够小，由确定的 σ 定义时，可以认为误差几乎是正确的且损失以更快的速率减小。 使用 dynamic batches 是具有挑战性的，这里的原因很多。即使我们试图维持前景锚点和背景锚点之间的平衡比例，但这并不总是可能的。根据图像上的真实目标以及锚点的大小和比例，可能会得到零前景锚点。在这种情况下，我们转而使用对于真实框具有最大 IoU 值的锚点。这远非理想情况，但是为了总是有前景样本和目标可以学习，这还是挺实用的。 非极大抑制（Non-maximum suppression）： 由于锚点经常重叠，因此建议最终也会在同一个目标上重叠。为了解决重复建议的问题，我们使用一个简单的算法，称为非极大抑制（NMS）。NMS 获取按照分数排序的建议列表并对已排序的列表进行迭代，丢弃那些 IoU 值大于某个预定义阈值的建议，并提出一个具有更高分数的建议。 虽然这看起来很简单，但对 IoU 的阈值设定一定要非常小心。太低，你可能会丢失对目标的建议；太高，你可能会得到对同一个目标的很多建议。常用值是 0.6。 建议选择： 应用 NMS 后，我们保留评分最高的 N 个建议。论文中使用 N=2000，但是将这个数字降低到 50 仍然可以得到相当好的结果。 独立应用程序 RPN 可以独立使用，而不需要第二阶段的模型。在只有一类对象的问题中，目标性概率可以用作最终的类别概率。这是因为在这种情况下，「前景」=「目标类别」以及「背景」=「不是目标类别」。 一些从独立使用 RPN 中受益的机器学习问题的例子包括流行的（但仍然是具有挑战性的）人脸检测和文本检测。 仅使用 RPN 的优点之一是训练和预测的速度都有所提高。由于 RPN 是一个非常简单的仅使用卷积层的网络，所以预测时间比使用分类基础网络更快。 在 RPN 步骤之后，我们有很多没有分配类别的目标建议。我们接下来要解决的问题就是如何将这些边框分类到我们想要的类别中。 最简单的方法是采用每个建议，裁剪出来，然后让它通过预训练的基础网络。然后，我们可以用提取的特征作为基础图像分类器的输入。这种方法的主要问题是运行所有 2000 个建议的计算效率和速度都是非常低的。 Faster R-CNN 试图通过复用现有的卷积特征图来解决或至少缓解这个问题。这是通过用兴趣区域池化为每个建议提取固定大小的特征图实现的。R-CNN 需要固定大小的特征图，以便将它们分类到固定数量的类别中。 兴趣区域池化 一种更简单的方法（被包括 Luminoth 版本的 Faster R-CNN 在内的目标检测实现方法所广泛使用），是用每个建议来裁剪卷积特征图，然后用插值（通常是双线性的）将每个裁剪调整为固定大小（14×14×convdepth）。裁剪之后，用 2x2 核大小的最大池化来获得每个建议最终的 7×7×convdepth 特征图。 选择这些确切形状的原因与下一模块（R-CNN）如何使用它有关，这些设定是根据第二阶段的用途得到的。 基于区域的卷积神经网络（R-CNN）是 Faster R-CNN 工作流的最后一步。从图像上获得卷积特征图之后，用它通过 RPN 来获得目标建议并最终为每个建议提取特征（通过 RoI Pooling），最后我们需要使用这些特征进行分类。R-CNN 试图模仿分类 CNNs 的最后阶段，在这个阶段用一个全连接层为每个可能的目标类输出一个分数。 R-CNN 有两个不同的目标： 1. 将建议分到一个类中，加上一个背景类（用于删除不好的建议）。 2. 根据预测的类别更好地调整建议的边框。 在最初的 Faster R-CNN 论文中，R-CNN 对每个建议采用特征图，将它平坦化并使用两个大小为 4096 的有 ReLU 激活函数的全连接层。 然后，它对每个不同的目标使用两种不同的全连接层： 一个有 N+1 个单元的全连接层，其中 N 是类的总数，另外一个是背景类。 一个有 4N 个单元的全连接层。我们希望有一个回归预测，因此对 N 个类别中的每一个可能的类别，我们都需要 Δcenterx、Δcentery、Δwidth、Δheight。 R-CNN 架构 R-CNN 的目标与 RPN 的目标的计算方法几乎相同，但是考虑的是不同的可能类别。我们采用建议和真实边框，并计算它们之间的 IoU。 那些有任何真实边框的建议，只要其 IoU 大于 0.5，都被分配给那个真实数据。那些 IoU 在 0.1 和 0.5 之间的被标记为背景。与我们在为 RPN 分配目标时相反的是，我们忽略了没有任何交集的建议。这是因为在这个阶段，我们假设已经有好的建议并且我们对解决更困难的情况更有兴趣。当然，这些所有的值都是可以为了更好的拟合你想找的目标类型而做调整的超参数。 边框回归的目标是计算建议和与其对应的真实框之间的偏移量，仅针对那些基于 IoU 阈值分配了类别的建议。 我们随机抽样了一个尺寸为 64 的 balanced mini batch，其中我们有高达 25% 的前景建议（有类别）和 75% 的背景。 按照我们对 RPN 损失所做的相同处理方式，现在的分类损失是一个多类别的交叉熵损失，使用所有选定的建议和用于与真实框匹配的 25% 建议的 Smooth L1 loss。由于 R-CNN 边框回归的全连接网络的输出对于每个类都有一个预测，所以当我们得到这种损失时必须小心。在计算损失时，我们只需要考虑正确的类。 与 RPN 相似，我们最终得到了很多已经分配了类别的目标，在返回它们之前需要进一步处理。 为了实施边框调整，我们必须考虑哪个类别具有对该建议的最高概率。我们也需要忽略具有最高概率的背景类的建议。 在得到最终目标和忽略被预测为背景的目标之后，我们应用基于类的 NMS。这通过按类进行分组完成，通过概率对其排序，然后将 NMS 应用于每个独立的组。 对于我们最后的目标列表，我们也可以设置一个概率阈值并且对每个类限制目标的数量。 在最初的论文中，Faster R-CNN 是用多步法训练的，独立地训练各部分并且在应用最终的全面训练方法之前合并训练的权重。之后，人们发现进行端到端的联合训练会带来更好的结果。 把完整的模型放在一起后，我们得到 4 个不同的损失，两个用于 RPN，另外两个用于 R-CNN。我们在 RPN 和 R-CNN 中有可训练的层，我们也有可以训练（微调）或不能训练的基础网络。 是否训练基础网络的决定取决于我们想要学习的目标特性和可用的计算能力。如果我们想检测与在原始数据集（用于训练基础网络）上的数据相似的目标，那么除了尝试压缩我们能获得的所有可能的性能外，其他做法都是没有必要的。另一方面，为了拟合完整的梯度，训练基础网络在时间和必要的硬件上都是昂贵的。 用加权和将四种不同的损失组合起来。这是因为相对于回归损失，我们可能希望给分类损失更大的权重，或者相比于 RPN 可能给 R-CNN 损失更大的权重。 除了常规的损失之外，我们也有正则化损失，为了简洁起见，我们可以跳过这部分，但是它们在 RPN 和 R-CNN 中都可以定义。我们用 L2 正则化一些层。根据正在使用哪个基础网络，以及如果它经过训练，也有可能进行正则化。 我们用随机梯度下降的动量算法训练，将动量值设置为 0.9。你可以轻松的用其他任何优化方法训练 Faster R-CNN，而不会遇到任何大问题。 学习率从 0.001 开始，然后在 50K 步后下降到 0.0001。这是通常最重要的超参数之一。当用 Luminoth 训练时，我们经常从默认值开始，并以此开始做调整。 在一些特定的 IoU 阈值下，使用标准平均精度均值（mAP）来完成评估（例如，mAP@0.5）。mAP 是源于信息检索的度量标准，并且常用于计算排序问题中的误差和评估目标检测问题。 我们不会深入讨论细节，因为这些类型的度量标准值得用一篇完整博客来总结，但重要的是，当你错过了你应该检测到的框，以及当你发现一些不存在的东西或多次检测到相同的东西时，mAP 会对此进行惩罚。 到目前为止，你应该清楚 Faster R-CNN 的工作方式、设计目的以及如何针对特定的情况进行调整。如果你想更深入的了解它的工作原理，你应该看看 Luminoth 的实现。 Faster R-CNN 是被证明可以用相同的原理解决复杂的计算机视觉问题的模型之一，在这个新的深度学习革命刚开始的时候，它就展现出如此惊人的结果。 目前正在建立的新模型不仅用于目标检测，还用于基于这种原始模型的语义分割、3D 目标检测等等。有的借用 RPN，有的借用 R-CNN，还有的建立在两者之上。因此，充分了解底层架构非常重要，从而可以解决更加广泛的和复杂的问题。 原文地址： https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/ "
215,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738397&idx=5&sn=11288d49870ff49a2312ad184d7b3a93&chksm=871acaa3b06d43b53c4d1a358b732f21f51e3fab55eed5bf49279d5538fa76b982e0532192d9&scene=27,学界 | 英特尔提出新型压缩技术DeepThin，适合移动端设备深度神经网络,arXiv 近日，英特尔的研究者提出新型深度神经网络压缩技术 DeepThin，适合移动端设备，性能优于其他压缩技术。 论文：DeepThin: A Self-Compressing Library for Deep Neural Networks 论文链接：https://arxiv.org/abs/1802.06944 摘要：随着业界在移动设备上部署越来越大、越来越复杂的神经网络，这些设备的内存和计算资源所面临的压力也越来越大。深度压缩（或深度神经网络权重矩阵压缩）技术为此类场景扩展了应用资源。现有的压缩方法无法高效压缩模型，压缩 1-2% 都比较困难。我们开发了一种新的压缩技术 DeepThin，该技术基于低秩分解领域的现有研究。我们将秩分解和向近似函数添加非线性的重塑过程结合起来，从而识别和打破由低秩近似造成的人工约束。我们将 DeepThin 部署为一个与 TensorFlow 相整合的 plug-gable 库，使用户无缝压缩不同粒度的模型。我们在两个顶尖的声学模型 TFKaldi 和 ZXC DeepSpeech 上评估 DeepThin，将其与之前的压缩方法（剪枝、HashNet 和秩分解）、实证有限研究方法和手动调整模型进行了对比。在 TFKaldi 上，DeepThin 网络的词错率（WER）在几乎所有测试压缩率情况下优于其他方法，平均优于秩分解 60%，优于剪枝 57%，优于等大小的手动调整网络 23%，优于计算成本高昂的 HashNet 6%。在 DeepSpeech 上，DeepThin 压缩网络比所有其他压缩方法的测试损失都低，优于秩分解 28%，优于剪枝 27%，优于手动调整同样大小网络 20%，优于 HashNet 12%。DeepThin 还使推断速度提升了 2 倍到 14 倍，提升幅度取决于压缩率和平台缓存大小。 近年来，机器学习算法越来越广泛地应用于消费者产品中，如个人助手中的语音识别。这些算法依赖于大型权重矩阵将网络中的不同节点之间的关系进行编码。完美情况下，这些算法将直接在客户端设备上运行，如 Amazon Echo [20] 和 Google Home [14]。 不过，此类设备通常是移动、低功耗设备，因此运行此类对内存、性能和能耗有很高要求的算法并不可行。 为了解决该问题，很多开发者致力于在高性能云服务器上执行推断模型，和在客户端和服务器之间传输模型输入和输出。但是，该解决方案带来了很多问题，如高昂的运算成本、移动网络上的大量数据迁移、用户隐私担忧，以及延迟增加。 近期研究调查了可将模型压缩至能够在客户端设备上直接高效执行的方法。此类压缩方法必须在基本不影响预测准确率、运行时性能或工程时间量的前提下降低模型空间需求。我们的研究基于低秩分解领域的现有研究，我们开发了一种新型压缩方法和 DeepThin 库，该方法： 使用辅助中间矩阵和高效的重新布局操作，解决了机器学习模型参数极低秩矩阵分解的基础对称性问题。 整合了流行和常用的 TensorFlow 框架，使用户无缝压缩不同粒度的模型。我们在该库中实现了之前的压缩技术，以对比不同压缩方法的准确率损失。 在同样大小的网络上，比其他压缩方法的准确率更高。 在我们基于 MKL [11] 的自定义 C++ TensorFlow 操作帮助下，实验证明其推断性能加速比未压缩的模型提高 2 倍到 14 倍。 标准的深度神经网络包含一系列有序连接的层级（layer），输入数据依次通过各层直到获得想要的输出。每个层计算先前层输出与当前层权重矩阵之间的矩阵乘积。在计算完矩阵乘积之后，将结果加上偏置项并馈送到非线性激活函数而得到输出。 对有时间依赖性的数据，可使用循环神经网络。尽管有很多不同类型的 RNN，但它们都涉及一种包含若干（通常 3 或 4）类似于上述计算步骤的模型。这样的模型要比寻常的 DNN 更具参数效率，但仍旧需要特别大的权重矩阵来获得优秀的准确率，因此它们可以从压缩方法中得到巨大收益。 对视觉数据而言，卷积神经网络从输入数据中学习到滤波器组（权重），来提取常见特征。每层的正向传播步骤都类似于上面描述的层运算。在此论文中，我们重点放在了 RNN 和前馈 DNN。然而，把 DeepThin 压缩方法应用到 CNN 也没有任何基础限制。 在该研究中，我们将该压缩方法单独应用到每层的权重矩阵。具备非线性激活函数 a、权重 W、偏置项 B 的单个层可定义为：Y = a(X.W + B) (1)，其中 W 和 B 是必须存储在该网络内的可学习参数。B 的大小与 W 相比可以忽略不计，因此这里我们只考虑 W 参数的压缩（不过我们在评估中也压缩偏置项）。 DeepThin 架构可压缩任意存储大型权重矩阵（如公式 1 中的 W）的模型，不过准确率会有些微损失。 图 2. 打破分解创建的人工结构约束。该变换具备两个可学习参数：低秩因子 X^f 和 W^f。 表 1. 与其他四种压缩方法相比，DeepThin 的平均提升。TFKaldi 数值是关于词错率下降，DeepSpeech 数值是关于测试误差减少。这里，我们看到不同的压缩方法在不同的数据集上各有偏重，而 DeepThin 在几乎所有测试情况中打败了其他压缩方法。 表 2. TFKaldi 和 DeepSpeech 上，DeepThin 模型在不同压缩大小和机器情况中的执行速度对比。不同机器之间的压缩大小略有不同，但是准确程度在 0.0001 以内。所有结果都以比未压缩的基线模型速度「X faster」的形式呈现。我们发现最大的提升来自缓存较小的平台，使用 DeepThin 可持续降低所有测试配置中的执行时间，使之更适合延迟和电量使用比较重要的环境。 
216,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738149&idx=3&sn=092ec4283138ee6dc9b1264d1355a24a&chksm=871ac99bb06d408dcbb5b5d6bff12e9c8e6a575e95172c93489198ebee9823b1b98136c21c01&scene=27,业界 | 百度提出机器阅读理解技术V-NET，登顶MS MARCO数据集榜单,  据机器之心了解，百度自然语言处理团队研发的 V-Net 模型以 46.15 的 Rouge-L 得分登上微软的 MS MARCO（Microsoft MAchine Reading COmprehension）机器阅读理解测试排行榜首。 MS MARCO 排行榜 MS MARCO 官方 twitter 发出的祝贺 说到机器阅读理解，业内人士必然会想到今年 1 月份「机器阅读理解打破人类记录」的新闻：微软和阿里巴巴的机器阅读理解系统在最新的 SQuAD 数据集测评结果中取得了并列第一的成绩。其中阿里使用的 SLQA 系统和微软的 R-Net 都是集成模型。 此次百度登顶的数据集是微软基于搜索引擎 BING 构建的大规模英文阅读理解数据集 MS MARCO，包含 10 万个问题和 20 万篇不重复的文档。MARCO 数据集中的问题全部来自于 BING 的搜索日志，根据用户在 BING 中输入的真实问题模拟搜索引擎中的真实应用场景，是该领域最有应用价值的数据集之一。 据介绍，相比 SQuAD，MARCO 的挑战难度更大，因为它需要测试者提交的模型具备理解复杂文档、回答复杂问题的能力。对于每一个问题，MARCO 提供多篇来自搜索结果的网页文档，系统需要通过阅读这些文档来回答用户提出的问题。但是，文档中是否含有答案，以及答案具体在哪一篇文档中，都需要系统自己来判断解决。更有趣的是，有一部分问题无法在文档中直接找到答案，需要阅读理解模型自己做出判断；MARCO 也不限制答案必须是文档中的片段，很多问题的答案必须经过多篇文档综合提炼得到。这对机器阅读理解提出了更高的要求，需要机器具备综合理解多文档信息、聚合生成问题答案的能力。 此次百度 NLP 在 MARCO 提交的是一种全新的模型 V-NET，它使用了一种新的多候选文档联合建模表示方法，通过注意力机制使不同文档产生的答案之间能够产生交换信息，互相印证，从而更好地预测答案。值得注意的是，此次百度只凭借单模型（single model）就拿到了第一名，并没有提交多模型集成（ensemble）结果。 关于百度提出的这一全新的机器阅读理解模型，目前还没有更多的技术细节放出。但据机器之心了解，相关论文会在之后公布。 对于此次登顶 MS MARCO 数据集榜单，百度自然语言处理首席科学家兼百度技术委员会主席吴华表示，「此次在 MARCO 的测试中取得第一，只是百度机器阅读理解技术经历的一次小考。我们希望能够与领域内的其他同行者一起，推进机器阅读理解技术和应用的研究，使 AI 能够理解人类的语言、用自然语言与人类交流，让 AI 更『懂』人类。」 
217,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738167&idx=1&sn=dee090882e74e44bb5dc62b2f3ef270c&chksm=871ac989b06d409f8ece6d8c9431cc777117373d1061c3576ffbba149c814b01b13b2523e9a1&scene=27,中科院、阿里云联合发布11量子比特云接入超导量子服务，郭光灿团队64比特量子仿真打破IBM Q记录,"2 月 22 日下午，在安徽合肥中科大举办的「中国科学院量子信息与量子科技创新研究院 2018 年度工作会议」上，中科院/中科大团队发布量子计算云平台最新成果。潘建伟院士正式发布中科院联合阿里云打造的 11 量子比特超导量子计算的云平台，这是继 IBM 后全球第二家向公众提供 10 量子比特以上超导量子计算云服务的系统。会上，郭光灿院士团队也介绍其本源量子计算云平台已成功上线 32 比特量子虚拟机，并已经实现了 64 量子比特的量子电路模拟打破 IBM Q 的 56 位仿真记录。 潘建伟团队介绍，目前其正式上线的云平台开放 11 比特的量子计算服务，其单比特门保真度达 99.7%、双比特门保真度达 94.9%。同时已经成功研制出 12 比特的超导量子计算处理器。 该量子计算云平台前端对用户提供云端的量子算法开发测试环境，后端连接经典计算仿真环境或者真实超导量子计算。阿里云方面将提供云计算资源支撑经典计算仿真环境。通过量子计算云平台，用户可以在云端的超导量子处理器上运行自定义的各种量子线路代码，下载相关运行结果。 该平台将吸引更多人在量子计算云平台上运行量子算法，完成初步试验，不仅能用于了解处理器的性能、技术瓶颈等重要特性，还将帮助到下一代处理器开发，为优化应用积累经验。而在云端提供量子计算的创新服务方式，也能从中知悉面临的技术挑战和机会。 现有国内外量子计算云平台主要提供两大类量子计算算力：通过经典计算资源构造的虚拟的量子计算模拟器，和真实的通用量子计算机。中科大阿里量子计算云平台同时提供两种算力服务，在云端可实现经典计算仿真环境与真实量子处理器的完整后端体验，用户可以登录阿里云官网使用。 阿里量子计算云平台用户界面： 图 1：在 11 比特量子处理器上实现三量子比特 GHZ（Greenberger-Horne-Zeilinger）态的电路图，最后一层是对电路结果进行基于|0>,|1>状态的测量。 图 2： 对图 1 电路执行 30000 次后测量到|000>,|001>,...,|111>状态的概率图。 除用户端服务外，阿里云还提供量子计算的经典仿真，并参与设计测量处理器性能的方案。经典仿真能准确算出量子处理器上的实验结果，与真实处理器结果进行比对，用以测量后者的性能、验证正确性等。 会上，同样来自中科大的郭光灿院士团队成员郭国平介绍，其团队开发的本源量子计算云平台也已成功上线 32 位量子虚拟机，用户可以在搭建了 32 位量子虚拟机的云平台上自行编写和运行量子程序，并可以观察已编辑程序的图像化显示效果，在远程量子服务器上完成编译、执行与测量后在本地获得运算结果。 郭光灿团队同时称已经实现了 64 量子比特的量子电路模拟（使用经典 GPU 虚拟的量子模拟器，属于经典计算机仿真环境），相关论文《64-Qubit Quantum Circuit Simulation》已发表在 arXiv 上 [1]。 本来，普遍观点是超过 50 量子比特的量子计算机无法被经典计算机模拟，甚至是最强大的超级计算机也做不到。但 IBM 打破了这个观念，通过特定的电路模拟方法，他们实现了 56 量子比特的模拟电路。郭光灿团队则利用巧妙的优化方法进一步将这一极限提高到了 64 比特，实际上在论文中他们甚至表示实现了 72 量子比特的模拟电路。 在这项工作中，他们通过分解两比特量子门和将原电路转换成并行子电路的方法，分别模拟了 42、56 和 64 量子比特的 randomized quantum circuit。其中 42 和 56 量子比特的电路只需要一台配置了显卡的个人计算机设备就可以完成模拟计算，而 64 量子比特的电路需要 64 个节点的计算机集群来模拟，但是其消耗的硬件资源相比之前的量子云平台已经大大降低。 图 3：转换两个两比特量子门（图中两个虚线框包含的 CZ 量子门）并生成替换的并行子电路。 比如，去年十月份 IBM 发布了 56 量子比特的量子计算云平台，在相同的超级计算机设备条件下，中科大本源的 56 量子比特模拟电路只需要 987 秒（16.4 分钟）就可以完成模拟计算，IBM 则需要两天的时间。 图 4：不同模拟量子比特数的不同层数（每个层表示当前时间步执行的量子门）的模拟计算时间。 国内目前有三大量子计算云平台，分别来自中科院与阿里云、本源量子计算公司和清华大学 NMRCloudQ 团队。三家于 2017 年 10 月 11 日同时发布上线。 清华大学采用的是核磁共振量子计算机，包含了四个量子比特，保真度超过 98%，这是国际上首个基于核磁共振的量子云计算平台。在中文版上线后的 24 小时内，网站主页独立 IP 访问次数近 2000 次，来自清华大学、中国科大、九院等不同单位近 150 个的注册用户，提交的在线计算任务 80 余次。 量子计算云服务促进量子计算商业化 现阶段各家量子计算机仍然主要为研究人员和科学家做研究和应用探索服务，量子计算云服务平台的面世也标志着量子计算已经走到了商用化的边缘。 IBM 曾于 2016 年 5 月向公众开放了业界首个量子计算云平台，用户可简单通过 API 和 SDK 接口访问 IBM 云端平台连接的 5 量子比特超导计算机以及基于经典计算机编程的量子计算机仿真器，进行量子算法或实验模拟。IBM 也对 Quantum Experience 进行了升级，新的模拟器可以对 20 个量子比特进行建模；并计划推出完整的 SDK，以便开发者能够在 Quantum Experience 之上构建简单的量子计算应用。 IBM 希望通过打造 IBM Q 系统提高其在量子计算应用领域的地位，计划与其他机构合作，共同开发对量子系统专门优化的配套程序。 谷歌也于 2017 年 7 月表示计划将 D-wave 量子退火机接入其云平台。此前其与美国量子计算初创新秀 Rigetti Computing 等机构合作推出 OpenFermion 开源量子计算软件平台，该软件包含一个量子算法库，适用于化学、材料方面的研究工作。OpenFermion 可兼容数种不同的量子计算机，其中包括谷歌、Rigetti 和 IBM 开发的机型，也意在建立以 OpenFermion 为标准的社区，吸引众多开发者使用量子计算机。谷歌还开设了一个「量子孵化数据中心」，用来寻找量子计算在现实生活中的新用例。 Rigetti 也于 2017 年发布「Forest」云计算平台，提供 36 个量子比特的模拟量子处理器，允许开发者通过代码访问 Rigetti 的虚拟量子计算机以及周边的量子计算设备，并表示通过建立量子计算云平台的方式为更多科研用户打造一个量子计算交流的社区比制造出一款成功的量子计算机产品更加重要。 未来展望 量子计算云平台是提供量子计算服务的云计算平台，其出现为广大研究人员和开发者提供了一个更好的量子程序设计和应用开发环境。 现有连接真实通用量子计算机的量子计算云平台通常愿意开放其量子计算中的量子电路层和量子算法层，甚至控制层，而连接经典计算机仿真环境开放的仅有量子算法层。 虽然对于开发真实量子计算机的研究人员来说，现有小数量量子比特的量子计算算力服务远远不够，但是用户可以通过该服务学习量子计算的基本算法思想，并可以在大规模物理硬件实现之前运行和试验量子算法。 同时对于制造量子计算机的科研人员，更希望量子云平台的服务开放到控制层，研究针对量子计算机物理层系统的脉冲控制及优化，来提升量子逻辑门的保真度。 潘建伟院士曾在去年 10 月的云栖大会上表示，量子计算云平台将在未来 5-10 年实现数百个量子比特的相干操纵，届时对特定问题的计算能力将达到目前全世界计算能力总和的 100 万倍。对揭示高温超导、惯性约束核聚变、高效氮固化机制等重大问题，以及指导相关产业技术开发将起到较大的作用，每年将获得数百亿美元的直接经济效益。 "
218,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738167&idx=2&sn=431874d519b45856f1584006859d982d&chksm=871ac989b06d409f7b7b006d95b1c93d2956caa415e92552e12d08bc10a6dc6dd4e84610d80d&scene=27,专访 | 百度8年，3次进阶，这是实践者王海峰的AI技术征途,过去一年，从技术向产业，有哪些值得记住的人和事？未来一年，AI 场景化落地还有哪些可能性？ 8 位 AI 行业局内人，向我们讲了讲他们的故事和看法。 早在 2010 年加入百度时，王海峰就开始了围绕 AI 技术体系的「实践」。 在那之前， 陆续拓展自然语言处理、机器翻译、机器学习、语音、图像、深度学习、个性化推荐等技术方向，尽管如他所说，当时「其实也不明确能用来做什么」。 不过，去解锁如此多元的技术领域，也不是王海峰最初的职能定位，实际上，他是以自然语言处理专家的身份被邀请加入百度。王海峰从 1993 年开始专注研究机器翻译与自然语言处理，成就斐然，在自然语言处理与计算语言学领域最顶级国际学术组织 ACL（Association for Computational Linguistics）50 多年历史上，他是唯一出任过主席（President）的华人。 即便是在技术研究上「战绩优异」，他没有停止边界的突破。2014 年，随着转战业务部门，三年多时间，王海峰负责了百度最核心的几个业务板块，比如搜索、手机百度、Feed 流……并在技术与业务互相协同的过程中，孵化促成了度秘等新业务。 那段时间，王海峰不仅迅速迎来个人在百度的两次晋升—— 2011 年升为基础技术首席科学家、2013 年进一步升至百度副总裁，也实实在在尝到了以技术突破带动业务产品的甜头。 「我们凭借持续的技术突破创新让百度搜索始终处于领先，我们凭 借技术的快速升级实现了 Feed 流量的高速增长，凭借技术的厚积薄发催生了度秘。业务需要解决的实际应用问题，以及业务中产生的大量真实数据，又促进了技术的继续创新突破。 」 这让 2017 年王海峰再一次完成转身显得顺理成章。无论哪一个方向的变化，几乎都暗合了他在百度 8 年时间里的升级过程，怎么看也都是他熟悉的「战场」： 2017 年 3 月，百度 AI 技术平台体系（AIG）成立，在这之前，百度并没有一个部门把所有 AI 相关技术都整合统一在一起。原本并不相互独立的百度研究院和八个应用技术部门，有了明确的分工，前者面向基础前瞻的技术研究，后者由应用目标来牵引。技术之外，百度在人工智能领域更大的变化是对应用、平台和生态的发力，促成商业化的落地。 很明显，应用导向成为百度在 AI 这件事上的重点，而王海峰经历了从零开始打下 AI 基础、深入核心业务线的过程后，也再次回归基础技术，组建 AI 技术平台体系，就任 AIG 总负责人并晋升为百度 Estaff 成员（百度最高决策层），成为百度 AI 战略的关键「局内人」。 以下是机器之能对王海峰的专访实录（机器之能做了不改变原意的编辑）：   「最重要的决定，第一步是加入了百度，第二步是除了百度让我做的自然语言处理，我还把 AI 一系列基础技术都建立起来了」 我们看到百度真正开始在 AI 方面体系化规模化的投入，应该是从 2010 年你加入之后。回看当时的情形，这 8 年时间主要经历了什么？ 我 2010 年 1 月份加入百度，到上个月 8 年整的时间，我走过了三个阶段，从奠基 AI 基础技术，到负责核心业务，再回来组建 AI 技术平台体系。这 8 年，对公司的贡献、我个人的收获，都是很大的。 第一阶段，那时候百度还没有这种横跨的独立的技术部门，只有各个产品部门里的技术团队。当然，最基础的数据中心这些肯定是横跨的。但 AI 类的技术还没有横跨的部门。我加入后开始着手组建这些部门。至少在中国的各个互联网公司中，这应该算是比较领先的一个布局。所以我在百度的前四年，就把这些都陆续建起来了，从 NLP 到语音、图像、机器学习、深度学习、个性化推荐，数据挖掘等。 这个阶段奠定的基础，使百度后来在人工智能领域既有先发优势，同时也有更完善的组织架构，有更深厚的积累。 但如果仅仅如此，我认为对一个互联网公司来说其实还是不完整的。互联网公司产品迭代特别快，数据特别大，而且都是来源于实际业务。所以，2014 年初，有一个契机，当时百度最核心的搜索业务特别需要，我就转岗去负责搜索，做了三年多。 这三年多时间，我最大的收获就是每天跟产品打交道，跟用户打交道，跟线上的各种需求打交道。同时，我一直在做技术，所以我也更清晰地知道，这些技术怎样才能更好地应用到产品里面去。 我这三年多的时间，第一年从当时份额最大的 PC 搜索开始；到下半年，开始有所扩展，接手了商业平台、糯米技术平台等；2015 年，我们将 PC 和移动搜索进行了整合，这在百度历史上也是非常重要的一步。整合之后，我们就可以开始做更多事情，比如度秘就是在 2015 年诞生的。2016 年我的职责范围中又增加了手机百度、Feed 流等重要业务，凭借技术的快速升级，半年的时间里 Feed 的流量就涨了几十倍。 去年 3 月份又回到基础技术的时候，我已经带过百度现在多数最重要的业务。业务中有什么实际应用问题急需解决，业务中产生的什么真实数据可供技术使用，新技术如何融合进原有的庞大系统，我都非常清楚。这些对技术的创新及应用都是非常重要的。 有从做技术到产品再回来负责技术这样一个过程，我会了解基础技术跟产品到底应该是什么样的分工协作关系，怎么样把基础技术做得既领先、前瞻，同时又能更好的支撑业务，不管是支撑现在的还是为未来做储备，都会做得更好。 这期间，你做过哪些现在看起来仍然关键的决定？ 如果说最重要的决定，第 1 个是加入百度；第 2 个是除了百度招我时希望我做的自然语言处理，我还把一系列人工智能技术方向都建立了起来；第 3 个是 2014 年初我去负责搜索；第 4 个当然是 2017 年 3 月组建 AIG。 事实上，刚加入百度时，公司并没有要求我把那些 AI 技术方向都建起来。最初进入百度的时候，我的定位就是自然语言处理专家，希望我把百度自然语言处理完善起来。因为搜索引擎对自然语言处理的依赖非常重，比如用户需求分析、网页分析等。但我并未局限于自然语言处理，在我加入百度的第一个季度，就先后为百度开创了机器翻译、语音识别等对当时的百度核心产品价值并不大而后来又证明很重要的技术方向，之后又陆续建立了推荐及个性化、数据挖掘、图像等方向。 2012 年初，又有了很关键的一步，我们正式立项开始做深度学习，这在中国公司里应该说是最领先的。当年，我们基于深度学习的语音、OCR 等系统先后都上线了。从大环境看，那时候我们真正要做深度学习是面临很大挑战的。 我记得 2011 年下半年，大家逐渐开始知道深度学习，当时，国内一些做机器学习的顶尖学者举行了一个小型研讨会，会上大家的感觉是，想要把这些技术应用落地还很困难。当时国外有人曾经尝试过将深度学习用于语音识别，训练一次要半年，这种状态是无法大规模产品化的。那时候我也负责做语音，我们和做系统的同事组成联合项目组，既攻关算法，又优化系统实现，半年多的时间就上线了基于深度学习的语音识别系统。 2014 年初，我开始负责搜索，这对我职业生涯是非常重要的一步。要知道，极少有人有机会在自己的职业生涯中，负责年收入数百亿的核心业务。那之后的三年，为公司核心业务做了大量贡献的同时，我自己也得到了非常多的锻炼和成长，也为去年重新负责 AI 基础技术，打造更好更有价值的技术平台奠定了基础。 在你刚加入百度还没有一个明确的职责方向时，还是既做了翻译又做语音这些暂时不太需要的事，是什么驱动你这么做？ 我个人的确有兴趣，比如我的前一份工作其实也是负责自然语言处理及语音技术相关工作。但是更重要的是基于两个方面的判断，一方面是产业发展趋势，另一方面是技术发展趋势。 以语音为例。 先说产业，虽然当时主流是 PC，但手机已经展现出迅速变强的趋势。所以在我看来，手机迟早会成为主流。而手机是为打电话而生的，人与手机交互最自然的方式就是对着手机说话，这为语音真正广泛应用打下了一个很好的基础，这是对产业的基本判断。 技术方面，过去很多年，语音技术在持续进步，每年错误率都在降低，有时候错误率相对降低 5% 左右，有时候好一点到 10%。积累到 2010 年前后，已经快要到突破的节点了。所谓节点，就是不断提升，过了某个临界点，相应的应用环境成熟，一定会爆发。基于这样一个判断，虽然当时我刚来不久，还是专门跟 Robin 做了一个汇报。他自己也很热爱和支持创新，很认同 AI 技术趋势，所以他很支持，然后就开始做。后来我们做很多事情都是这样的过程。 「要把学术思维和工程思维结合起来，先不管能不能说清楚，管用的就先把它做出来」 这些决策过程容易吗？百度已经是一个体系成熟的大公司。 在百度这个平台上比较容易，百度很看重技术，很鼓励创新，尤其是比较前瞻的技术创新。所以很多技术研究的开始都是如此，大概会有两个基本判断依据，一个技术发展趋势，一个产业趋势，这两个缺一不可。否则，技术做得再好，没有产业需求也没有用；反过来，产业需求有了，技术到不了，突破不了这个临界点，那也无法应用。在这样一个环境下，我们再做判断就不是太难。 一个技术的产业应用临界点，你是怎么判断的？ 首先，技术不是说要做到 100% 准确才能用，效果达到一定程度，就好用了。比如语音识别，人也做不到 100% 准确。现在的语音识别技术，很多场景下甚至已经比一个正常人听的准确率还要高，这显然已经可以有很多应用了。我们当时开始研发语音识别技术时候，研究了搜索、地图、输入法等目标应用，分析了语音识别准确率到什么程度可以满足这些应用的需要，结论是努努力是可能做得到的。 所以说，在合适的时机找合适的应用很重要。找应用的想法并不是我首创的，一直就有很多人有这种认识。我们前段时间引入了几位世界顶级的科学家，其中有一位也是研究自然语言的（Kenneth Ward Church）。他比我年纪还要大一些，1993 年我刚入行的时候，就看过他的文章，当时有一篇文章叫 Good Applications for Crummy Machine Translation，那时候机器翻译效果比现在差很多。但即使如此，也可以找到一些合适的应用场景，也能发挥很多作用。 内部在决定要进行或者是开展一项技术的研究，是怎么往前推的？ 有两方面，一些是从应用来的，也有一些暂时没有应用。我们看好某些前瞻方向，就会提前布局去做，尤其是越往底层，比如说研究院，不需要有一个直接的应用来牵引，但是我会有一个对未来的预测，认为未来这项技术会很有用，那我们就会提前布局。 你刚才也提到 2011 年下半年国内才开始有人讨论深度学习，具体是什么样的状况？ 那时候产业界基本上没人做，学术界基本上也是处于起步的状态，刚刚开始接触和测试深度学习这个概念。我们召开一些研讨会，大家一起讨论讨论到底是怎么回事。所以基本上也没有相应的人才。当时我们要做深度学习，也就是拿一些国外早期发表的几篇 Paper 看一看。Paper 上看不清楚的，就只能自己去摸索了。因为 CPU 训练太慢，我们联合做系统的同事一起搞 GPU。那个时候谁也不懂，就自己想办法搞清楚 GPU 编程到底怎么编，就是这样逐渐做起来的。 在外界这种状态下，百度内部对 AI 或者是对这些细分的技术方向有特别明确吗？ 那时候没有那么明确。当时我们建一些技术方向，大家还是从各自产品角度提出有哪些需求，看这些技术是否对产品有用。那时候没人提 AI 这个词，不像现在 AI 已经家喻户晓，我们最初在语音领域做深度学习，公司大多数人还不知道。 其实那时候我自己也在反复思考，深度学习为什么会起作用。我们搞研究特别希望把数学原理先搞清楚，推导出来。深度学习有一些东西，其实在当时看来还说不清楚。所以一定要把学术思维和工程思维结合起来，先不管能不能说清楚，管用的就先把它做出来。 当时有这么一个空间给你不停地去尝试？ 这个空间是有的。 2010 年到 2013 年，建立了这些不同的技术方向的部门。这些部门以及这些人，比较大的变化是怎么样的？ 部门更成熟了，积累更深厚了。如果说，早期是先把这些基本的技术都搭起来，基础的数据收集起来，能支持一些应用先做起来，现在这些基础已经很强大了。 比如 NLP，NLP 是我来百度之后建立的第一个部门，现在技术、数据及平台的积累都已经很强大了。NLPC 技术平台，每天公司内部的调用量就是千亿规模。在这种情况下，我们一方面持续加强技术平台，另一方面也会为一些重要应用进行定制化技术开发。此外，我们也更有基础去做些更前瞻的、未来的东西，例如通用对话系统、通用人工智能方面的探索。这些看上去离产品应用没那么近，但在百度我们一直有这个追求，再往前多看几步，持续探索通向未来之路。 团队方面，无论是人员能力还是数量，都已经比当年要强很多了。当年那些刚入门的小同学，现在都成顶梁柱了。 转岗去负责搜索相关业务，涉及到的多半是产品，对你来说是挑战吗？ 我是技术出身，其实一个纯做技术的人转到业务上，挑战还是很大的。但是我觉得我还是具备做业务所需的一些重要素质的，推动力和执行力都很强。这就是为什么我从纯做技术转到负责业务也能顶得住。 我接手搜索以后，和以前的确是不一样。我记得一开始，Robin 就问我，你怎么保证这个能做好？当时其实压力也挺大的，包括很多竞品追得很紧。我说人家要做，这个我是挡不住的。但是我会保证，我创新的速度更快。因为我技术积累多，我知道做什么会更快，能不断往前跑。这样的话，别人是追不上的，而且别人要一直追着我们，其实最后很容易乱掉他们自己的节奏。 做了一段时间以后，我更坚定了，我发现这样做是有效的，很快就见到效果了。2014 年初，百度搜索引擎虽然局部已经用了不少机器学习技术，但在主体上还是一个传统的、以规则为主的系统。我们迅速地持续不断地推进技术改造，直到现在每个模块都是 AI 技术在支撑。我也发现，用 AI 技术改造搜索的同时，也能衍生出一些其它东西来，这就是后来度秘诞生的原因。 在 2014 年底，我们搜索团队曾做过一次讨论，讨论搜索的未来到底是什么。当时我们就认为，搜索就是不断地满足用户需求，我认为一个比较终极、最自然的状态，其实就是一个像人一样的秘书。比如说你有一个秘书，什么问题，任何事情你跟他交待一句，甚至有一些你不用交待，他很了解你，你什么都不说，他可能就替你办好了。这就是为什么后来会有度秘，会有小度机器人。相当于做搜索这条主线的同时，衍生出了面向未来的旁线开始做。 「AI 会渗透到各行各业，会渗透到我们生活的方方面面，社会的每一个角落都会有」 在具体场景里，什么样的产品才是好的 AI 产品？它能更好地把技术和用户体验结合起来。 AI 产品已经真实存在了。搜索引擎就是一个典型的，而且是一个最大的 AI 产品，规模极其庞大，里面有大量的 AI 技术。我们做的几乎所有的 AI 技术都会用在搜索引擎里，所以搜索引擎和以前相比也变得越来越不一样。 度秘、无人车更典型了，它们和搜索引擎不一样。搜索引擎诞生的时候，人们并不把它看作是 AI 产品，后来我们把它逐渐演化成这样了。而度秘和无人车从诞生的那一天起，就被认为是 AI 产品。信息流背后也是 AI 技术，要对用户进行理解，对内容进行理解，将内容和用户做匹配进行个性化推荐，所有这些事情都是 AI。 除了百度一直在做的用户产品，我们现在也开始做 to B 业务，综合利用我们的 ABC 能力（AI、Big Data、Cloud）为客户赋能。例如，首钢应用集成了 PaddlePaddle 及计算机视觉能力的 ABC 一体机进行钢板质检。再如，几个学生基于 PaddlePaddle 平台开发了智能桃子分拣机。AI 会渗透到各行各业，会渗透到我们生活的方方面面，社会的每一个角落都会有。 百度发布小度机器人、DuerOS 平台商业化速度非常快，业务线的推动过程还挺快。 对，我认为这取决于两个因素，一是前期做了很多积累，很多东西虽然不止我们一家在做，但这些积累我们是独一无二的，尤其是基于搜索平台方面，还包括语音、自然语言处理、知识图谱等。另一方面，我认为去年我们制定的开放平台生态战略，是非常重要的。 不管是 DuerOS 还是 Apollo，基本上都是这样一个战略，我们提供最核心、最基础的东西，推动合作伙伴一起共建平台，让生态繁荣起来。跟平台伙伴的合作，我们比较灵活，有各种合作模式。比较独立的就是 DuerOS、Apollo 两个平台，而更庞大、完备的能力输出，是我们的 AI 开放平台。去年 7 月份开发者大会的时候，我们公布说百度当时有 60 项能力开放，这个数字现在变成 90 多了。 这背后的基本理念是什么？AI 是新的生产力，我把它比成第一次工业革命的蒸汽机，第二次工业革命的电，第三次工业革命的信息。不管源头是从哪个行业出现的，比如说第一次工业革命很大程度上是从纺织业，在 18 世纪是非常重要的产业，最后都会渗透到各行各业。后来电更不用说了，各行各业都开始用电。人工智能也是一样，人工智能之所以从互联网开始，首先是因为互联网行业应用场景特别多，有各种资源、人才、数据、计算能力，这些也是人工智能发展需要的基本要素。 但是它肯定不止于影响互联网，现在我们也看到了，各行各业都开始引入人工智能这些能力。这种情况下，任何一个公司都不可能把它完全包办了。从这个角度讲，我们也必须把这些能力开放出去，让很多合作伙伴一起，应用在他们的业务里面，一起让人工智能迅速在各行各业都开花结果。在这个过程中，我们支持大家做了这些事情，大家都是受益者，最终会共赢。 现在有多少开发者、合作方使用AI技术平台？ 去年 11 月份的百度世界大会，我们宣布是 37 万。这个数字增长非常快，我刚刚看到最新的数字差不多 50 万的样子，去年 11 月到现在两三个月的时间，又增长了十几万。 刚提出要做这个平台的时候，你们对这个平台的想法是怎么样的？后来确定下来之后有变化过吗？ 开始的时候只有大方向和一些基本做法，肯定要在过程中不断优化的。比如说开始时开放技术没那么多，我们也会讨论到底先开放哪些，后续的节奏怎样。开放出去以后，是不是就够了呢？也不见得。一方面，我们会继续改进已开放的技术，另一方面也会不断丰富各种支持，比如为了让用户更好地理解和运用这些能力，需要提供丰富的应用实例，也需要有相应教程和培训等，为此我们也组织了 AI 训练营等。这些都是在逐渐迭代完善的。 迭代。所以其实做开放平台这样的事情，与做一个用户产品相比，有很多不同的挑战。 对，更确切的说这是不同角度的挑战。产品就是不断满足每个用户的需求，平台是一个个合作伙伴和开发者，他们的要求肯定会有不一样的。 平台类的发展方案或路径，怎么样判断，或者说有什么标准去判断它是在朝着一个好的、对的结果走？ 产品就看是不是受用户欢迎，是不是用的人多，留存率是不是高，等等。平台也一样，比如开发者数量是不是越来越多，在上面开发的应用是不是越来越多，大家开发使用时的易用性怎么样，是不是有很多需求我们仍然满足不了？其实一样可以有很多指标可以去去衡量。 AI 技术开放平台、Apollo，还有 DuerOS 之后，接下来也会继续再逐渐开放其他的技术方向平台吗？下一个会是什么？ 还是沿着两条线来说，一个是技术上，开放数量从 60 涨到 90，今年还会继续增加，这是沿着技术线上。Apollo 已经不完全是一个独立的沿着通用 AI 技术线的开放平台，而是在汽车这个领域深挖，DuerOS 也类似。在垂直方向深耕，开拓一块大业务。这个方向有了 Apollo，DuerOS，是不是仍然有下一个？我们肯定会去持续探索。 刚刚提到的 Apollo、DuerOS 平台，这是百度把开放赋能确定为 AI 时代核心战略的一个结果，所以这个战略在落地方面有别于他人的基本方法论是怎么样的？现在所有人都在谈开放、搭平台。 对于平台来讲，每一个单项能力够强很重要，而综合能力够强、够完整则更重要。我们现在也看到，我们的很多合作伙伴，他们需要的 AI 能力，普遍都不是一个单点。 比如某公司说需要一个语音识别，另一家需要一个人脸识别，还有一家需要文字识别，实际上常常不是这样的。很多应用都是需要把很多技术放在一起用，都是综合应用。企业往往不是说需要什么技术，而是说要解决什么问题，需要一个完整的解决方案。这种完整的解决方案，里面包含方方面面的技术，不止是识别，可能还需要理解，这就需要自然语言理解，还需要知识。我们给运营商做自动客服，相当于客服机器人。这背后涉及到语音的识别、合成，涉及自然语言的理解，对话的能力，还包括知识。知识既有我们通用的知识图谱，又有具体行业应用自己的知识图谱，甚至是业务流程。这一整套东西都有。 另一方面，你还得有工程落地的支撑，需要有这样的能力。百度云是一个能落地的很重要的平台。当然我们也不止是云上，端上也有一些这样的能力。所以 AI 的公司有不同的类型，每种类型有它自己存在的价值。但是我对这个趋势的看法是，很多技术真正应用的时候，关心的都会是解决一个应用问题，它会是很多技术，很多能力的综合应用。 很多技术真正应用的时候，大家关心的还是如何解决一个问题，它会是很多技术、能力的综合应用。能举一个具体的案例吗？ 我们遇到任何一个任务，我们都需要先分析这个任务的本质是什么，包括分析用户的需求，这个任务相应的数据的特征和分布等。这样才能选择更好的方法去解决它。例如，现在语音是人工智能很重要的一个方向，但传统语音领域，比如你做语音识别、语音合成，会在云端训练一个大模型。 现在我们真正要解决 DuerOS 的各种应用，实际上很多问题已经不是传统意义上有一个更好的深度学习模型就行了。实际应用中会有很多问题，例如唤醒的识别率及误报问题，噪声问题，麦克风的数量及布置方式，腔体结构等。所以真实场景下要把一个产品做好，有很多方面的问题要解决。 组建完善 AIG 的同时，百度研究院招揽行业顶尖人才也有进展。你对 AI 技术平台体系发展的设想跟接下来一个明确的目标规划是怎么样的？ 这个体系，就是作为百度 AI 技术的平台，我们首先当然还是希望保持 AI 技术的领先。不管是现在业务需要的应用技术，还是对未来的布局，我们希望持续地往前走，持续地研发最领先的 AI 技术。 另一方面，我们要积极应用 AI 技术，不管是公司内还是公司外。公司内我们支持公司的各种重要业务，支持方式又分两种，一种是平台化的支持，一种是定制化的支持。 能力越来越强，平台化支持业务的比例也会提升。持续提升的话，单位资源支持的业务会更多，同时最重要的业务我们会以定制化的方式支持。对外的目标，就是我们的平台生态战略。同时，对于一些重要的合作伙伴，也和内部的机制类似，会有定制化的支持，双方一起联合开发。 整体上从技术到应用，就是这样一个逻辑。当然，要做好这些事，我们就要保持一个非常强大的，最好的团队，这也是刚才提到的我们在招揽人才。外界看到了我们发布的引进顶尖人才的信息，我们内部的人才也在不断成长。无论外部引进还是内部培养，都是在持续进行的。 你认为根据你想要做到的事情，需要有一些改变吗？比如说人的配备，或者是说架构上，还有一些部门之间的协调。 如果说改变，肯定持续会有。组建 AIG 到现在不到一年，其实已经有了一系列变化。这个变化随着技术的发展，业务需求的发展持续会有，但不会有大的波动。虽然有时候外面看到我们的某一个动作会觉得惊讶，但在我们看来，很多事都是水到渠成的，发展到那一步，就是要有那个变化了。 技术部门更强化技术的产出，一方面会有定制化，另一方面也会强化平台化的能力。相同的人力资源下，我们能支持的业务方向，会随着平台化能力增强而增加。因为很多业务不需要专门投入人力，直接调用平台上的技术就行了。 另一方面，研究院也在加强，上个月刚刚宣布新增加了两个实验室，而且大家可以看到，增加的思路是有变化的。之前 IDL、BDL，包括硅谷的 AI Lab，更多是以技术来命名的，就是深度学习、大数据、AI 技术。新增加的两个实验室应用色彩更重一些，一个是商业智能，一个是机器人与自动驾驶。这是因为在我们看来，偏前瞻基础的几个 Lab 做得不错，但我们同时也会强调应用导向。我们做商业智能，做机器人、自动驾驶，其实都是沿着这个思路在把研究院建得更完整。   「管理上有所为，有所不为，而且在不同阶段要有不同的所为和不为」 你其实一直都是负责多条业务线的，不管是最早的第一个阶段四年的时间，不同的技术方向，后面又是不同的产品业务方向，现在同时还要推进一些前沿技术的研究。你怎么平衡多线不同的任务？ 因为不是我一个人在战斗，我有团队。我每往前走一步的时候，后面那些事情都会有人给顶起来了。否则的话，我的精力已经被耗干，我不可能再开辟一个新的方向。 这个过程当中，团队的培养是非常重要的。我希望每一个位置上不断有人，有非常强的人能独挡一面，能顶住，而且他还有接班人，还有后备，每一步都是这样的。因此，虽然我负责的事越来越多，但每条线上都有一群很强的人，那些人都能独挡一面。这时候我才有可能不断的去开拓新的方向或者新的业务。 面对新方向或新业务，对我来说，除了选方向，定目标这些事情，非常重要的也要把团队搭建好。团队无非或者是自己培养，或者是引进。我们的团队，的确引进了很多非常顶尖的人才，但如果你要算比例，百度自己培养起来的肯定更多。 你的管理风格是怎么样的？ 首先我认为，团队也好，业务也好，或者甚至一个公司，如果出问题，很多时候其实都是在管理上出了差错。管理上有所为，有所不为，而且在不同阶段要有不同的所为和所不为。 初期的时候，我把目标定得比较细，会把团队各方面都理好。后期团队已经运行得比较成熟了，我开始往后退。退的意思是我抓最重要的，最宏观的事，甚至我选好几个最重要的人，盯一盯，其他事我就可以适当放手了。 这样我才可以做更多事，还有很重要的一点，只有充分授权，真正做事的人才有发挥空间，才有更大的积极性，才会快速成长。我一直强调，做管理除了把事要做好，特别要强调人的培养。如果下面的人没有成长，我自己也不可能再往前走，也没法做更多的事。 百度还是非常典型的技术工作者的方式。 我们总说「简单可依赖」，外边可能有些不完全理解，但是内部我们的确就是这样一种模式在工作，也就是讨论任何事都力求简单直接，有问题直截了当，就事论事，谈论问题，解决问题，基本上这个效率是很高的。 「AI 其实是一个进化的过程，不是一个简单的训练过程」 之前你在一个采访里面也说到，在每天数十亿次的用户请求千锤百炼之下，百度的人工智能技术已经是真正实用的人工智能的技术了。什么才叫真正实用的 AI 技术？ 如果要用一句话说，真正实用就是真正能解决某一个应用问题。为什么我说我们在千锤百炼下就更能解决这些问题？因为那些技术每天就是在解决各种应用的问题。像搜索里面，用户虽然最终看到的就是一个非常简洁清晰的结果，但背后要解决的问题非常多。 每一项技术，都是在具体应用问题上打磨，然后在真实的数据上训练，随着我们的应用越来越多，数据越来越多，训练的模型也会越来越好。同样一个算法，可能我们相配套训练出来的模型就是更好的。同时我们也积累了很多真实的解决问题的经验，比如说同样一个问题，我到底选择什么样的算法？怎样调节这个模型，甚至怎么样筛选数据？如果仅仅懂算法，是不足以把这些事情做好的。而现在我们因为有开放平台，同时也服务了很多合作伙伴的需求，我们也看到更多问题，这本身就是一个迭代进化的过程。 所以我认为 AI 其实是一个进化的过程，不是一个简单的训练过程。传统上我们搞人工智能是，有算法，有数据，有训练，训练出一个模型，就完成了，这是静态的。但在我看来，更多的是我们的 AI 系统在实际场景中去用，用的过程中，跟应用、跟用户去互动，数据也会不断增加，在这个过程中整个系统的模型、算法会变得更好。这是技术在场景里进化，不断改进的过程。这个过程不是闭环，而是进化，我不认为 AI 是在这样一个环里，它会不断地往前走。 你最近比较关注的一些技术点都是哪些？有哪些产业应用方向是你比较看好的？ AI 涉及到的技术非常多，就像我们刚才说到的算法、算力、数据；有感知层的，语音、图像、AR 等等；认知层的，自然语言处理、知识图谱、用户画像等，每个层面都会面临一些不同的问题。 比如说数据层面，你可以说它是技术的一部分，也可以说它是独立的，但跟技术是高度相关的。数据首先就是怎么样能收集更多的数据，更有效地去处理，更有效地去挖掘其中的价值。理论上来讲，在其它资源都是无限的情况下，数据越多越好，但现在其它资源毕竟不是无限的。 同时，计算这部分也有很大变化，从 CPU 到 GPU 再到 FPGA，以及现在各类定制化芯片。在我看来，会有越来越多种计算平台，异构计算会很重要，同时边缘计算也会很重要。人工智能很多东西最早都是跑在大服务器上，但是现在越来越多需要能在端上计算，所以算力方面，我们既要重视云的能力，也要重视端的能力，这是非常重要的一点。 感知层方面，过去这些年，深度学习这波浪潮首先是在感知层，像语音、图像这些领域获得成功的，同时这些领域也是受深度学习带来的红利影响进步最大的，基本上已经达到实用的水平。接下来要做什么？实际上更多会跟硬件去结合，跟传感器、摄像头、麦克风等结合，总之软硬结合会变得越来越重要。 认知层上，深度学习带来的提升还远不如感知层。这其中一个最本质的原因，我认为是，用深度学习技术解决语音、图像问题的时候，更多还是把它当成模式识别的问题来解决；但认知层的问题，远远大于模式识别。比如你讲一段语言，语言背后有丰富的涵义，语言是人对整个真实世界的认知、描述和表达，跟物理世界、人的精神世界、背景知识都是相关的。这些东西如果不具备，仅仅把它当成一个字符串用模式识别的方法来处理，可以解决一些问题，但是没有办法解决本质问题。所以到目前为止的深度学习技术，或者说人工智能技术还不足以完全刻画所有这些背后的东西。 人类的语言是可以去描述整个真实世界的。反过来，理解人类的语言，理解整个真实世界，要难得多。所以这也是我们为什么重视知识，花很大力气做知识图谱的原因。 百度的知识图谱到目前为止是我所知范围内最大的，有几千亿事实在里面，这是别人谁也没有的量级。我们在这方面的优势来源于百度传统的一些优势，比如说搜索，搜索把互联网所有的网页抓取回来，进行分析处理，并建索引。搜索引擎基本上构成了整个互联网的镜像。 而由于现在大量的需求和数据在持续不断的线上化，互联网又构成对真实世界的镜像，所以透过搜索引擎的这些数据，我们可以从中挖掘知识，对整个真实世界更好地建模，更好地进行知识的积累。我认为，有这样一个过程，认知层的技术才能逐渐往前走，朝着通用人工智能发展，这是很重要的一条路径。当然通用人工智能还是比较远的一件事。 现在国内的学者或学生发论文的数量越来越庞大了，有人也会觉得国内的比如说自然语言处理，跟美国的距离缩小了。你怎么看？ 首先差距是不是在缩小？肯定是在缩小，甚至有一些方面，我们已经反超。尤其是在应用技术领域，有些我们做得比别人更早。以机器翻译为例，现在主流都转到神经网络的翻译上了。我记得 2016 年 9 月底，Google 发布了他们的神经网络翻译。但是你上网了解就会发现，百度的神经网络翻译 2015 年上半年就已经上线了，比 Google 要早一年多。有些领域，我们的确是世界上最早开始的，对科技圈来讲，早一年多是一个很大的领先优势了。 但我们的基础研究还是相对落后的。比如现在这些主流的方法，深度神经网络、强化学习，还都是来自于欧美。我觉得现在越来越多的中国人进入这一行，越来越多的人在做研究，我相信慢慢一定会产生好的基础研究。但是到目前为止，我们的确还是落后的。 我们在看技术领先性时，会把它区别为一些问题是属于基础性的问题，一些问题可能是在应用的过程当中发现的。 是，解决应用的问题我们能力已经很强了。 但大家会有一个观念是，后面这种研究能力是弱于基础研究的。 不能说谁强谁弱，而是有一个先后关系的问题。就是说你没有基础研究，后面也不可能用它们来解决应用问题。但是只有基础研究也是不够的，类比一下，比如说爱因斯坦的确很伟大，相对论也好，质能方程也好，的确是非常重要的基础。但是有了这些，离真正造出原子弹，建成核电站还差得很远很远，中间还有非常长的路，这些应用之路其实也是非常重要的。而且一旦这些路走通了，走到应用了，大家看到应用的价值了，会有更多的人进入，会有更多的资源进入，会反过来再促进基础技术的发展。所以应用非常重要，在现实场景下能发现问题并解决掉，很多时候也是在找新方法。大家都已经看到它的重要性，大量的人，而且是聪明人都进来了，中国在这方面迟早会有突破。 8位AI行业局内人讲述对过去、对未来的看法 点击下方，阅读更多        |  
219,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738149&idx=2&sn=9cd30ca31a145737993ce173e5877878&chksm=871ac99bb06d408d58ba57f9a7dfbaf6fa6bdb62640e48dfb06ab4a16b0ad9bd158e65276951&scene=27,教程 | GitHub项目：利用不完整的数据样本补全不完整的图像,GitHub 大多数图像补完和生成模型需要完全被观察的样本来训练。但是，在 AmbientGAN 里，阐述了获取高分辨率样本对于一些应用来说是可能非常昂贵的或者是不切实际的。该 GitHub 项目结合了两篇论文 AmbientGAN 和 GLCIC 的思想，实现了用不完整图像样本训练的补全不完整图像的网络。 GitHub 地址：https://github.com/shinseung428/ImageCompletion_IncompleteData 这个 GitHub 项目里的模型融合了以下工作中的思想： AmbientGAN（来自有损测量生成的模型） Globally and Locally Consistent Image Completion（GLCIC，全局和局部一致的缺漏图像完整性技术） AmbientGAN 使我们可以直接用有噪声或者不完整的样本来训练生成模型。模型的生成器可以成功地用测量函数从正确的分布里预测样本。 另一方面，GLCIC 一文里的模型用了完全被观察的样本来训练网络。补完网络首先用了 mse 损失来预训练权重，之后用了一个判别器损失以完整地训练模型。 把 AmbientGAN 和 GLCIC 文章里的思想结合以后，这个项目中的模型学习仅用不完整的数据来填充不完整的区域（例如：被随机用 28*28 大小补丁覆盖的地方）。你可以看到，一些生成区域是不完美的。这个模型生成的图像仍然有缺陷，一些区域的颜色也不连贯。 网络 方法 现在假定我们已经有不完整图片的样本，且我们知道添加到样本的噪声类型。我们可以使用加上掩码的图像作为补完网络（completion network）的输入而不是使用随机隐向量。假设补完网络成功生成了掩盖的区域，那么生成的图像块将使用掩码信息 X_g 与输入图像相结合。 随后，补完的图像 X_g 将会被馈送到度量函数中。如 AmbientGAN 原论文所述，度量函数将尝试模拟生成对象 X_g 上的随机度量。这很有可能实现，因为我们知道添加到完整图像的噪声类型。此外，我们也可以创建一个度量函数以模拟添加到图像中的噪声。 在将度量函数和不完整样本 Y_r 馈送到判别器以从假的度量方式中鉴别出真正的度量方法，最后可生成图像 Y_g。在补完网络和判别网络进行对抗性的训练后，补完网络学习到如何生成图像块以补完不完整样本的确实部分。 数据集 该项目最终使用 CelebA 数据集。为了创建不完整图像的数据集，我们将原版的 CelebA 图像居中剪裁为 32*32 的图像块，并调整尺寸为 64*64，然后将 28*28 的空白图像块（图像块的值填充为 1）随机添加到图像中。 项目效果 论文：AmbientGAN: Generative models from lossy measurements 论文地址：https://openreview.net/forum?id=Hy7fDog0b 摘要：生成模型提供了一种对复杂分布的结构进行建模的方法，并已被证明在很多现实任务中很有用。然而，目前的训练生成模型的技术需要访问充分观测的样本。在很多设置中，获得充分观测的样本是代价昂贵的甚至不可能完成的，但是从部分的、带噪声的观测中获取样本则是经济的。我们考虑了仅给定从感兴趣分布得到的样本的有损耗测量来学习隐式生成模型的任务。我们证明真实的潜在分布可以被实证地恢复，即使每个样本都存在信息损耗时也可行（根据特定的测量模型）。基于此，我们提出了一种训练生成对抗网络的新方法，称为 AmbientGAN。在三个基准数据集上，我们用不同的测量模型证明了该模型无论定性上还是定量上都能获得大幅提升。用我们的方法训练的生成模型可以获得 2-4 倍于基线模型的性能。 论文：Globally and Locally Consistent Image Completion 论文地址：http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/ 摘要：我们在本文中展示了一种新的图像补完的方法，可以在图像中获得局域的和全局的一致性。利用一个全卷积神经网络，我们可以通过填充任意形状的缺失区域对任意分辨率的图像补完。为了训练该图像补完网络获得一致性，我们使用了全局的和局域的语境判别器（分辨真实图像和被补完的图像）。全局判别器观察完整的图像以评估它是否整体上一致，而局域判别器仅观察中心位于补完区域的小块区域以确保生成补丁的局域一致性。然后图像补完网络被训练以欺骗这两个语境判别器网络，即它需要生成和真实图像无法区分的无论是整体还是细节上都保持一致性的图像。我们证明了该方法可以用于补完大量类型的。此外，和基于补丁的方法（例如 PatchMatch）不同的是，我们的方法可以生成在图像其它地方不存在的碎片，这使得我们可以自然地用熟悉的和特定的结构（例如人脸）补完物体的图像。 
220,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738093&idx=2&sn=a4e88b2f2719f29d555f5fa1b4c01270&chksm=871ac9d3b06d40c5e68d6f2839acb8d38bd6c22d41edf23a7a1e51367832fecd2a4acef9d901&scene=27,入门 | 机器学习研究者必知的八个神经网络架构,"本文简述了机器学习核心结构的历史发展，并总结了研究者需要熟知的 8 个神经网络架构。 我们为什么需要「机器学习」？ 机器学习对于那些我们直接编程太过复杂的任务来说是必需的。有些任务很复杂，以至于人类不可能解决任务中所有的细节并精确地编程。所以，我们向机器学习算法提供大量的数据，让算法通过探索数据并找到一个可以实现程序员目的的模型来解决这个问题。 我们来看两个例子： 写一个程序去识别复杂场景中照明条件下新视角的三维物体是很困难的。我们不知道编写什么程序，因为我们并不了解它在我们大脑中运作的机制，即便知道如何实现，写出来的程序也可能会非常复杂。 写一个程序去计算信用卡诈骗的概率是很困难的。因为可能没有任何既简单又可靠的规则，我们需要结合大量的弱规则去判别。欺骗是可以转移目标的，程序需要不断更改。 接着出现了机器学习方法：我们不需为每个特定的任务手动编程，只要收集大量的样本，为给定的输入指定正确的输出。机器学习算法利用这些样本去生成完成指定工作的程序。学习算法产生的程序可能与典型的手写程序非常不同，它可能包含数百万个数字。如果我们做得正确，这个程序将像处理训练集上的样本一样来处理新样本。如果数据改变，程序也可以通过训练新数据改变。你应该注意到，目前大量的计算比支付给程序员编写一个特定任务的程序便宜。 鉴于此，机器学习最适用任务的例子包括： 模式识别：真实场景中的物体，面部识别或面部表情，口语。 异常识别：不寻常的信用卡交易序列，核电站传感器读数的异常模式。 预测：未来股票价格或货币汇率，一个人喜欢什么电影。 什么是神经网络？ 神经网络是机器学习文献中的一类模型。例如，如果你参加了 Coursera 的机器学习课程，很可能会学到神经网络。神经网络是一套特定的算法，它彻底改变了机器学习领域。他们受到生物神经网络的启发，目前深度神经网络已经被证实效果很好。神经网络本身是一般的函数逼近，这就是为什么它们几乎可以应用于任何从输入到输出空间复杂映射的机器学习问题。 以下是说服你学习神经计算的三个理由： 了解大脑是如何工作的：它非常大且很复杂，一旦破坏就会脑死亡，所以我们需要使用计算机模拟。 了解受神经元及其适应性连接启发的并行计算风格：这种风格与序列计算截然不同。 使用受大脑启发的新颖学习算法来解决实际问题：即使不是大脑的实际工作方式，学习算法也非常有用。 在完成吴恩达的 Coursera 机器学习课程后，我开始对神经网络和深度学习产生兴趣，因此寻找最好的网上资源来了解这个主题，并找到了 Geoffrey Hinton 的机器学习神经网络课程。如果你正在做深度学习的工程或想要踏入深度学习/机器学习的领域，你应该参加这个课程。Geoffrey Hinton 毫无疑问是深度学习领域的教父，在课程中给出了非凡的见解。在这篇博客文章中，我想分享我认为任何机器学习研究人员都应该熟悉的八个神经网络架构，以促进他们的工作。 一般来说，这些架构可分为三类： 1. 前馈神经网络 这是实际应用中最常见的神经网络类型。第一层是输入，最后一层是输出。如果有多个隐藏层，我们称之为「深度」神经网络。他们计算出一系列改变样本相似性的变换。各层神经元的活动是前一层活动的非线性函数。 2. 循环网络 循环网络在他们的连接图中定向了循环，这意味着你可以按照箭头回到你开始的地方。他们可以有复杂的动态，使其很难训练。他们更具有生物真实性。 目前如何高效地训练循环网络正在受到广泛关注。循环神经网络是模拟连续数据的一种非常自然的方式。它们相当于每个时间片段具有一个隐藏层的深度网络；除此之外，它们在每个时间片段上使用相同的权重并且在每个时间片段上输入。它们可以长时间记住隐藏状态的信息，但很难训练其使用这个潜能。 3. 对称连接网络 对称连接网络有点像循环网络，但是单元之间的连接是对称的（它们在两个方向上权重相同）。比起循环网络，对称连接网络更容易分析。这个网络中有更多的限制，因为它们遵守能量函数定律。没有隐藏单元的对称连接网络被称为「Hopfield 网络」。有隐藏单元的对称连接的网络被称为玻尔兹曼机。 下面介绍研究者需要熟知的 8 个神经网络架构。 1. 感知器 第一代神经网络出现时，感知机（perceptron）仅仅是单个神经元的计算模型，其在二十世纪六十年代初被美国计算机科学家 Frank Rosenblatt 推广。其学习算法非常强大，并且宣称可以学习很多事情。1969 年，Minsky 与 Papert 出版了一本名为《感知机》的书，这本书分析了这些算法可以做什么，并阐释了其局限性。许多人就将这个局限性放大到所有的 NN 模型。然而，感知机学习过程仍广泛用于具有包含数百万特征的大特征向量的任务。 在统计模式识别的标准范例中，我们首先将原始输入向量转换为特征激活向量。然后，基于大家的共识手动编程来定义特征。接下来，我们学习如何对每个特征激活进行加权以获得单一的标量。如果这个标量超过了某个阈值，我们认为输入向量是目标集中的一个正样本。 标准的感知机架构遵循前馈模型，输入被发送到神经元中，经处理后输出。在下图中，表示为网络自下而上读取：底部输入，顶部输出。 但是，感知机确实存在局限性：如果您使用手动设置特征，并且使用了足够多的特征，那么你几乎可以做任何事情。对于二进制输入向量，我们可以为指数级多的二进制向量分别设置一个特征单元，因此我们可以对二进制输入向量进行任何可能的区分。但是，一旦确定了手动编程的特征，感知器可以学习的东西就非常有限。 这个结果对于感知机是毁灭性的，因为模式识别是去识别在变换情况下的模式。Minsky 和 Papert 的「组不变性定理」认为感知机的学习部分无法去学习当转换来自于一个组的情况。为了识别上述那种情况，需要更多的特征单元去识别那些模式中包含的子信息。所以模式识别的技巧部分必须由手动编码的特征检测器来解决，而不是学习过程。 没有隐藏单元的网络在其可以学习建模的输入输出映射中是非常有限的。简单地增加一些线性单元无济于事，因为结果还是线性的。固定的输出非线性也不够，因此，我们需要多层自适应非线性隐藏单元。问题是怎样对这样的网络进行训练。我们需要一种适应所有权重的有效方式，而不仅仅是最后一层，所以这很难。学习进入隐藏层的权重等同于学习特征，这是非常困难的，因为没有人直接告诉我们隐藏层应该怎么做。 2. 卷积神经网络 机器学习研究已经广泛地集中在物体检测问题上。有各种各样的事情使识别物体变得困难： 图像分割：真实场景中总是掺杂着其它物体。很难判断哪些部分属于同一个对象。对象的某些部分可以隐藏在其他对象的后面。 物体光照：像素的强度被光照强烈影响。 图像变形：物体可以以各种非仿射方式变形。例如，手写也可以有一个大的圆圈或只是一个尖头。 情景支持：物体所属类别通常由它们的使用方式来定义。例如，椅子是为了让人们坐在上面而设计的，因此它们具有各种各样的物理形状。 视角：标准学习方法无法应对的视点变化导致的图像变化，得到的信息随输入维度（即像素）的变化而变化。 维度跳变：设想一个医疗数据库，通常用来学习体重的神经元，现在忽然用来学习病人的年龄！要应用机器学习，我们首先要消除这个维度跳跃。 复制特征方法是目前神经网络解决目标检测问题的主要方法。在不同的位置使用相同的特征提取器。它也可以在尺寸和方向上复制，这很需技巧并且很贵。复制大大减少了要学习的自由参数的数量。它使用几种不同的特征类型，每种都有自己的复制检测器图像。它也允许以各种方式表示每个图像块。 那么复制特征检测器是如何实现的呢？ 激活值等变化量：复制特征的方法并不能使神经元激活值不变，但是能够使激活值改变量相同。 知识不变量：如果在训练中一个特征在某些位置有效，则在测试过程中，特征检测器在各个位置有效。 1998 年，Yann LeCun 和他的合作者开发了 LeNet 的手写数字识别器。它在前馈网中使用反向传播，这个前馈网络不仅仅是一个识别器，它有许多隐藏层，每个层有许多复制单元的映射，汇集附近复制单元的输出，有一个即使重叠也能够同时处理几个字符的宽网，以及训练一套完整的系统的巧妙方式。后来正式命名为卷积神经网络。一个有趣的事实是：这个网络被用来读取北美地区约 10％的支票。 卷积神经网络可用于从手写数字到 3D 物体的与物体识别有关的所有工作。但是，从网下载的彩色照片中识别真实物体要比识别手写数字复杂得多。它百倍于手写数字的类别（1000：10），数百倍于手写数字的像素（256×256 色：28×28 灰度），是三维场景的二维图像，需要分割混乱场景，而且每个图片有多个对象。这样的情况下，相同类型的卷积神经网络会起作用吗？ 之后在 ImageNet 2012 年的 ILSVRC 竞赛（这个比赛被誉为计算机视觉的年度奥林匹克竞赛）中，题目是一个包含大约 120 万张高分辨率训练图像的数据集。测试图像不显示初始注释（没有分割或标签），算法要产生指定图像中存在什么对象的标签。来自 Oxford、INRIA、XRCE 等机构的先进计算机视觉小组用已有的最好计算机视觉方法应用于这个数据集。通常计算机视觉系统是复杂的多级系统，往往需要在早期阶段通过手动调参来优化。 比赛获胜者 Alex Krizhevsky（NIPS 2012）开发了由 Yann LeCun 开创的深度卷积神经网络类型。其架构包括 7 个隐藏层（不包括池化层）。前五层是卷积层，后面两层是全连接层。激活函数在每个隐藏层中被修正为线性单元。这些训练比 Logistic 单元更快，更有表现力。除此之外，当附近的单元有更强的活动时，它还使用竞争规范化来压制隐藏的活动，这有助于强度的变化。 有一些技术手段可以显著提高神经网络的泛化能力： 从 256×256 图像中随机挑选 224×224 块图像以获取更多数据，并使用图像的左右反射。在测试时，结合 10 个不同的图像：四个角落加上中间，再加上它们水平翻转的五个。 使用「dropout」来调整全局连接层（包含大部分参数）的权重。Dropout 指的是随机移除每个训练样本一层中的一半隐藏单元，使其不再过多地依赖其它隐藏单元。 就硬件要求而言，Alex 在 2 个英伟达的 GTX 580 GPU（超过 1000 个快速小内核）上使用了非常高效的卷积网络实现。GPU 非常适合矩阵乘法，并且具有非常高的内存带宽。这让它在一周内训练了网络，并在测试时快速结合了 10 个图像块的结果。如果我们可以足够快地交换状态，我们可以在很多内核上传播一个网络。随着内核越来越便宜，数据集越来越大，大型神经网络将比老式计算机视觉系统发展得更快。 3. 循环神经网络 为了理解循环神经网络，我们需要对序列建模进行简要概述。当机器学习应用于序列时，我们经常希望将输入序列转换为不同域中的输出序列；例如，将一系列的声压转换成单词序列。当没有单独的目标序列时，我们可以通过尝试预测输入序列中的下一项作为网络的学习目标。目标输出序列是输入序列的下一步。比起试图根据一个图片的其他像素来预测一个像素或根据图像的其余部分来预测一个部分，这样似乎更自然。预测序列中的下一项模糊了有监督学习和无监督学习之间的区别。它使用专为监督学习而设计的方法，但它不需要单独的目标数据。 无记忆模型是这个任务的标准方法。具体而言，自回归模型可以通过使用「延迟抽头」从固定数量的前几项预测下一项，而且前馈神经网络是使用一层或多层非线性隐藏单元的广义自回归模型。但是，如果我们给生成模型一些隐藏的状态，使这个隐藏的状态内部是动态的，我们就会得到一个更有趣的模型：它可以长时间地将信息存储在隐藏状态。如果隐状态的动态从隐状态中生成输出是有噪声的，我们将永远无法知道它确切的隐藏状态。我们所能做的是推断隐状态矢量空间的概率分布。这种推断只适用于 2 种隐藏状态模型。 循环神经网络是非常强大的，因为它们结合了两个属性：1）分布式隐状态，允许其有效地存储大量有关过去的信息； 2）非线性动态，使他们能够以复杂的方式更新隐状态。有了足够的神经元和时间，RNN 可以计算任何计算机可以计算出来的东西。那么 RNN 可以表现什么样的行为呢？它们可以振动，可以稳定在点吸引子，从而表现地很混乱。还可以通过使用隐状态的不同子集执行许多不同的小程序，每个小程序捕获一块知识，而且所有的这些都能并行运行并以更复杂的方式交互。 然而，RNN 的计算能力使它们很难训练。由于梯度爆发和梯度消失，训练一个 RNN 是相当困难的。当我们进行多层反向传播时，梯度的大小会发生什么变化？如果权重很小，则梯度将指数缩小。如果权重很大，梯度将成指数增长。典型的前馈神经网络可以应付这些指数效应，因为它们只有很少的隐藏层。然而，在训练长序列的 RNN 中，梯度很容易爆炸或消失。即使初始权重选的很好，也很难检测到依赖于多个时间步长前的输入的当前目标输出，所以 RNN 难以处理序列中的长程依赖。 基本上有 4 种有效的方法来学习 RNN： 长短期记忆：将 RNN 用于长期记忆值的小模块。 Hessian Free 优化：通过使用酷炫的优化器来处理梯度消失问题，该优化器可以检测具有更小曲率的微小梯度。 回声状态网络：通过谨慎地初始化层之间的连接（输入 ->隐层、隐层 ->隐层、输出 -> 隐层），确保隐藏状态有巨大的弱耦合震荡存储，可以通过输入选择性地驱动这些振荡器。 利用动量进行良好的初始化：像回声状态网络一样进行初始化，然后使用动量学习所有连接。 4. 长短期记忆网络 Hochreiter 和 Schmidhuber（1997）通过构建长短期记忆网络，解决了获取 RNN 长时间记忆（如数以百计的时间步长）的问题。他们使用具有乘法相互作用的逻辑和线性单元来设计存储器单元。每当「写入」门打开时，信息就会进入单元。当「保持」门打开，信息将在单元中保持。信息可以通过打开「读取」门而从单元中读取。 手写草书识别是一个特别适合 RNN 的任务。输入是笔尖的（x，y，p）坐标序列，其中 p 表示笔是向上还是向下。输出是一个字符序列。Graves 和 Schmidhuber（2009）表明，带有 LSTM 的 RNN 是目前草书识别的最佳系统。简而言之，其使用一系列小图像代替笔坐标作为输入。 5. Hopfield 网络 循环网络的非线性单元通常很难分析。它们可以表现为不同的方式：稳定到稳定的状态，振荡，或遵循不可预测的混沌轨迹。一个 Hopfield 网络由二元门限单元组成，它们之间有连续的连接。1982 年，John Hopfield 认识到，如果连接是对称的，就有一个全局能量函数。整个网络的每个二进制「结构」都有能量，而二进制阈值的决策规则使网络可以得到能量函数的最小值。利用这类计算的一种简洁方法是使用记忆作为神经网络的能量最小值。使用能量最小值来表示记忆从而内存可寻址。可以只知道一个项目的部分内容来访问这个项目。这对硬件损坏是极大的。 每当记忆一次配置，我们希望创造一个新的能量最小值。但是，如果在中间位置附近有两个最小值怎么办呢？这限制了 Hopfield 网络的容量。那么如何增加 Hopfield 网络的容量呢？物理学家认为他们已知的数学知识可以解释大脑的工作机制。物理学期刊上发表了许多关于 Hopfield 网络及其存储容量的论文。最终，Elizabeth Gardner 认为，有一个更好的存储规则——使出权重的「浑身解数」。这不是一次性存储向量，而是多次循环训练集，并利用感知机收敛过程来训练每个单元，使其具有正确的状态，给定该向量中所有其他单元的状态。统计学家称这种技术为「伪可能性」。 Hopfield 网络还有另一个计算功能。我们不再用网络来存储记忆，而是用它来构建感官输入的信息。用可见单元表示输入，用隐藏节点的状态来表达输入节点的信息，用能量表示信息的不好（低的能量状态来表达一个好的 interpretation）。 6. 玻尔兹曼机网络 玻尔兹曼机是一种随机递归神经网络。它可以被看作是 Hopfield 网络随机生成的对应物。它是第一个能够学习内部表示的神经网络之一，能够表示并解决难的组合问题。 玻尔兹曼机学习算法的学习目标是最大化玻尔兹曼机分配给训练集中二进制向量概率的乘积。这等同于最大化玻尔兹曼分配给训练向量的对数概率之和。也就是说，如果我们做了如下的事情，最大化我们得到 N 个训练案例的概率：1）让网络在没有外部输入的情况下在不同时间稳定分布; 2）每次采样一次可见向量。 2012 年，Salakhutdinov 和 Hinton 提出了玻尔兹曼机的高效小批量学习程序。 对于正相位，首先将隐藏概率初始化为 0.5，将可见单元上的数据向量进行钳位，然后并行更新所有隐藏单元，使用平均场方法并行更新隐藏单元直到收敛。在网络收敛之后，记录每个连接的单元对 Pi Pj，并在最小批量中对所有数据取平均。 对于负相位：首先保留一组「幻想粒子」（就是一对 (Si,Sj) 形成的系统？）。每个粒子都有全局配置中的一个值。然后串行更新几次每个幻想粒子中的所有单元。对于每一个连接的单元，对所有的幻想粒子的 SiSj 取平均。 在普通玻尔兹曼机中，单元的随机更新需要连续。有一个特殊的体系结构允许更有效的交替并行更新（层内没有连接，没有跳层连接）。这个小批量程序使玻尔兹曼机的更新更加并行。这就是所说的深度玻尔兹曼机（DBM），一个缺失很多连接的普通玻尔兹曼机。 2014 年，Salakhutdinov 和 Hinton 为他们的模型提出了一个升级版，称之为受限玻尔兹曼机（RBM）。他们通过限制连通性使推理和学习变得更容易（隐藏单元只有一层，隐藏单元之间没有连接）。在 RBM 中，当可见单元被钳位时只需要一步就能达到热平衡。 另一个有效的小批量 RBM 学习程序是这样的： 对于正相位，首先将可见单元的数据矢量钳位。然后计算所有可见和隐藏单元对的<ViHj>的确切值。对于每个连接的单元对，对小批量中的所有数据<ViHj>求平均。 对于负相位，也保留一组「幻想粒子」（就是一对 (Vi，Hj) 形成的系统？）。然后交替并行更新几次每个幻想粒子中的所有单元。对于每一个连接的单位对，所有幻想粒子 ViHj 求平均。 7. 深度信念网络 反向传播被认为是人工神经网络中的标准方法，用于在处理一批数据之后计算每个神经元的误差贡献。但是，使用反向传播存在一些重要的问题。首先，它需要有标签的训练数据；而几乎所有的数据都没有标签。其次，学习时间不够理想，这意味着隐藏层数多的网络很慢。第三，它可能会陷入局部最小的局面，所以对于深度网络来说，它们还差得很远。 为了克服反向传播的限制，研究人员已经考虑使用无监督的学习方法。这有助于保持使用梯度方法来调整权重的效率和简单性，还可以用它来对传感输入的结构进行建模。特别是，他们调整权重使生成模型产生感官输入的概率最大化。问题是我们应该学习什么样的生成模型？可以是像玻尔兹曼机这样的能量模型吗？还是由理想化的神经元组成的因果模型？或是两者的混合？ 信念网是由随机变量组成的有向无环图。使用信念网我们可以观察到一些变量。我们想要解决 2 个问题：1）推理问题：推断未观测变量的状态; 2）学习问题：调整变量之间的交互作用，使网络更有可能产生训练数据。 早期的图形模型的图形结构和条件概率是专家定义的。那时，这些图形连接稀疏，所以研究人员一开始把重点放在做正确的推论，而不是学习上。对于神经网络来说，学习是关键，手写知识并不酷，因为知识来自于学习训练数据。神经网络的目的不在于可解释性或通过稀疏连接性使其便于推理。不过，还有神经网络版本的信念网络。 由随机二元神经元组成的生成神经网络有两种类型：1）基于能量利用对称连接来连接二元随机神经元得到玻尔兹曼机; 2）我们通过因果关系在一个有向无环图中连接二元随机神经元获得 SBN。这两种类型的描述超出了本文的范围。 8. 深度自动编码器 最后，我们来讨论深度自动编码器。深度自动编码器是非线性降维的非常好的方法，原因如下：它们提供了两种灵活的映射方式。训练事例的学习时间是线性的（或更好的），最终的编码模型相当紧凑和快速。然而，使用反向传播来优化深度自动编码器很难。在初始权重较小的情况下，反向传播梯度消失。现在我们有了更好的方法来优化它们，要么使用无监督的逐层预训练，要么像回声状态网络那样谨慎地初始化权重。 对于预训练任务，实际上有三种不同类型的浅自动编码器： 受限玻尔兹曼机作为自动编码器：当我们用一步对比散度训练受限玻尔兹曼机时，它试图使重建看起来像数据。它就像一个自动编码器，但是通过使用隐藏层中的二进制活动来强化正则化。经过最大可能性训练后，受限玻尔兹曼机不像自动编码器。我们可以用浅自动编码器堆栈来代替用于预训练的 RBM 堆栈；然而，如果浅自动编码器通过惩罚平方权重而被正规化，那么预训练就不是有效的（对于随后的辨别）。 去噪自动编码器：通过将其许多分量设置为 0（例如输入的数据丢失的情况），将噪声添加到输入向量。他们仍然需要重建这些分量，因此他们必须提取捕获输入之间相关性的特征。如果我们使用表示自动编码器的堆栈，那么预训练是非常有效的。与 RBM 预训练一样好或者更好。由于我们可以很容易地计算目标函数的值，所以评估预训练也更简单。它缺乏我们用 RBM 获得的良好的变分边界，但这只是理论上的问题。 压缩自动编码器：使自动编码器正规化的另一种方法是尽可能使隐藏单元的活动对输入不敏感，但是他们不能忽视这些输入，因为他们必须重建这些输入。我们通过惩罚每个隐藏活动相对于输入的平方梯度来达到这个目的。压缩自动编码器预训练时工作得很好。代码倾向于使一小部分隐藏单元对输入变化敏感。 简而言之，现在有很多不同的方法来进行功能的逐层预训练。对于没有大量标签化的事例的数据集来说，预训练有助于后续的判别式学习。对于非常大的有标签的数据集，使用无监督预训练初始化监督学习中使用的权重进行并不必要，即使对于深度网络也是如此。预训练是初始化深度网络权重的第一个不错的方法，但现在还有其他方法。但是，如果我们让网络很大，我们将再次离不开预训练！ 最后的福利 神经网络是有史以来最美丽的编程范例之一。在传统的编程方法中，我们告诉计算机做什么，将大问题分解成计算机可轻松执行的许多精确定义的小任务。相比之下，在神经网络中，我们不需要告诉计算机如何解决我们的问题，而是让它通过观测数据学习，找出解决手头问题的办法。 如今，深度神经网络和深度学习在计算机视觉、语音识别和自然语言处理等许多重要问题上有着出色的表现。它们正在被谷歌、微软和 Facebook 等公司大规模部署。 我希望这篇文章能帮助你学习神经网络的核心概念，包括深度学习的现代技术。你可以从我的 GitHub 库（https://github.com/khanhnamle1994/neural-nets）中获取 Hinton 的 Coursera 课程所有课程幻灯片，研究论文和编程作业。祝你好运！ 原文链接：https://medium.com/@james_aka_yale/the-8-neural-network-architectures-machine-learning-researchers-need-to-learn-2f5c4e61aeeb "
221,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738149&idx=5&sn=4d0f7a3d08b332e34f3e004f124e3d86&chksm=871ac99bb06d408decbd0085cf4ab561a4cb6218bfdfccc5ecfe48c652ce05d57f09d1d6a350&scene=27,ICLR 2018 | 阿姆斯特丹大学论文提出球面CNN：可用于3D模型识别和雾化能量回归,"arXiv 通过类比平面CNN，本文提出一种称之为球面CNN的神经网络，用于检测球面图像上任意旋转的局部模式；本文还展示了球面 CNN 在三维模型识别和雾化能量回归问题中的计算效率、数值精度和有效性。 1 引言 卷积神经网络（CNN）可以检测出图像任意位置的局部模式。与平面图像相似，球面图像的局部模式也可以移动，但这里的「移动」是指三维旋转而非平移。类比平面 CNN，我们希望构造一个神经网络，用于检测球面图像上任意旋转的局部模式。 如图 1 所示，平移卷积或互相关的方法不适用于分析球面信号。那么最明显的解决办法是改变互相关的定义，将滤波器平移改为滤波器旋转。然而这时我们会遇到问题，平面和球面之间存在一个细微却重要的差异：平面的移动空间（二维平移）与该平面是同构的，而球面的移动空间（三维旋转）是一个与球面不同构的三维流形，称为 SO(3)。因此球面互相关的结果（即输出的特征映射）是 SO(3) 上的信号，而非球面 S^2 上的信号。鉴于此，我们在一个球面 CNN 较高的几层中实现 SO(3) 群内的互相关（Cohen and Welling, 2016）。 图 1：对球面信号的平面投影都会产生畸变。球面信号的旋转效果无法用其平面投影的平移效果来模仿。 球面 CNN（S^2-CNN）的实现存在两大挑战。首先，平面上像素组成的方形栅格具有离散的平移对称性，而球面上不存在完全对称的栅格，所以很难对球面滤波器旋转一个像素的距离作出简单定义。为了旋转球面滤波器，我们需要做某种形式的插值。另一个挑战是计算效率，由于 SO(3) 是一个三维流形，简单实现 SO(3) 群内的互相关，算法的时间复杂度为 O(n^6)。 本文采用非交换谐波分析领域的方法（Chirikjian and Kyatkin, 2001; Folland, 1995）解决上述问题。这一领域给出的广义傅里叶变换影响深远，不仅适用于球面信号，也适用于旋转群上的信号。已知 SO(3) 群内互相关关于 SO(3) 群内的傅里叶变换满足傅里叶定理，而本文对球面（S^2）互相关的定义也满足傅里叶定理，因此可以用广义的快速傅里叶变换（FFT）算法高效实现 S^2 的互相关和 SO(3) 的群内互相关。 本文是首个在多层神经网络中针对连续群做互相关的研究，因此就连续理论预测的数学性质在实际离散化实现中的适用程度，我们进行了严格的评估。 同时，通过对三个数据集的实验，本文展示了球面 CNN 在旋转不变分类问题和回归问题中的应用。第一项实验显示，球面 CNN 对球面 MNIST 图像做旋转不变分类的效果远优于平面 CNN；第二项实验采用 CNN 实现三维形状的分类；第三项实验将球面 CNN 模型用于分子能量回归分析，这是计算化学中的一类重要问题。 贡献 本文的主要贡献如下：一是球面 CNN 理论；二是对于球面 S^2 和三维特殊正交群 SO(3) 的广义傅里叶变换，本文给出了首个可自动微分的实现，开源的 PyTorch 代码使用简单、运算快速，且内存使用效率高。三是就球面 CNN 对旋转不变类学习问题的适用性，本文给出了实证支持。 图 2：频域内实现球面互相关。信号 f 和局部支持的滤波器 ψ 经过傅里叶变换，分块表示为张量，在各输入通道上求和，最终做傅里叶逆变换。注意，由于滤波器是局部支持的，用矩阵乘法（离散傅里叶变换 DFT）比用 FFT 算法更快。本文用球坐标 α 和 β 将球面参数化，用 ZYZ（外旋）欧拉角 α，β 和 γ 将 SO(3) 参数化。 图 3：∆ 作为栅格分辨率和网络层数的函数 图 4：用球极平面投影将两个 MNIST 数字投影到球面。如果再投影回平面，则会产生非线性畸变。 图 5：光线从球面向球心投射，与模型（椅子）的第一个交汇点处可计算球面信号值。右侧两幅图是用球坐标 (α, β) 表示的两个球面信号。右上：从球面到模型第一个交汇点处的光线线段长度。右下：光线与模型法线夹角的余弦；红点对应左图中投射的红色光线。 表 1：平面 CNN 与球面 CNN 在球面 MNIST 数据集上的精度。这里 R 表示经过旋转的图像，NR 表示未旋转的图像，X/Y 表示网络用 X 数据集训练，用 Y 数据集评估。 表 2：SHREC17 竞赛（三维形状分类）最佳方法与本文方法的结果对比 图 6：势函数 Uz 产生五通道的球面信号。分子中可包含的五类原子：H（红色）、C（绿色）、N（橙色）、O（棕色）和 S（灰色），对应原子序数 z ∈ {1, 6, 7, 8, 16}。 表 3：左：QM7 任务（对分子的原子化能做回归预测）实验结果，作者 (a) 为 Montavon et al. (2012)，作者 (b) 为 Raj et al. (2016)。右：用于分子能量回归任务的 ResNet 架构。 论文：SPHERICAL CNNS 论文链接：https://openreview.net/pdf?id=Hkbd5xZRb 卷积神经网络（CNN）已成为二维平面图像学习问题的首选方法。然而，近期研究中出现的大量问题需要面向球面图像的分析模型，应用包括无人机、机器人和自动驾驶汽车的全向视觉，分子能量的回归分析，以及全球气象建模等。将卷积神经网络简单应用于球面图像的平面投影这一方法注定会失败，因为投影时的空间变化会引入变化的畸变，在平移卷积核时，权值共享不再有效。 本文提供了构建球面 CNN 的基本要素，并提出了球面互相关的定义，表达力强且具有旋转等变性。如此定义的球面互相关满足广义傅里叶定理，因此可以用广义（非交换的）快速傅里叶变换（FFT）算法高效计算。本文展示了球面 CNN 在三维模型识别和雾化能量回归问题中的计算效率、数值精度和有效性。 "
222,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738149&idx=1&sn=1ca56d8365988b77e09a636f8bbcde7e&chksm=871ac99bb06d408d0254da27fac8c437fe916a7493f6c6bf689fa527d32c07a2b3b0551864bd&scene=27,想击败英伟达？可能得十家以上不同垂直领域的深鉴科技，这就是AI芯片的「行情」,"过去一年，从技术向产业，有哪些值得记住的人和事？未来一年，AI 场景化落地还有哪些可能性？ 8 位 AI 行业局内人，向我们讲了讲他们的故事和看法。 「我认为 2018 年，将会是各种芯片产品『百家争鸣』的一年。」计算机架构领域著名教授 David Patterson 说。 果不其然，新年伊始，芯片领域就重磅消息不断。继 John Hennessy 出任 Alphabet 公司董事长之后，谷歌宣布全面开放 TPU 云。紧接着，ARM 发布 Trillium 项目、亚马逊被曝正在开发 Echo 专用 AI 芯片。 算法走向成熟，自然会寻求算力改造。而就在两三年前，芯片领域的创业融资「仍犹如一场噩梦」：一家创业公司凭什么与英特尔竞争？深度神经网络的走红，让创业投资 (VC) 资金重新回到半导体领域。 腾讯发布的《中美两国人工智能产业发展全面解读》显示，目前（2017 年）处在基础层（芯片/处理器）公司中，美国有 33 家，中国约 14 家（实际数字可能会更多）。虽然无法媲美全球 3000 多家（中国 1000 多家）的 AI 公司数量，但对半导体领域来说，这个数字已属惊人。 「过去 10 到 15 年以来，任何一个细分领域都不曾出现过这样的情况：超过 15 家半导体公司同时涌现。」一位连续创业者感叹道。 2017 年，创业投资在芯片创业公司的投资额超过 15 亿美元，几乎是两年前投资的两倍。其中，深鉴科技分别获得数千万美元（ A 轮）、4千万美元（A+ 轮）融资、耐能获超千万美元融资（A 轮）、ThinkForce 宣布完成 4.5 亿元融资（A轮） 。寒武纪、地平线融资规模达 1 亿多美元，而2017 年实现这一融资规模的 AI 芯片创业公司只有三家：Cerebras（2018 年年初）、Graphcore 以及 Wave Computing。 一些活跃的芯片公司投资人通常也活跃在 AI 领域（比如红杉资本），甚至自己就是将 AI 作为未来战略重点的大公司。 投资了顶尖算法公司商汤、旷视的阿里巴巴，于 2017 年连投三家 AI 芯片公司——寒武纪、深鉴科技和耐能。而在此之前（2016），公司还投资了杭州中天微和 Barefoot Networks。 对于中国公司来说，这是一次难得的超车机会。 AI、物联网等长尾和分散的新兴市场，为中国芯片创业公司带来重要的入场机会。 过去十年中，中国芯片产业大环境的重大变化也降低了初创公司做芯片的门槛。 成立于 2016 年3月的深鉴科技，正是最早进入这一领域的重要创业公司之一。从 2012 年开始，他们就一直沿着做有指令集的深度学习处理器这条路在走。2017 年 10 月，深鉴科技推出自研的 AI 芯片「听涛」与「观海」，并计划于今年上市。 以下是姚颂的专访实录（机器之能做了不改变内容的编辑）： David Patterson 与 John Hennessy 是《计算机体系架构：量化研究方法》作者，也是计算机架构领域鼎鼎大名的教授。近期，John Hennessy 接替施密特出任  Alphabet 公司董事长。在你看来，这对芯片行业传递着一种什么样的信号？ 我觉得是一个相当大的利好。 符合整体事物发展逻辑。谷歌，这么大一个体量公司，单凭搜索再去把业务扩大，我觉得非常困难。所以大家一定要优化内部结构，打开一些新市场。AI 这个领域，软硬兼施，协同很多，谷歌一定得要去做 TPU，才可能把它的 AI 服务做得特别好。要不然所有人因为用 GPU，成本高，性能功耗也有瓶颈，差异化拉不开。这也是公司将超级大牛邀请过去的原因。 这跟 AI 的整体逻辑也是一样的，要优化整体的内部结构，才能把这个业务做到成本最低，效果最优，最后可能还得做自己的芯片。 对创业公司来说，会有什么样的影响？ 对创业公司的影响分两个层面。 业务层面，我觉得没有太多影响。不同创业公司，做的事差异也很大，业务方向上看，没有太多冲突。甚至有一些利好，很多公司可能没有这样一个时间周期去等他们内部的 TPU 出来与谷歌 TPU 竞争，大家先在云上做的可能是 FPGA。其实，谷歌最开始也是先上了几千颗 FPGA，量大以后，才把它变成芯片。 另一方面，如果真得越来越重视硬件层面，从投资大环境来说，大家也会越来越重视这个领域，肯定也是一些间接利好。 如何评价 Hinton 近期提出的 CapsNet 框架？会是一个变革性的成果吗？ 从学术界来说，有可能。但从工业界来说，我觉得还比较难。 从爆发至今，深度学习花了好些年才让大家都适应。哪怕 CapsNet 超越深度学习，也要花好几年的时间才可能完全波及到工业界。另外，我觉得深度学习之所以波及这么快，是因为在很多实际场景中，完全超越了以往所有传统机器学习，在很多的问题上是 0 和 1 的区别。但是，如果一个问题，比如人脸识别，现在已经可以做到八个九了，再来一个算法可以做到九个九，大家会觉得意义不大。 所以从工业界角度来看，这时候再来一个变革，推广速度可能会比原来还要稍微慢一点。如果真是颠覆性技术，要在整个工业界盛行，还需要好几年时间。 如何看待目前这个领域里的泡沫现象？ 整体泡沫很多。但只要是创业界，不管哪个行业都会有类似情况。 比如，共享单车出来了，全国会出现好几十家做共享单车的公司；还有千团大战。创业圈，不管哪个细分领域，永远都是这样：只要一个事情有前景，一定会有很多人前赴后继地投进来。只不过现在换成技术创业以后，大家发现理解这个事情并不像原来理解团购、共享单车、打车商业模式那么简单，需要换一个新思路去理解，但本质上没有什么区别，大家都希望能够分到一杯羹。 大家在前赴后继投进的时候，还是需要分析这个行业最关键的竞争要素是什么。 在芯片领域，设计门槛确实在变低，一颗深度学习处理器芯片设计复杂度可能只是一颗 CPU 芯片的 1/10。但是，单纯芯片本身可能不是最核心的竞争要素，竞争一定是系统层级的事情。 对 AI 芯片行业来说，2018年意味着什么？ 我觉得是很关键的一年，要验证大家到底有没有真材实料。对于整个 AI 行业来说，都是特别关键的一年。 2018 年以前，大家还是可以凭借着团队背景、学术能力等一些较虚的东西融到资，吸引人才加入，进一步往前走。从 2018 年开始，大家必须要有实际的产品和订单，要有实际的销售。 所以，这个行业里的马太效应会越来越明显。如果技术不是那么强，越往后越没办法。在商业竞争中，产品卖不出去，甚至因为公司能力不够，产品都做不出来，这样一批公司会越来越难融到资。 如果有新的创业公司要在今年进场做芯片，还有机会吗？ 我觉得几乎没有了。 从 2015 年往后、2016 年，这个时间窗口就已经过去了。2017 年往后成立的公司，已经很难有机会。 谷歌 2014 年甚至更早开始做 TPU 这个事情，我们和寒武纪是 2012 年开始做，地平线是 2015 年开始做。谁都逃不过芯片硬件数年的研发周期。硬要逞强赶着量产芯片，研发周期短会导致很难设计出性能很好、或者通用性较好的架构，结果，芯片能效比可能比我们的 FPGA 还要差十倍。 未来十年，芯片产业的各个环节，会以算力来去配合算法为中心，软件算法定义硬件？ 肯定是这样。 原来之所以可以一直延续通用芯片，是因为有摩尔定律，大家一直不断地写原来的程序就行了，反正每隔两年性能就翻倍，成本会下来一些。 现在，摩尔定律无已为继，要获取更高性能，更低功耗，只能做得越来越专用。越来越专用，就要牵扯到跟算法跟应用场景，要了解应用和场景。此时，很多的事情就是算法软件给定的。 所以，我觉得这肯定是一个非常大的趋势。其实，从 2014 年开始，就有人在提这个趋势。比如，百度从 2014 年就开始提一个概念，叫做 software-defined accelerator，赛灵思也有类似的口号。未来一定是系统决定芯片，很难像 CPU 这样一个架构做所有事情。 谈到 AI 芯片方案，一般都会提及 FPGA 和 ASIC，不同路线都有大厂商在走， 深鉴科技选择芯片方案的逻辑是什么？ 我们也在做自己的处理器芯片，很快了，也是专用芯片。其实，我们做的都是一套东西。我一直很强调：我们一直做的是一个深度学习处理器，是有指令集的处理器，而不是将某几个算法写死到一颗芯片上，或者在 FPGA 上。我们从 2012 年开始做这个事，一直沿着这条路在走。 当然，我们这个通用性是局限在深度学习的 Inference。这是我们的一个核心东西，不论 ASIC 还是 FPGA，核心架构都是一条深度学习处理器。 这是首先需要澄清的。 至于为什么用 FPGA？因为对于创业公司来说，有好几个特别难的地方。 第一，做一颗处理器级别芯片，姑且不论量产周期多少时间，核心架构至少也需要 3 到 4 年。而仅芯片的生产制造流程，量产的话，都得走一年半到两年，创业公司耽误不起这个时间。 第二，芯片的一次性研发投入很高。比如我们做一个 28 纳米的芯片，一次性研发投入可能是 400 多万美金，还不算人的成本和后续量产成本。如果算上人，这个钱至少要双倍。但一个公司不可能只留了工资和做这个东西的钱，这些钱一定只能是公司总预算的一小部分，还有其他很多地方要花钱。 第三，全行业都面临一个特别大的挑战：怎样去定义芯片？ 原来定义芯片，比如 X86 的 CPU，告诉你是 X86 指令集，什么算法都能跑，这个定义比较明确。蓝牙芯片、wifi 芯片情况都差不多，芯片功能和协议固定下来，做芯片优化就可以了。 但对于 AI 芯片来说，AI 怎么定义？一个大的 AI 里面有一部分是机器学习，机器学习里面一部分是深度学习，深度学习里面有各种 Net，有各种各样的卷积和尺寸。深度学习又分 Training 和 Inference，而且在不同场景，大家跑的不仅仅是神经网络，因为深度学习大于神经网络这个概念。所以，芯片应该是做成什么功能，能支持哪些 Net，通用程度是怎样的，也很难定义清楚。 FPGA 能解决这几个问题。 一方面，FPGA 芯片已经在这里了（赛灵思已经做好了），芯片研发已经结束， 把研发的新品结构烧进 FPGA 后就可以直接上产品，上市时间（time to market）特别快，可能仅需三个月。 另一方面，你是赛灵思或者 Altera 那里按片购买 FPGA， 不用一开始就投入 400 多万美金，所以，也没有一次性投入太高的问题。 第三，芯片没法定义这个问题，也好解决。FPGA 是可重构的。将每一代的处理器架构重新烧结上去，只要向上兼容就好了。上一代架构写的程序，还能在新一代架构上跑起来，向上兼容就行了。 但是，FPGA 确实还存在两个痛点，这也是我们自己做芯片的原因。 有一个问题，也是一直以来存在的一个问题：在量大的情况下，成本高。如果有一个几十万片的订单，芯片成本摊下来后，就比 FPGA 低了。虽然一次性研发投入成本特别高，但后续成本很低：只要几十万片，成本就可能摊到 10 美金以下，这时候卖就能够挣钱了。 另外，在 FPGA 上实现处理器芯片，它的性能和功耗，通常会比直接把芯片做出来，要差十倍左右。因为 FPGA 必须考虑它可以不断擦写，可以烧各种各样的芯片，所以，许多地方达不到最优。 比如做一个芯片，可以跑 1GHz、1.2GHz，甚至 1.5GHz。在 FPGA 上，可能只能跑 200MHz 或者 300MHz。FPGA 也没办法把功耗优化得特别低，因为要考虑很多通用的事情。 对于 AI 来说，核心竞争力并不是这颗芯片真正的性价比怎么样，最核心要素是软件环境。...... 深鉴科技很重视安防领域，中国安装的摄像头数量巨大，大约 1 个多亿。具体使用到了 AI 的市场到底有多大？似乎仅为 1% 不到 2%？ 现在的 AI 渗透率低，硬件是一个最主要原因。 大家都在用 GPU，但会有几个问题。其中一个问题是 GPU 只能在服务器用。 举个例子，我们公司也有几百颗 GPU ，用来训算法，做压缩。我们自己做服务器托管时，北京很多 IDC 机房都去试用了一遍。他们都说，我的供电能够做得特别稳定，空调制冷也能做得特别好，后来发现都或多或少有问题，因为 GPU 不稳定，经常会需要重启服务器。 所以，现在要用上 GPU 必须要搭一个条件特别好的机房。但是，一个小的派出所，或者超市和小区没这个条件。另外，公安也不能接受一个经常丢视频的产品。 因此，现在的智能安防更多还是局限在一些大型智慧城市项目，只有这样的项目才有一笔预算去建一个机房，采购服务器。 第二，GPU 价格很贵。如果所有 AI 全往云端，带宽成本、存储成本都很贵，平摊下来，成本太高，每路视频可能是在大几千到万这个量级。 中国一年出货的安防监控芯片大概是 2 亿颗。能够承受得起这么高昂成本的，可能只有那些极少数、预算最高的智慧城市项目，这就是 AI 渗透率很低的原因。所有新技术应用，一定都是从能够承受很高成本的地方开始。 深鉴科技做的事情，其实是在把每路成本降低到几百人民币、一百美金的级别。我们产品的稳定性比 GPU 好得很多，是一个低功耗方案，发热量比较少，整体稳定性很好。之前因为硬件原因没办法覆盖到的市场，现在都能覆盖到。公司这一块的订单量，起来地也挺快。 我们就是在做这样一个事：在新的市场上，让它渗透率变得更高。 安防领域有不同类型的想做芯片的竞争者，比如深耕行业多年的大公司，还有创业公司（比如算法公司和硬件芯片公司），还有传统芯片公司。你比较看好什么类型的竞争对手？ 我只看好海康威视，其他公司或多或少有一些缺陷。 首先，算法类公司会遇到的问题在于：芯片的难度和周期可能超乎大家想象。研发周期加上量产周期，至少是两到三年时间。先把算法做好，然后直接把算法写死在一颗芯片上，即使是这种最简单的做法也要花两年多时间。我觉得，这可能会是大家意料不到的一个事情。 在这两年多的时间里，必须持续不断砸钱，一点水花都看不到。这是一个比较大的考验。虽然肯定能做，但是芯片出来后，上面的算法可能是一两年前的，算法优势会消失。芯片上面的算法落后于当前研究界最好的新算法，精确度肯定会稍微低一些，算法公司会面临这样的竞争。 第二个问题是没有办法更新算法，如果需要更换算法，就要重新做一个芯片。实际上，这也是在赌：算法在很长一段时间内不会被替代，然后可以出大量的芯片，另外，用户也觉得低成本更关键。 其实，FPGA 价格不一定很贵，还是需要看场景、公司设计能力，很好公司开发 FPGA，成本可能只是其他公司的三分之一，但能做到的性能是一样的。赛灵思之所以投深鉴就是因为同样一颗芯片，我们能发挥出的纯硬件能力比别人至少强三倍。FPGA 最本质的、上面跑的芯片架构设计最为核心，它就是一个载体、一个乐高积木，用来拼那块 DPU 芯片，归根到底取决于芯片设计能力。 至于传统芯片厂商，他们可能也会比较难理解这个事。比如，一个很重要的问题就是怎么去定义芯片，怎么让你这颗芯片具有竞争力。 AI 芯片的玩法跟传统半导体行业的玩法很不一样。有一种可能的打法是，别人有一颗怎样的芯片，我去做一颗类似的，然后通过高性价比的方式去占领这个市场。大家都觉得，过两年芯片就能重新做一代，这款芯片不成功，再推一款，只要能成功地打开市场就行。 但对于 AI 来说，核心竞争力并不是这颗芯片真正的性价比怎么样，最核心要素是软件环境。 单纯从芯片性价比角度，GPU 不一定有优势，但上面的软件生态环境差异特别大。各种各样的库，各种各样的工具，可以非常简单的让小白用起来。大家要开发一个怎样的新研究项目，可以在网上找到很多支持。行业的竞争壁垒导致大家用惯了这套软件以后，别人的芯片如果不是一个彻底颠覆性的东西，就很难把你替代掉。 这是传统半导体公司需要去意识到的一个事情，包括我每次和 Movidius CEO Remi 聊天时候都是聊软件。这个行业的玩法可能会颠覆他们原来习惯了的玩法。 另一位朋友 Naveen 可能就遇到了一些类似的问题。英特尔在 2016 年收购了 Nervana。说 2016 年年底要出一个 AI 芯片，17 年年初又说要推出芯片，结果推迟到 17 年底。有了这个芯片，还要有完整配套的软件，做一个真有竞争力的产品，真的很难。 至于海康威视，最大的优势是他有明确的场景需求，知道自己要什么芯片。 深耕行业多年，海康知道把算法做到哪个程度，用户就能接受。在上面加哪些功能，未来几年这些用户都能够接受。他能够清楚地定义出这个芯片要什么和不要什么。这是算法公司和其他想要切到这个行业的公司，都没法掌握的东西。 另外，他们也有很强的渠道，能够保证芯片大量卖出去，把成本摊薄，大量赚钱。这是我比较看好的，也是传统做 ASIC 的思路。 现在再看马斯克开芯片，我就觉得符合正常逻辑。他要做自动驾驶。现在用的 GPU，但 GPU 很贵；一年能卖几十万辆车出去，做芯片能摊薄成本；他也了解这些自动驾驶功能，知道自己想要什么，不想要什么。 总之，要符合两个条件，一个是需求明确，一个是确实有非常大的量。这两点都满足，开芯片就没有什么逻辑问题。 对于算法公司来说，要在这样一个场景内生存的话，或者最后取胜，我觉得只有一条路走：变成行业公司。这也是我现在看到这几家公司在做的事情。每个行业公司都知道这个系统长什么样，它不仅仅是一颗芯片，是一个非常庞杂的大系统。 比如，考虑到芯片, 还有整个软件环境，网络录像机，服务器，视频管理软件等，这样是非常庞杂的一个整体系统。在这个系统里，AI 是核心竞争要素，但除此之外，99% 的工作量是传统的东西。真要挣钱活下去、活得好，必须变成一个行业公司。 谁能击败英伟达？可能是十家或者几十家像深鉴科技这样的公司在不同垂直领域里分别击败英伟达。 有人认为，对于 AI 芯片创业公司来说，卖 IP 这样的模式很不错，如果去造芯片，等于在爬坡。你怎么看？ 我觉得这种模式很难。IP 很难赚到大钱。所有 IP 公司中，第一名叫 ARM，全球第二名呢？我不知道还有没有任何一家单独依靠 IP 业务，就超过 10 亿美金的公司。像 Cadence 这样的 EDA 巨头，如果单独看 IP 这个业务，可能就是一个几亿美金的公司。 IP 是一个竞争门槛、壁垒相对单一的生意。为了做 IP，必须把接口定义特别清楚，一旦接口定义清楚，人家照这个接口就能把你换掉，除非别人没有，你有。现在，虽然你的深度学习 IP 可以做得很强，但 DSP 也能做，DSP 卖得很便宜，很大程度上，这就是在拼纯商业、纯销售。 所以一直以来，IP 是一个非常难做的生意，它属于这个行业最上游肉最少的那个地方。只有 ARM 能分到肉吃，因为它有全球生态。而现在，这些 AI 公司已经没有时间去布局全球生态。ARM 可以花几十年时间做出现在的规模，是因为当时做这个事情时机更好，竞争对手也少一些。当然，他们技术能力确实很强，能够把包括 Atom 在内的竞争者扫出去。 所以，走 IP 模式的创业公司要问自己两个问题。第一，是否有能力扫掉全球所有的竞争对手，这就是一个直接、面对面的竞争。第二，是不是处在一个合适的时间点。我觉得，稍微有机会的时间点是 2014 年，后面的可能性就很低了。在这样一个群狼环伺的环境中，创业公司最终长成一个 ARM ，非常困难。 我觉得很难用 IP 核的层面去一统江湖，在 AI 芯片领域，几乎没有可能。 其实还是回到一个问题：谁能打败 ARM？ 我曾经和很多人讨论的一个问题，包括跟 David Patterson 他们。他们觉得没有任何人能打败 ARM，也没有任何人能够正面竞争英伟达。 能打败 ARM 是一个什么样的概念？可能是 100 家用 RISC-V 的公司（RISC-V 是近几年推出的一套开源处理器架构，全部开源，很多人正在用来开发 CPU ，能效比确实不错）。当时大家讨论，能够打败 ARM 的就是 100 家使用 RISC-V 的公司在每个垂直领域里将 ARM 击败，才可能最终击败 ARM。 现在，英伟达也属于这种情况。它的软件环境做得很牛，技术积累也非常深厚。全球正面击败英伟达？我觉得几乎没可能。谁能击败英伟达？可能是十家或者几十家像深鉴科技这样的公司在不同垂直领域里分别击败英伟达。 比如，摄像头芯片功耗不能高于三瓦，这就将英伟达从这个市场扫出去，因为它最低功耗是十瓦。做服务器端的 Inference，一个很重要的指标是延时，英伟达又会因为这一点被打出去一些。自动驾驶领域，英伟达如果克服不了功耗和成本的问题，可能会因为这一点又被打出一些，但是，他肯定还是会有一部分市场，不可能被彻底扫出去，因为它的软件生态在那里。 所以，没有任何一家公司有可能用一个架构来直接推翻英伟达。 ARM 在这个时候会怎么玩？ ARM 一统 IP 江湖，这是有可能的。如果 ARM 做出一个很好的深度学习 IP，免费跟他的 CPU 一起卖，所有的做这方面 IP 生意的公司都有可能死掉。 近期去世的定位大师杰克·特劳特说过，要么定位，要么死亡。能否分享分享一下深鉴科技寻找自己利基市场过程中的经验或者教训？ 举个例子。最开始（ 2016 年的时候），我们做过一些机器人跟无人机的生意。当时分析这个行业，发现有一些很好的地方。 产品做稳定是特别难的一个事，降成本也是一件挺难的事儿。但是，我们发现机器人和无人机行业对成本不是那么敏感，因为产品单价至少都在几千、几万元。至于稳定性，跑个几十分钟就要换电池了，也不需要特别稳定。对新技术实践来说，这些都挺友好。 但是做了后，我们发现这个市场好难做。比如，无人机有 100 万架的量，结果 70 多万都是大疆的，剩下的其他公司都特别少，跟大家合作起来，都起不了太大的量。 所以，我觉得在考虑行业的时候，必须要考虑短期内能否切得进去，这是很重要的一点。除了短期内收益是多少，也要长远地看这个市场规模是多少。 不同行业和公司对采用创业公司产品的看法，可能会不一样。大公司就特别在意供货的稳定性，一旦供货，大公司通常会要求对方保证至少连续五年供货。比如，赛灵思推出某一款 FPGA 芯片，会承诺这个芯片会量产到 2025 年。他也必须这样说，大家才会持续不断用他的产品。 最注重这个的是汽车行业。车厂特别注重：对方必须稳定地给车厂供货几年，车厂跟 Tier 1 和汽车电子供应商都是几十年的合作关系，这是一个信任度的问题。 选择安防领域，也是考虑到它是一个特别长尾的市场，市场中的中小型公司对采用创业公司的产品不会在意那么多。 对于中国来说，AI 芯片真是一次实现弯道超车的机会吗？ 我觉得这是一次换道超车的机会。 对大家来说，这会是一个非常大的新挑战，战略和格局层面的挑战。 过去，我们都是跟在后面拼命追。无论是 CPU、GPU 还是 FPGA，国产走的都是这样一条路，追得特别费力。硬件的东西不会开源，开源来的代码，你也读不懂，只能自己踩着坑前进，很难追得上。 不过，有一个成功的传统例子，京东方。大家突然发现他全球领先了，为什么？因为他直接砸钱布局几年后才会量产的下一代工艺，比如柔性显示。当国际上做柔性显示的时候，他在下一代工艺上就已经国际领先了。 这跟 AI 有点像。虽然咱们整个人才积累和工业底子还是比国外的薄，但从视野角度来说，这个事情确实是超前国外的。比如，2012 年我们做研究时，全球都没什么人看重这个事情。哪怕到了2016 年，全球也没有什么人看重。2015 年底、2016 年时，也没有什么人真开始创业做。 我们比别人做得早，而且一直在投入，所以我觉得有可能超过去。弯道超车和换道超车，我觉得是完全不同的思路，但我觉得在 AI 的这个事情上，芯片这个行业确实有可能超越国外。 有投资人曾经说，在中国做创业和投资，一定要做长尾市场，你怎么看？ 我觉得，创业公司的切入肯定是以长尾市场为主。 这其实是品牌的交锋。淘宝最初建立时，吸引的是那些晚上有业余时间，希望开店赚点小外快的人，大品牌不会去淘宝上开店。有了足够多的人群后，一些小品牌开始在上面设店面，淘宝又做天猫，越来越多的大品牌开始出现在淘宝平台上。 所以，一家创业公司早期是没有品牌的，最多是靠投资方的一些新闻加一些背书，形成一个还不错的品牌，但这个品牌和海康这样的品牌完全不是一回事。因此，一开始很难直接给最大的公司服务，哪怕宣传中说有一些服务，但事实上很难直接从他们那里拿到钱。 所以，一定都是小的做起，然后逐步往大的打。但是，一定要有一颗要做到最好的心。 8位AI行业局内人讲述对过去、对未来的看法 点击下方，阅读更多             "
223,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738093&idx=3&sn=38ba91b09e5bed86d8fb07f5df756ec2&chksm=871ac9d3b06d40c5ecad19dac5fcbb58bed4b06dda1d3fe03b4a3fcbb9b66d6852ca25979300&scene=27,业界 | 百度人机交互新研究：仅用少量样本生成高质量多说话者语音,语音复制（voice cloning）是个性化语音接口的非常急需的功能。在此论文中，百度介绍了一种能以少量音频样本作为输入的神经语音复制系统。 在百度研究院，我们的目标是用最新的人工智能技术革新人机交互界面。我们的  在一年前启动，致力于教会机器从文本生成更加类人的语音。 通过超越单个说话者语音合成的局限，我们证明了单个系统可以学习生成几千个说话者身份，每个说话者只需要少于半小时的训练数据。我们通过在说话者之间学习共享的和区分的信息获得了这种性能。 我们希望走得更远，并尝试从仅仅少量的话语（即，仅有数秒持续时间的句子）中学习说话者特征。这个问题通常称为「语音复制」（voice cloning）。语音复制有望在人机交互界面的个性化方向中得到重要的应用。 在这项研究中，我们聚焦于两种基本的方法，以解决语音复制的问题：说话者适应（speaker adaptation）和说话者编码（speaker encoding），这两种技术都可以通过 应用于一个多说话者生成语音模型，而不会降低语音质量。关于语音的自然性以及它和原始说话者的相似性，这两种方法都可以获得很好的性能，即使只有少量的复制音频。 复制语音样本的地址：https://audiodemos.github.io./ 说话者适应基于用少量复制样本微调一个多说话者生成模型，使用基于反向传播的优化方法。适应机制可以应用于整个模型，或仅应用于低维说话者嵌入向量。后者可以用非常少的参数数量表示每个说话者，尽管它需要更长的复制时间，且音频质量也更低。 说话者编码基于训练一个独立的模型以直接从复制音频中推理出一个新的说话者嵌入向量，然后应用于多说话者生成模型。说话者编码模型有一个时域和频域处理模块以从每个音频样本中恢复说话者的身份信息，还有一个注意力模块以将它们以最优的方式进行组合。说话者编码的优势包括快速的复制时间（只需要数秒）以及只需要少量的参数数量表示每个说话者，使其更适用于资源稀缺部署的场景。除了准确得评估说话者嵌入之外，我们还观察到说话者编码器可以用有意义的方式将不同的说话者映射到嵌入空间中。例如，将来自不同区域口音或不同性别的说话者聚集到一起，这是通过在学习到的隐空间中转换说话者的性别或口音而得到的。我们的结果（见复制语音样本的地址）表明，这种方法在为说话者生成语音和变换说话者特征时非常有效。 关于语音复制的更多细节，请参见论文。 论文：Neural Voice Cloning with a Few Samples  论文链接：https://arxiv.org/pdf/1802.06006.pdf 摘要： 语音复制（voice cloning）是个性化语音接口的非常急需的功能。基于神经网络的语音合成已被证明可以为大量的说话者生成高质量的语音。在本文中，我们引入了一种神经语音复制系统，其以少量音频样本作为输入。我们研究了两种方法：说话者适应（speaker adaptation）和说话者编码（speaker encoding）。说话者适应基于用少量复制样本微调一个多说话者生成模型。说话者编码基于训练一个独立模型以直接从复制音频中推理出一个新的说话者，然后应用于多说话者生成模型。关于语音的自然性以及它和原始说话者的相似性，这两种方法都可以获得很好的性能，即使只有少量的复制音频。虽然说话者适应方法可以达到更好的自然性和相似性，但是说话者编码方法的复制时间和内存占用显著更少，使其更适用于资源稀缺部署的场景。 原文链接：http://research.baidu.com/neural-voice-cloning-samples/ 
224,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738093&idx=1&sn=f8615a55f35f76540ca85bb65010f0f6&chksm=871ac9d3b06d40c5ad30ed58a43316873cdc73e89f05448cb541afe87f477598db3ab07df3d7&scene=27,波士顿动力放出新视频：谁都挡不住机器狗开你的门,Matt Simon 还记得那只像狗一样的机器人 SpotMini 吗？上周波士顿动力发布了 ，视频中这只「机器狗」可以给同伴开门了！刚刚，波士顿动力发布了一则新视频，一个人拿着一根曲棍球棒努力阻止机器人开门，比如拖拽，机器人开始不停地挣扎。但是人类的阻止并没有用。这只机器狗赢了，成功地出门。 视频中最让人印象审核的细节是：这只机器人几乎是完全自主地做出这些行为，至少根据视频的描述来看是这样。波士顿动力是一家出了名的嘴紧的公司，因此这则视频中仅有的几幅场景已经相当于金矿一样了。这则视频描述了一名人类操作员把机器人赶到门前，然后命令它开门。SpotMini 抓住门把手，人试图关门，它支撑住自己的身体，然后更大力地开门。人拽住它背后的绳子，大力向后拽，但是机器人来回晃动，并最终挣脱，这完全出于它自己的意愿。 机器人能够摆脱外力的错误引导，一直努力完成任务。正如视频名称所述，波士顿动力在「测试稳健性」。即，机器人处理不同环境的能力。让机器人不摔倒已经够难了，但与人类「搏斗」，并若无其事地继续完成自己的事更难。 现在，我们无法确定 SpotMini 的自主性如何。人类可能仍然在用操纵杆控制它，但是机器人真的可以自主完成这些吗？「我认为可能是的，因为事实上遥控指挥机器人做出这样的行为是非常具有挑战性的。」Built Robotics 创始人和 CEO Noah Ready-Campbell 如此说道，「毫无疑问，这非常棒！」 如果你还在寻求安慰，想一想 SpotMini 的自主能力可能大受限制。人类仍然擅长做人类的事情，比如规划（让机器人来到门前），而机器越来越擅长做重复的任务（如开门）。事实上已经有大量机器人在自然环境下与人类合作：比如，安保机器人，担任人类保安的眼睛和耳朵，以及在酒店和医院运送食物的机器人。但是在那些情景中，人类起重要作用。机器人无法自己到处走动，这导致召唤中心的大幅增长，在那里遇到困难的机器人可以得到人类遥控操作员的帮助。 所以除了开门、保持自我平衡，SpotMini 还能做什么？它可以走出这栋楼然后在其他楼里找到特定的一间屋子吗？Ready-Campbell 说，「我有所怀疑。它可能遇到各种各样的障碍，例如上楼梯、不同形状的门、各种各样的东西，它极可能会发生故障。」 说到用曲棍球棒戳机器人，波士顿动力似乎已经透露过。2016 年，该公司放出一则视频，视频中一个人用曲棍球棒欺负人形机器人 Atlas。对此，有些人感到不适，觉得视频中的人太烦人了，机器人太辛苦了。所以这一次，波士顿动力在视频中明确注明：「注意：该测试并未激怒或者伤害机器人。） 怎么使用 SpotMini 还不清楚，但值得注意的是波士顿动力之前开发的机器人 BigDog 是用于军用负重（后来因噪声太大而被拒用）。同时还值得注意的是，该公司从未像现在这样密集地放出视频，也几乎从未像现在这样在视频描述中给出这么多信息。可能它准备好向市场发布机器人产品了？ 时间会证明一切。但在你害怕机器人破门而入之前，请记得机器人是为了帮助人类，无论我们用曲棍球棒怎么击打他们。可能为它们敞开大门，更为明智。 原文链接：https://www.wired.com/story/watch-a-human-try-to-fight-off-that-door-opening-robot-dog/ 
225,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738083&idx=3&sn=f57af7f2f36dbe100850dbfe56428e81&chksm=871ac9ddb06d40cb9c52902e79277def16ae370f55beccae169d549cffc83af8b112df3d7ba7&scene=27,入门 | 一文看懂卷积神经网络,Medium   本文主要介绍了神经网络中的卷积神经网络，适合初学者阅读。 深度学习和人工智能是 2016 年的热词；2017 年，这两个词愈发火热，但也更加容易混淆。我们将深入深度学习的核心，也就是神经网络。大多数神经网络的变体是难以理解的，并且它们的底层结构组件使得它们在理论上和图形上是一样的。 下图展示了最流行的神经网络变体，可参考这篇博客 (http://www.asimovinstitute.org/neural-network-zoo/)。 本文介绍卷积神经网络（CNN）。在开始之前，我们首先了解一下感知机。神经网络是一些被称作感知机的单元的集合，感知机是二元线性分类器。 如上图所示，输入 x1 和 x2 分别和各自的权重 w1 和 w2 相乘、求和，所以函数 f=x1*w1+x2*w2+b（偏置项，可以选择性地添加）。函数 f 可以是任意的运算，但是对于感知机而言通常是求和。函数 f 随后会通过一个激活函数来评估，该激活函数能够实现期望分类。Sigmoid 函数是用于二元分类的最常见的激活函数。如果您想进一步了解感知机，推荐阅读这篇文章（https://appliedgo.net/perceptron/）。 如果我们把多个输入堆叠在一起，并且使用函数 f 将其与位于另一层的多个堆叠在一起的单元连接在一起，这就形成了多个全连接的感知机，这些单元（隐藏层）的输出成为最后一个单元的输入，再通过函数 f 和激活函数得到最终的分类。如下图所示，这个就是最简单的神经网络。 神经网络有一个独特的能力，被称作「泛逼近函数」（Universal Approximation function），所以神经网络的拓扑和结构变体是很多样化的。这本身就是一个很大的话题，Michael Nielsen 在文章中做了详细的描述（http://neuralnetworksanddeeplearning.com/chap4.html）。读完这个我们可以相信：神经网络可以模拟任何函数，不管它是多么的复杂。上面提到的神经网络也被称为前馈神经网络（FFNN），因为信息流是单向、无环的。现在我们已经理解了感知机和前馈神经网络的基本知识，我们可以想象，数百个输入连接到数个这样的隐藏层会形成一个复杂的神经网络，通常被称为深度神经网络或者深度前馈神经网络。 那么深度神经网络和卷积神经网络有什么不同呢？让我们来探讨一下。 CNN 由于被应用在 ImageNet 等竞赛中而广受欢迎，最近也被应用在自然语言处理和语音识别中。需要记住的关键点是，其他的变体，如 RNN、LSTM、GRU 等，基于和 CNN 类似的结构，不过架构存在一些差异。 CNN 由三种不同的层组成，即「卷积层」、「池化层」、「密集层或全连接层」。我们之前的神经网络都是典型的全连接层神经网络。如果想了解更多卷积和池化层的知识，可以阅读 Andrej Karpathy 的解释（https://cs231n.github.io/convolutional-networks/）。现在继续我们关于层的讨论，下面我们来看一下卷积层。 （在下面的内容里，我们会以图像分类为例来理解卷积神经网络，后面再转移到自然语言处理和视频任务中。） 卷积层：假设一张图像有 5*5 个像素，1 代表白，0 代表黑，这幅图像被视为 5*5 的单色图像。现在用一个由随机地 0 和 1 组成的 3*3 矩阵去和图像中的子区域做乘法，每次迭代移动一个像素，这样该乘法会得到一个新的 3*3 的矩阵。下面的动图展示了这个过程。 上述的 3*3 的矩阵被称作「滤波器」，它的任务是提取图像特征，它使用「优化算法」来决定 3*3 矩阵中具体的 0 和 1。我们在神经网络的卷积层中使用好几个这样的滤波器来提取多个特征。3*3 矩阵的每一个单个步骤被称作「步幅」（stride）。 下图展示了使用两个三通道滤波器从三通道（RGB）图像中生成两个卷积输出的详细过程。 滤波器 w0 和 w1 是「卷积」，输出是提取到的特征，包含这些滤波器的层叫做卷积层。 池化层：这个层主要使用不同的函数为输入降维。通常，最大池化层（max-pooling layer）出现在卷积层之后。池化层使用 2*2 的矩阵，以卷积层相同的方式处理图像，不过它是给图像本身降维。下面分别是使用「最大池化」和「平均池化」的示例。 全连接层：这个层是位于之前一层和激活函数之间的全连接层。它和之前讨论过的简单「神经网络」是类似的。 注意：卷积神经网络结果也会使用正则化层，不过本文将分开讨论。此外，池化层会损失信息，所以也不是首选的。通常的做法是在卷机层中使用一个较大的步幅。 ILSVRC 2014 的亚军 VGGNet 是一个流行的卷积神经网络，它使用 16 个层来帮助我们理解 CNN 中深度的重要性，AlexNet 是 ILSVRC 2012 的冠军，它只有 8 层。Keras 中有可以直接使用的模型 VGG-16。 在 Keras 中加载了这个模型之后，我们可以观察每一层的「output shape」来理解张量维度，观察「Param#」来了解如何计算参数来得到卷积特征。「Param#」是每一次获取卷积特征时的所有权重更新。 现在我们已经熟悉了卷积神经网络的结构，理解了每一层都是如何运行的，那么我们可以进一步去理解它是如何用在自然语言处理和视频处理中的了。您可以在这个链接中了解自 2012 年以来所有获得 ImageNet 竞赛冠军的 CNN 模型（https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html）。 
226,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738083&idx=4&sn=5de32e1960cdad0d60edb7a8f537b821&chksm=871ac9ddb06d40cb715aac7065b8d5711c9ab4fed8cb3e10e88a36e357b38a71db34dbc7a12f&scene=27,深度 | 论文解读：神经网络修剪最新研究进展,"inference 本文解读了两篇近期关于神经网络修剪的论文，分别是 L_0 正则化方法和 Fisher 修剪方法。作者对两种方法的工作机制进行了精简的总结和解释，可以帮助我们快速领会论文的方法论思想。 我想简单介绍近期两篇关于神经网络修剪的论文（免责声明，其中一篇是我们自己的论文）： Christos Louizos, Max Welling, Diederik P. Kingma (2018) 《Learning Sparse Neural Networks through L0Regularization》 Lucas Theis, Iryna Korshunova, Alykhan Tejani, Ferenc Huszár (2018) 《Faster gaze prediction with dense networks and Fisher pruning》 标题中所写的「修剪」通常是指减少或控制非零参数的数量或神经网络中频繁使用的特征图数量。在较高的层次上，至少有三种方法可以做到这一点，而修剪只是其中的方法之一： 正则化（regularization）：修改目标函数/学习问题，所以优化后可能会得到一个参数较少的神经网络。参见 Louizos et al, (2018) 修剪（pruning）：面向大规模神经网络，并删除某些意义上冗余的特征或参数。参见 (Theis et al, 2018) 增长（growing）：虽然这一方法传播得不够广泛，但是也可以采取这第三种方法，从小型网络开始，按某种增长标准逐步增加新的单元。参见「学界 | 为数据集自动生成神经网络：普林斯顿大学提出 NeST」 对网络进行修剪有不同的原因。最显然的理由是，修剪能在保持相同性能的前提下降低计算成本。删除那些在深度网络结构中不真正使用的特征可以加速推断和训练的过程。你也可以认为，修剪是一种架构搜索的形式：找出每层需要多少特征才能获取最佳性能。 第二种观点是，通过减少参数数量可以减少参数空间的冗余，从而提高泛化能力。正如我们在对深度网络泛化的近期研究中所看到的，参数的原始数量（L_0 范数）实际上并不是泛化能力的充分预测因子。也就是说，我们凭借经验发现修剪网络有助于泛化。同时，社区也正在不断开发（或者，可能在某些情况下是重新发现）新的参数相关量对泛化进行预测和描述。Fisher-Rao 范数就是一个很好的例子。有趣的是，Fisher 修剪 (Theis et al, 2018) 和 Fisher-Rao 范数有着密切的联系，这可能暗示了修剪、参数冗余性和泛化能力深层次的关联（参见「深度 | 可视化线性修正网络：看 Fisher-Rao 范数与泛化之间的关系」）。 我认为 Christos Louizos 等人关于 L_0 的论文（第一篇）非常有意思，它可以被视作我几个月前在 machine learning cookbook 中提到的机器学习问题转化的直接应用。这是一个很好的例子，它可以说明如何通过这些一般性的思想，将棘手的机器学习优化问题转化为可执行的 SGD 过程。 因此，我会把该论文总结为一系列的步骤，每个步骤都会对优化问题作出一些推进： 1. 从可能难以优化的理想损失函数入手：通常训练损失加上参数的 L_0 范数，进行线性组合。L_0 范数简单地计算向量中的非零项，是一个不可微的分段常值函数。这是一个困难的组合优化问题。 2. 应用变分优化将不可微函数变成可微函数。这通常需要引入关于参数 θ 的概率分布 p_ψ(θ)。即使目标函数对任何 θ 都不可微，平均损失函数 p_ψ 也可能关于 ψ 可微。为了找到最优的 ψ，通常可以使用强化梯度估计，这也就导致了进化策略的应用。但是，进化策略一般方差很高，因此我们需要进行下一步。 3. 对 p_ψ 应用参数重设技巧构建一个低方差梯度估计器。但是，这只适用于连续变量。为了处理离散变量，我们需要执行第 4 步。 4. 应用 concrete relaxation，通过连续近似的方法逼近离散随机变量。现在，我们就有了一个低方差梯度估计器（相比于强化梯度估计而言），可以通过反向传播和简单的蒙特卡罗采样进行计算。你可以将这些梯度用于 SGD（Adam），这也就是论文中所做的工作。 有趣的是，其中并没有提到 Eq.(3) 和进化策略或变分优化之间的关联。取而代之，其中提到了与基于奖励机制的 spike-and-slab prior 的不同关联。我建议可以带着这种相关性思想阅读这篇论文。 作者接下来表明，这的确能起作用，并且与其他用于减少参数数量的方法相比更具优势。 根据上述步骤，你可以顺着从一个问题向另一个问题转换的思路来领会本文，这能让你对这个方法进行推广和改进。例如，REBAR 或 RELAX 梯度估计器能为 concrete relaxation 提供一个无偏、低方差的替代方案，可能也能获得不错的性能。 Fisher 修剪 我想谈的第二篇文章来自我们自己的实验室。该论文更加注重建立快速神经网络以预测图像显著性的具体应用，而不是一个纯粹的方法类论文。修剪后的网络能捕捉 Twitter 裁剪照片背后的特征关联。 我们的目标也是降低网络中的计算成本，特别是迁移学习设置中的计算成本：如果在预训练过的神经网络上进行构建，你将同时继承解决原始的源任务所需的大量复杂性，这其中大部分对于解决目标任务而言是多余的。我们的高级修剪目标与上一篇论文存在区别：与 L_0 范数或组稀疏度不同的是，我们使用了一个略显复杂的公式直接估计该方法正向传播所需的时间。这是相邻层参数数量的二次函数。有趣的是，这将导致网络架构倾向于交替堆叠厚层和薄层，如下所示： 通过一次减少一个卷积特征图，我们贪婪地修剪训练好的网络。选择下一个需要修剪的特征图有一个重要原则，即最小化由此带来的训练损失函数值的增加。从这个标准出发，对损失函数进行二阶泰勒展开，并添加更多的假设，我们会得到以下保持参数 θ_i 的修剪信号： 其中，F_i 表示 Fisher 信息矩阵的第 i 个对角项。上面的公式仅处理单一参数，但是我们可以通过推广得到移除整个特征图的方法。修剪过程即每次迭代中去除具有最小 Δ 的参数或特征图，此外，在迭代步骤之间要对网络进行重新训练。如果你想了解更多详情，请参阅论文。 除了论文中提到的内容之外，我还想指出一些 Fisher 修剪和之前在博客中提到的想法之间的一些联系。 Fisher-Rao 范数 首先，Fisher 修剪与 Fisher-Rao 范数之间存在一些联系。让我们首先假设 Fisher 信息是对角的——理论上来说这是一个过于简化且不合理的假设，但实际上这种简化能获得很有帮助的算法。在上述假设下，θ 的 Fisher-Rao 范数变成下面的形式： 写成这种形式，你就很容易意识到 FR 范数和 Fisher 修剪标准之间的联系。根据所使用的 Fisher 信息的特定定义，你可以将 FR 范数近似地解释如下： 移除一个随机参数后，训练对数似然度（经验 Fisher 信息）的期望值下降 移除一个参数后，模型（模型 Fisher 信息）定义的条件分布的近似变化 在现实世界中，Fisher 信息矩阵并不是对角的，这实际上是理解泛化的一个重要方面。一方面，只考虑对角元素使得 Fisher 修剪对网络的某些参数重设（非对角雅克比矩阵）是敏感的。不过，Fisher-Rao 范数和参数冗余之间可能还有着更深的联系，尚有待发掘。 弹性权重固化 利用 Fisher 对角信息值指导修剪也和弹性权重固化（Elastic Weight Consolidation）有相似之处（Kirkpatrick et al, 2017）。在弹性权重固化（EWC）中，Fisher 信息值用于确定哪些权重对于源任务而言比较重要。在 EWC 中，该算法是从贝叶斯在线学习的角度派生而来的，不过你也可以从泰勒展开的角度推导它，就像 Fisher 修剪那样。 我用「共享硬盘」的比喻来理解和解释 EWC。（注意：就如同所有的比喻一样，这可能完全没有意义。）神经网络的参数就像是一个硬盘或某种存储容器。在某一任务上训练神经网络需要对训练数据进行压缩，并将它们保存到硬盘当中。如果你没有一个防止数据重写的机制，数据就很可能被覆盖：在神经网络中，灾难性遗忘也以相同的方式发生。EWC 就像是多个用户之间共享硬盘的协议，用户不能相互覆盖彼此的数据。Fisher 信息值可以被视为一个软性的「不可覆盖」标志。在对第一个任务进行神经网络训练之后，我们可以计算 Fisher 信息值，该值能显示哪些参数存储了有关该任务的关键信息。Fisher 值较低的参数是冗余的，它们可以被覆盖然后重新用来存储新的信息。在这个比喻当中，Fisher 信息值的总和可以衡量硬盘驱动器满载的程度，而修剪就像是抛弃实际上不用于存储任何东西的硬盘驱动部分。 本文中，我提到近期的两种方法，它们通过计算应该去掉哪些参数或特征自动学习（修改）神经网络架构。在我看来，方法和文章本身都很有趣。L_0 方法更像是一个简单的优化算法，它可能比 Fisher 修剪中每次去除一个特征的迭代方案更可取。但是，如果你从一个很大的预训练模型开始，想进行迁移学习的设置，Fisher 修剪可能更加适用。 原文链接：http://www.inference.vc/pruning-neural-networks-two-recent-papers/ "
227,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738083&idx=1&sn=b2582e000f7226bea9b42b77b532a53d&chksm=871ac9ddb06d40cbcbb5f10855efaa426de6dfd67557d6b12d9a986cad85996792f79222de74&scene=27,超越Adam，从适应性学习率家族出发解读ICLR 2018高分论文,"最近，ICLR 2018 高分论文讨论了 Adam 等适应性学习率算法的收敛性缺点，并提出了一种新的 Adam 变体。为此，我们从 AdaGrad 开始，依次分析了 AdaDelta、RMSProp 和 Adam 等适应性学习率算法家族，并在最后结合该 ICLR 2018 高分论文讨论 Adam 的非收敛性和修正的方法。 随机梯度下降是当前训练深度网络的主流方法，该方法通过在小批量数据上计算损失函数的梯度而迭代地更新权重与偏置项。特别的，SGD 的一类变体通过使用历史梯度某种形式的范数而调整学习率取得了很大的成功，因为它们能针对不同的参数采用不同的学习率。一般来说，适应性学习率算法的基本思想是若损失函数对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。这一类算法第一个比较流行的是 AdaGrad（Duchi et al., 2011; McMahan & Streeter, 2010），该算法要比一般的 SGD 算法在性能上有显著性的提升，尤其是当梯度比较稀疏或比较小的情况下。 尽管 AdaGrad 在稀疏梯度的情况下工作良好，但由于在更新中使用了所有的历史梯度信息，所以该算法在损失函数非凸和梯度比较密集的情况下会引起学习率的快速衰减。此外，模型的高维空间更进一步加剧了这种缺点，因此 AdaGrad 的性能也会受到很大的影响。为了解决这种问题，目前已经有许多研究者提出了 AdaGrad 方法的变体，如 RMSPROP（Tieleman & Hinton, 2012）、Adam（Kingma & Ba, 2015）、AdaDelta（Zeiler, 2012）和 NAdam（Dozat, 2016）等。它们都使用历史梯度平方的指数移动均值来缓解学习率过快衰减的现象，这基本上就限制更新会更多地依赖于过去几次迭代的梯度信息。 虽然这些算法能成功地应用于一些实践和开发中，但它们在一些环境下并不会起作用。例如我们通常会观察到有一些小批量数据会提供较大的梯度，虽然这种批量非常少，但这些较大的梯度会提供非常多的下降信息，它们在指数移动均值中会存在和影响很长一段时间，因此也就造成了算法收敛到一个比较差的最优解。 为了理解和分析这种局限性，Sashank J. Reddi、Satyen Kale 和 Sanjiv Kumar 等人发表了一篇 ICLR 2018 论文《ON THE CONVERGENCE OF ADAM AND BEYOND》。该论文被接收为 ICLR 2018 的 Oral 论文，且最终双盲评分一直在前几名。为了对该论文做一些简要的分析与讨论，我们会从基本的适应性学习率算法开始，然后再进一步讨论它们的局限性与修正方法。 其实适应性学习率方法早在 80 年代就有学者进行了一定的研究，Delta-bar-delta（Jacobs, 1988）这一启发式方法基于很简单的想法，即如果损失函数对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。如果损失函数对于该参数的偏导变换了符号，那么学习率就应该减少。这种学习算法只能适应于全批量梯度下降，而后 AdaGrad（Duchi et al., 2011）第一次将适应性学习率算法的性能提升到顶尖的水平，并广泛应用于带有稀疏梯度的模型。 AdaGrad 亦称为适应性梯度（Adaptive Gradient），它允许学习率基于参数进行调整，而不需要在学习过程中人为调整学习率。AdaGrad 对具有较大梯度的参数相应地有一个快速下降的过程，而具有小梯度的参数在学习率上有相对较小的下降速度。因此，AdaGrad 成了稀疏数据如图像识别和 NLP 的天然选择。然而 AdaGrad 的最大问题在于，某些案例中的学习率会变得太小，学习率的单调下降也会使得网络停止学习过程。在经典的动量算法和 Nesterov 中，加速梯度参数更新是对所有参数进行的，并且学习过程中的学习率保持不变。在 Adagrad 中，每次迭代中每个参数使用的都是不同的学习率。以下是 AdaGrad 的参数更新式： 如下伪代码所示，AdaGrad 首先抽取 m 个训练样本和对应的样本标注，然后基于这些样本计算损失函数对特定参数的梯度。将该梯度的平方加入历史梯度项 r，然后再以 1/sqrt(r) 缩放学习率，从而根据历史梯度的大小为每个参数定制学习率，其中 δ 是为防止分母为零的常量。 AdaGrad 算法最大的特点是将历史梯度 L2 范数的倒数作为缩放学习率的因子。因此从直观上来说，一直拥有较大梯度的参数处于较陡的位置，它离最终收敛还有一段距离，因此我们可以选择较大的学习率而加快梯度下降的速度。而对于较小的梯度，它的位置相对而言比较平坦，且很可能处于接近收敛的范围，因此我们需要减小学习率而更谨慎地搜索最优解。 AdaDelta AdaDelta 使用短期历史梯度的信息而缩放学习率，它类似经典的动量算法累积历史的更新而加速学习。AdaDelta 可以有效克服 AdaGrad 的学习率过快减少至零，因为它将累积历史梯度信息的范围限制在固定窗口 w 内，从而不再采用动量算法累积所有历史梯度的做法。在时间 t 计算的 E[g^2](t) 依赖于历史梯度的信息和当前的梯度值。因此，该移动平均的计算可以表示为： 其中 γ 在实践中通常设为 0.9 左右。我们可以将这种移动平均的计算方法与随即梯度下降相结合，SGD 更新的表达式为： Adagrad 的更新为： 使用历史梯度的信息 E[g^2]_t 替换对角矩阵 G_i，得到 其中分母是梯度的平方根误差， 用  替换先前更新规则中的学习率 α，得到 如上是 AdaDelta 的更新方式及简单的推导。如以下伪代码，其描述了 AdaDelta 的详细计算与更新过程： 我们首先会初始化梯度与超参数，然后计算当前时间步某个参数的梯度，再如上 E[g^2] 所示的移动均值计算式获取短期历史梯度的信息。在利用历史梯度的移动均值情况下，我们可以根据短期梯度信息为每个参数设计学习率。但正如论文 ON THE CONVERGENCE OF ADAM AND BEYOND 所述，这种方法可能在某些情况下只能收敛到次优点。 RMSProp RMSProp 算法（Hinton，2012）修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。RMSProp 与 Adadelta 的移动均值更新方式十分相似： RMSProp 的更新规则如下： 在 RMSProp 中，学习率需要除以梯度平方的指数衰减平均值以为不同的参数缩放学习率。 RMSProp 的标准式如下所示： 如上为 RMSProp 的标准算法，首先我们需要初始化全局学习率和控制移动平均长度范围的超参数 ρ。在算法的循环体中，首先需要采样一个批量的数据，并计算损失函数对所有参数的梯度而作为梯度向量。随后根据超参数 ρ 决定需要遗忘的历史梯度范围，即计算指数移动均值，并储存在变量 r 中。最后只需要利用历史梯度信息缩放对应的学习率就能完成梯度下降，在这些计算过程中，⊙表示对应元素之间的乘积。 我们同样可以结合 Nesterov 动量而提升经典 RMSProp 算法的效果： 如上所示，前面的初始化会多一个动量系数 α。在循环体内，我们抽取一个批量的数据会先计算临时的更新，也就是先在动量方向上更新一步。然后我们需要计算预更新后，该位置的梯度 g，并利用梯度 g 的平方与历史梯度更新新的历史梯度信息。在利用历史梯度信息缩放学习率后，我们就能更新 Nesterov 动量的速度变量，且利用该变量最终更新参数。 RMSProp 是 Hinton 在公开课上提出的最优化算法，其实它可以视为 AdaDelta 的特例。但实践证明 RMSProp 有非常好的性能，它目前在深度学习中有非常广泛的应用。 Adam 最后一种就是近来非常流行的 Adam 算法，虽然 ON THE CONVERGENCE OF ADAM AND BEYOND 重点分析了 Adam 的收敛性缺点，但仍然不能否认它是目前最高效和「自动化」的最优化方法之一。下面我们会根据 Adam 的原论文简要介绍该算法。 Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。 Adam 算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即： 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。 Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。具体来说，算法计算了梯度的指数移动均值（exponential moving average），超参数 beta1 和 beta2 控制了这些移动均值的衰减率。 移动均值的初始值和 beta1、beta2 值接近于 1（推荐值），因此矩估计的偏差接近于 0。该偏差通过首先计算带偏差的估计而后计算偏差修正后的估计而得到提升。 如下伪代码展示了 Adam 的基本算法： 如上算法所述，在确定了参数α、β1、β2 和随机目标函数 f(θ) 之后，我们需要初始化参数向量、一阶矩向量、二阶矩向量和时间步。然后当参数θ没有收敛时，循环迭代地更新各个部分。即时间步 t 加 1、更新目标函数在该时间步上对参数θ所求的梯度、更新偏差的一阶矩估计和二阶原始矩估计，再计算偏差修正的一阶矩估计和偏差修正的二阶矩估计，然后再用以上计算出来的值更新模型的参数θ。 该算法的核心是 m_t hat 除以 sqrt(v_t hat)，其实它可以看作一种信噪比，即算法对 m_t hat 是否符合真实梯度方向存在的不确定性。若该比值很小，表示 m_t hat 符合真实梯度方向有很大的不确定性，从而令有效步长接近于 0，令目标函数收敛。 上图伪代码为展现了 Adam 算法的基本步骤。假定 f(θ) 为噪声目标函数：即关于参数θ可微的随机标量函数。我们对怎样减少该函数的期望值比较感兴趣，即对于不同参数θ，f 的期望值 E[f(θ)]。其中 f1(θ), ..., , fT (θ) 表示在随后时间步 1, ..., T 上的随机函数值。这里的随机性来源于随机子样本（小批量）上的评估和固有的函数噪声。而  表示 f_t(θ) 关于θ的梯度，即在实践步骤 t 下 ft 对θ的偏导数向量。 该算法更新梯度的指数移动均值（m_t）和平方梯度（v_t），而参数 β_1、β_2 ∈ [0, 1) 控制了这些移动均值（moving average）指数衰减率。移动均值本身使用梯度的一阶矩（均值）和二阶原点矩（有偏方差）进行估计。然而因为这些移动均值初始化为 0 向量，所以矩估计值会偏差向 0，特别是在初始时间步中和衰减率非常小（即β接近于 1）的情况下是这样的。不过初始化偏差很容易抵消，因此我们可以得到偏差修正（bias-corrected）的估计 m_t hat 和 v_t hat。此外，m_t 和 v_t 可以通过数学归纳法求出对应的偏差修正项。 注意算法的效率可以通过改变计算顺序而得到提升，例如将伪代码最后三行循环语句替代为以下两个： 在这篇 ICLR 2018 的 Oral 论文中，研究者分析了这些基于指数移动均值方法的缺点，并严格地证明了少数较大梯度会影响收敛的效果。限制更新只依赖于少数历史梯度确实会引起显著的收敛性问题，该论文的作者也明确表明他们该论文的主要贡献为： 通过提供一个简单的凸优化问题作为案例来说明 RMSPROP 和 ADAM 中的指数移动均值会如何导致算法不收敛，即 RMSPROP 和 ADAM 在该案例中不会如同动量法那样收敛到相对最优解。该分析很容易扩展到其它使用指数移动均值的最优化方法，如 AdaDelta 和 NAdam。实际上，该分析足够灵活以扩展到更一般的情况，即那些根据固定窗口大小而采用平均梯度的算法，这些算法一般会采用窗口范围内的梯度信息而抛弃窗口外的梯度信息。但是该论文并没有讨论这些扩展情况，因而分析将变得更加明晰。 本论文的分析结果表明，为了保证收敛性，优化算法必须具有历史梯度的「长期记忆」。具体而言，作者们指出了 Kingma & Ba (2015) 关于 Adam 算法收敛性证明的一个问题。为了解决这个问题，他们提出了一个 Adam 的新变体，它依赖于历史梯度的长期记忆，且和原 Adam 算法有相同的计算时和空间要求。Sashank 等人最后基于 Kingma & Ba (2015) 的分析展示了该新变体的收敛性分析。 Sashank 等人还对该变体进行了初步的实证研究，并表明它在常见机器学习问题中有相似或更优秀的性能。 以下是详细的论文内容： 论文地址：https://openreview.net/pdf?id=ryQu7f- RZ 近来提出的几种随机优化方法已经成功地应用于深度网络的训练，如 RMSPROP、ADAM、ADADELTA 和 NADAM 等方法，它们都是基于使用前面迭代所产生梯度平方的指数滑动平均值，在对该滑动平均值取平方根后用于缩放当前梯度以更新权重。根据经验观察，这些算法有时并不能收敛到最优解（或非凸条件下的临界点）。研究者证明了导致这样问题的一个原因是这些算法中使用了指数滑动平均（exponential moving average）操作。本论文提供了一个简单的凸优化案例，其中 ADAM 方法并不能收敛到最优解。此外，研究者还描述了过去文献中分析 ADAM 算法所存在的精确问题。他们的分析表明，收敛问题可以通过赋予这些算法对前面梯度的「长期记忆」能力而得到解决。因此该论文提出了一种 ADAM 算法的新变体，其不仅解决了收敛问题，同时还提升了经验性能。 在论文的第二章节中，研究者们重点讨论了一般适应性方法和基于指数移动均值的适应性方法的表述： 一般适应性方法 原作者为适应性方法提供了一个通用性框架，它展示了不同适应性方法之间的区别，并有利于我们理解 Adam 等方法的缺陷。以下算法 1 给出了一般适应性框架的伪代码，该算法仍然非常抽象，因为「均值」函数φ_t 和 ψ_t 并没有明确指定。其中φ_t 为 d 维向量，而ψ_t 为 d*d 维正定矩阵。为了便于展示，作者将 α_t 设为下降步大小，将 α_t * V_t^(-1/2) 设为算法的学习率。此外，作者将算法 1 封装的适应性方法限制为对角方差矩阵，即 V_t = diag(v_t)。我们发现因为φ_t(g_1, . . . , g_t) = g_t 、 ψ_t(g_1, . . . , g_t) = I，标准的随即梯度下降会失效，其中对于所有的 t ∈ [T]，α_t = α/sqrt(t)。 尽管缩减下降步长是算法收敛的先决条件，但如此暴力的学习率衰减方式会典型地收敛到较差的解，因此它会有较差的经验性能。适应性方法的关键思想是选择适当的均值函数而实现优良的收敛性。例如推动新研究的第一个适应性学习率算法 Adagrad（Duchi et al., 2011）使用以下均值的函数： 这和 SGD 的α_t = α/sqrt(t) 不一样，Adagrad 使用更加优秀的衰减策略，即对于 j ∈ [d]，α/sqrt(∑sqre(g^2))。当梯度是稀疏的情况下，算法的收敛性能得到非常大的提升（Duchi et al. 2011），这在一些非稀疏的梯度设定中也能获得很大的提升。 基于指数移动均值的适应性方法 基于指数移动均值的 AdaGrad 变体在深度学习社区中非常流行，RMSProp、Adam、NAdam 和 AdaDelta 都是这种变体的先驱研究。它们和 AdaGrad 最大的区别就是使用指数移动均值作为函数ψ_t，而不是使用简单的均值函数。Adam 是最流行的一种变体，它使用的均值函数如下： 根据前面章节的问题设定，我们将在本章节讨论基于指数移动均值的 Adam 优化算法的基本缺点。该论文的研究者展示了 Adam 即使在简单的一维凸问题上都有可能不能收敛到最优解。这些不收敛的案例与（Kingma & Ba, 2015）声明的收敛性相矛盾，主要的问题在于以下表达式的数值： 该数值基本上衡量了自适应学习率的倒数相对于时间的变化趋势。一个关键点是，SGD 和 AdaGrad 对于所有 t ∈ [T] 都有Γ_t⪰0，这只是从 SGD 和 AdaGrad 的更新规则推导而出。特别的，这些算法的更新规则并不会导致学习率的增加，即「non-increasing」学习率。然而，这对于其它如 Adam 和 RMSProp 等基于指数移动均值的方法是不成立的，即对于 t ∈ [T]，Γ_t 可能是不确定的。研究者们证明了这种违反正定性的属性将导致 Adam 和 RMSProp 出现不期望的收敛情况。若我们考虑以下 F = [−1, 1] 的简单线性函数序列： 其中 C>2。对于这个函数序列，我们很容易了解到 x=-1 提供了最小的回退。以下结果表明，当 β_1 = 0 和 β_2 = 1/(1 + C 2 ) 时，Adam 收敛于 x = +1 的高度次优解。直观上的推理如下，该算法每三步会获取一个较大的梯度 C，其它两步则会观察到梯度-1 而导致错误的方向。较大的梯度 C 不能抵消这种影响，因为对于给定的 β_2 值，x 几乎只会以常量 C 来缩小，因此算法只能收敛到 1 而不是-1。 在该章节的后面部分和附录中，研究者对这一非收敛现象进行了更形式化和细致地分析。 在这一章节中，Sashank 等研究者开发了一种新型指数移动均值变体，并提供了它的收敛性分析。他们的目标是设计一种保证收敛性的新策略，且能同时保留 Adam 和 AMSProp 的优势。为了更好地理解这种算法，我们需要重新审视表达式（2）中的 Γ_t。对于 Adam 和 RMSProp，该变量的数值仍然可能为负。Adam 原论文错误地假设 Γ_t 为半正定，这一错误在该论文的附录 D 有详细讨论。首先，研究者会修正这些算法以满足额外的约束。随后，我们会探索新的方法以令 Γ_t 在给定随时间 t 而改变 β_1 和 β_2 的情况下为半正定。 与 Adam 相比，AMSGrad 使用较小的学习率，但只要Γ_t 为半正定，那么它就保留了缓慢减弱历史梯度对学习率的影响这一直观属性。算法 2 展示了 AMSGrad 的伪代码，它与 Adam 的关键不同点就是它会保留所有 v_t 的最大值到当前时间步，并使用该最大值归一化梯度的移动均值，而不是使用 Adam 中的 v_t。通过这样的修正，AMSGrad 可以保持非递增的下降步大小，并避免了 Adam 和 RMSProp 的缺陷，即对于常数 β_2 和所有 t ∈ [T] 都有 Γ_t⪰0。此外在算法 2 中，实践上会典型地使用常数 β_1t，而实际证明要求一个递减的方案来保证收敛性。 实验 "
228,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738018&idx=3&sn=6df76bbe18d99d65ccb94fcaf178bd9d&chksm=871ac91cb06d400a0cae8db1538f62379226e7a2a876d0a4e8ce885ae1619bf7583f90ea5b00&scene=27,活动 | 深度学习大讲堂推出免费AI课程,课程视频地址： http://study.163.com/provider/400000000374020/course.htm 2017 年的年度热词排行榜第一名，一定是「人工智能」。无论是互联网巨头、AI 创业公司，还是政府机构、行业公司，无不将 AI 视为提升自身核心竞争力的根本性战略。毋庸置疑，AI 将深刻改变人类社会生活、改变世界。 在这个正在被 AI 改变的时代，最贵的依然是人才。随着越来越多的行业+AI 产品的落地生根，人才缺口还会进一步加剧。想要成为这个 AI 时代的一员，从知识武装自己开始。 深度学习大讲堂，携手中科视拓、中科院计算所 VIPL 研究组，共同制作了这份诚意满满的深度学习视频课程，内容由浅入深，从理论到实践，覆盖了深度学习数学基础、人工智能基础概念、深度学习基础概念、深度学习基本理论、卷积神经网络、深度目标检测技术、TensorFlow、MXNet、Dragon 深度学习框架。 本课程的内容来源于社区，服务于社区，全部 11 节课程均可以免费观看，所有讲义在网易云课堂都可下载。合作媒体：机器之心。 课程大纲 讲师介绍 如何观看 深度学习系列课程，已上线网易云课堂，观看方法如下： 2. 网易云课堂首页搜索     网易云课堂首页，搜索课程「深度学习大讲堂」，进入观看。 3. 浏览器打开链接    http://study.163.com/provider/400000000374020/course.htm 读者服务群 深度学习大讲堂课程团队正在招募 AI 爱好者，您将享有如下权益： 全部视频课程永久免费观看； 与深度学习领域大牛交流思想、碰撞火花； 有机会参与课程内容制作，获得奖励红包。 首期开放 50 个名额，请在「深度学习大讲堂」公众号留言，可以告诉我们您对深度学习的认识与困惑，留言须包括如下内容： 个人微信号+Github 地址 (可选)+联系电话； 工作单位 (城市+名称) 或就读学校 (名称)。 通过选拔后，我们将在 2 月 11 日 17:00 前给您回复。 此外，欢迎加入学习交流群。 深度学习基础课程 QQ 群：708785248 深度学习数学课程 QQ 群：707843700 让世上没有难学的 AI， 我们与您一起 定义未来。 
229,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738018&idx=2&sn=dfeb197e54391af4034120eceff8b38c&chksm=871ac91cb06d400ac8e760ba24dfe1e36ca61aec5ead57e9bd38e6702e8755fbf4ca82050e18&scene=27,深度 | 可视化线性修正网络：看Fisher-Rao范数与泛化之间的关系,"inFERENCe Ferenc Huszár  深度模型的泛化性能一直是研究的主题。最近，Twitter 的机器学习研究员 Ferenc Huszár 发表了一篇文章专门讨论泛化与 Fisher-Rao 范数之间的关系，它先分析了提出 Fisher-Rao 度量方法的论文，包括带偏置项和不带偏置项的分段线性网络的可视化，然后再讨论了其它如 Fisher-Rao 范数是否会成为有效的正则化器等问题。 在上周发布的关于泛化之谜的文章之后，有研究者向我介绍了最近将 Fisher-Rao 范数度量与泛化联系起来的工作： Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes (2017) Fisher-Rao Metric, Geometry, and Complexity of Neural Networks：https://arxiv.org/abs/1711.01530 合著者 Sasha Rakhlin 的这篇关于泛化的简短材料也是值得关注的，尽管我必须承认很多关于学习理论的引用都有所丢失。虽然我不够理解第四节中所描述的有界性证明，但我想我已经理解了大概，所以我将尝试在下面的部分总结要点。另外，我想补充一些图表，它们能帮助我理解作者所使用的受限模型和这种限制引起的「梯度结构」。 基于范数的容量控制 本文的主要观点与 Bartlett (1998) 的结果是一致的，他观察到在神经网络中，泛化与权重的大小有强相关，而与权重的数量没有多大关系。这个理论的基础是使用诸如权值衰减、甚至提前终止等技术，因为两者都可以被看作是保持神经网络权重向量有较小值的方法。根据一个神经网络权重向量的大小量级或范数而推理其泛化能力称为基于范数的容量控制。 Liang 等人（2017）的主要贡献是提出 Fisher-Rao 范数作为衡量网络权重有多大的指标，从而作为网络泛化能力的度量方法。它的定义如下： 其中 I 是 Fisher 信息矩阵： Fisher 信息矩阵有很多形式，因此 Fisher-Rao 范数的形式取决于期望的分布情况。经验样本 x 和 y 都来自经验数据分布。模型从数据中抽取样本 x，若假定损失是概率模型的对数损失，那么我们可以从这个模型中抽样 y。 重要的是，Fisher-Rao 范数是依赖于数据分布的（至少是 x 的分布）。它在参数重设时也是不变的，这意味着如果有两个参数 θ_1 和 θ_2 实现相同的功能，那么它们的 FR 范数就是相同的。最后，这是一个与平坦度相关的度量，因为 Fisher 信息矩阵在某些条件下逼近损失函数的 Hessian 矩阵。 本论文主要观点： 在不带偏置项的修正线性网络中，Fisher-Rao 范数有一个更容易计算和分析的等价形式。 以前提出的其它用于限制模型复杂性的范数可以用 Fisher-Rao 来界定。 对于没有修正的线性网络（实际上只是一个线性函数），Rademacher 复杂性可以用 FR 规范来界定，且边界不依赖于层数或每层单元的数量。这表明 Fisher-Rao 范数可能是泛化性能非常好的间接度量方法。 作者没有证明更一般模型的泛化边界，但是该论文基于与其它范数的关系提供了直观的论点。 作者还做了大量的实验来展示 FR 规范如何与泛化性能相关联。他们研究了一般 SGD 和 二阶随机法 K-FAC。他们研究了如果我们将标签随机地混合到训练中会发生什么，并发现最终解决方案的 FR 范数似乎能追踪到泛化差距。 这里仍然有一些未解决的问题，例如解释是什么具体使 SDG 选择更好的极小值，以及该极小值如何伴随着批量大小的增加而变化。 我认为能在这篇论文中扩展的地方，就是作者研究特定模型类型（不带偏置项的修正神经网络）的一些细节。这种限制最终保证了一些非常有趣的属性，而不伤害网络的经验性能（所以作者声明并在一定程度上证明了这一点）。 首先我们可以可视化具有偏置项的修正多层感知器的输出数据。我使用了 3 个隐藏层，每个层都有 15 个 ReLU 单元，并且使用了 PyTorch 默认的随机初始化。网络的输入是 2D 的，输出是 1D 的，所以我可以很容易的绘制梯度的等高面： 左图显示函数本身。它旁边的图分别显示了该损失函数对 x_1 和 x_2 的梯度。该函数是分段线性的（这很难观察，因为有很多的线性块），这意味着梯度是分段恒定的（这在视觉上更明显）。 f 的分段线性结构变得更加明显，我们在 f 本身的等高面（红—蓝）上叠加了梯度的等值线图（黑色）。 这些函数显然非常灵活，通过增加更多的层数，线性块的数量呈指数增长。重要的是，如果我把函数的输出作为 θ 的两个成员的函数绘制，保持 x 固定，上面的图看起来非常相似。 现在让我们看看当我们从网络中删除所有的偏置项仅保留权重矩阵时会发生什么： 哇，现在函数看起来很不一样，不是吗？在 x=0 时，它总是等于 0。它由楔形（或在更高维度，广义金字塔形）区域组成，在这个区域中函数是线性的，但每个楔形的斜率是不同的。然而表明仍然是连续的。让我们再来做一张叠加图： 从这些图中不太清楚，为什么像这样的函数能够模拟数据，以及为什么如果我们添加偏置项会得到更一般的分段线性函数。在高维上理解是有帮助的，因为高维度中两个随机采样的数据点落入同样的「pyramind」（即共享相同的线性区域）的概率是非常小的。除非你的数据有一定结构，使得对于很多数据点同时可能发生这种情况，否则我想你不需要担心。 此外，如果我的网络有三个输入维度，但是我只用两个维度 x_1 和 x_2 来编码数据并固定第三个坐标 x_3=1，我可以在我的输入上实现相同类型的功能。这被称为使用齐次坐标，而一个具有齐次坐标且不带偏置项的网络在其所能建模的函数方面，几乎与有偏置的网络一样强大。下面是一个使用齐次坐标时，不带偏置项的修正神经网络例子。 这是因为第三个变量 x_3=1 乘以它的权重实际上成为了第一个隐藏层的偏置。第二个观察结果是，我们可以将 f_θ (x) 作为特定层权重矩阵的函数，保持其它所有的权重和输入相同，函数的行为与输入是 x 时的行为完全相同。如果我把它绘制为一个权重矩阵的函数（即使权重矩阵很少是 2D 的，所以我不能真的把它绘制出来），在 f 中我们将观察到相同的辐射形状。 梯度结构 作者指出，这些函数满足以下公式： 此外，我认为这些都是上述等式成立的唯一连续函数，但我把它留给聪明的读者来证明或反驳。注意到网络输入和权重矩阵之间的对称性，可以建立一个关于参数 θ 的相似等式： 其中 L 是层级数。 为什么是这样的？ 以下是我的解释，与作者给出的简单证明略有不同。正如讨论的那样，一个通常的修正线性网络对于 x 是分段线性的。当我们改变 θ 时，线性分段的边界和斜率随之改变。若我们固定 θ，那么只要 x 和一些 x_0 落在相同的线性区域内，函数在 x 处的值等于它在 x_0 的泰勒展开式： 现在，如果我们不使用偏置项，所有的线段总是楔形的，并且它们在原点 x=0 处相遇。所以，我们可以把上述泰勒级数的极限作为 x_0→0 时的极限。（我们只能从技术上取极限，因为函数在 x=0 处不可微分。）因为 f_θ (0)=0，我们发现 就像我们想要的那样。现在将 l 层的权重 θ (l) 视为后续层级网络的输入，而将前一层的激活视为权重乘以这些输入，我们可以推导出由 θ (l) 表示的类似公式： 对所有层 l=1…L+1 应用此公式，并取平均值，我们得到： 我们根据隐藏层 L 加上输出层得到了 L+1 层。 Fisher-Rao 范数的表达式 使用上面的公式和链式法则，我们可以简化 Fisher-Rao 范数的表达式： 在这种形式中我们可以很清楚地看出，Fisher-Rao 范数只取决于函数 f_θ (x) 的输出和损失函数的性质。这意味着如果两个参数 θ_1 和 θ_2 实现相同的输入-输出函数 f，他们的 F-R 范数将是相同的。 我认为这篇文章对修正线性网络的几何结构提出了一个非常有趣的见解，并强调了几何学信息和基于范数的泛化之间的一些有趣的联系。 我认为目前缺少的是解释为什么 SGD 能够找到低 F-R 范数的解决方案，或一个解决方案的 F-R 范数是如何被 SGD 的批量大小影响的（如果有的话）。另一个缺少的是 F-R 范数是否能够成为一个有效的正则化器。似乎对于没有任何偏置参数的特定类型网络，模型的 F-R 范数可以相对便宜地计算并作为正则化项加入损失函数，因为我们已经计算了网络的前向传播。 "
230,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738028&idx=1&sn=0f1cc6d647d26c9de58e1aeb2eb4ddea&chksm=871ac912b06d40046a5729e3d9467842078ab4f1d9b74714f00e8fc4e09cd1943f69310bcf57&scene=27,Science组织了一场尖锐的Reddit问答，Yann LeCun的回答还是那么耿直,今日，AAAS 在 reddit 上组织了一场问答，谷歌、微软、Facebook 人工智能实验室的相关负责人一起回答了提问者的众多尖锐问题，话题包括量子计算、隐私、前沿研究方向、伪人工智能等。有趣的是，Yann LeCun 对量子计算与机器学习、伪人工智能等问题的回答相当耿直。机器之心对其中的部分问题进行了编译，感兴趣的读者可从文末链接查看所有讨论。 回答者： Yann LeCun，Facebook 人工智能研究中心首席科学家 Eric Horvitz，微软研究院负责人 Peter Norvig，谷歌研究总监 问题 1：你认为对下一代而言，哪些工作将会被人工智能取代，哪些工作是安全的？我是作为一位经常给学生提供就业选择建议的高中教师提问的。很多人谈论人工智能对工作的颠覆时谈到的主要是驾驶汽车等方面，而排除了其它领域。我现在有一位计划成为飞行员的学生。我告诉他考虑一下无人驾驶飞机，但他认为这不是什么威胁。我告诉学生们进入贸易行业比较保险，尤其是需要大量流动性的贸易。另外还有哪些领域现在看来比较安全？ Peter Norvig ：我认为更有意义的角度是看任务是什么，而不是着眼于职业。如果一位雄心勃勃的商业飞行员在 1975 年寻求建议，那么应该建议：你喜欢起飞和降落吗？未来很多年你都可以这样建议。你喜欢长时间稳定飞行吗？很遗憾，这个任务将几乎全部实现自动化。所以我认为大多数领域都是安全的，但你在任何工作中所做的任务组合将会发生变化，不同职业之间的报酬差异也将改变，而且每种工作所需要的人数也会发生变化。我们很难预测这些变化。举个例子，现在有很多人驾驶卡车。未来某个时候，大多数长途驾驶都将被自动化。我认为车里面仍还会有一个人，但他们的工作将更注重装卸货物和客户关系/推销，而不是驾驶。如果他们可以在车辆移动的时候在车里睡觉（终于可以了）和/或编排更大规模的卡车车队，那么你可能就会想我们所需的卡车司机更少了，但如果卡车运输的成本相对于铁路或海路运输下降了，那么需求也会增长。所以现在很难预测几十年后的事情，最好的建议是保持灵活性并为学习新事物做好准备——不管是某个职业内任务发生变化还是更换职业。 Eric Horvitz ：人工智能的发展将会对经济中的劳动力产生多重影响。我相信某些改变会是颠覆性的并且可能会以相对快的方式发生——这样的颠覆可能会出现在驾驶汽车和卡车等工作上。其它影响还包括工作执行的方式和人们执行不同领域的任务的方式。总的来说，关于人工智能发展对工作分配和工作性质的影响，我的看法是正面的。我看到很多任务都得到了更复杂自动化的支持，而不是被它们所取代。其中包括艺术和科学探索领域内的工作以及需要精细身体操作的工作，另外还有很多工作总是需要人之间的相互合作和照顾——包括教学、指导、医疗照护、社会工作和抚养孩子成人。关于后者，我希望在这个日益自动化的世界里看到更显著的「关怀经济（caring economy）兴起并获得支持。 有人可能有兴趣了解近期一些思考未来状况的研究。这里有一份很有意思的研究，思考了机器学习进步在特定功能方面对工作的影响：http://science.sciencemag.org/content/358/6370/1530.full。我推荐这篇文章是因为这是一个很好的范例，能帮助人们了解如何将某些结构组合起来预测人工智能和工作就业的未来。 顺便一提：昨天在奥斯汀的 AAAS 上我们有个环节就是关于人工智能的进步对人类能力的增强和任务的变革的。 Yann LeCun ：还有很长时间我们才能有机器人管道工、木匠、零杂工、理发师等等。一般而言，人工智能不会取代工作，但会改变工作。最终而言，人工智能会让每个工作都更高效。但需要人类创造力、交互能力、情感智能的工作很长时间内都不会消失。科学、工程、艺术、手工制作等创造性工作还将继续保留。 问题 2：目前很多机器学习研究似乎都转向了深度学习。1）这对机器学习研究的多样性会有负面影响吗？2）为了支持深度学习研究，需要将其他范式的研究例如概率图模型、支持向量机等完全抛弃吗？有可能这些模型在当前表现不是很好，但在未来会出现突破，正如深度学习在上世纪 90 年代的状况。 Yann LeCun ：随着我们 AI 技术的成长，我的感觉是深度学习只是其中的一部分。在复杂的（可能是动态的）图中集成参数化模块并根据数据优化参数的思想并没有过时。在这层意义上，只要我们还没有找到不需要使用梯度来优化参数的高效方法，深度学习也不会过时。因此，深度学习并不足以构建完整的 AI。我认为定义动态深度架构（即按程序定义计算图，其结构随着新的输入而改变）的能力可以将深度学习推广为可微编程。 至于问题 2)，深度学习和图模型之间并不存在对立。你可以这样使用图模型例如因子图，其中的因子是完整的神经网络。它们是互不相关的概念。人们曾经在深度学习框架之上构建概率编程框架。例如 Uber 的 Pyro 就建立在 PyTorch 之上（概率编程可以看成是图模型的推广，类似于可微编程是深度学习的推广）。事实证明在图模型中使用反向传播梯度进行推理是很有用的。当数据匮乏并可以手动特征化时，SVM/核方法、树模型等更好用。 Eric Horvitz ：深度神经网络在分类和预测任务上的表现有很多亮点。我们也见证了目标识别、语音识别、翻译，甚至学习最优策略（结合强化学习思想）的准确率在不断提高。然而，AI 是一个很宽泛的领域，有大量潜在的分支学科，并且 AI 的机器学习分支也有大量的分支。 我们需要继续深度开发有潜力的 AI 技术（并结合各自的优势），包括概率图模型、决策理论分析、逻辑推理、规划、算法博弈论、元推理和控制论等已有的丰富成果。我们还需要将领域进行扩展，例如将有限理性模型推广到开放世界中研究智能体的限度。 问题 3：如何将任务特定的 AI 突破到更加通用的智能？目前我们看起来正花费大量的精力用于在围棋中赢得胜利，或使用深度学习执行特定的科学任务。这种进展很不错，但相比大多数人们心目中的 AI 来说还很狭隘。我们如何构建通用智能，使得可以适应任意的任务呢？我认为简单地集成数百万个任务特定的应用并不能构建通用的智能。 Yann LeCun ：我认为，让机器通过观察来学习预测模型是通用人工智能（AGI）的最大障碍。人类婴儿和很多动物似乎都可以通过观察世界并与其交互获得一种常识（虽然相比我们的强化学习系统，他们只需要很少量的交互）。我的直觉是，大脑中有很大一部分是预测机器。它训练自身以预测所有事物（从已见事物预测未见事物）。通过学习预测，大脑精心构建了层次化的表征。预测模型可以在和世界的最小量交互中用于规划和学习新的任务。目前的「无模型」强化学习系统，例如 AlphaGo Zero，需要与「世界」进行大量的交互来学习（虽然它们学习得很不错）。它们在围棋或象棋中表现得很好，但这样的「世界」很简单、很确定性，并且可以同时使用多个计算机快速运行。和这样的「世界」交互很容易，但无法推广到现实世界中。你不能在驾驶汽车时通过 5 万次撞击学习「不能撞击」的规则。人类甚至只需要一次经验就能学习到这样的规则。我们需要让机器学习这样模型。 Eric Horvitz ：没错，目前的人工智能现状就是：智能而狭隘的「学者」。 我们对于人类智能的认识还远远不足，其中包括人类如何在开放世界中学习（以无监督的方式）、我们形成「常识」的机制以及我们轻易地泛化到新任务的秘密。 我认为有两种很重要的方法可以促进对通用智能的发展：一种方法是将多种特定应用有机结合起来，然后探索这些应用的关联性问题；另一种方法是集中研究一种核心方法论例如 DNN，然后探索其中更加普遍的结构。有一篇论文可以为我们提供一个有趣的通向 AGI 的框架和方向：http://erichorvitz.com/computational_rationality.pdf 问题 4：我是一个正计划转向 AI 研究的核工程/等离子体物理学研究生。 关于 AI 领域：AI 研究的下一个里程碑会是什么？目前为了达到这些里程碑的挑战是什么？ 关于该领域的专业技能发展：我需要具备哪些关键技能/知识才能获得成功？你对刚入门的人有什么一般性的建议或推荐学习资源吗？ Yann LeCun ：下一个里程碑：深度无监督学习、可以进行推理的深度学习系统。无监督学习的挑战：学习世界的层次化表征，以理解变化的解释因素。我们需要让机器学习如何在不完全可预测的世界中进行预测。关键技能：对连续型数学（线性代数、多变量微积分、概率统计、优化学等）的掌握/良好直觉。熟练的编程技能。熟练的科学方法论。总之：创造力和直觉。 Peter Norvig ：我对能真正理解人类语言并能进行实际对话的助理很感兴趣，这将是很重要的里程碑事件。其中很大的挑战是将模式匹配（我们对此很在行）结合抽象推理和规划，目前我们只能在非常形式化的领域如象棋中才能做的很好，而在现实世界中还远远不够。 作为物理学家是你的一大优势，有很适合的数学背景以及实验、建模和处理不确定性、误差的思维。我见识过很多物理学家在这个领域做出很棒的工作。 问题 5：有哪些场景背后是人工智能支持的而我们却未意识到？举个例子。 Eric Horvitz ：有相当一些人工智能系统和服务「位于引擎盖下」。我最喜欢的一个例子是我们在微软研究院与 Windows 团队紧密合作所得到的一个成果，这项进展叫做 Superfetch。如果你用的是 Windows 机器，你的系统就在使用机器学习来学习了解你的工作模式和下一步行动（这个过程是隐私的，在本地进行），它会持续地进行预测，通过预加载和预存取应用来最好地管理内存。因为你的机器会在幕后推理你的下一步动作，而且很快还将能推理你在一天的某个时候和一周的某天会做的事，所以速度会更快，非常神奇。这些方法一直在运行，而且自在 Windows 7 上的第一版以来一直在越来越好。微软研究院的人与 Windows 团队组成了一个联合团队共同努力——使用真实负载来进行实验让我们发展很快，能帮助我们选出最好的方法。 Yann LeCun ：过滤令人反感的内容、使用卫星图像构建地图、帮助内容设计师优化设计、使用紧凑的特征向量来表示内容（图像、视频、文本）以便索引和搜索、图像中文本的识别…… Peter Norvig ：任何有数据的地方，都有优化的可能性。有些事情你可能已经知道了。另一些则永远不会被用户注意到。比如说，我们做了很多工作来优化我们的数据中心——我们如何构建数据中心、我们如何让工作负载流过它们、我们如何冷却它们等等。我们应用了各种各样的技术（深度学习、运筹学研究模型、凸优化等）；你可以自己决定将这些看作是「人工智能」或「只是统计学」。 问题 6：我是一位博士学生，我没有足够多的资金来投入多个 GPU 和大型（就计算能力而言）深度学习平台。作为一位学生，我有发表论文的压力（我的研究领域是计算机视觉/机器学习），而且我知道我没法在论文截至日期之前足够快地在我的「新式模块的」网络上测试完所有超参数。而在 Facebook/谷歌等企业进行研究的人有多得多的资源可用，可以快速出成果发论文。在会议上，我们得到的评价标准都是一样的——所以我毫无胜算。如果我可以按时做完实验然后发表的唯一途径是在大公司做实习生——你们难道不认为这有很大的问题吗？我住在美国，还好一点。其它国家的人又该怎么办？对于解决这个问题，你们有什么想法？ Peter Norvig ：我们可以提供支持：你的教授可以申请谷歌云：https://cloud.google.com/edu/，其中包括 1000 个 TPU。 如果你的目标是开发一个端到端的计算机视觉系统，那么作为一个学生，你将难以与公司竞争。这不是深度学习领域独有的情况。我记得我在读研究生的时候有一位做 CPU 设计的朋友，而且他们知道他们无法与英特尔竞争。要完成一个大型工程开发项目，需要数百人开发数百个组件，如果有任何一个组件失败了，你就不是最领先的。但一个学生可以有更好地实现一个组件的新想法并将其展示出来（也许可以使用开源模型并展示由你的新组件所带来的改进）。 Yann LeCun ：我有两个头衔：Facebook 的首席人工智能科学家和纽约大学教授。我在纽约大学的学生可以使用 GPU，但没有在 FAIR 做实习生所能使用的 GPU 多。你可不要让自己与大型行业团队直接竞争，而且有很多不竞争也能做出好研究的方法。很多（甚至大多数）的创新思想仍然来自于学术界。比如说，在神经机器翻译中使用注意机制的思想来自 MILA。这种方法像飓风一样席卷了神经机器翻译领域，并且在几个月之内就得到了主要公司的采纳。在那之后，Yoshua Bengio 告诉 MILA 的成员停止竞争数据更好的翻译结果，因为与谷歌、Facebook、微软和百度等公司竞争是没有意义的。几十年前，在字符识别和语音识别领域也曾发生过这样的事。 Eric Horvitz ：微软和其它公司正在努力实现人工智能的民主化，开发工具和服务来帮助大公司之外的人轻松地在人工智能领域做出伟大成就。我能理解有关计算的问题会出现。在各种项目中，你可能会发现 Azure for Research 和 AI for Earth 很有价值，这能帮你获取微软的计算资源。 问题 7：作为一名 ML 从业者，我对最近漫天遍野的「fake AI」越来越厌倦。比如： Sophia，一个设置预编答案的木偶，却被呈现为活生生的有意识的存在。涉及机器学习的工作机会中 95% 并不是 AI 职位，只是加上了「AI」或「机器学习」这种流行词来使该公司看起来更有吸引力罢了。 对我来说，世界上只有很少的几千人从事机器学习工作，但是却有 100 倍的人在假装做 AI。这是一种病，它伤害了所有人，还抢走了近期 ML 真正做出的成果。我们能采取什么措施制止这种行为吗？ Peter Norvig ：不要担心。不只是 AI 领域有这种情况。每次出现一个热词，一些人就想用不恰当的方式利用它。AI 和 ML 如此，「有机」（organic）、「无麸」（gluten-free）、「范式转移」（paradigm shift）、「瓦解」（disruption）、「中枢」（pivot）等也是如此。他们只能得到一些短期的注意力，最终会消失。 Eric Horvitz ：我同意 Peter 的观点。看到大家对 AI 研究的热情很棒，但是确实存在一些过热、误解和隔阂，就像那些以各种方式跳上风口的人一样（包括给所有事情都加上一个「AI」:-)）。 马克·吐温有一句名言：「历史不会重复，但会押韵。」在 1980 年代中期专家系统时代也成出现 AI 过热的场面。1984 年，一些 AI 科学家提醒大家：被误导的狂热和无法达到期望可能导致兴趣和资金的崩溃。确实，几年后，我们进入了一些人所说的「AI 寒冬」。我不认为这次也必然出现这样的结果。我认为这次大火中会有炽热的余烬，闪耀着推动 AI 领域前进，但是 AI 科学家继续教育多个领域中的人们关于我们确实能够达到的成果也很重要，以及自「人工智能」一词首次使用这 65 年来我们努力试图解决的困难问题。 Yann LeCun ： 严肃的 ML/AI 专家在看到这种情况时，不用犹豫可以直接大喊「bull shit」。我自己一直是这样做的。是的，「AI」已经成为一个商业热词，但是今天 AI/ML 领域仍有大量严肃、超酷的工作。 问题 8：贵司会为保存竞争优势而保留一些算法／架构机密吗？我知道数据集会带来很大的竞争优势，那么算法也会吗？也就是说，如果你的公司在某个算法／架构上取得了突破，比如下一代 CNN 或下一代 LSTM，你们会为了科学发展而公开它呢，还是会为了保存竞争优势而保留机密呢？ Peter Norvig ：截至目前，你可以看到我们这三家公司（以及其他公司）发布了很多通用算法，我认为我们也将继续这样做。我认为原因有三：首先，我们相信科学发展；其次，竞争优势来自于我们使用算法所做的艰辛工作以及围绕创造某个产品所有的过程，而非核心算法本身；第三，你无法将它们作为机密保存，如果我们能想到，同一研究社区的其他人也能想到。 Yann LeCun ：在 FAIR，我们公开我们所做的所有事情。原因如下： （1）正如 Peter 所说，「我们相信科学发展；竞争优势来自于我们使用算法所做的艰辛工作以及围绕创造某个产品所有的过程，而非核心算法本身。」我还要加一句，竞争优势还来自于将算法／模型转换成产品或服务的速度。 （2）今天 AI 的主要问题不是一家公司是否领先于另一家（没有一家公司可以永远大幅领先），而是 AI 领域本身需要在一些重要方向实现快速进展。我们都不希望孤独地去解决这一问题，我们需要整个研究社区合作来实现进步。 （3）只有允许科学家发布成果，你才能吸引到做优秀的科学家；只有以其对更广阔的研究社区的学术影响力来评估他们（至少占一部分），你才能保留住他们。 （4）只有告诉他们必须发布成果，才能得到可靠的研究结果。如果不打算公开结果的话，人们通常会更加草率。 （5）公开创新性研究有助于将该公司塑造成领导者和创新者，这有助于招募最优秀的人才。在技术行业中，吸引最优秀人才的能力意味着一切。 Eric Horvitz ：自 1991 年设立以来，微软研究院就是一个开放的研究实验室。我们实验室的一个重要基础就是研究人员自己决定是否发布研究成果、共享 idea 与学识，这个基础深入我们实验室的 DNA。看到其他公司也在此方向前进，我觉得非常棒。在 Peter 的基础上，我想说的是，伟大的创新和 IP 是围绕不同领域实际产品化实现的细节开发出来的，这些可能无法像核心技术进展那样共享出来。 问题 9：量子计算的进步会驱动人工智能背后的研究吗？你如何看待未来二者的融合？ Peter Norvig ：我想要做的很多事情都没有量子计算的帮助。我经常想要通过一个相对简单的算法来处理海量文本，而量子计算对此并无帮助。 然而，量子计算可能有助于更高效地搜索深度网络的参数空间。我不知道是否有人做出了这样的量子算法，不用考虑硬件机器能否实现它，但理论上可能是有帮助的。 Yann LeCun ：驱动（driving)？当然不。对我而言，根本不清楚量子计算能对人工智能有任何影响。在短时间内更不可能。 问题 10：传统统计模型的价值在于易于理解模型的行为、如何得出结论以及推断／预测的不确定性。而新型深度学习方法在预测方面取得了很好的成果，但我认为它们通常是「黑箱」。目前我们对 ANN 等模型的内部机制有多大程度的理解呢？以及您认为理解其内部机制的重要度如何？我认为这在模型用于制定重大决策时尤为重要，比如汽车驾驶或临床决策。 Peter Norvig ：这是当前研究的重要部分。你可以从 Big Picture 博客 或 Chris Olah 的 博客看到谷歌为此努力的多个例子。我认为理解的难度更多地来自于「问题」本身的难度，而非解决方案技术的难度。二维线性回归很好理解，但它对于不具备好的线性模型的问题来说并无太大用处。类似地，人们说随机森林或标准 Python/Java 代码中的「if/then」规则易于理解，但是如果真的易于理解，代码就不会有 bug 了。而代码往往存在 bug。因为这些易于理解的模型同样易于出现确认偏差（confirmation bias）。 我更倾向于不只用「理解」（understanding）来描述这件事，还有「值得信任」（trustworthiness）。当我们可以信任一个系统时，尤其是该系统作出重大决策时，可以思考以下多个方面： 我能够理解该代码／模型吗？ 它是否长期在大量示例上得到验证？ 我是否确信世界不会变化，将我们带到模型从未见过的状态？ 该模型是否能够抵抗对抗攻击？ 该模型是否能够抵抗退化测试（degradation test），即我们故意削弱其中的一部分，查看其他部分如何运作。 是否存在类似的技术，在过去被证明是成功的？ 该模型是否能够被连续监控、验证和更新？ 该模型外部存在哪些检查？输入和输出都被其他系统检查吗？ 我使用哪种语言与该系统交流？我可以询问它在做什么吗？我可以向它提建议吗？如果它犯了错，我只能提供数千个新的训练样本，还是可以说「不，你把 X 弄错了，因为你忽略了 Y」。 …… 这是一个伟大的研究领域，我希望能看到更多这方面的研究成果。 问题 11：你觉得 Capsule 网络怎么样？除了 MultiMNIST，你们成功地在其他数据集上应用过它吗？输入更多数据时，它能够替代 CNN 吗？ Yann LeCun ：这样的想法在大型数据集上实践需要时间，Capsule 是个非常酷的想法。Geoff Hinton 已经思考了几十年（例如，他的学生 Rich Zemel 的博士论文主题是 TRAFFIC 模型）。找到在 MNIST 上有效的方法已经花费他很多时间了，所以使 Capsule 在 ImageNet 数据集（或者别的数据集）上有效也要花费一些时间。此外，还不清楚它是否有性能优势，在训练样本数量上的优势在实践中是否有效。Capsule 网络可被看作是一种以特殊方式做池化的卷积网络。 问题 12：我是个 13 岁的学生，我喜欢用 JS 和 Python 自己做游戏和编程。我想要做自己的音乐和机器学习程序，对我这样的年轻开发者有什么建议吗？ Yann LeCun ：再学习学习数学和物理。 Peter Norvig ：除了学习，做一些开源项目。要么自己在 Github 上开源一个，或者参与到已有的有趣项目中。 问题 13：Peter，谷歌一直在研究辅助识别图像的人工智能，效果也相当好，但仍有奇怪的地方。去年我用你们的 API，输入一张猫的图像，很简单，效果也不错。但因为尾巴从头上面露了出来，API 也猜测是它是一只独角兽。这种错人类不会犯，而人工智能会，特别是输入 2D 图像时，你觉得 AI 会克服这种问题吗？ Peter Norvig ：识别图像也就这几年才做到的，发展很稳定，但就像你说的，即使在一些任务上人工智能有超越人类的表现，它也会犯一些尴尬的错误。随着我们有更多经验、更多数据，这种情况会有所改进，且有希望做迁移学习，以便于不用从头开始做每个模型。相比于静态图像，视频可能有更大优势，这一点提的非常好。我们的计算能力呈指数级增长，但还没达到能够输入大体量视频的程度。等到能做到的那一天，你会看到极好的进步。 问题 14：能定义下「专家系统」与「人工智能」吗？你研究更多的是专家系统还是人工智能，或者二者都有？你研究专家系统或者人工智能的目标或者成功标准是什么？ Peter Norvig ：我认为专家系统是通过采访一位专家，把他所知道的进行编码的一种程序，包括所在领域的本体论、关于要做什么、何时达到目标的程序性知识。然后，给定一个新目标，程序可尝试模仿专家的行为。专家系统在 1980 年代达到顶峰。 相比之下，规范系统只尝试做「正确的事」，换言之就是「最大化预期效用」，不关心对专家行为的模仿。 此外，「机器学习」系统是通过收集来自全世界的数据建立的，而不是通过手动编码规则。 今天，我们专注于规范的机器学习系统，因为事实证明它要比专家系统更稳健。 问题 15：你们明显在致力于人类最终的衰败。你们为什么这么做，有什么理由？ Yann LeCun ：相反，我们致力于让人类变得更好。人工智能是对人类智能的扩充。火、弓箭、农业的出现让人类衰败了吗？ 原文链接：https://www.reddit.com/r/science/comments/7yegux/aaas_ama_hi_were_researchers_from_google/ 
231,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738018&idx=1&sn=924c5a619bfe34d04073f7dc407b74db&chksm=871ac91cb06d400afa5a7f3dc75f56a2dd4300acdc82e5ff0f63cad6c1b3eb683f73a7b89a7b&scene=27,2017年，机器之心贡献过的开源项目,"机器之心目前有三个 GitHub 项目，即旨在构建 AI 领域术语库的「Artificial-Intelligence-Terminology」、旨在提供高质量模型实现与试验的「ML-Tutorial-Experiment」和旨在开放 AI 企业评选的「AI00」。 机器之心 GitHub 项目地址：https://github.com/jiqizhixin 目前「Artificial-Intelligence-Terminology」共有 750 个专业术语，我们将机器之心在编译技术文章和论文过程中所遇到的专业术语记录下来，希望有助于大家查阅和翻译。同时也希望大家能积极指出我们编译的不当之处。 项目地址：https://github.com/jiqizhixin/Artificial-Intelligence-Terminology 在该项目中，读者可通过 README 界面表盘查看自己想要了解的专业词汇。在单个首字母中，表格的组织形式为：英文／缩写、汉语、来源&扩展。 来源&扩展是对该词汇的注解，内容为机器之心往期的相关文章。例如字母 A 中的「算法」，我们关联到的三篇文章是《回归、分类与聚类：三大方向剖解机器学习算法的优缺点》和《机器学习算法附速查表》和《深度学习算法全景图：从理论证明其正确性》。因此，我们希望不仅能提供相对应的术语，同时还希望能为读者提供每一个术语的来源和概念上的扩展。 我们最近的工作很大程度上都在增加每个术语的对应链接，我们希望每个术语不仅有对应的翻译和表达方式，同时还能提供相关的文章。 本项目中所有英文专业词汇对照的中文都来自机器之心编译的文章和系列机器学习教科书（如周志华的《机器学习》和李航的《统计学习方法》和 Ian Goodfellow 的《深度学习》中译版等），我们力求在提供准确翻译的同时保留最常用的形式。 目前该术语库重点更新了各词汇的链接文章，并直接体现在项目中。但由于当前仍在继续添加与完善词汇对应的链接，因此我们并没有直接更新到词汇展示页。我们的词汇展示页可以很便捷地查找便捷的 AI 词汇，并且我们很快会添加扩展链接以方便读者索引对应的词汇和对应的术语解释，以下是术语的展示页面： 展示地址：https://jiqizhixin.github.io/AI-Terminology-page/ 目前机器学习算法实现项目共获得 976 次收藏，是我们今年将要重点关注和完善的项目。它目前有三篇详细的教程文章，即 CNN 的实现、经典 GAN 的推导与实现和 CapsNet 的解读。我们希望能提供高质量和能实现的技术文章，在这些文章中，我们所使用的代码块或整体实现都是我们预先测试的。且我们提供的 Jupyter Notebook 都带有代码注释，非常适合初学者随文章阅读。 项目地址：https://github.com/jiqizhixin/ML-Tutorial-Experiment 在从零开始用 TensorFlow 搭建卷积神经网络的文章中，我们主要介绍的是 CNN 的实现，而并不会从理论和概念上详细解释深度神经网络、卷积神经网络、最优化方法等基本内容。值得注意的是，这篇文章不仅从最基础的 TensorFlow 安装开始，同时还详细描述了 TensorFlow 的基本操作和构建方法。 文章地址： 该教程主要从全连接网络到 LeNet-5 介绍了如何构建基本的 CNN，如下展示了最初的 LeNet-5 架构： 第二个分析与实现的主题是 GAN，该文章从原论文出发，借助 Goodfellow 在 NIPS 2016 的演讲和台大李弘毅的解释，而完成原 GAN 的推导与证明。其主要分为四部分，第一部分是描述 GAN 的直观概念，第二部分描述概念与优化的形式化表达，第三部分将对 GAN 进行详细的理论推导与分析，最后一部分将实现前面的理论分析。 文章地址： 在这篇文章中，我们最后描述了整个算法的训练过程： 对于每一次迭代： 从真实数据分布 P_data 抽取 m 个样本 从先验分布 P_prior(z) 抽取 m 个噪声样本 将噪声样本投入 G 而生成数据  通过最大化 V 的近似而更新判别器参数θ_d，即极大化   ，且判别器参数的更新迭代式为  以上是学习判别器 D 的过程。因为学习 D 的过程是计算 JS 散度的过程，并且我们希望能最大化价值函数，所以该步骤会重复 k 次。 从先验分布 P_prior(z) 中抽取另外 m 个噪声样本 {z^1,...,z^m} 通过极小化 V^tilde 而更新生成器参数θ_g，即极大化  ，且生成器参数的更新迭代式为  以上是学习生成器参数的过程，这一过程在一次迭代中只会进行一次，因此可以避免更新太多而令 JS 散度上升。 除了文章中所描述的 Keras 实现，后续我们还在补充资料中添加了 TensorFlow 实现。如下所示定义了生成器与判别器： 第三篇描述的是 Hinton 等人提出来的 CapsNet，其旨在解释 CapsNet 的网络架构与实现。为了解释 CapsNet，该文章从卷积层与卷积机制开始，从工程实践的角度解释卷积操作的过程与输出，这对进一步理解 Capsule 层的处理十分有利，后面文章基于对 Capsule 层的理解解释 Geoffrey Hinton 等人最近提出来的 CapsNet 架构，并根据 naturomics 的实现进行测试与解释。 文章地址： 根据 Dynamic Routing Between Capsules，整个层级间的传播与分配可以分为两个部分，第一部分是下图 u_i 与 u_j|i hat 间的线性组合，第二部分是 u_j|i hat 与 s_j 之间的 Routing 过程： 如上所示，该图展示了 Capsule 的层级结构与动态 Routing 的过程。最下面的层级 u_i 共有两个 Capsule 单元，该层级传递到下一层级 v_j 共有四个 Capsule。u_1 和 u_2 是一个向量，即含有一组神经元的 Capsule 单元，它们分别与不同的权重 W_ij（同样是向量）相乘得出 u_j|i hat。例如 u_1 与 W_12 相乘得出预测向量 u_2|1 hat。随后该预测向量和对应的「耦合系数」c_ij 相乘并传入特定的后一层 Capsule 单元。不同 Capsule 单元的输入 s_j 是所有可能传入该单元的加权和，即所有可能传入的预测向量与耦合系数的乘积和。随后我们就得到了不同的输入向量 s_j，将该输入向量投入到「squashing」非线性函数就能得出后一层 Capsule 单元的输出向量 v_j。然后我们可以利用该输出向量 v_j 和对应预测向量 u_j|i hat 的乘积更新耦合系数 c_ij，这样的迭代更新不需要应用反向传播。 在最近 Sara 等研究者开放了论文 Dynamic Routing between Capsules 的官方实现代码后，我们也对核心代码做了简要的解析。这一份实现是 Sara 等研究者获得顶尖结果的依据，它与其它研究者所复现的代码主要有三个区别。首先，Sara 的实现会添加一个 leaky_routing 函数，按照该函数的定义，它会添加额外的维度以路由分对数（logits）。如果需要执行路由的张量维度与上层任意 Capsule 单元不匹配，那么该函数将允许激活的 Capsule 单元在额外的维度中进行路由。 其次是 Sara 的实现调用了两次动态路由算法（余弦相似度），而我们从论文上理解为只有在 DigitCaps 层才会使用动态路由，因此相对于其它研究者直接将卷积后的张量投入 Squash 非线性函数，Sara 的实现会先迭代精炼几次卷积结果。最后一点是 naturomics 所表明的偏置项问题，他表明 Sara 等人实现的路由算法在投入到 Squash 非线性激活函数时会添加一个偏置项。我们将官方核心代码的解读添加到了补充资料中。 文章地址： 目前这三个分析与实现并没有成体系，比如果我们没有从基础的机器学习算法开始，一步步完善各算法的试验与分析。在新的一年，我们将重点关注于该项目而提供高质量的教程与实现。 项目地址：https://github.com/jiqizhixin/AI00 这不仅是一份榜单，更是一个人人可以参与的开源项目，人工智能是一个复杂庞大的体系，涉及众多学科，也关乎技术、产品、行业和资本等众多要素，本报告的写作团队只代表他们的专业观点，有自己的局限性，需要更多行业专家参与进来加以修正和完善。 人工智能技术和行业的发展瞬息万变，而报告的制作周期较长，其中的内容和数据势必会落后于行业的最新进展，无法同时满足时效性和高质量的要求。而领域内参与者的及时更新可以解决这个问题。 我们深刻地理解在没有专业用户反馈的情况下所做出报告的质量局限性，所以希望用工程界「Agile Development」的理念来对待我们的报告，不断收集专业反馈来持续提升报告质量。 以上是机器之心现有的一些 GitHub 项目，我们还会继续扩展并发现对读者有帮助的新计划，例如汉化或制作机器学习工具包的速查表、从《Deep Learning》或其它书籍衍生出的知识点等等。在新的一年里，机器之心希望读者能和我们共同完善这一些目标。 目前 AI00 已经走过了一年多的时间，从 2016 年 10 月的第一份榜单开始，该项目已经更新了 15 次。我们每一次都会根据读者与业界的反馈更新各上榜公司的主营业务、融资情况和所属领域等。以下是自第一次榜单以来的更新情况： 机器之心「AI00」十一月最新榜单：两家国内公司新上榜 机器之心「AI00」十二月最新榜单：四家公司新上榜 机器之心「AI00」一月最新榜单：英国最火的机器学习创业公司 机器之心「AI00」二月最新榜单：新增赛灵思、Argo AI和Gamalon 机器之心「AI00」三月最新榜单：新增DataRobot与值得关注的Neuralink 机器之心「AI00」四月最新榜单：新增人工智能网络安全公司Cylance和Sift Science 机器之心「AI00」颁奖晚宴闭幕，五月最新榜单发布 机器之心「AI00」六月最新榜单：算法商店Algorithmia与网络安全公司Spark 机器之心「AI00」七月榜单：「AI不止语音助手」的华为 机器之心「AI00」八月榜单：脱胎于谷歌TPU团队的Groq 机器之心「AI00」九月榜单：1400万美元A轮融资的TalkIQ 机器之心「AI00」十月榜单：卡耐基梅隆大学数位博士开创的Solvvy 机器之心「AI00」十一月榜单：OpenAI科学家创立Embodied Intelligenc e 机器之心「AI00」十二月榜单：紧盯AI人才的京东 机器之心「AI00」一月榜单：前谷歌自动驾驶首席工程师创立Nuro "
232,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738028&idx=3&sn=123cce22491189a6f6788614804b7498&chksm=871ac912b06d4004d6be9298f4ca429eddfb33269c9303ad30db161aab88b715b0a24c41ac91&scene=27,ICLR 2018 | 深度可逆网络i-RevNet：信息丢弃不是泛化的必要条件,"本文介绍了一种可逆网络架构 i-RevNet，证明对于分类网络的泛化能力，通过信息丢弃构造信息瓶颈并不是必要条件，该结论甚至对 ImageNet 这样的大型数据集也是成立的；此外，通过保留中间表征的所有信息，使得逆向完全地恢复原图变得可行。 虽然卷积神经网络（CNN）在进行图像分类的时候特别有效（He et al., 2016; Krizhevsky et al., 2012），但是非线性算子和线性算子的级联在揭示内部表征对分类的贡献方面却是很有限的。学习过程的主要特点是能够稳定地减少图片中大量的无信息可变性（uninformative variability），同时揭示图像类别的本质特征。普遍认为这个过程是基于逐步丢弃与问题输入相对应的无信息可变性 (Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Shwartz-Ziv & Tishby, 2017; Achille & Soatto, 2017)。然而，关于抛弃信息的程度信息在某些中间非线性过程中丢失了。在这篇论文中，研究者通过提出一种可逆卷积神经网络来提供关于可变性减少过程的一些洞见，这个可逆卷积神经网络不会损失关于输入的任何信息。 很多常用的网络结构都面临着从隐藏表征中恢复图片的困难 (Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016)。这引发了一个问题：在成功的分类模型中，大量的信息损失是否必要。本文将证明，没有信息是必须被丢弃的。通过使用同胚层（homeomorphic layers），不变性可以仅仅在最后一层通过投影的方式建立。 Shwartz-Ziv 和 Tishby 的研究中（2017）建议采用最少而充足的统计量来解释可变性的减少。Tishby 和 Zaslavsky2015 年的研究介绍了信息瓶颈原则——为了尽可能多地减少无信息可变性，一个最优的表征必须减少输入与其表征之间的互信息。同时，为了有效地防止一个类别被混淆到其他类别，网络还应该最大化表征与其期望输出之间的互信息。Shwartz-Ziv & Tishby (2017) 和 Achille & Soatto (2017) 基于一些小数据集阐述了信息瓶颈效应。然而，本文将证明，这并不是一个必要条件，而且通过构建级联的同胚层，可以保留输入和隐藏表征之间的互信息，并且证明信息损失可以仅仅发生在最后一层。以此，我们可以证明，在诸如 ImageNet 这样的大规模数据集上也可以避免信息损失的问题。 减少可变性的一种方式就是逐步地收缩中间表征的 L2 范数对应的可变性。 有几项研究已经注意到了在有限数据集上训练的非可逆网络存在逐步分离和收缩的现象 (Oyallon, 2017; Zeiler & Fergus, 2014)。这种逐步提升性能的现象可以被解释为逐步增强不变性，以改善分类结果。理想情况下，这种收缩不应该太暴力，以避免从中间信号中移除重要信息。这证明区分度和不变性之间的权衡必须逐步建立。在这篇论文中，作者将 Zeiler & Fergus (2014) 和 Oyallon (2017) 的工作扩展到了 ImageNet (Russakovsky et al., 2015) 上，并且最重要的是，他们证明了在逐步收缩的过程中，信息损失是可以避免的。 Mallat (2016) 讨论了不同类别的不变性和区分度之间的二重性。这里用李群为类内部可变性建模，类内部可变性可以通过在这些对称性中执行并行传递来处理。在学习过程中可以将卷积核适应到数据集的特定偏差上，进而可以避免沿着可区分方向的收缩。然而，使用不属于欧几里得范畴的群进行图像分类是很困难的。主要原因是与这些抽象可变性相关的群是难以评估的，因为它们具有高维度的属性以及需要合适自由度的不变性。Mallat(2012) 通过散射变换给出了这个框架在欧氏群上的一个描述，它在一定程度可恢复的同时建立了小幅度转译的不变性。在这篇论文中，作者引入了一个网络结构，它不会在除最后一层的其他地方丢弃任何信息，同时他们还定量地展示了信号类别中的渐进收缩和分离。 研究者引入了 i-RevNet，这是一种可逆的深度网络，i-RevNets 在除最后一层的所有中间表征中保留了输入信号的所有信息。该架构架构是基于最近提出的 RevNet(Gomez et al., 2017) 建立的，用可逆组件代替了原始 RevNets 结构中的非可逆组件，i-RevNet 在 ImageNet 上达到了与非可逆 RevNet 和 ResNet 相同的性能 (Gomez et al., 2017; He et al., 2016)。在这个架构中，本文证明：在学习可以泛化到陌生数据的表征时，信息损失并不是必要条件。 为了揭示学习表征泛化能力的机制，作者证明了 i-RevNets 随着深度的增加会逐渐分离和收缩信号。结果表明：通过使用对可恢复输入进行收缩，可以有效地减少可变性。 论文链接：https://openreview.net/forum?id=HJsjkMb0Z 普遍认为，卷积神经网络的成功是基于对问题输入的无信息可变性的逐渐丢弃。在绝大多数常见的网络架构中，难以从图像的隐藏表征恢复图像，经验地支撑了上述观点。我们在这篇论文中证明了这种信息丢失不是泛化到复杂问题 (如 ImageNet ) 上的必要条件。通过使用级联同胚层，我们建立了 i-RevNet，这是一个可以完全倒转到类别的最终投影上的网络，也就是说，不会丢弃任何信息。建立一个可逆架构是很困难的，因为局部可逆是病态的，我们通过一种显式的逆向过程克服了这个问题。通过对 i-RevNet 学习到的表征的分析，我们提出了一种通过渐进收缩和按深度的线性分离来解释良好准确率的方法。为了揭示 i- RevNet 学习模型的性质，我们重构了自然图像表征之间的线性插值。 "
233,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737992&idx=4&sn=b9ad780ef35cd84bb3fc9f1290b9e00f&chksm=871ac936b06d4020a9601cb5b5796b1f5d1759751981e3e4d5b666994b1990aa1e3694c15fea&scene=27,学界 | 纽约大学：参考儿童认知发展，通过简单神经网络学习归纳偏置,"借助先验知识，也就是归纳偏置，人类得以有效学习关于世界的新知识。本文发现，简单神经网络在观察 4 个物体类别的 3 个实例之后，便可以发展出一种形状偏置，这预示着神经网络开始快速学习词汇，与儿童的认知发展过程相一致。本文启发了一种参考生物认知发展过程以初始化模型，然后逐渐泛化到更复杂数据集的模型开发范式。 论文：Learning Inductive Biases with Simple Neural Networks 论文链接：https://arxiv.org/pdf/1802.02745.pdf 摘要： 为了高效学习新概念，人类大量使用关于世界的先验知识。这些先验——也被称作「归纳偏置」（inductive biases）——归属于学习器的内部模型空间，可帮助学习器做出超出已观察数据之外的推断。最近一项研究发现，针对目标识别而优化的深度神经网络可以发展出形状偏置（Ritter et al., 2017），一种在儿童早期词汇学习之中扮演重要角色的归纳偏置。然而，这些神经网络使用了大量的训练数据，并且这些偏置发展所需的条件也很不清晰。此外，上述网络的学习机制如何与儿童发育过程相关尚不清楚。我们使用抽象模式和合成图像的可控数据集来研究神经网络中形状偏置的发展和影响，使得我们可以系统地改变提供给学习算法的经验数据的数量和形式。我们发现简单神经网络在观察 4 个物体范畴的 3 个实例之后就发展出一种形状偏置。这些偏置的发展预示着网络开始快速学习词汇，这与儿童的认知发展过程相一致。   综合讨论 通过一组可控的综合实验，本论文提供了有关允许在神经网络中 learning-to-learn 的环境条件的新见解。在 Colunga & Smith (2005) 的工作基础之上，我们表明简单神经网络可从 4 个类别的 3 个实例的抽象模式的刺激之中学习形状偏置。因此，这些网络在偏置发展所需的样本复杂性中同时采用 HBMs 和人类儿童的方法。通过扩展 Ritter et al. (2017) 的研究成果，我们表明通过高维彩色图像训练的简单 CNN 架构可以通过 8 个目标类别的 6 个实例学习形状偏置，然而不是所有的刺激属性都相同。我们的彩色图像训练实验表明 CNN 的学习动态取决于其训练时接受的属性。这一结果凸显了把刺激扩展到更多复杂数据的重要性，其中不同刺激属性有不同的物理性质。最后，我们的新结果表明一个已训练网路对形状的敏感性如何随着输入而变化参数。 在最近的一项研究中，Hill et al. (2017) 训练一个神经网络智能体导航虚拟 3D 环境，并使用类似于我们的简化人工物体刺激根据基于名称的语言指令收集物体。作者从有关人类儿童的研究中获得启发，其结果值得注意。然而，我们的工作目标在某些重要方面很不同。本实验中的智能体一起接收视觉和语言的联合输入，并且必须输出导航决策。因此网络被要求同时学习不同的任务——即，视觉感知、语言理解和导航。因此论文的学习曲线反映了一种多方面学习的形式。本论文中我们的主要兴趣是研究神经网络学习归纳偏置所需的精确数据量。我们的框架旨在独立研究这一问题，以最大限度地减少外部因素的干扰。 人类儿童的形状偏置的发展据知与改进的单词学习能力相关联，这一现象在我们的网络之中有所反映。这一发现说明，有可能将模型通过形状偏置训练进行初始化之后，可能更加高效地训练大规模图像识别模型。在未来的工作中，我们希望通过本文启发的初始化框架，在 ImageNet 一般规模的 DNN 中验证这一假设。 "
234,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737992&idx=3&sn=5eaf613506b59cfaf9a9c7b76fcb02cf&chksm=871ac936b06d402039e1c17403b9ef7dcb8aa38637b0f080702e806346fa1274ced097db583b&scene=27,入门 | 区块链vs传统数据库：分布式运行有何优势？,区块链（blockchain）这一概念正因比特币等虚拟货币的兴盛而变得火热起来，实际上，这种技术因为特殊的设计思路也可以应用于很多其他领域中。作为一种容错率很高的分布式数据存储模式，区块链与传统数据库有哪些不同之处？想要回答这个问题，我们需要看看它们的运行机制。 传统数据库 传统数据库使用客户端-服务器网络架构。在这种结构中，用户（或称为客户端）可以修改存储在中央服务器中的数据。数据库的控制权保留在获得指定授权的机构处，他们会在用户试图接入数据库前对其身份进行验证。由于授权机构对于数据库的管理负责，如果授权机构的安全性受到损害，则数据面临被修改、甚至被删除的风险。 区块链数据库 区块链数据库由数个分散的节点组成。每一个节点都会参与数据管理：所有节点都会验证新加入区块链的内容，并将新数据写入数据库。对于加入区块链的新内容，大多数节点必须达成一致才能成功写入。这种共识机制保证了网络安全，让篡改内容变得非常困难。 区块链最引人关注的实例就是比特币（Bitcoin）了，在比特币中，共识通过「挖矿」达成（使用计算机计算复杂 hashing 问题的解）；而在另一个著名的案例以太坊中，共识机制是由股权证明（POS）来完成的。想要了解这两种机制的详细内容，可以参阅本文作者此前的文章：https://hackernoon.com/what-is-proof-of-stake-8e0433018256。 完整性与透明度 区块链技术区别于传统数据库技术的一大特点就是其具备公开可验证性，这是通过完整性与透明度来实现的。 完整性：每名用户都可以得到这样的保证——他们所检索的数据自被记录的那一刻起不会遭到损坏或改写。 透明度：每名用户都可以获知并验证区块链内容是如何随着时间推移而变化的。 CRUD vs 读取 & 写入操作 传统数据库中，客户可以对数据执行四种操作：创建、读取、更新和删除（通称为 CRUD 命令）。 而区块链只能增加。用户只能以附加块的形式添加数据，所有先前的数据被永久存储，无法更改。因此，区块链仅能执行以下操作： 读取：用区块链查询和获取数据； 写入：向区块链添加更多数据。 验证和写入 区块链具备这两个功能：交易验证和新交易写入。交易是一种改变区块链上数据状态的操作。区块链上之前的 entries 永远保持不变，而新的 entry 可以改变之前 entries 中数据的状态。例如，如果区块链记录我的比特币钱包中有一百万比特币，该数字永久存储在区块链中。当我花费 20 万比特币时，该交易也被记录在区块链上，我的余额为 80 万比特币。但是，由于区块链只能不断加长，因此这次交易之前的余额 100 万比特币仍然永久保存在区块链上。这就是为什么区块链通常指不可更改的分布式账本。 总之，区别在于去中心化控制 去中心化控制消除了中心化控制的风险。任何能够充分访问中心化数据库的人都可以摧毁或破坏其中的数据，因此用户依赖于数据库管理员的安全基础架构。 区块链技术使用去中心化数据存储来避开这一问题，从而在自己的结构中建立安全性。 区块链技术很适合记录某些种类的信息，传统数据库更适合记录另外一些种类的信息。对于每个组织而言，理解它想从数据库中获得什么非常关键，我们需要在选择数据库之前，判断每种数据库的优缺点。 原文链接：https://towardsdatascience.com/blockchains-versus-traditional-databases-e496d8584dc 
235,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737992&idx=2&sn=753da163e9d4d35e838fcc628922c45a&chksm=871ac936b06d40202dc982d420f7e2052a0b61cabd2be08764291a3419165b9453b24a325c84&scene=27,灵魂追问 | 教程那么多，你……看完了吗？,"2017年，人工智能技术出现了很多新的技术和发展，在这一年中机器之心发布了很多教程类文章，有适合入门学习者的，有适合已经具备专业知识和实践经验的从业者的；有关于语言的，有关于框架的，有关于硬件配置的，甚至还有关于猫片、漫画的…… 教程那么多，你……看完了吗？ 本文对这一年来机器之心发布的教程进行总结，共分为 What 和 How 两大部分，在两大板块下又进行细分，目录如下： What 概念 机器学习基础 深度模型基础 强化学习基础 数学 How 致初学者 课程 算法实现 机器学习基础实现 深度网络基础实现 计算机视觉实现 自然语言处理实现 强化学习实现 深度学习框架 工具方法 云端 边缘设备 硬件 吃喝玩乐撸撸猫 Money, Money, Money 概念 1. 机器学习基础 一文读懂机器学习、数据科学、人工智能、深度学习和统计学之间的区别 人人都能读懂的无监督学习：什么是聚类和降维？ 如何解读决策树和随机森林的内部工作机制？ 教程 | 拟合目标函数后验分布的调参利器：贝叶斯优化 入门 | 区分识别机器学习中的分类与回归 深度 | 思考VC维与PAC：如何理解深度神经网络中的泛化理论？ 教程 | 理解XGBoost机器学习模型的决策过程 业界 | 似乎没区别，但你混淆过验证集和测试集吗？ 教程 | 初学者如何学习机器学习中的L1和L2正则化 机器学习算法集锦：从贝叶斯到深度学习及各自优缺点 入门 | 机器学习新手必看10大算法 教程 | 详解支持向量机SVM：快速可靠的分类算法 干货 | 详解支持向量机（附学习资源） 教程 | 遗传算法的基本概念和实现（附Java实现案例） 教程 | 利用达尔文的理论学习遗传算法 深度 | 详解可视化利器t-SNE算法：数无形时少直觉 入门 | 如何构建稳固的机器学习算法：Boosting&Bagging 资源 | 神经网络调试手册：从数据集与神经网络说起 观点 | 三大特征选择策略，有效提升你的机器学习水准 教程 | 如何为单变量模型选择最佳的回归函数 机器学习老中医：利用学习曲线诊断模型的偏差和方差 教程 | 如何为时间序列数据优化K-均值聚类速度？ 入门 | 将应用机器学习转化为求解搜索问题 从重采样到数据合成：如何处理机器学习中的不平衡分类问题？ 2. 深度模型基础 从零开始：教你如何训练神经网络 教程 | 深度学习初学者必读：张量究竟是什么？ 解读 | 通过拳击学习生成对抗网络（GAN）的基本原理 干货 | 直观理解GAN背后的原理：以人脸图像生成为例 教程 | 从基本概念到实现，全卷积网络实现更简洁的图像识别 资源 | 初学者指南：神经网络在自然语言处理中的应用 教程 | 深度学习：自动编码器基础和类型 入门 | 请注意，我们要谈谈神经网络的注意机制和使用方法 教程 | 经典必读：门控循环单元（GRU）的基本概念与原理 入门 | 迁移学习在图像分类中的简单应用策略 解读 | 如何从信号分析角度理解卷积神经网络的复杂机制？ 教程 | 无监督学习中的两个非概率模型：稀疏编码与自编码器 深度 | 从任务到可视化，如何理解LSTM网络中的神经元 教程 | 将注意力机制引入RNN，解决5大应用领域的序列预测问题 教程 | 听说你了解深度学习最常用的学习算法：Adam优化算法？ 教程 | 如何解决LSTM循环神经网络中的超长序列问题 教程 | 一个基于TensorFlow的简单故事生成案例：带你了解LSTM 教程 | 如何判断LSTM模型中的过拟合与欠拟合 教程 | 如何估算深度神经网络的最优学习率 教程 | 如何为神经机器翻译配置编码器-解码器模型？ 教程 | 如何用深度学习处理结构化数据？ 改进卷积神经网络，你需要这14种设计模式 3. 强化学习基础 从强化学习基本概念到Q学习的实现，打造自己的迷宫智能体 教程 | Keras+OpenAI强化学习实践：深度Q网络 一份数学小白也能读懂的「马尔可夫链蒙特卡洛方法」入门指南 入门 | 蒙特卡洛树搜索是什么？如何将其用于规划星际飞行？ 教程 | Keras+OpenAI强化学习实践：行为-评判模型 从贝叶斯定理到概率分布：综述概率论基本定义 想了解概率图模型？你要先理解图论的基本定义与形式 数学 干货 | 机器学习需要哪些数学基础？ 深度神经网络中的数学，对你来说会不会太难？ 观点 | Reddit 热门话题：如何阅读并理解论文中的数学内容？ 教程 | 基础入门：深度学习矩阵运算的概念和代码实现 从概率论到多分类问题：综述贝叶斯统计分类 机器之心最干的文章：机器学习中的矩阵、向量求导 致初学者 教程 | Kaggle CTO Ben Hamner ：机器学习的八个步骤 教程 | Kaggle初学者五步入门指南，七大诀窍助你享受竞赛 从零开始，教初学者如何征战Kaggle竞赛 只需十四步：从零开始掌握Python机器学习（附资源） 如何从初入行者进阶为人工智能先锋青年？ 观点 | 如何从一名软件工程师转行做人工智能？ 教程 | 如何转行成为一名数据科学家？ 初学者怎么选择神经网络环境？对比MATLAB、Torch和TensorFlow 教程 | 初学者如何选择合适的机器学习算法（附速查表） 经验之谈：如何为你的机器学习问题选择合适的算法？ 资源 | 企业应该怎样选择数据科学&机器学习平台？ 实验研究工作流程详解：如何把你的机器学习想法变成现实 观点 | 机器学习新手工程师常犯的6大错误 教程 | 如何用Docker成为更高效的数据科学家？ 从标题到写作流程：写好一篇论文的十条基本原则 论文格式排版你真的做对了吗? 常用格式及其LaTeX书写方法介绍 课程 蒙特利尔大学开放MILA 2017夏季深度学习与强化学习课程视频（附完整PPT） 斯坦福CS231n Spring 2017开放全部课程视频（附大纲） 斯坦福大学秋季课程《深度学习理论》STATS 385开讲 资源 | CMU统计机器学习2017春季课程：研究生水平 教程 | 斯坦福CS231n 2017最新课程：李飞飞详解深度学习的框架实现与对比 三天速成！香港科技大学TensorFlow课件分享 四天速成！香港科技大学 PyTorch 课件分享 吴恩达Deeplearning.ai课程学习全体验：深度学习必备课程（已获证书） 算法实现 1. 机器学习基础实现 教程 | 从头开始：用Python实现带随机梯度下降的线性回归 初学TensorFlow机器学习：如何实现线性回归？（附练习题） 教程 | 从头开始：用Python实现带随机梯度下降的Logistic回归 教程 | 从头开始：用Python实现随机森林算法 教程 | 从头开始：用Python实现基线机器学习算法 教程 | 从头开始：用Python实现决策树算法 听说你用JavaScript写代码？本文是你的机器学习指南 教程 | 如何使用JavaScript构建机器学习模型 教程 | 初学文本分析：用Python和scikit-learn实现垃圾邮件过滤器 教程 | 如何通过牛顿法解决Logistic回归问题 每个Kaggle冠军的获胜法门：揭秘Python中的模型集成 教程 | 如何在Python中快速进行语料库搜索：近似最近邻算法 2. 深度网络基础实现 教程 | 初学者入门：如何用Python和SciKit Learn 0.18实现神经网络？ 教程 | 如何用30行JavaScript代码编写神经网络异或运算器 教程 | 使用MNIST数据集，在TensorFlow上实现基础LSTM网络 教程 | 如何使用Keras集成多个卷积网络并实现共同预测 教程 | 在Python和TensorFlow上构建Word2Vec词嵌入模型 教程 | 详解如何使用Keras实现Wassertein GAN 机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络 教程 | 如何基于TensorFlow使用LSTM和CNN实现时序分类任务 作为TensorFlow的底层语言，你会用C++构建深度神经网络吗？ 入门 | 十分钟搞定Keras序列到序列学习（附代码实现） 入门 | 想实现DCGAN？从制作一张门票谈起！ 教程 | 通过PyTorch实现对抗自编码器 教程 | 基于Keras的LSTM多变量时间序列预测 3. 计算机视觉实现 教程 | TensorFlow从基础到实战：一步步教你创建交通标志分类神经网络 教程 | 如何使用TensorFlow和自编码器模型生成手写数字 教程 | 无需复杂深度学习算法，基于计算机视觉使用Python和OpenCV计算道路交通 教程 | 深度学习 + OpenCV，Python实现实时视频目标检测 教程 | 如何通过57行代码复制价值8600万澳元的车牌识别项目 教程 | 百行代码构建神经网络黑白图片自动上色系统 教程 | 盯住梅西：TensorFlow目标检测实战 深度 | 从数据结构到Python实现：如何使用深度学习分析医学影像 仅需15分钟，使用OpenCV+Keras轻松破解验证码 教程 | 如何使用TensorFlow API构建视频物体识别系统 教程 | 经得住考验的「假图片」：用TensorFlow为神经网络生成对抗样本 先读懂CapsNet架构然后用TensorFlow实现，这应该是最详细的教程了 教程 | 如何使用深度学习去除人物图像背景 资源 | 如何通过CRF-RNN模型实现图像语义分割任务 4. 自然语言处理实现 如何解决90％的自然语言处理问题：分步指南奉上 谷歌开放GNMT教程：如何使用TensorFlow构建自己的神经机器翻译系统 教程 | 从头开始在Python中开发深度学习字幕生成模型 资源 | 谷歌全attention机器翻译模型Transformer的TensorFlow实现 教程 | 如何使用TensorFlow构建、训练和改进循环神经网络 教程 | Kaggle网站流量预测任务第一名解决方案：从模型到代码详解时序预测 教程 | 用于金融时序预测的神经网络：可改善移动平均线经典策略 教程 | 如何用PyTorch实现递归神经网络？ 教程 | 用数据玩点花样！如何构建skip-gram模型来训练和可视化词向量 教程 | 利用TensorFlow和神经网络来处理文本分类问题 5. 强化学习实现 教程 | 深度强化学习入门：用TensorFlow构建你的第一个游戏AI 资源 | 价值迭代网络的PyTorch实现与Visdom可视化 解读 | 如何使用深度强化学习帮助自动驾驶汽车通过交叉路口？ 6. 深度学习框架 分布式TensorFlow入坑指南：从实例到代码带你玩转多机器深度学习 资源 | TensorFlow极简教程：创建、保存和恢复机器学习模型 快速开启你的第一个项目：TensorFlow项目架构模板 TensorFlow初学者指南：如何为机器学习项目创建合适的文件架构 教程 | 七个小贴士，顺利提升TensorFlow模型训练表现 教程 | 如何从TensorFlow转入PyTorch 教程 | 如何利用C++搭建个人专属的TensorFlow 谷歌云大会教程：没有博士学位如何玩转TensorFlow和深度学习（附资源） 教程 | 维度、广播操作与可视化：如何高效使用TensorFlow 教程 | PyTorch内部机制解析：如何通过PyTorch实现Tensor 贾扬清撰文详解Caffe2：从强大的新能力到入门上手教程 教程 | TensorFlow 官方解读：如何在多系统和网络拓扑中构建高性能模型 教程 | 如何使用TensorFlow中的高级API：Estimator、Experiment和Dataset 教程 | Docker Compose + GPU + TensorFlow 所产生的奇妙火花 工具方法 教程 | 如何优雅而高效地使用Matplotlib实现数据可视化 教程 | 如何用百度深度学习框架PaddlePaddle做数据预处理 教程 | 一文入门Python数据分析库Pandas 代码优化指南：人生苦短，我用Python 资源 | 从数组到矩阵的迹，NumPy常见使用大总结 教程 | Python代码优化指南：从环境设置到内存分析（一） 资源 | 如何利用VGG-16等模型在CPU上测评各深度学习框架 教程 | 手把手教你可视化交叉验证代码，提高模型预测能力 教程 | 如何使用Kubernetes GPU集群自动训练和加速深度学习？ 教程 | Prophet：教你如何用加法模型探索时间序列数据 初学机器学习的你，是否掌握了这样的Linux技巧？ 云端 教程 | 新手指南：如何在AWS GPU上运行Jupyter noterbook？ 教程 | 只需15分钟，使用谷歌云平台运行Jupyter Notebook 入门 | 完全云端运行：使用谷歌CoLaboratory训练神经网络 边缘设备 教程 | BerryNet：如何在树莓派上实现深度学习智能网关 机器之心实操 | 亚马逊详解如何使用MXNet在树莓派上搭建实时目标识别系统 手把手教你为iOS系统开发TensorFlow应用（附开源代码） 教程 | 如何使用Swift在iOS 11中加入原生机器学习视觉模型 教程 | 如何使用谷歌Mobile Vision API 开发手机应用 开源 | 深度安卓恶意软件检测系统：用卷积神经网络保护你的手机 专栏 | 手机端运行卷积神经网络实践：基于TensorFlow和OpenCV实现文档检测功能 资源 | 用苹果Core ML实现谷歌移动端神经网络MobileNet 教程 | 如何用TensorFlow在安卓设备上实现深度学习推断 深度 | 向手机端神经网络进发：MobileNet压缩指南 硬件 成本14,000元，如何自己动手搭建深度学习服务器？ 资源 | 只需1200美元，打造家用型深度学习配置（CPU+GTX 1080） 教程 | 从硬件配置、软件安装到基准测试，1700美元深度学习机器构建指南 从硬件配置到框架选择，请以这种姿势入坑深度学习 从零开始：深度学习软件环境安装指南 这是一份你们需要的Windows版深度学习软件安装指南 教程 | 一步步从零开始：使用PyCharm和SSH搭建远程TensorFlow开发环境 实用指南：如何为你的深度学习任务挑选最合适的 GPU?（最新版） 深度 | 英伟达Titan Xp出现后，如何为深度学习挑选合适的GPU？这里有份性价比指南 Titan XP值不值？一文教你如何挑选深度学习GPU 吃喝玩乐撸撸猫 教程 | 你来手绘涂鸦，人工智能生成「猫片」：edges2cats图像转换详解 教程 | 萌物生成器：如何使用四种GAN制造猫图 学界 | 宅男的福音：用GAN自动生成二次元萌妹子 深度 | 如何使用神经网络弹奏出带情感的音乐？ 深度 | 人工智能如何帮你找到好歌：探秘Spotify神奇的每周歌单 解读 | 艺术家如何借助神经网络进行创作？ 教程 | 用生成对抗网络给雪人上色，探索人工智能时代的美学 圣诞快乐——Keras+树莓派：用深度学习识别圣诞老人 教程 | 摄影爱好者玩编程：利用Python和OpenCV打造专业级长时曝光摄影图 教程 | 基于遗传算法的拼图游戏解决方案 教程 | AI玩微信跳一跳的正确姿势：跳一跳Auto-Jump算法详解 Money, Money, Money 教程 | 从零开始：如何使用LSTM预测汇率变化趋势 自创数据集，使用TensorFlow预测股票入门 资源 | 利用深度强化学习框架解决金融投资组合管理问题（附 GitHub 实现） 比特币突破8000美元，我们找到了用DL预测虚拟货币价格的方法 教程 | 如何使用深度学习硬件的空余算力自动挖矿 "
236,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737956&idx=2&sn=d357bb2c10490b1c9234f56c8e6d9ec8&chksm=871ac95ab06d404c46393324a9a11f47c920b6bd1c369a3098f7d880437e8be83434cb23e35f&scene=27,深度 | Pedro Domingos总结机器学习研究的12个宝贵经验,Medium 本文是对《终极算法》作者，华盛顿大学教授 Pedro Domingos 的一篇名为《A Few Useful Things to Know about Machine Learning》论文的解读，总结了机器学习研究者和从业者的 12 个宝贵经验。 机器学习算法可以通过从样本中泛化的方式执行重要任务。与手动编程相比，这通常可行且具有成本效益。随着可用数据的日益增加，机器学习可以解决更多重大问题。因此，机器学习被广泛应用于计算机等领域。然而，如果想开发成功的机器学习应用，你需要使用大量难以在教科书中找到的「魔法」。 我最近阅读了华盛顿大学 Pedro Domingos 教授一篇名为「A Few Useful Things to Know about Machine Learning」的论文。这篇论文很了不起，它总结了机器学习研究者和从业者的 12 个宝贵经验，其中包括需要避免的陷阱、值得关注的重点问题、常见问题的答案。我希望能在本文中分享这些经验。在你考虑解决接下来要面对的机器学习问题时，这些经验将有很大帮助。 1. 学习=表征+评估+优化 机器学习算法通常仅由 3 部分构成： 表征（Representation）：分类器必须用计算机可以处理的形式化语言来表示。相反地，为训练模型选择一个表征就等同于选择可训练分类器的集合。这个集合称为训练模型的「假设空间」。如果分类器不在「假设空间」中，那么它就不能由训练所得到。一个相关的问题是如何表征输入，即使用哪些特征。 评估（Evaluation）：需要一个评估函数来区分分类器的好坏。算法内部使用的评估函数可能与分类器优化的外部评估函数不同，这是为了便于优化，并且是由我们下一节所要讨论的问题导致的。 优化（Optimization）：最后，我们要用一种方法搜索得分最高的分类器。优化方法的选择对于提升模型的效率非常关键。此外，如果评估函数具有一个以上的最优值，则优化方法有助于确定最后产生的分类器。新的训练模型一开始常常使用现有的优化器，后来常会转而使用自定义的优化器。 2. 泛化能力很关键 机器学习的基本目标是对训练集之外的样本进行泛化。这是因为，无论我们有多少数据，我们都不太可能在测试中再次看到完全相同的例子。在训练集上具有良好表现很容易。机器学习初学者最常犯的错误是把模型放在训练数据中进行测试，从而产生成功的幻觉。如果被选择的分类器在新的数据上进行测试，通常而言，结果往往和随机猜测相差无几。所以，如果你雇佣他人建立分类器，一定要留一些数据给你自己，以便在他们给你的分类器中进行测试。反过来说，如果有人雇佣你建立一个分类器，请在一开始保留一些数据，只用这些数据对你的分类器进行最终测试。 3. 仅有数据是不够的 将泛化能力作为目标，还有另一个重要的后果：仅有数据是不够的，不管你拥有多少数据。 这个消息听起来真叫人沮丧。那么，我们怎么能奢求它学到东西呢？所幸，现实世界中我们想学习的函数并不都是从数学上可能的函数中提取出来的！事实上，使用一般假设——例如平滑性、相似样本有相似分类、有限的依赖性或有限复杂度——往往能做得足够好，这也正是机器学习能如此成功的大部分原因。正如演绎一样，归纳（训练模型所做的）是一个知识杠杆——它将少量知识输入转化为大量知识输出。归纳是一个比演绎更为强大的杠杆，仅需更少的知识就能产出有用的结果。不过，它仍然需要大于零的知识输入才能工作。而且，正如任何一个杠杆一样，我们输入得越多，得到的也越多。 这样回想起来，训练过程中对知识的需求没什么好惊讶的。机器学习并非魔术，它无法做到无中生有，它所做的是举一反三。如同所有的工程一样，编程需要做大量的工作：我们必须从头开始构建所有的东西。训练的过程更像是耕种，其中大部分工作是自然完成的。农民将种子与营养物质结合起来，种植作物。训练模型将知识与数据结合起来，编写程序。 4. 过拟合具有多面性 如果我们所拥有的知识和数据不足以完全确定正确的分类器，分类器（或其中的一部分）就可能产生「幻觉」。所获得的分类器并不是基于现实，只是对数据的随机性进行编码。这个问题被称为过拟合，是机器学习中棘手的难题。如果你的训练模型所输出的分类器在训练数据上准确率是 100％，但在测试数据上准确率只有 50％，那么实际上，该分类器在两个集合上的输出准确率总体可能约为 75％，它发生了过拟合现象。 在机器学习领域，人人都知道过拟合。但是过拟合有多种形式，人们往往不能立刻意识到。理解过拟合的一种方法是将泛化的误差进行分解，分为偏差和方差。偏差是模型不断学习相同错误的倾向。而方差指的是不管真实信号如何，模型学习随机信号的倾向。线性模型有很高的偏差，因为当两个类之间的边界不是一个超平面时，模型无法做出调整。决策树不存在这个问题，因为它们可以表征任何布尔函数。但是另一方面，决策树可能方差很大：如果在不同训练集上训练，生成的决策树通常差异很大，但事实上它们应该是相同的。 交叉验证可以帮助对抗过拟合，例如，通过使用交叉验证来选择决策树的最佳规模用于训练。但这不是万能的，因为如果我们用交叉验证生成太多的参数选择，它本身就会开始产生过拟合现象。 除交叉验证之外，还有很多方法可以解决过拟合问题。最流行的是在评估函数中增加一个正则化项。举个例子，这样一来就能惩罚含更多项的分类器，从而有利于生成参数结构更简单的分类器，并减少过拟合的空间。另一种方法是在添加新的结构之前，进行类似卡方检验的统计显著性检验，在添加新结构前后确定类的分布是否真的具有差异。当数据非常少时，这些技术特别有用。尽管如此，你应该对某种方法完美解决了过拟合问题的说法持怀疑态度。减少过拟合（方差）很容易让分类器陷入与之相对的欠拟合误差（偏差）中去。如果要同时避免这两种情况，需要训练一个完美的分类器。在没有先验信息的情况下，没有任何一种方法总能做到最好（天下没有免费的午餐）。 5 高维度会挫伤直觉 除了过拟合，机器学习中最大的问题就是维度灾难。这一名词是由 Bellman 在 1961 年提出的，指的是当输入维度很高时，许多在低维工作正常的算法将无法正常工作。但是在机器学习中，它的意义更广。随着样本维度（特征数量）的增加，进行正确泛化变得越来越难，因为固定大小的训练集对输入空间的覆盖逐渐缩减。 高维的一般问题是，来自三维世界的人类直觉通常不适用于高维空间。在高维度当中，多元高斯分布的大部分数据并不接近平均值，而是在其周围越来越远的「壳」中；此外，高维分布的大部分体积分布在表面，而不是体内。如果恒定数量的样本在高维超立方体中均匀分布，那么在超越某个维数的情况下，大多数样本将更接近于超立方体的一个面，而不是它们的最近邻。此外，如果我们通过嵌入超立方体的方式逼近一个超球面，那么在高维度下，超立方体几乎所有的体积都在超球面之外。这对于机器学习来说是个坏消息，因为一种类型的形状常常可以被另一种形状所逼近，但在高维空间中却失效了。 建立二维或三维分类器容易；我们可以仅通过视觉检查找出不同类别样本之间的合理边界。但是在高维中，我们很难理解数据的分布结构。这又反过来使设计一个好的分类器变得困难。简而言之，人们可能会认为收集更多的特征一定不产生负面作用，因为它们最多只是不提供有关分类的新信息而已。但事实上，维度灾难的影响可能大于添加特征所带来的利益。 6 理论保证与实际的出入 机器学习论文中充斥着理论保证。最常见的保证就是关于保持模型良好泛化能力的训练样本数量约束问题。首先，该问题显然是可证的。归纳通常与演绎相对：通过演绎，你可以确保结论是正确的; 在归纳中，所有臆想都被摒弃。或许这就是传世的古老智慧。近十年的主要突破就是认识到归纳的结果是可证的这一事实，尤其在我们愿意给出概率保证时。 我们必须斟酌这类约束意味着什么。这并不意味着，如果你的网络返回与某个特定训练集一致的假设，那么这个假设就可能具有很好的泛化能力。而是，给定一个足够大的训练集，你的网络很可能会返回一个泛化能力好的假设或无法得到一致的假设。这类约束也没有教我们如何选择一个好的假设空间。它只告诉我们，如果假设空间包含好的分类器，那么随着训练集的增大，网络训练出一个弱分类器的概率会减小。如果我们缩小假设空间，约束条件作用会增强，但是训练出一个强分类器的概率也会下降。 另一种常见的理论保证是渐进性：假如输入的数据规模是无穷大的，那么网络肯定会输出一个强分类器。听起来靠谱，但是由于要保证渐近性，选择某个网络而非另一个就显得过于轻率。在实践中，我们很少处于渐近状态（也被称为「渐进区」）。而且，由上面讨论的偏差 - 方差权衡可知，如果网络 A 在具有海量数据时比网络 B 好，则在有限数据情况下，B 往往比 A 好。 理论保证在机器学习中存在的意义不仅仅是作为评判实际决策的标准，而且是理解的方法及设计算法的动力。鉴于此，它十分有用。事实上，这么多年以来，正是理论联系实际促进了机器学习的飞跃式进步。但要注意：学习是一个复杂的现象，它在理论上说得通，在实际工作中可行，也并不表示前者是导致后者的原因。 7 特征工程是关键 最后，有些机器学习项目大获成功，有些却失败了。这是什么造成的？最重要的影响因素就是使用的特征。如果你获取到很多独立的且与所属类别相关的特征，那么学习过程就很容易。相反，若某一个类是特征的极其复杂的函数，你的模型可能无法学习到该函数。通常来说，原始数据格式很不适合学习，但是可以基于它来构建特征。这正是机器学习项目最重要的部分，通常也是最有趣的部分，直觉、创造力、「魔术」和技术同样重要。 初学者常常会惊讶于机器学习项目实际上花在机器学习上的时间很少。但是当你将收集、整合、清洗和预处理数据以及将数据重构成特征过程中解决错误等琐事所消耗的时间考虑在内就不奇怪了。而且，机器学习并不只是构建数据集跑一次模型就没事了，它通常是一个跑模型、分析结果、修改数据集/模型的迭代式过程。学习是其中最快的部分，但这取决于我们已经可以熟练运用它！特征工程因为针对特定的领域，所以很难做，而模型架构的适用范围更广泛。但是，这二者之间并没有清晰的界线，这通常可以解释那些整合了领域知识的模型具有更好的性能。 8 数据量为王 在计算机科学的大多数领域，时间和内存是两大紧缺资源。但在机器学习中，数据集俨然是第三个紧缺资源。随着时间的推移，瓶颈之争也在不断改变。在 20 世纪 80 年代，数据通常是瓶颈。而如今时间更为宝贵。我们今天有海量的数据可用，但是却没有充足的时间去处理它，这些数据因此被搁置。这就产生了一个悖论：即使在原则上讲，大量的数据意味着可以学习到更复杂的分类器，但在实践中，我们往往采用更简单的分类器，因为复杂的分类器意味着更长的训练时间。部分解决方案是提出可以快速学习到复杂分类器的方法，且今天在这一方向上确实取得了显著的进展。 使用更智能的算法的收益不如期望的部分原因是，第一次取近似值时，它跟其它算法无异。当你认为表征方式之间的区别与规则、神经网络之间的区别类似时，这会让你惊讶。但事实是，命题规则可以轻易地编码进神经网络，并且其它的表征方式之间也有类似的关系。模型本质上都是通过将近邻样本分到相同的类别而实现的，关键差异在于「近邻」的含义。对于非均匀分布的数据，模型可以产生广泛不同的边界，同时在重要的区域（具有大量训练样例的区域，因此也是大多数文本样例可能出现的区域）中产生相同的预测。这也能解释为什么强大的模型可能是不稳定的但仍然很准确。 一般来说，我们首先要考虑最简单的模型（例如，先考虑朴素贝叶斯而非 logistic 回归，先考虑 K-近邻而非支持向量机）。模型越复杂越诱人，但是它们通常很难使用，因为你需要调整很多的节点以获得好的结果，同时，它们的内部构造极其不透明。 模型可以分为两种主要类型：一种是规模固定的模型，例如线性分类器，另一种是表征能力随数据集增强的模型，例如决策树。固定规模的模型只能利用有限的数据。规模可变的模型理论上可以拟合任何函数，只要有足够大的数据集，但是现实很骨感，总存在算法的局限性或计算成本。而且，由于维度灾难，现有的数据集可能不够。鉴于这些原因，更智能的算法—那些充分利用数据和计算资源的算法--如果你愿意努力去调试，最终会得到好的结果。在设计模型与学习分类器之间并没有十分清晰的界线；但是，任何给定的知识点都可以编码进模型或从数据中学习到。因此，模型设计往往是机器学习项目中的重要组成部分，设计者最好拥有相关专业背景。 9 不单单学习一个模型 在机器学习发展的早期，大家都有各自喜爱的模型，用一些先验的理由说明它的优越性。研究员对模型开发了大量的变体并从中挑选一个最优的模型。随后，系统的经验比较表明，最好的模型随应用的改变而改变，开始出现了包含许多不同模型的系统。现在的研究开始尝试调试多个模型的不同变体，然后挑选表现最好的那一个。但研究人员开始注意到，不选择找到的最佳变体，而是结合多个变体，却得到了更好的结果（通常会好很多），而且这没有增加工作量。 现在，模型集成已经是标准方法。其中最简单的技术叫 bagging 算法，我们仅通过重采样来生成训练数据集的随机变体，再基于这些变体分别学习分类器，并通过投票整合这些分类器的结果。此法的可行性在于它大幅减少了方差，且只微微提升了一点偏差。在 boosting 算法中，训练样例有权重，而且这些权重各不相同，因此每个新分类器都把重点放在前面的模型会出错的样例上。在 stacking 算法中，每个单独的分类器的输出作为「高层」模型的输入，这些高层模型会以最佳方式组合这些模型。 还有很多其它的方法，就不一一列举了，但是总的趋势是规模越来越大的集成学习。在 Netflix 的奖金激励下，全世界的团队致力于构建最佳视频推荐系统。随着竞赛的推进，竞赛团队发现通过结合其它团队的模型可以获得最佳结果，同时这也促进团队的合并。冠军和亚军模型都是由 100 多个小模型组成的集成模型，两个集成模型相结合可进一步提高成绩。毫无疑问，将来还会出现更大的集成模型。 10 简单不意味着准确 奥卡姆剃刀原理指出，如无必要，勿增实体。在机器学习中，这通常意味着，给定两个具有相同训练误差的分类器，两者中较简单的分类器可能具有最低的评估误差。关于这一说法的佐证在文献中随处可见，但实际上有很多反例用来反驳它，「没有免费午餐」定理质疑它的真实性。 我们在前文中也看到了一个反例：集成模型。即使训练误差已经达到零，通过增加分类器，增强集成模型的泛化误差仍然可以继续减少。因此，与直觉相悖，模型的参数数量与其过拟合趋势并没有必然的联系。 一个巧妙的观点是将模型复杂性等同于假设空间的大小，因为较小的空间允许用较短的编码表征假设。类似理论保证部分中的界限可能被理解成较短的假设编码有更好的泛化能力。通过在有先验偏好的空间中对假设进行较短的编码，我们可以进一步细化这一点。但是把这看作准确率和简单性之间的权衡的证明则是循环论证：我们通过设计使偏爱的假设更简单，如果它们准确率不错，那是因为偏爱假设的正确，而不是因为在特定表征下假设的「简单」。 11 可表征并不意味着可学习 所有运用于非固定规模的模型表征实际上都有「任意函数都可以使用该表征来表示或无限逼近」之类的相关定理。这使得某表征方法的偏好者常常会忽略其它要素。然而，仅凭可表征性并不意味着模型可以学习。例如，叶节点多于训练样本的决策树模型就不会学习。在连续的空间中，通常使用一组固定的原语表征很简单的函数都需要无限的分量。 进一步讲，如果评估函数在假设空间有很多局部最优点（这很常见），模型可能就找不到最优的函数，即使它是可表征的。给定有限的数据、时间及存储空间，标准的模型只能学到所有可能函数集的一个很小的子集，且这个子集随所选的表征方法的不同而不同。因此，关键问题不在「模型是否可表示」，而「模型是否可学习」以及尝试不同的模型（甚至是集成模型）是很重要的。 12 相关性并不意味着因果关系 相关性并不意味着因果关系这一点被频繁提起，以至于都不值得再批评。但是，我们讨论的某类模型可能只学习相关性，但是它们的结果通常被看作是表征因果关系。有问题吗？如果有，那么大家为何还这么做？ 通常是不对的，预测模型学习的目标是用它们作为行动的指南。如果我们发现人们在买啤酒的时候也会买纸尿布，那么把啤酒放在纸尿布旁边或许会提高销量。但如果不实际进行实验则很难验证。机器学习通常用于处理观测数据，其中预测变量不受模型的控制，和实验数据相反（可控的）。一些学习算法也许可以通过观测数据挖掘潜在的因果关系，但是实用性很差。另一方面，相关性只是潜在的因果关系的标识，我们可以用它指导进一步的研究。 结论 跟任何学科一样，机器学习有很多难得的「人类智慧」，但对模型的成功至关重要。Domingos 教授的论文总结了一些金科玉律。机器学习的非理论介绍《终极算法》也是个不错的学习宝库。  原文链接：https://medium.com/@james_aka_yale/12-useful-things-to-know-about-machine-learning-c599be92c98d 
237,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737992&idx=1&sn=5cb431b3789e18ee99445c2cb1a6c418&chksm=871ac936b06d40201051c1974c37895d61f7e043a136a60d3b97c56fb37e4d4bd91edd41ef08&scene=27,陈天奇等人提出TVM：深度学习自动优化代码生成器,"TVM 是由华盛顿大学在读博士陈天奇等人提出的深度学习自动代码生成方法，去年 8 月 。该技术能自动为大多数计算硬件生成可部署优化代码，其性能可与当前最优的供应商提供的优化计算库相比，且可以适应新型专用加速器后端。近日，这项研究的论文《TVM: End-to-End Optimization Stack for Deep Learning》终于完成，内容包含新方法的介绍与讨论，以及 TVM 在英伟达、AMD 的 GPU、树莓派及一些 FPGA 上的性能评估。 项目链接：https://github.com/dmlc/tvm 深度学习模型可以识别图像、处理自然语言，以及在部分具有挑战性的策略游戏中击败人类。在其技术发展的过程中，现代硬件稳步推进的计算能力扮演了不可或缺的作用。很多目前最为流行的深度学习框架，如 TensorFlow、MXNet、Caffe 和 PyTorch，支持在有限类型的服务器级 GPU 设备上获得加速，这种支持依赖于高度特化、供应商特定的 GPU 库。然而，专用深度学习加速器的种类越来越多，这意味着现代编译器与框架越来越难以覆盖所有的硬件。 显而易见，以现有的点到点方式实现不同深度学习框架对所有种类的硬件进行后端支持是不现实的。我们的最终目标是让深度学习负载可以轻松部署到所有硬件种类中，其中不仅包括 GPU、FPGA 和 ASIC（如谷歌 TPU），也包括嵌入式设备，这些硬件的内存组织与计算能力存在着显著的差异（如图 1 所示）。考虑到这种需求的复杂性，开发一种能够将深度学习高级程序降低为适应任何硬件后端的低级优化代码的优化框架是最好的方法。 目前的深度学习框架依赖于计算图的中间表示来实现优化，如自动微分和动态内存管理 [3,7,4]。然而，图级别的优化通常过于高级，无法有效处理硬件后端算子级别的转换。另一方面，目前深度学习框架的算子级别库通常过于僵化，难以轻松移植到不同硬件设备上。为了解决这些问题，我们需要一个可实现从计算图到算子级别的优化，为各种硬件后端带来强大性能的编译器框架。 图 1：CPU、GPU 与类 TPU 加速器需要不同的片上存储架构和计算基元。在生成优化代码时我们必须考虑这一问题。 图 2：TVM 堆栈图。目前的堆栈支持多种深度学习框架以及主流 CPU、GPU 以及专用深度学习加速器。 优化的四大基本挑战 深度学习的优化编译器需要同时展示高级别与低级别的优化，在论文中，研究人员总结了在计算图级别与张量算子级别上的四大基本挑战： 高级数据流复写：不同的硬件设备可能具有截然不同的内存层次结构，因此，融合算子与优化数据布局的策略对于优化内存访问至关重要。 跨线程内存复用：现代 GPU 与专用加速器的内存可被多个计算核心共享，传统的无共享嵌套并行模式已不再是最优方法。为优化内核，在共享内存负载上的线程合作很有必要。 张量计算内部函数：最新的硬件带来了超越向量运算的新指令集，如 TPU 中的 GEMM 算子和英伟达 Volta 架构中的 Tensor Core。因此在调度过程中，我们必须将计算分解为张量算术内部函数，而非标量或向量代码。 延迟隐藏（Latency Hiding）：尽管在现代 CPU 与 GPU 上，同时拥有多线程和自动缓存管理的传统架构隐藏了延迟问题，但专用的加速器设计通常使用精简控制与分流，这为编译器堆栈的调度带来了复杂性。所以，调度仍需仔细，以隐藏内存访问延迟。 TVM：一个端到端优化堆栈（见图 2），该端到端优化编译器堆栈可降低和调整深度学习工作负载，以适应多种硬件后端。TVM 的设计目的是分离算法描述、调度和硬件接口。该原则受到 Halide [22] 的计算／调度分离思想的启发，而且通过将调度与目标硬件内部函数分开而进行了扩展。这一额外分离使支持新型专用加速器及其对应新型内部函数成为可能。TVM 具备两个优化层：计算图优化层，用于解决第一个调度挑战；具备新型调度基元的张量优化层，以解决剩余的三个挑战。通过结合这两种优化层，TVM 从大部分深度学习框架中获取模型描述，执行高级和低级优化，生成特定硬件的后端优化代码，如树莓派、GPU 和基于 FPGA 的专用加速器。该论文做出了以下贡献： 我们构建了一个端到端的编译优化堆栈，允许将高级框架（如 Caffe、MXNet、PyTorch、Caffe2、CNTK）专用的深度学习工作负载部署到多种硬件后端上（包括 CPU、GPU 和基于 FPGA 的加速器）。 我们发现了提供深度学习工作负载在不同硬件后端中的性能可移植性的主要优化挑战，并引入新型调度基元（schedule primitive）以利用跨线程内存重用、新型硬件内部函数和延迟隐藏。 我们在基于 FPGA 的通用加速器上对 TVM 进行评估，以提供关于如何最优适应专用加速器的具体案例。 我们的编译器可生成可部署代码，其性能可与当前最优的特定供应商库相比，且可适应新型专用加速器后端。 论文：TVM: End-to-End Optimization Stack for Deep Learning 论文链接：https://arxiv.org/abs/1802.04799 摘要： 可扩展框架，如 TensorFlow、MXNet、Caffe 和 PyTorch 是目前深度学习领域中最流行和易用的框架。但是，这些框架只对窄范围的服务器级 GPU 进行优化，要把工作负载部署到其他平台，如手机、嵌入式设备和专用加速器（如 FPGA、ASIC），则需要大量手动工作。我们提出了 TVM，一个端到端的优化堆栈，具备图形级和算子级的优化，以为多种硬件后端提供深度学习工作负载的性能可移植性。我们讨论了 TVM 所解决的深度学习优化挑战：高级算子融合（operator fusion）、多线程低级内存重用、任意硬件基元的映射，以及内存延迟隐藏。实验结果证明 TVM 在多个硬件后端中的性能可与适应低功耗 CPU 和服务器级 GPU 的当前最优库相比。我们还通过针对基于 FPGA 的通用深度学习加速器的实验，展示了 TVM 对新型硬件加速器的适应能力。该编译器基础架构已开源。 "
238,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737956&idx=3&sn=d3597e73bd7457d608488cf1087389f4&chksm=871ac95ab06d404c1952b6fd59d0ec59267e5647cd52b39d2a9e8adf070b2c4c372dd8f0e219&scene=27,ICLR 2018 | 谷歌大脑Wasserstein自编码器：新一代生成模型算法,"arXiv 变分自编码器（VAE）与生成对抗网络（GAN）是复杂分布上无监督学习主流的两类方法。近日，谷歌大脑 Ilya Tolstikhin 等人提出了又一种新思路：Wasserstein 自编码器，其不仅具有 VAE 的一些优点，更结合了 GAN 结构的特性，可以实现更好的性能。该研究的论文《Wasserstein Auto-Encoders》已被即将在 4 月 30 日于温哥华举行的 ICLR 2018 大会接收。 表示学习（representation learning）领域最初由监督式方法实现，使用超大标注数据集得到了突出的结果。而之前通过无监督方式生成的模型往往使用概率方法处理低维数据。近年来，这两种方法逐渐结合。在交叉点形成的新领域，出现变分自动编码器（VAE）[1] 这一成熟的方法，虽然理论成熟，但应用于自然图像时会生成模糊的样本。相比之下，生成对抗网络（GAN）[3] 在模型采样的图像的视觉质量方面更加突出，但它的缺点是没有编码器，更难训练，并且有「模式崩溃」（mode collapse）的问题，最终的模型无法捕获真实数据分布的所有变化。此前的研究中，研究人员已经分析过很多 GAN 结构和 VAE、GAN 组合结构的问题，但我们还没有发现一个把 GAN 和 VAE 的优点适当结合的统一框架。 谷歌大脑的这项工作建立在 L. Mescheder 等人 [11] 提出的理论分析的基础上。根据 Wasserstein GAN 和 VEGAN，我们从最佳传输（OT：optimal transport）的角度来看生成建模。最佳传输成本（The OT cost）[5] 是一种测量概率分布之间距离的方法，且比其它方法（包括与原始 GAN 算法相关的 f 增益（f-divergences））的拓扑更弱。这在应用里面非常重要，因为在输入空间 X 中，数据通常是靠低维流形支持的。因此，更强烈的距离概念（如捕获分布间密度比率的 f 增益）往往最大，没有给训练提供有用的梯度。相比之下，有人称 OT 会有更好的表现 [4, 7]，尽管在其 GAN 类的实现中，需要在目标中增加约束项或正则项。 这篇文章中，我们的目标是最小化实际（但未知）的数据分布 PX 、由隐藏代码（latent codes）Z ∈ Z 的先验分布规定的隐变量模型 PG 和数据点 X ∈（X|Z）的生成模型 PG(X|Z) 之间的 OT Wc(PX, PG)。我们的主要贡献如下（参见图 1）： Wasserstein 自动编码器（WAE），一个新的正则化自动编码器家族（算法 1，2 和等式 4），可以最小化任何成本函数 c 的最佳传输 Wc（PX，PG）。与 VAE 类似，WAE 的目标由两项组成：c-重构成本（c-reconstruction cost）和一个正则化矩阵，正则化矩阵用于惩罚 Z：PZ 中的两个分布和编码数据点的分布矛盾，即 QZ := EPX [Q(Z|X)]。当 c 是成本的平方，DZ 是 GAN 目标时，WAE 与 [2] 中的对抗自编码器一致。 WAE 通过成本平方 c(x, y) = ||x−y||2 在 MNIST 和 CelebA 数据集上进行评估。研究员的实验表明，WAE 保持了 VAE 的良好特性（训练稳定，编码器-解码器架构和一个好的潜在流形结构），同时生成了质量更好的样本，接近 GAN 生成的样本。 我们提出并检验了两个不同的正规化矩阵 DZ（PZ，QZ）。一个基于 GAN 和隐空间（latent space）Z 的对抗训练，另一个利用最大均值差异（maximum mean discrepancy），可以很好地用于匹配高维标准正态分布 PZ[8]。 最后，《From optimal transport to generative modeling: the VEGAN cookbook》[11] 中和用来推导 WAE 目标的理论考虑本身可能会很有趣。特别是，定理 1 表明在生成模型的情况下，Wc（PX，PG）的原始形式相当于涉及优化概率编码器 Q（Z | X）优化的问题。 本文结构如下。第二部分我们回顾了一个新的自动编码器公式，用来计算 PX 和 [11] 中推导的隐变量模型 PG 之间的 OT。放宽了最终的约束优化问题（Wasserstein 自动编码器的目标）。我们得出了两种不同的正则化矩阵，得出 WAE-GAN 和 WAE-MMD 算法。第三部分讨论相关的工作。第四部分是实验结果，并以未来工作有前景的方向结束。 表 1：CelebA 中样本的 FID 得分（数字越小越好）。 论文：Wasserstein Auto-Encoders 论文链接：https://arxiv.org/abs/1711.01558 摘要： 我们提出了 Wasserstein 自动编码器（WAE）——一种用于构建数据分布生成模型的新算法。WAE 将模型分布与目标分布之间的 Wasserstein 距离的惩罚形式最小化，导出了与变分自动编码器（VAE）所使用的不同的正则化矩阵 [1]。此正则化矩阵鼓励编码的训练分布与之前的相匹配。我们比较了我们的算法和其它几种技术，表明它是对抗自动编码器（AAE）的推广 [2]。我们的实验表明，WAE 具有 VAE 的许多特性（训练稳定，编码器-解码器架构，良好的潜在流形结构），同时生成了通过 FID 得分衡量的质量更好的样本。  "
239,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738149&idx=4&sn=0d02fc17c1e8eb96b15ecf5709223e21&chksm=871ac99bb06d408d341623b62e17978d38b5d405202fb4305de35e7ec8b966513d3b677f9026&scene=27,报告 | 牛津、剑桥、OpenAI 等多家机构发布重磅报告，论述恶意人工智能的「罪与罚」,OpenAI 日前，OpenAI 联合牛津大学、剑桥大学等多家机构发布了一份报告，该报告调查了恶意使用人工智能的潜在安全威胁图景，并提出了多种预测、预防和缓解威胁的方式。报告分析了 AI 在数字安全、物理安全和政治安全领域对威胁图景的影响，并向 AI 研究者和其他利益相关者提出了四种高级推荐做法。报告还提出了多个有前景的未来研究领域，可扩展防御的范围，或者减弱攻击的有效性和实施可能。最后，报告讨论了攻击者和防御者的长期平衡，但并未解决该问题。机器之心编译了该报告的部分内容，包括执行摘要和第二章的主要内容。详情请查看原文链接。 报告链接：https://arxiv.org/pdf/1802.07228.pdf 目录   人工智能和机器学习的能力以前所未有的速度飞速发展。这些技术已经有很多优秀的应用，从机器翻译到医疗图像分析，还有更多应用正在开发中。但是长期以来，很少有人注意人工智能被恶意使用的情况。本报告调查了恶意使用人工智能技术的潜在安全威胁，并提出更好地预测、预防和缓解这些威胁的方法。我们分析攻击者和防御者之间的长期平衡，但并未得到最终的解决方案。我们主要关注在未实施充分的防御时可能出现的攻击。 对于不断改变的威胁环境，我们推荐以下四种高级做法： 1. 决策者应与技术研究者紧密合作，调查、预防和缓解恶意使用人工智能的潜在安全威胁。 2. 人工智能领域的研究者和工程师应严肃对待其工作的双刃剑性质，考虑错误使用 AI 对研究优先级和规范的影响，以及在可预见有害的应用时，先发制人地找到相关的参与者。 3. 研究领域应该用更成熟的方法确认最佳实践，以解决对 AI 双刃剑性质的担忧，如计算机安全。且最佳实践应该用于适合的场景中。 4. 积极扩展更多利益相关者和领域专家参与讨论这些难题。 随着 AI 变得越来越强大和普遍，我们预测 AI 系统的广泛使用会给威胁图景带来如下改变： 现有威胁的扩展。攻击的成本可能因 AI 系统的扩展使用而降低（用于完成往常需要人类劳动、智力和专业知识的任务）。这将扩大实施特定攻击的参与者的范围、实施攻击的速度，以及潜在目标的类型。 带来新的威胁。AI 系统用于完成人类不太可能完成的任务时，可能带来新的攻击。此外，恶意使用者可能会利用防御者部署的 AI 系统的弱点。 威胁的典型特征发生改变。我们认为有理由预测 AI 广泛使用带来的攻击会更加高效、精准定位、难以归因，且可能会利用 AI 系统的弱点。 我们从以下三个安全领域进行分析，通过具备代表性的例子阐述这些领域可能的威胁变化： 数字安全。使用 AI 自动化完成涉及网络攻击的任务将缓解攻击的规模和效能之间的现有矛盾。这可能扩大与劳动密集型网络攻击（如鱼叉式网络钓鱼）有关的威胁。我们还预测会出现利用人类弱点的新攻击（如利用语音合成技术进行模仿）、利用现有软件漏洞的新攻击，或利用 AI 系统漏洞的新攻击（如通过对抗样本和数据下毒进行攻击）。 物理安全。使用 AI 自动化完成使用无人机和其他物理系统（如自动武器系统）的任务可能会扩大与这些攻击相关的威胁。我们还预测会出现颠覆网络物理系统（如导致自动驾驶汽车翻车）或涉及无法远程指挥的物理系统（如数千架微型无人机）的新攻击。 政治安全。使用 AI 自动化完成涉及监控（如分析收集的数据）、劝导（如进行针对性宣传）和欺骗（如操纵视频）的任务可能会扩大与隐私侵犯和社会操纵相关的威胁。我们还预测新攻击会利用不断提升的能力，基于获取数据分析人类的行为、情绪和信仰。这些担忧在威权国家中最严重，但是也可能削弱民主。 除了上文列举的高级推荐做法以外，我们还在以下四种主要研究领域中探索了多个开放性问题和潜在的干预方法： 从网络安全社区学习。在网络安全和 AI 攻击的交集处，我们需要探索和潜在地实现「红队判研」（red teaming）、正式验证、安全工具、安全硬件，以及负责任地揭露 AI 漏洞。 探索不同的开放性模型。随着 AI 和 ML 的双刃剑属性逐渐明显，我们需要重新设置有关研究开放性的规范和制度，从特别关注的技术领域中发表前的风险评估、中央访问授权模型开始，共享支持安全保障的制度，以及从其他双刃剑技术中学到的经验。 提升责任意识。AI 研究者和组织在塑造 AI 赋能世界的安全图景中占据独特位置。我们强调教育、伦理声明和标准、框架、规范和期望。 开发技术和政策解决方案。我们调查了大量颇有前景的技术和政策干预方法，可能有助于构建更安全的 AI 未来。进一步研究的高级领域包括隐私保护、维护公众安全的协调使用 AI、监控 AI 相关资源，以及其他的立法和监管措施。 这些干预方法不仅需要 AI 研究者和企业的注意和行动，还需要立法者、政府官员、监管机构、安全研究者和教育者的协作。挑战很大，风险很高。 人工智能的安全相关属性 人工智能是一种双刃剑技术。AI 系统及其设计知识可有民用和军用两种目的，进而带来或好或坏的后果。一些需要智能的任务是良性的，一些则不是。人工智能如同人类智能一样，也是一把双刃剑。对于人工智能研究员来说，避免创造可直接服务于有害目的的研究成果和系统是很难的（尽管一些案例中，基于特定领域的属性，需要特别小心）。很多可以自动化执行的问题本身就是双刃剑。举例来说，用来检查软件漏洞的系统既有攻击性也有保护性的应用，自动送包裹的无人机和自动送炸弹的无人机的区别也并不是很大。此外，旨在提高我们对 AI、AI 能力和人类对 AI 控制力的认识的基础研究，也显现出了天然的双重用处的属性。 AI 系统通常既高效且可扩展。这里，如果 AI 系统训练好且部署之后，它可以比人更快的或者更廉价地完成特定的任务，那么我们说它是「高效的」。如果给出一个特定任务，增加这个系统的算力或复制更多的系统，可以完成更多的任务，则我们说这个 AI 系统是「可扩展的」。例如，一个典型的人脸识别系统既是高效的也是可扩展的；一旦它被训练好并且部署之后，它可用于处理很多不同的摄像机原始数据，完成和人类分析师一样的分任务，还更加廉价。 AI 系统可以超过人类的能力。具体来说，一个 AI 系统可以比人更好地完成一个给定的任务。举例来说，如上所述，现在 AI 系统在象棋和围棋比赛中超越了顶级的人类玩家。在其他任务里，不管是良性的还是恶性的，没有任何明确的证据说明，目前观察到的人类水平是能达到的最高水平，即使是近期表现最好且保持稳定的领域也是如此。 AI 系统可以增加匿名性和心理距离。很多任务涉及和他人交流、观察或者被观察、决定如何回应对方，或者物理上和他们一起出现。AI 系统允许这样的任务被自动化处理，从而使得攻击参与者保持他们的匿名性和与目标所受影响的心理距离。比如，某人用自动化武器系统实施暗杀，与手枪相比，这避免了出现在现场和观察受害者。 AI 发展带来了快速的扩散。尽管攻击者可能会发现获取或复制 AI 系统所需硬件（比如强大的电脑或无人机）非常昂贵时，但获取软件和相关的科学研究通常比较容易。确实，很多新的 AI 算法用几天或者几周的时间就可以复制。此外，AI 研究文化的特征之一就是高程度的开放性，很多论文都带有源码。即使事实证明限制特定技术发展的扩散是必要的，这也很难达到。 今天的 AI 系统经历了大量新的未解决漏洞，包括数据下毒攻击（输入可使学习系统犯错的训练数据）、对抗样本（造成机器学习系统错误分类的输入）和利用自治系统目标设计中的缺陷。这些缺陷和传统软件缺陷（如缓冲溢出）很不同，同时也证明 AI 系统在很多方面可以超越人，但也会犯很多人不会犯的错误。 威胁图景的一般影响 根据上文讨论的内容，我们得出 AI 威胁图景的三个高级影响方面。在缺少充足防御的情况下，AI 的发展会： 扩大现有威胁 带来新的威胁 威胁的典型特征发生改变   具体来说，我们预测攻击将会更高效、更精准、更难以归因，也更可能利用 AI 系统的漏洞。 这些变化使「Interventions」部分所讨论的积极响应办法成为必然。   扩展现有威胁 对于许多熟悉的攻击，我们预测 AI 的发展会扩大实施攻击者的范围、参与者实施攻击的速度和可攻击目标的范围。该观点根据 AI 系统的效能、可扩展性和易扩散性得来。具体来说，高效 AI 系统的扩散可以增加可实施特定攻击的参与者的数量。如果相关的 AI 系统具备可扩展性，则已经具备实施攻击资源的参与者可能获得更高速实施攻击的能力。最后，由于这两项发展，攻击那些原本从成本收益分析角度来看并不划算的目标也是可行的。 举例来说，可能以这种方式扩展的威胁有，鱼叉式网络钓鱼攻击造成的威胁。这些攻击利用个性化信息提取个人敏感信息或钱财，攻击者通常以目标朋友、同事或专业联系人的姿态出现。最先进的鱼叉式网络钓鱼攻击需要大量高技术劳动，攻击者必须确定高价值目标，研究目标的社交和职业网络，然后生成适合该语境的信息。 如果相关的研究和合成任务可以自动化完成，则可能会有更多的人参与鱼叉式网络钓鱼。例如，攻击者和目标说同样的语言可能都不再是必需条件。攻击者可能还有能力以一种目前不可行的方式参与大规模鱼叉式网络钓鱼，因此他们在选择目标时更加「一视同仁」。类似的分析也可用于大部分网络攻击，以及目前需要大量人类劳动的物理安全或政治安全威胁。 AI 的发展还可能增加攻击参与者实施特定攻击的意愿，从而扩展现有威胁。该观点源于不断增加的匿名性和心理距离。如果攻击参与者知道某项攻击不会被追踪到自己，如果他们对目标的怜悯更少，他们经历过的道德挣扎更少，那么他们也更有可能实施该攻击。心理距离的重要性可以通过这个例子来描述：即使是军用无人机操作员，也必须观察目标，「开始射击」，因工作而频繁出现创伤性压力。因此，心理距离的不断扩大将对潜在攻击者的心理产生很大的影响。 我们还应该注意到，通常情况下，AI 的发展不是扩展现有威胁的唯一力量。机器人技术的发展和硬件成本下降（包括算力和机器人）也很重要。例如，廉价业余无人机的普及（可以轻松装运爆炸物）使得近期非国家组织（如伊斯兰国）发动空袭成为可能。   带来新威胁 AI 的发展带来多种新的攻击。这些攻击可能使用 AI 系统比人类更好地完成某些任务，或者利用 AI 系统的漏洞。 首先，AI 系统不受人类能力约束的特性使得攻击参与者可以实施之前不可行的攻击。例如，大部分人无法逼真模仿别人的声音或手动创建模仿人类录音的音频文件。但是，近期语音合成系统出现了巨大进步，可以学习模仿人类的声音（该技术目前尚未商用）。这些系统的输出与原始录音很难分辨，除非使用专门的认证手段。这类系统将为扩散假消息和假冒他人提供新的方法。 此外，AI 系统还可用于控制机器人和恶意软件的行为，而这对于人类来说不太可行。例如，没有人类团队能够实际地为一群无人机中的每一架都选择正确的飞行路线，来进行物理攻击。人类控制在其他情况中也可能不可行，因为没有可靠的通信渠道来指挥相关的系统；使用 Stuxnet 软件扰乱伊朗核计划时，一个专门设计用来更改气隙计算机（air-gapped computer）行为的病毒一旦感染这些计算机就无法接收命令。有限通信的挑战还出现在水下和存在信号干扰器的情况下。 第二，处理未解决漏洞的特性表明，如果人们开始部署新型 AI 系统，那么他们可能使自己遭受利用这些漏洞的攻击。例如，使用自动驾驶汽车为攻击者提供机会，他们通过向汽车提供对抗样本的方式引发车祸。以某种方式改变停车牌图像中的几个像素就可以误导 AI 系统。如果多个机器人被在一个中央服务器上运行的单个 AI 系统控制，或多个机器人被多个同样的 AI 系统控制，并在同样的刺激下出现，则单次攻击还可能引起大规模的同步失败。最糟糕的情况可能是攻击用于指挥自主武器系统的服务器，可导致大规模友军误伤或平民受伤。 威胁的典型特征发生改变   目前，我们的分析表明威胁图景将会改变现有的威胁，形成新的威胁。我们还预测威胁的典型特征将会以不同方式改变。具体来说，我们认为 AI 主导的攻击将会非常高效、目标精准、难以归因，以及利用 AI 系统的漏洞。 首先，高效、可扩展性和超人能力表明高效攻击会变得更加典型（至少缺少足够的预防措施）。攻击者经常面临攻击频率和规模与高效之间的权衡。比如，鱼叉式网络钓鱼比通常的钓鱼更加高效，后者不需要为个体定制信息，但是鱼叉式网络钓鱼更昂贵，而且无法批量使用。一般的钓鱼攻击也可以获利，虽然其成功率极低，但规模较大。通过提高某些攻击（包括鱼叉式网络钓鱼）的频率和扩展性，AI 系统可以让类似的权衡变得不那么尖锐。结果是攻击者可以实施更高频率、更大规模的更高效攻击。攻击高效性的增长这一预测也和 AI 系统可能具备超越人类的潜力相符。   第二，高效和可扩展性，尤其是识别和分析潜在目标的性能，也表明精准攻击将变得更加普遍。攻击者通常喜欢攻击具备某些特定属性的目标，比如高净资产或者与某些政治组织有关，也有兴趣针对目标的属性定制攻击。但是，攻击者通常面临攻击效率和可扩展性与精准打击程度之间的权衡。该权衡与高效性权衡密切相关，同样的逻辑表明我们应该期望其相关性减弱。与其他钓鱼攻击相比，鱼叉式网络钓鱼攻击的相对增加也表明了这一趋势。另一个例子是无人机群使用人脸识别技术杀死人群中的某个人，取代攻击不那么精准的暴力形式。 第三点，匿名性的增加表明难以归因的攻击将会更加普遍。同样可以用攻击者用自动化武器系统展开攻击而不是他们亲自去的例子来说明。 最后，我们预测利用 AI 系统漏洞的攻击会变得越来越普遍。该预测基于未解决的 AI 系统漏洞以及 AI 系统会变得更普遍的结论。 
240,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737949&idx=2&sn=73fba487380973257f1b171a23ecda0f&chksm=871ac963b06d4075a1264d23ca8bb18d9c43e5a45d463ba1c98fd95a75799ac7eac563f9281f&scene=27,业界 | Facebook发布Tensor Comprehensions：自动编译高性能机器学习核心的C++库,今天，Facebook 人工智能实验室宣布发布 Tensor Comprehensions，这是一个 C++库和数学语言，它能帮助缩小使用数学运算的研究人员和专注在各种硬件后端运行大规模模型的工程师之间的距离。Tensor Comprehensions 的主要区别特征是它使用一种独特的准时化（Just-In-Time）编译来自动、按需生成高性能的代码，这正是机器学习社区所需要的。 生产力的数量级增长 创造全新高性能机器学习模型的典型工作流需要花费数天或者数周来完成两个流程： 在 NumPy 层级的使用上，研究人员编写一个全新层，并在 PyTorch 这样的深度学习库中链接已有运算，然后在小规模实验上进行测试。代码实现的表现需要通过数量级的加速来运行大规模试验； 然后工程师使用该层，并为 GPU 和 CPU 编写有效的代码： 工程师需要成为高性能计算的专家，现在这方面的人才极为有限； 工程师需要明了这个环境，筹划出相关策略，编写代码以及做 debug； 将代码移到与实际任务相关的后端（例如检查冗长的评论），并添加样板融合代码。 结果导致，过去几年深度学习社区一直依赖 CuBLAS、MKL、CuDNN 这样的高性能库来获得在 GPU 和 CPU 上的高性能代码。想要实验不依赖于这些库的新思路需要一定量级的工程量，这对研究人员来说可能是惊人的。 开源一种能将这一流程从数天或者数周缩减到数分钟的工具包，我们觉得有非常大的实用价值。有了 Tensor Comprehensions，我们想象的是研究人员能够以数学符号的方式编写自己的想法，这种符号能被我们的系统自动编译并调整，且结果是有很好表现的专业代码。 在此发布中，我们能提供： 一种以简单语法形式表达大量机器学习 idea 的数学符号； 一个基于 Halide IR 的 C++前端，面向此数学符号； 一个基于 Integer Set Library（ISL）的多面准时化（polyhedral Just-in-Time /JIT）编译器； 一个基于进化搜索的多线程、多 GPU 的自动调节器。 早期研究 近期，高性能图像处理领域中一种语言逐渐开始流行，即 Halide。Halide 使用类似的高级函数式语法描述图像处理流程，接着在独立的代码块中，将其明确调度到硬件上，详细说明操作是如何被平铺、矢量化、并行化和融合的。对于具有架构技能的人来说，Halide 是一个非常有生产力的语言，但是却难以为绝大多数机器学习从业者所用。Halide 的自动调度是一个活跃的研究领域，但对于 GPU 上运行的 ML 代码还没有很好的解决方案。 Tensor Comprehension 将 Halide 编译器作为所要调用的库。我们构建了 Halide 的中间表征（intermediate representation/IR）和分析工具，并将其与多面编译（polyhedral compilation）技术配对，因此你可以使用相似的高阶句法编写层，而无需搞懂其运行原理。我们也发现了使语言更加简洁的方法，而无需为缩减运算指定循环边界。 细节 Tensor Comprehensions 使用 Halide 和多面编译（Polyhedral Compilation）技术通过委托内存管理与协调自动合成 CUDA 内核。该编译可对一般的运算符混合、快速局部内存、快速缩减和 JIT 专业化进行优化。由于我们并不尝试获取所有权或优化内存管理，因此我们的工作流可以简单而高效地集成到任何能调用 C++函数的 ML 框架和语言中。 与经典编译器技术和函数库所采用的方法相反，多面编译允许 Tensor Comprehensions 为每个网络按需调度单个张量元素的计算。在 CUDA 层级，它结合了仿射循环转换、fusion/fission 和自动并行化，且能同时确保数据正确地流经内存层级。 图中的数字表示最初计算张量元素的顺序，箭头表示它们之间的依赖关系。在该案例中，图像的旋转对应着允许深层运算符混合的循环交换。 为了推动搜索过程，我们同样提供了一个集成的多线程、多 GPU 自动调优的库，它使用进化搜索来生成和评估数千种实现方案，并选择性能最好的方案。只要在 Tensor Comprehension 上调用 tune 函数，就能实时地查看性能提升，并在满意的时候终止进程。最好的策略是通过 protobuf 执行序列化，并立即或离线的情况下复用。 在性能方面，虽然我们在工作上还有很多地方需要提升，但 Tensor Comprehensions 已经能相匹配或胜过当前 ML 框架与手动调参库所集成的性能。这主要通过将代码生成策略适应于特定问题而解决。下面的条形图说明了将 Tensor Comprehensions 自动生成的内核与 Caffe2 和 ATen 等相对比时得到的性能提升。更多性能提升的细节，请查看以下 Tensor Comprehensions 的研究论文。 随着我们扩大对更多硬件终端的贡献，Tensor Comprehension 将补足由英伟达和英特尔编写的快速库，并将与 CUDNN、MKL 或 NNPack 等库联合使用。 下一步计划 该工具的出现让研究者与程序员们可以使用符号编写层，这种方式与论文中使用的，用以描述程序的简洁数学表达方式相同。这意味着人们可以快速方便地将这种表达转化为实现，此前需要数天的任务目前仅需数分钟的时间。 FAIR 将在未来放出 Tensor Comprehensions 的 PyTorch 集成版本。 FAIR 致力于向科学领域作出贡献，并一直积极与机器学习社区合作。Tensor Comprehensions 已经在 Facebook、India、ETH Zurich 和 MIT 开始了应用。目前，这项工作还处于开发的初始阶段，FAIR 将在未来对其进行进一步改进。 论文：Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions 论文地址：https://arxiv.org/abs/1802.04730 摘要： 卷积和循环模型的深度学习网络已经相当普及，可以分析大量的音频、图像、文本和图表数据，并应用于自动翻译、语音到文本转换、场景理解、用户偏好排序、广告定位等。用于构建这些网络的深度学习框架，如 TensorFlow、Chainer、CNTK、Torch/PyTorch、Caffe1/2、MXNet 和 Theano，都在可用性和表达性、研究或产品导向以及硬件支持之间探索不同的权衡。它们在运算符的 DAG 上运行；封装了高性能的库，如 CUDNN（NVIDIA GPU）或 NNPACK（多种 CPU）；以及自动化的内存分配、同步化和分布式。当计算无法调用已有的高性能库时，需要使用自定制的运算符，这通常需要很高的工程代价。当研究者发明了新的运算符时，这很常见：而这种运算符会使得运行性能大幅降低，导致其创新意义受限。此外，即使存在这些框架可以使用的运行时调用库，对于用户的特定网络和数据集也无法提供最优的性能，因为它没有优化运算符之间的关系，也不存在对数据的大小和形状进行优化的工具。我们的贡献包括了（1）构建了一种和深度学习的数学过程很接近的语言，即 Tensor Comprehensions，提供了命令式和声明式的语言；（2）构建了一个多面即时（JIT）编译器，以将表征深度学习的有向无环图数学描述转换成一个具备内存管理和同步化功能的 CUDA 核，并且提供了运算符融合和指定特定数据大小等优化功能；（3）构建了一个由模组填充的高速编译缓存。特别是，我们证明了多面框架可以有效地针对 GPU 上的当前最佳深度学习模型构建领域特定的优化器。我们的工作流相比将英伟达库应用在机器学习社区常用的的核，以及应用在 Facebook 的真实产品级模型，要快了 4 倍。它集成了主流的框架 Caffe2（产品导向）、PyTorch（研究导向），以及 ATen 非共时张量库。 
241,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737949&idx=3&sn=56fe5f0660a93a914c96063eb58dd85e&chksm=871ac963b06d407549c240b3e09825e6fb8fcb565570206f049a936e40ab804368f8b1133090&scene=27,业界 | 外媒称英伟达下月将发布新型GPU「Turing」，挖矿虚拟货币,据路透社最新报道，英伟达将在下月发布新一代显卡 GPU，代号「Turing」，专门用于虚拟货币挖矿。在虚拟货币挖矿大量使用其显卡导致价格抬高的情况下，这是英伟达为保证游戏显卡供应所做的努力。 去年 5 月英伟达发布新一代显卡芯片架构「Volta」，包括 GV100 芯片和针对数据中心的 Tesla V100，该 GPU 并未出现在游戏产品中。日前，外媒报道英伟达可能会在 3 月 26 日开始的 GTC 大会上发布适合游戏市场的新型显卡，代号「Turing」。 之前已经有消息称，架构代号很有可能是 Ampere。DigitalTrends 网站作者 Kevin Parrish 分析，Ampere 可能要替代英伟达当前基于 Pascal 的 GeForce GTX 10 系列显卡，Ampere GV104 芯片可作为新的高端解决方案。基于 Ampere 芯片的显卡有望在 4 月 12 日发售，这与英伟达即将在 GTC 2018 上发布游戏产品显卡的报道相一致。 而路透社的文章称他们已经得到消息，Nvidia 将会在 3 月份发布全新的显卡，其架构代号并不是之前认为的「Ampere」，而是「Turing」。该文章主要介绍，在虚拟货币挖矿大量使用其显卡导致价格抬高的情况下，英伟达为保证游戏显卡供应所做的努力。CFO Colette Kress 说游戏 GPU 出现历史新低。   Kevin Parrish 在文章中分析称： 这指引我们想到这三个代号：Volta、Ampere 和 Turing。我们已经知道英伟达的新 Volta 架构使用 Tesla V100 显卡。我们还知道英伟达通常不会为不同的市场创造不同的架构。如果有什么不同的话，英伟达创建了新一代架构，并针对适合不同市场的设计生产多个不同芯片。 最有可能的仍然是 Volta。但是 Ampere 和 Turing 代号可能用于在新形势下描述两种不同市场的显卡：游戏和虚拟货币挖矿。之前的谣言称 Ampere 代号游戏显卡，而 Turing 可能针对虚拟货币挖矿显卡。这两个代号也可能是颠倒的，但是可能性不大。 为什么？Turing 这一代码名称来自 Alan Turing，一名英国计算机科学家、理论生物学家、数学家和密码破译专家。鉴于 Alan Turing 在密码破译方面的贡献，将其名字用于一类专门用于虚拟货币挖矿的 GPU 也是适得其所。Alan Turing 曾帮助破解纳粹军发送的编码信息，为盟军赢得二战胜利做出了重大贡献。 鉴于以游戏为中心的插件卡处于历史最低水平，这一基于 Volta 的三层方案很可能是英伟达计划在 3 月讨论的。同时有推测，英伟达已彻底取消 Ampere 代码的名称，因为该名称已被基于 ARM 的服务器制造商使用。这无关紧要，因为 Volta 仍然是底层的 GPU 架构，而 Ampere 和 Turing 的名称仅仅区分了两波主流人群。 最终，英伟达将会发布什么新芯片，让我们在新的一年里拭目以待。 
242,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737956&idx=1&sn=31804a962ac859cb9c24661e4ad79801&chksm=871ac95ab06d404c91c616236c5b6ae24d4981e790a2fb3471978b1f68e76c0ba32607f1f8cb&scene=27,Zero to Hero：2017年机器之心AI高分概述文章全集,新年快乐！在狗年的第一天，我们全面梳理了过去一年机器之心发布的基础知识和深度概述性文章，希望能对大家的研究和学习有所帮助。 过去一年，人工智能技术不仅经历了快速发展，也因为各家科技公司的大量投入而不断落地，在人们的眼里，AI 眼前的道路正在逐渐变得清晰。2017 年里，我们整理并发布了很多来自学界、业界的概述性文章，其中不仅有盘点机器学习、计算机视觉和自然语言处理等领域现有技术的长文，也有面向深度学习框架、机器学习相关硬件方面的研究，其中包含多篇颇具价值的概述性论文。时值春节，我们将其分类整理，以方便大家参考和学习。 这些深度文章包括并不限于以下关键词：进化策略、迁移学习、卷积神经网络、知识图谱、聚类、回归、PCA、t-SNE、贝叶斯、决策树、随机森林、遗传算法、强化学习、马尔科夫、概率图、LSTM…… Note：想要了解去年机器之心的盘点，请点击—— 入门 深度 | 迁移学习全面概述：从基本概念到相关研究 深度 | 理解深度学习中的卷积 综述 | 知识图谱研究进展 盘点 | 机器学习入门算法：从线性模型到神经网络 深度神经网络全面概述：从基本概念到实际模型和硬件基础 机器理解大数据的秘密：聚类算法深度详解 想了解概率图模型？你要先理解图论的基本定义与形式 人工智能能骗过人类吗？愚人节特写：这不是玩笑 三张图读懂机器学习：基本概念、五大流派与九种常见算法 LSTM 入门必读：从基础知识到工作方式详解 从语言学到深度学习 NLP，一文概述自然语言处理 最全的 DNN 概述论文：详解前馈、卷积和循环神经网络技术 从贝叶斯定理到概率分布：综述概率论基本定义 追根溯源！一图看尽深度学习架构谱系 徒手实现 CNN：综述论文详解卷积网络的数学本质 读懂概率图模型：你需要从基本概念和参数估计开始 从零开始：教你如何训练神经网络 开发者必读：计算机科学中的线性代数 理论 机器学习 学界 | 定量研究：当前机器学习领域十大研究主题 机器学习和深度学习引用量最高的 20 篇论文（2014-2017） 从贝叶斯角度，看深度学习的属性和改进方法 良心 GitHub 项目：各种机器学习任务的顶级结果（论文）汇总 深度 | 从朴素贝叶斯到维特比算法：详解隐马尔科夫模型 揭秘深度学习成功的数学原因：从全局最优性到学习表征不变性 深度 | 从 AlexNet 到残差网络，理解卷积神经网络的不同架构 从 Pix2Code 到 CycleGAN：2017 年深度学习重大研究进展全解读 前沿方向 OpenAI 详解进化策略方法：可替代强化学习 从自编码器到生成对抗网络：一文纵览无监督学习研究现状 资源 | 从文本到视觉：各领域最前沿的论文集合 从决策树到随机森林：树型算法的原理与实现 从概率论到多分类问题：综述贝叶斯统计分类 从遗传算法到 OpenAI 新方向：进化策略工作机制全解 GAN 综述 | 一文帮你发现各种出色的 GAN 变体 资源 | 生成对抗网络及其变体的论文汇总 生成对抗网络综述：从架构到训练技巧，看这篇论文就够了 计算机视觉 干货 | 物体检测算法全概述：从传统检测方法到深度神经网络框架 重磅 | 自动驾驶计算机视觉研究综述：难题、数据集与前沿成果（附 67 页论文下载） 神经风格迁移研究概述：从当前研究到未来方向（附论文和代码） 深度学习目标检测模型全面综述：Faster R-CNN、R-FCN 和 SSD 计算机视觉这一年：这是最全的一份 CV 技术报告 计算机视觉这一年：2017 CV 技术报告 Plus 之卷积架构、数据集与新趋势 深度 | 2017 CV 技术报告之图像分割、超分辨率和动作识别 深度 | 2017CV 技术报告：从 3D 物体重建到人体姿态估计 自然语言处理 语音合成到了跳变点？深度神经网络变革 TTS 最新研究汇总 资源 | 从全连接层到大型卷积核：深度学习语义分割全指南 学界 | 词嵌入 2017 年进展全面梳理：趋势和未来方向 深度 | 一文概述 2017 年深度学习 NLP 重大进展与趋势 推荐系统 学界 | 一文综述所有用于推荐系统的深度学习方法 使用深度学习构建先进推荐系统：近期 33 篇重要研究概述 深度学习框架 业界｜谷歌 TensorFlow 的一份全面评估报告：好的坏的及令人讨厌的 初学者怎么选择神经网络环境？对比 MATLAB、Torch 和 TensorFlow 硬件 业界 | 剖析用于深度学习的硬件：GPU、FPGA、ASIC 和 DSP 神经形态计算与神经网络硬件最全调查：从研究全貌到未来前景 从 GPU、TPU 到 FPGA 及其它：一文读懂神经网络硬件平台战局 优化 从浅层模型到深度模型：概览机器学习优化算法 综述论文：当前深度神经网络模型压缩和加速方法速览 深度 | 从修正 Adam 到理解泛化：概览 2017 年深度学习优化算法的最新研究进展 一文概览深度学习中的五大正则化方法和七大优化策略 代码实现 从强化学习基本概念到 Q 学习的实现，打造自己的迷宫智能体 回归、分类与聚类：三大方向剖解机器学习算法的优缺点（附 Python 和 R 实现） 基于 TensorFlow 理解三大降维技术：PCA、t-SNE 和自编码器 一文读懂遗传算法工作原理（附 Python 实现） 10 大深度学习架构：计算机视觉优秀从业者必备（附代码实现） 从算法到训练，综述强化学习实现技巧与调试经验 2017 年度盘点：15 个最流行的 GitHub 机器学习项目 
243,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737949&idx=1&sn=36cdcc2cd71f5f7417c2b1d8cdc20d26&chksm=871ac963b06d40754df3260a3cc299514da7766476e8b30792f7d229be2fc88efd05fab51fed&scene=27,新年新气象，低调的机器之心决定高调一次！,从深度学习的崛起到AlphaGo横空出世，人工智能、机器学习、深度学习成为了媒体笔下的常见词汇，人工智能的报道也越来越多。但一直以来，机器之心秉持「专业」、「不炒作」、「对从业者有价值」的心态，一边学习一边为科学家、专业严谨的企业决策者、工程师和学生等报道及推荐人工智能内容。 过去，机器之心几乎把所有的时间都放在内容、产品、和产业服务上，把所有的精力都用来为从业者服务。所以在读者和合作伙伴看来有些「太过低调」，以至于很多人不知道我们的用户量、业务范围及成绩、获得的荣誉等。 因此，在辞旧迎新之际，机器之心首次系统性的向大家做个自我介绍，并感谢所有读者、合作伙伴及投资人的支持！ 
244,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737918&idx=3&sn=99316c2c2f384054eeea698c5ce50004&chksm=871ac880b06d4196ab19c6c03a8e7b77a5501ccab6ca4819357d5ef56dcf6aac738c871997e8&scene=27,前沿 | BAIR开发现实环境的RL机器人，通过与人类的物理交互学习真实目标,"BAIR 作者： Andrea Bajcsy 可交互机器人通常将人类干预当成干扰，在干预撤除后随即恢复原来的轨迹，像弹簧一样执拗，无法根据人类偏好优化动作。伯克利近日开发出可交互学习的机器人系统，以类似强化学习的范式（目标函数不确定），能根据人类干预对自身轨迹进行修正，以最大化奖励，从而可以实时学习人类偏好。 人类每天都在进行彼此间的物理交互—从某人快要撒掉饮料时扶住他/她的手到将你的朋友推到正确的方向，身体上的物理互动是一种用来传达个人喜好和如何正确执行一个任务的直观方式。 那么，我们为什么不和当下的机器人像人一样进行物理交互呢？人类和机器人之间进行无缝的物理交互需要很多条件：轻量级的机器人设计、可靠的力学传感器、安全和反应式的控制方案、预测人类协作者意图的能力，等！幸运的是，机器人学在专门为人类开发的个人机器人设计方面已经取得了很多进步。 然而，再推敲一下我们刚开始就列举的第一个例子，即你在朋友快要撒掉饮料的时候扶住了他/她的手。现在假定你那位即将撒掉饮料的朋友（而不是你）是一个机器人。因为在目前最先进的机器人的规划和控制算法中，通常会将人类的物理干预视为外部扰动，一旦你放开机器人，它将恢复它那错误的轨迹，继续洒出饮料。这种差距的关键在于机器人是如何思考与人类之间的物理交互的：绝大多数机器人会在交互结束之后恢复其初始行为，而不是思考人类为什么根据需求对它进行物理干预并重新规划。 我们认为机器人应该将人类的物理干预视为和它应该如何执行任务相关的有用的信息。我们将机器人对物理干预的反应形式化为一种目标（奖励）学习问题，并且提出了一个解决方案，使得机器人在执行一个任务的时候能够根据在这些交互中得到的信息来改变它们的行为。 对物理交互的推理：未知的干扰与有意义的信息 物理人机交互（pHRI）领域研究的是共享工作空间里亲密的物理交互中出现的设计、控制和规划问题。之前的 pHRI 研究已经开发出了应对机器人在执行任务时面对物理交互的应对方法。由 Hogan（http://summerschool.stiff-project.org/fileadmin/pdf/Hog1985.pdf）等人提出的阻抗控制是常用的方法之一，阻抗控制可以让机器人在有人存在的空间里朝着期望的轨迹移动。使用这个控制方法时，机器人就像一个弹簧一样：它允许人推它，但是在人停止施力之后，它会移回到原来的期望位置。尽管这种策略非常快速，并且能够让机器人安全地适应人类的力量，但是机器人并不会利用这种干预去更新它对任务的理解，机器人将继续以与人类交互之前规划好的方式执行任务。 为什么会是这种情况呢？这可以归结为机器人对任务知识以及它所感知到的力的理解。通常，任务的概念是以一种目标函数的形式被赋予机器人的。这个目标函数为任务的不同方面编码奖励，例如「到达位置 X」，或者「在远离人类的同时朝着桌子移动」。机器人使用它的目标函数来生成可以满足任务所有方面的动作：例如，机器人会朝着目标 X 移动，同时选择靠近桌子和远离人类的路径。如果机器人最初的目标函数是正确的，那么任何外部干扰对它而言都是对它正确路径的干扰。因此，为了安全起见，机器人应该允许物理交互来干预它，但是它最终会返回到计划的最初路径，因为它固执地认为最初的规划是正确的。 相比之下，我们认为人类的干预往往是有目的的，并且是在机器人出错的时候才去干预它。虽然机器人的原始行为相对其预定义好的目标函数可能是最优的，但是需要人类干预的事实则意味着最初的目标函数并不是特别正确。所以，物理的人类干预不再是扰动了，而是对机器人应该呈现的真实目标函数的有用观察。基于这种考虑，我们从逆强化学习（IRL）（http://ai.stanford.edu/~ang/papers/icml00-irl.pdf）中获得一些灵感，即机器人观察到了一些行为（例如被推离了桌子），并且尝试着去推理新的目标函数（例如，「远离桌子」）。请注意，虽然很多 IRL 方法集中在让机器人在下一次做得更好，而我们则关注于让机器人正确地完成当前的任务。 形式化对 pHRI 的反应 基于对物理人机交互的认识，我们可以用一个动态系统来描述 pHRI，其中机器人不能确定正确的目标函数，人类的交互将给它提供信息。这种形式定义了一类广泛的 pHRI 算法，包括现有的阻抗控制方法，使得我们能够得到一种新颖的在线学习方法。 我们将会集中讨论这种方法的两个部分：（1）目标函数的结构；（2）机器人通过给定的人类物理交互推理目标函数的观察模型。让 x 代表机器人的状态（例如位置和速度），uR 代表机器人的动作（例如施加到关节的扭矩）。人类可以通过外部的力矩来与机器人产生物理交互，称作 uH，机器人通过它的动力运动到下一个状态。 机器人的目标：在最少的人类交互下正确地完成任务 在 pHRI 中，我们希望机器人能够学习人类，但同时我们也不想让人类在持续的物理交互中负担过重。所以，我们可以为机器人定下这么一个目标，既能完成任务，也能最小化所需的交互数量，最终在这则两者之间进行权衡。 这里，ϕ(x,uR,uH) 对任务相关的特征进行编码（例如，「到桌子的距离」、「到人类的距离」、「到目标的距离」），θ决定每种特征的相对权重。这个函数中，θ封装了真正的目标——如果机器人准确地知道如何给任务的各个方面进行加权，那么它就可以计算出如何以最佳的方式执行任务。然而，机器人并不知道这个参数！机器人并不总会知道执行任务的正确方式，更不用说人类喜欢的方式了。 观测模型：从人类的交互中推理正确的目标函数 正如我们讨论的，机器人应该观察人类的动作来推理位置的任务目标。为了把机器人测量的直接人力与目标函数联系起来，机器人采用了观测模型。在最大熵逆强化学习（IRL）（https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf）中的现有工作和人类行为认知科学模型（http://web.mit.edu/clbaker/www/papers/cogsci2007.pdf）中的玻尔兹曼分布的基础上，我们将人类的干预建模为：机器人在处于状态 x 并采取 uR+uH 的行动时，能够将机器人期望的奖励近似最大化的矫正。这个期望的奖励包含即时奖励和未来奖励，并且由 Q 值描述。 直觉地看，这个模型的解释是，人类更可能选择这样一种物理交互，它能够与机器人的动作结合起来，以形成一个期望的行为（具有高奖励值的行为）。 从人类的物理交互中进行实时学习 就像教一个人类一样，我们希望机器人能够在我们与它交互的时候持续地学习。然而，我们提出的学习框架需要机器人求解一个部分可观测马尔科夫决策过程（POMDP，partial observable markov decision process）；不幸的是，我们知道，精确地求解 POMDP 需要昂贵的计算代价，而且在最坏的情况下是无法解决的。然而，我们可以从这种形式中推导它的近似值，这些近似值可以使机器人在与人类交互的同时进行学习和行动。 为了实现这种任务内学习，我们做了三个近似，归纳如下： 1）把求解最优控制策略和估计真实目标函数区分开来。这意味着机器人要在每一个时间步更新它对θ的可能值的置信度，然后重新规划一个满足新分布的最优控制策略。 2）将控制和规划区分开来。计算一个最优控制策略意味着要在连续状态、动作和置信空间中的每个状态计算出一个要采取的最佳行动。尽管在每一次交互之后实时重新计算出一个完全的最优策略是很难的，但是我们可以在当前的状态实时重新计算出一个最优轨迹。这就是说，机器人首先会规划出一个最符合当前估计的轨迹，然后用一个阻抗控制器追踪这个轨迹。我们前面描述过的阻抗控制提供了需要的良好属性，在交互期间，人们可以物理地修改机器人的状态，同时还能保证安全。 回顾一下我们的估计步骤，我们将对轨迹空间进行类似的变换，并且修改我们的观测模型来反映这一点： 现在我们的观测模型仅仅依赖于在一个轨迹上的累积奖励 R，R 可以通过对所有步骤中的奖励进行求和计算得到。在这个近似中， 在推理真实目标函数的时候，在给定当前执行轨迹 ξR 以后，机器人仅须考虑与人类偏好轨迹 ξH 的似然度。 但是，人类的偏好轨迹 ξH 又是什么呢？机器人仅仅会直接测量人类施加的力 uH。一种用来推理人类偏好轨迹的方式是在机器人的当前轨迹上传播人类的力。图 1 建立了基于 Losey 和 O'Malley 之前的工作的轨迹形变，开始于机器人的原始轨迹，然后施加外力，然后施加形变以产生 ξH。 3）使用θ的最大后验（MAP）估计进行规划。最后，因为θ是一个连续变量，并且可能会具有较高的维度，加之观测模型是非高斯的，所以我们会仅使用 MAP 估计进行规划，而不是对θ的完全置信。我们发现，在高斯先验条件下，机器人当前轨迹的二阶泰勒级数展开下的 MAP 估计相当于执行在线梯度下降： 在每一个时间点，机器人会根据其当前最优轨迹和人类的偏好轨迹之间的累积特征差  来更新它对θ的估计。在示例学习的文献中，这个更新规则类似于在线最大间距规划（https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf）；它也类似于合作学习（https://arxiv.org/pdf/1601.00741.pdf），在合作学习中，人类会修正当前任务的轨迹点来为未来的任务学习一个奖励函数。 最终，将这三步结合起来就得到了原始 POMDP 的一个优雅的近似解决方案。在每一个时间步骤中，机器人规划一个轨迹 ξR，然后开始移动。人类可以进行物理交互，使得机器人能够感知到人类施加的力量 uH。然后，机器人利用人的力量使其原始轨迹发生形变，并生成人类期望的轨迹 ξH。然后机器人会推理其原始轨迹和人类期望的轨迹在任务的哪些方面存在不同，并在这种差别的方向上更新 θ 的值。然后，机器人使用新的特征权重重新规划一个更加符合人类偏好的轨迹。 您可以阅读我们在 2017 年机器人学习会议上的论文（http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf）来了解我们的形式化和近似的全面描述。 为了评价任务内学习在现实个人机器人上的好处，我们招募了 10 名参与者进行用户研究。每位参与者都与运行我们提出的在线学习方法的机器人进行交互，同时将没有从物理交互中学习，只是简单运行阻抗控制方法的机器人作为对比基准。 图 2 展示了三个实验性的居家操作任务，在每一个任务中，机器人开始时都被初始化为一个不正确的目标函数，参与者必须对其进行校正。例如，机器人会把杯子从架子上移动到桌子上，但它不会考虑杯子倾斜（它不会注意到杯子里是否有液体）。 图 2. 初始目标函数被标记为黑色的线，真实目标函数的期望轨迹标记为蓝色线条。参与者需要校正机器人，教它将杯子保持直立（左边），使其朝着桌子移动（中间），并避免经过笔记本电脑（右边）。 我们测量了机器人相对真实目标的性能、参与者付出的努力、交互时间以及 7 点 Likert 量表调查的响应。 在任务 1 中，看到杯子倾斜时，要教机器人使杯子保持直立，参与者必须进行物理干预（图左的阻抗控制不会将杯子保持修正后的状态，图右的在线学习则能实时修正杯子变得直立）。 任务 2 让参与者教机器人靠近桌子（阻抗控制的机器手确实像弹簧，非常执拗） 对于任务 3，机器人的原始轨迹会经过笔记本电脑上方。为了教机器人避免从笔记本电脑上方经过，参与者必须进行物理干预。 我们的用户研究结果表明，从物理交互中学习能够以较少的人力获得更好的机器人任务性能。当机器人正在执行任务期间积极地从交互中学习的时候，参与者能够使机器人以更少的努力和交互时间更快地执行正确的行为。此外，参与者相信机器人能够更好地理解人类的偏好，能够减少他们互动的努力，参与者相信，机器人是一个更具协作性的合作伙伴。 最终，我们认为机器人不应该将人类的交互作为一种干扰，而应该将其作为提供信息的动作。我们证明，具有这种推理能力的机器人能够更新他们对正在执行的任务的理解并正确地完成任务，而不是依赖于人们引导他们直至任务的完成。 这项工作只是探索从 pHRI 中学习机器人目标的一个简单尝试。很多未解决的问题仍然存在，包括开发能处理动态方面的解决方案（例如关于移动时间的偏好），以及如何/何时将所学的目标函数推广到新任务中。此外，机器人的奖励函数经常会有一些任务相关的特征，人类的交互也许仅仅给出了关于相关权重的一个特定子集的信息。我们在 HRI 2018 中的最新工作研究了机器人如何通过一次只学习一个特征权重来消除对人们试图纠正的错误的歧义。总之，我们不仅需要能够从与人类的物理交互中进行学习的算法，而且这些方法还必须考虑到在尝试动觉地（肌肉运动感觉）教一个复杂的（可能不熟悉的）机器人系统时人类需要面对的固有难度。 这篇博客的主要内容基于以下两篇论文： A. Bajcsy, D.P. Losey, M.K. O』Malley, and A.D. Dragan. Learning Robot Objectives from Physical Human Robot Interaction. Conference on Robot Learning (CoRL), 2017. A. Bajcsy , D.P. Losey, M.K. O』Malley, and A.D. Dragan. Learning from Physical Human Corrections, One Feature at a Time. International Conference on Human-Robot Interaction (HRI), 2018. "
245,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737918&idx=4&sn=9b5820415d94d418e3449ac698af7685&chksm=871ac880b06d419608aff694fd7e1b1247b3854fc8efe53d144b56b1bbb8b39d6ea12764ebbb&scene=27,学界 | Jeff Dean等人提出ENAS：通过参数共享实现高效的神经架构搜索,"arXiv 本文提出超越神经架构搜索（NAS）的高效神经架构搜索（ENAS），这是一种经济的自动化模型设计方法，通过强制所有子模型共享权重从而提升了NAS的效率，克服了NAS算力成本巨大且耗时的缺陷，GPU运算时间缩短了1000倍以上。在Penn Treebank数据集上，ENAS实现了55.8的测试困惑度；在CIFAR-10数据集上，其测试误差达到了2.89%，与NASNet不相上下（2.65%的测试误差）。 神经架构搜索（NAS）已成功用来设计图像分类和语言建模模型架构 (Zoph & Le, 2017; Zoph et al., 2018; Cai et al., 2018; Liu et al., 2017; 2018)。在 NAS 中，RNN 控制器进行循环训练：控制器首先采样候选架构，即一个子模型（child model），接着训练它收敛以测量其在所需任务上的表现。控制器接着把子模型表现作为指导信号以发现更好的架构。这一过程需要重复迭代很多次。尽管其实证表现令人印象深刻，NAS 的算力成本巨大且耗时，比如 Zoph et al. (2018) 使用 450 块 GPUs 耗时 3-4 天（即 32,400-43,200 GPU 小时）。同时，使用的资源越少，所得结果的竞争力就越小 (Negrinho & Gordon, 2017; Baker et al., 2017a)。我们发现 NAS 的计算瓶颈是训练每个子模型至收敛，只测量其精确度同时摈弃所有已训练的权重。 本文研究做出的主要贡献是通过强制所有子模型共享权重而提升了 NAS 的效率。这个想法明显存在争议，因为不同的子模型利用权重的方式也不同，但本文受到先前迁移学习和多任务学习工作的启发，即已确定一个特定任务的特定模型所学习的参数可用在其他任务的其他模型之上，几乎无需做出修改（Razavian et al., 2014; Zoph et al., 2016; Luong et al., 2016）。 研究者证明，不仅子模型之间共享参数是可能的，而且表现也很出色。尤其在 CIFAR-10 上，他们的方法取得了 2.89% 的测试误差，相比之下 NAS 为 2.65%。在 Penn Treebank 上，他们的方法实现了 55.8 的测试困惑度，明显优于 NAS 62.4 的测试困惑度 (Zoph & Le, 2017)，这是 Penn Treebank 在不使用后训练处理的方法中所实现的一个新的当前最优结果。重要的是，在本研究所有使用单个 Nvidia GTX 1080Ti GPU 的实验中，搜索架构的时间都少于 16 小时。相较于 NAS，GPU 运算时间缩短了 1000 倍以上。鉴于其高效率，我们把这一方法命名为高效神经架构搜索（ENAS）。 ENAS 思想的核心是观察到 NAS 最终迭代的所有图可以看作更大图的子图。换句话说，我们可以使用单个有向无环图（DAG）来表征 NAS 的搜索空间。图 2 是一个通用实例 DAG，其架构可通过采用 DAG 的子图而实现。直观讲，ENAS 的 DAG 是 NAS 搜索空间之中所有可能的子模型的叠加，其中节点表征局部计算，边缘表征信息流。每一个节点的局部计算有其自己的参数，这些参数只有当特定计算被激活时才使用。因此在搜索空间中，ENAS 的设计允许参数在所有子模型（即架构）之间共享。 图 2：该图表征了整个搜索空间，同时红箭头定义了其中一个由控制器决定的模型。这里，节点 1 是模型的输入，节点 3 和 6 是模型的输出。 2.1 设计循环单元 为了设计循环单元，作者使用了有 N 个结点的有向无环图（DAG），其中每个节点代表局部运算，而每条边代表 N 个节点中的信息流。ENAS 的控制器是一个 RNN，它会控制：1）哪一条边处于激活状态；2）在 DAG 中的每一个结点会执行哪些运算。 图 1：搜索空间中带有四个计算节点的循环单元案例。左图为对应循环单元的计算 DAG，其中红色的边代表图中的信息流。中间为循环单元。右图为 RNN 控制器的输出结果，它将会生成中间的循环单元和左边的 DAG。注意节点 3 和 4 永远不会被 RNN 采样，所以它们结果是平均值，且可以作为单元的输出。 2.3 设计卷积网络 在卷积网络的搜索空间中，RNN 控制器在每一个决策模块上会制定两组决策：1）前一个单元需要连接什么；2）它需要什么样的运算过程。这些决策共同构建了卷积网络中的不同层级。 图 3：搜索空间中一个循环单元的实例运行，该空间带有 4 个计算节点，表征卷积网络的 4 个层。顶部：控制器 RNN 的输出。左下：对应于网络架构的计算 DAG。红箭头表征激活的计算路径。右下：完整的网络。虚线箭头表征跳跃连接。 2.4 设计卷积单元 本文并没有采用直接设计完整的卷积网络的方法，而是先设计小型的模块然后将模块连接以构建完整的网络（Zoph et al., 2018）。图 4 展示了这种设计的例子，其中设计了卷积单元和 reduction cell。接下来将讨论如何利用 ENAS 搜索由这些单元组成的架构。 图 4：连接卷积单元和 reduction 单元，以构建完整的网络。 图 5：控制器在卷积单元搜索空间中运行的示例。上：控制器的输出。在卷积单元的搜索空间中，节点 1 和节点 2 是单元的输入，因此控制器只需要设计节点 3 和节点 4。左下：对应的 DAG，其中红边代表激活的连接。右下：由控制器采样得到的卷积单元 。 3.1 在 Penn Treebank 数据集上训练的语言模型 图 6：ENAS 为 Penn Treebank 数据集发现的 RNN 单元。 表 1：ENAS 为 Penn Treebank 数据集发现的架构的测试困惑度以及其它的基线结果的比较。（缩写说明：RHN 是 Recurrent Highway Network、VD 是 Variational Dropout、WT 是 Weight Tying、ℓ2 是 Weight Penalty、AWD 是 Averaged Weight Drop、MoC 是 Mixture of Contexts、MoS 是 Mixture of Softmaxes。） 3.2 在 CIFAR-10 数据集上的图像分类实验 表 2：ENAS 发现的架构在 CIFAR-10 数据集上的分类误差和其它基线结果的对比。在这个表中，第一块展示了 DenseNet，由人类专家设计的当前最佳架构。第二块展示了设计整个网络的方法。最后一块展示了设计模块单元以构建大型模型的技术。 图 7：ENAS 从大型搜索空间中发现的用于图像分类的网络架构。 ENAS 用了 11.5 个小时来发现合适的卷积单元和 reduction 单元，如图 8 所示。 图 8：ENAS 在微搜索空间中挖掘新的单元。 论文：Efficient Neural Architecture Search via Parameters Sharing 论文链接：https://arxiv.org/abs/1802.03268 我们在本文中提出高效神经架构搜索（ENAS），这是一种高效和经济的自动化模型设计的方法。在 ENAS 中，有一个控制器通过在一个大型计算图中搜索一个最优的子图以学习发现最优神经网络架构的方法。控制器采用策略梯度进行训练，以选择最大化验证集期望奖励的子图。同时，和所选子图对应的模型将进行训练以最小化标准交叉熵损失。由于子模型之间的参数共享，ENAS 的速度很快：它只需要使用少得多的 GPU 运算时间就能达到比当前的自动化模型设计方法好很多的经验性能，尤其是，其计算成本只有标准的神经架构搜索（NAS）的千分之一。在 Penn Treebank 数据集上，ENAS 发现了一个新颖的架构，其达到了 55.8 的测试困惑度，这是未经后处理而达到当前最佳性能的新方法。在 CIFAR-10 数据集上，ENAS 设计了一个新颖的架构，其测试误差达到了 2.89%，与 NASNet（Zoph et al., 2018）不相上下（2.65% 的测试误差）。 "
246,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737918&idx=2&sn=6b8f2f981c7abc2d6b237e0db51698d4&chksm=871ac880b06d41960b76e6c71fad47cbfc22c56ca94b7dd9014b3d41e083a15fa95a8bef4c84&scene=27,业界 | ARM放大招发布Trillium项目：包含神经网络软件库和两种AI处理器,作者： Tony Peng、Michael Sarazen 90% 的 AI 设备都是用 Arm 的架构设计的，现在 Arm 在人工智能领域厚积薄发，发布了 Trillium 项目，包括一款为移动设备而设计的机器学习处理器、一款目标检测处理器和一个神经网络软件库。 现在 90% 的 AI 设备都是基于 Arm 架构开发的，Arm 是一家英国芯片知识产权提供商，以 CPU 和 GPU 处理器而被熟知。为了提高机器学习的影响力，今天这家公司宣布了 Trillium 项目，包括一个机器学习处理器、一个目标检测处理器，和一个神经网络软件库的 Arm IP 套件。 Trillium 项目是这家公司在人工智能领域极富雄心的一次举措，通过集成设计提升 AI 设备的效率和性能，预计到 2028 年，这些设备的数量将从现在的 3 亿增长到 32 亿。 Arm 在机器学习领域的努力可以追溯到 2013 年，那时它开始探索 AI 市场并实施了一系列的战略收购。2017 年，这家公司宣布建立机器学习事业群，并任命 Jem Davies 担任总经理。在一次独家专访中，Davies 告诉 Synced 他认为「没有一个细分市场还没有或者不会被 AI 所冲击」。 「 AI 几乎影响所有……手机、相机、智能音箱，甚至温度控制器。谁会想到房间温度控制器是一个智能装置呢？ 」 Davies 说。 今天发布的机器学习芯片是 Arm 的第一代针对移动设备推断的 AI 芯片。该芯片使移动设备每平方毫米的运算性能不低于每秒 4.6 万亿次，在现实优化应用中实现 2 到 4 倍的吞吐量提升，以及在热度和能耗有限的环境里也能够实现超过 3 TOPs/W 的效率。 Davies 说其机器学习处理器背后的架构是全新的，根植于多年的研究成果。该架构为 16 位整数运算进行了优化。 新架构将为 CPU 和 GPU 遇到的挑战提供解决方案，Davies 说。「卷积神经网络非常普遍。重点是传统架构，不管是 CPU、GPU 还是 DSP，都要进行大量中间结果存储和加载。因此，我们生产了一种全新的架构，该架构使用智能存储系统。」 目标检测处理器是基于 Arm 现有的 IP 族 Spirit 的迭代。Spirit 是主导 Hive 安防摄像头的目标检测加速器，于 2016 年 Arm 收购 Apical 后不久发布，Apical 是一家为超过 15 亿设备提供计算机视觉和图像处理器的公司。 Arm 的第二代处理器可以全高清、60fps 实时检测无限数量的目标。其详细的人体模型提供了丰富的元数据，使方向、轨迹、姿势和动作检测成为可能。 Arm 提供集成解决方案，包括机器学习处理器和目标检测处理器。在实时目标识别任务中，目标检测处理器首先分离出感兴趣区域，比如人脸。这样，机器学习处理器就能够分析更少的像素，以实现更快、更精细的结果。 Arm 的神经网络库是一系列图像、视觉和机器学习工作负载构造块的集合。开发者可使用该软件，以及 Arm 现有的实现工具，如加速算法和应用的 Compute Library，或最大化边缘设备上性能的 CMSIS-NN。该库支持主流框架，如 TensorFlow、Caffe，且已经为 Arm Cortex CPU、Mali GPU 和新型机器学习处理器进行了优化。 Arm 机器学习处理器将于今夏发售，目标检测处理器将于本季度末上市。 
247,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737918&idx=1&sn=f1b71135f4014a64fa3d7005c10d972a&chksm=871ac880b06d4196a85f1bf098fbb1c8287d0b687a2f23e9ca387f9617e2d9b582ae6faecb4f&scene=27,从1维到6维，一文读懂多维数据可视化策略,towardsdatascience 数据聚合、汇总和可视化是支撑数据分析领域的三大支柱。长久以来，数据可视化都是一个强有力的工具，被业界广泛使用，却受限于 2 维。在本文中，作者将探索一些有效的多维数据可视化策略（范围从 1 维到 6 维）。 描述性分析（descriptive analytics）是任何分析生命周期的数据科学项目或特定研究的核心组成部分之一。数据聚合（aggregation）、汇总（summarization）和可视化（visualization）是支撑数据分析领域的主要支柱。从传统商业智能（Business Intelligence）开始，甚至到如今人工智能时代，数据可视化都是一个强有力的工具；由于其能有效抽取正确的信息，同时清楚容易地理解和解释结果，可视化被业界组织广泛使用。然而，处理多维数据集（通常具有 2 个以上属性）开始引起问题，因为我们的数据分析和通信的媒介通常限于 2 个维度。在本文中，我们将探索一些有效的多维数据可视化策略（范围从 1 维到 6 维）。 「一图胜千言」 这是一句我们熟悉的非常流行的英语习语，可以充当将数据可视化作为分析的有效工具的灵感和动力。永远记住：「有效的数据可视化既是一门艺术，也是一门科学。」在开始之前，我还要提及下面一句非常相关的引言，它强调了数据可视化的必要性。 「一张图片的最大价值在于，它迫使我们注意到我们从未期望看到的东西。」 ——John Tukey 本文假设一般读者知道用于绘图和可视化数据的基本图表类型，因此这里不再赘述，但在本文随后的实践中，我们将会涉及大部分图表类型。著名的可视化先驱和统计学家 Edward Tufte 说过，数据可视化应该在数据的基础上，以清晰、精确和高效的方式传达数据模式和洞察信息。 结构化数据通常包括由行和特征表征的数据观测值或由列表征的数据属性。每列也可以被称为数据集的某特定维度。最常见的数据类型包括连续型数值数据和离散型分类数据。因此，任何数据可视化将基本上以散点图、直方图、箱线图等简单易懂的形式描述一个或多个数据属性。本文将涵盖单变量（1 维）和多变量（多维）数据可视化策略。这里将使用 Python 机器学习生态系统，我们建议先检查用于数据分析和可视化的框架，包括 pandas、matplotlib、seaborn、plotly 和 bokeh。除此之外，如果你有兴趣用数据制作精美而有意义的可视化文件，那么了解 D3.js（https://d3js.org/）也是必须的。有兴趣的读者可以阅读 Edward Tufte 的「The Visual Display of Quantitative Information」。 闲话至此，让我们来看看可视化（和代码）吧！ 别在这儿谈论理论和概念了，让我们开始进入正题吧。我们将使用 UCI 机器学习库（https://archive.ics.uci.edu/ml/index.php）中的 Wine Quality Data Set。这些数据实际上是由两个数据集组成的，这两个数据集描述了葡萄牙「Vinho Verde」葡萄酒中红色和白色酒的各种成分。本文中的所有分析都在我的 GitHub 存储库中，你可以用 Jupyter Notebook 中的代码来尝试一下！ 我们将首先加载以下必要的依赖包进行分析。 我们将主要使用 matplotlib 和 seaborn 作为我们的可视化框架，但你可以自由选择并尝试任何其它框架。首先进行基本的数据预处理步骤。 我们通过合并有关红、白葡萄酒样本的数据集来创建单个葡萄酒数据框架。我们还根据葡萄酒样品的质量属性创建一个新的分类变量 quality_label。现在我们来看看数据前几行。 葡萄酒质量数据集 很明显，我们有几个葡萄酒样本的数值和分类属性。每个观测样本属于红葡萄酒或白葡萄酒样品，属性是从物理化学测试中测量和获得的特定属性或性质。如果你想了解每个属性（属性对应的变量名称一目了然）详细的解释，你可以查看 Jupyter Notebook。让我们快速对这些感兴趣的属性进行基本的描述性概括统计。 葡萄酒类型的基本描述性统计 比较这些不同类型的葡萄酒样本的统计方法相当容易。注意一些属性的明显差异。稍后我们将在一些可视化中强调这些内容。 单变量分析基本上是数据分析或可视化的最简单形式，因为只关心分析一个数据属性或变量并将其可视化（1 维）。 使所有数值数据及其分布可视化的最快、最有效的方法之一是利用 pandas 画直方图（histogram）。 将属性作为 1 维数据可视化 上图给出了可视化任何属性的基本数据分布的一个好主意。 让我们进一步可视化其中一个连续型数值属性。直方图或核密度图能够很好地帮助理解该属性数据的分布。 可视化 1 维连续型数值数据 从上面的图表中可以看出，葡萄酒中硫酸盐的分布存在明显的右偏（right skew）。 可视化一个离散分类型数据属性稍有不同，条形图是（bar plot）最有效的方法之一。你也可以使用饼图（pie-chart），但一般来说要尽量避免，尤其是当不同类别的数量超过 3 个时。 可视化 1 维离散分类型数据 现在我们继续分析更高维的数据。 多元分析才是真正有意思并且有复杂性的领域。这里我们分析多个数据维度或属性（2 个或更多）。多变量分析不仅包括检查分布，还包括这些属性之间的潜在关系、模式和相关性。你也可以根据需要解决的问题，利用推断统计（inferential statistics）和假设检验，检查不同属性、群体等的统计显著性（significance）。 可视化 2 维数据（2-D） 检查不同数据属性之间的潜在关系或相关性的最佳方法之一是利用配对相关性矩阵（pair-wise correlation matrix）并将其可视化为热力图。 用相关性热力图可视化 2 维数据 热力图中的梯度根据相关性的强度而变化，你可以很容易发现彼此之间具有强相关性的潜在属性。另一种可视化的方法是在感兴趣的属性之间使用配对散点图。 用配对散点图可视化 2 维数据 根据上图，可以看到散点图也是观察数据属性的 2 维潜在关系或模式的有效方式。另一种将多元数据可视化为多个属性的方法是使用平行坐标图。 用平行坐标图可视化多维数据 基本上，在如上所述的可视化中，点被表征为连接的线段。每条垂直线代表一个数据属性。所有属性中的一组完整的连接线段表征一个数据点。因此，趋于同一类的点将会更加接近。仅仅通过观察就可以清楚看到，与白葡萄酒相比，红葡萄酒的密度略高。与红葡萄酒相比，白葡萄酒的残糖和二氧化硫总量也较高，红葡萄酒的固定酸度高于白葡萄酒。查一下我们之前得到的统计表中的统计数据，看看能否验证这个假设！ 因此，让我们看看可视化两个连续型数值属性的方法。散点图和联合分布图（joint plot）是检查模式、关系以及属性分布的特别好的方法。 使用散点图和联合分布图可视化 2 维连续型数值数据 散点图在上图左侧，联合分布图在右侧。就像我们提到的那样，你可以查看联合分布图中的相关性、关系以及分布。 如何可视化两个连续型数值属性？一种方法是为分类维度画单独的图（子图）或分面（facet）。 使用条形图和子图可视化 2 维离散型分类数据 虽然这是一种可视化分类数据的好方法，但正如所见，利用 matplotlib 需要编写大量的代码。另一个好方法是在单个图中为不同的属性画堆积条形图或多个条形图。可以很容易地利用 seaborn 做到。 在一个条形图中可视化 2 维离散型分类数据 这看起来更清晰，你也可以有效地从单个图中比较不同的类别。 让我们看看可视化 2 维混合属性（大多数兼有数值和分类）。一种方法是使用分图\子图与直方图或核密度图。 利用分面和直方图\核密度图可视化 2 维混合属性 虽然这很好，但是我们再一次编写了大量代码，我们可以通过利用 seaborn 避免这些，在单个图表中画出这些图。 利用多维直方图可视化 2 维混合属性 可以看到上面生成的图形清晰简洁，我们可以轻松地比较各种分布。除此之外，箱线图（box plot）是根据分类属性中的不同数值有效描述数值数据组的另一种方法。箱线图是了解数据中四分位数值以及潜在异常值的好方法。 2 维混合属性的有效可视化方法——箱线图 另一个类似的可视化是小提琴图，这是使用核密度图显示分组数值数据的另一种有效方法（描绘了数据在不同值下的概率密度）。 2 维混合属性的有效可视化方法——小提琴图 你可以清楚看到上面的不同酒品质类别的葡萄酒硫酸盐的密度图。 将 2 维数据可视化非常简单直接，但是随着维数（属性）数量的增加，数据开始变得复杂。原因是因为我们受到显示媒介和环境的双重约束。 对于 3 维数据，可以通过在图表中采用 z 轴或利用子图和分面来引入深度的虚拟坐标。 但是，对于 3 维以上的数据来说，更难以直观地表征。高于 3 维的最好方法是使用图分面、颜色、形状、大小、深度等等。你还可以使用时间作为维度，为随时间变化的属性制作一段动画（这里时间是数据中的维度）。看看 Hans Roslin 的精彩演讲就会获得相同的想法！ 可视化 3 维数据（3-D） 这里研究有 3 个属性或维度的数据，我们可以通过考虑配对散点图并引入颜色或色调将分类维度中的值分离出来。 用散点图和色调（颜色）可视化 3 维数据 上图可以查看相关性和模式，也可以比较葡萄酒组。就像我们可以清楚地看到白葡萄酒的总二氧化硫和残糖比红葡萄酒高。 让我们来看看可视化 3 个连续型数值属性的策略。一种方法是将 2 个维度表征为常规长度（x 轴）和宽度（y 轴）并且将第 3 维表征为深度（z 轴）的概念。 通过引入深度的概念来可视化 3 维数值数据 我们还可以利用常规的 2 维坐标轴，并将尺寸大小的概念作为第 3 维（本质上是气泡图），其中点的尺寸大小表征第 3 维的数量。 通过引入尺寸大小的概念来可视化 3 维数值数据 因此，你可以看到上面的图表不是一个传统的散点图，而是点（气泡）大小基于不同残糖量的的气泡图。当然，并不总像这种情况可以发现数据明确的模式，我们看到其它两个维度的大小也不同。 为了可视化 3 个离散型分类属性，我们可以使用常规的条形图，可以利用色调的概念以及分面或子图表征额外的第 3 个维度。seaborn 框架帮助我们最大程度地减少代码，并高效地绘图。 通过引入色调和分面的概念可视化 3 维分类数据 上面的图表清楚地显示了与每个维度相关的频率，可以看到，通过图表能够容易有效地理解相关内容。 考虑到可视化 3 维混合属性，我们可以使用色调的概念来将其中一个分类属性可视化，同时使用传统的如散点图来可视化数值属性的 2 个维度。 通过利用散点图和色调的概念可视化 3 维混合属性 因此，色调作为类别或群体的良好区分，虽然如上图观察没有相关性或相关性非常弱，但从这些图中我们仍可以理解，与白葡萄酒相比，红葡萄酒的硫酸盐含量较高。你也可以使用核密度图代替散点图来理解 3 维数据。 通过利用核密度图和色调的概念可视化 3 维混合属性 与预期一致且相当明显，红葡萄酒样品比白葡萄酒具有更高的硫酸盐含量。你还可以根据色调强度查看密度浓度。 如果我们正在处理有多个分类属性的 3 维数据，我们可以利用色调和其中一个常规轴进行可视化，并使用如箱线图或小提琴图来可视化不同的数据组。 通过利用分图小提琴图和色调的概念来可视化 3 维混合属性 在上图中，我们可以看到，在右边的 3 维可视化图中，我们用 x 轴表示葡萄酒质量，wine_type 用色调表征。我们可以清楚地看到一些有趣的见解，例如与白葡萄酒相比红葡萄酒的挥发性酸度更高。 你也可以考虑使用箱线图来代表具有多个分类变量的混合属性。 通过利用箱线图和色调的概念可视化 3 维混合属性 我们可以看到，对于质量和 quality_label 属性，葡萄酒酒精含量都会随着质量的提高而增加。另外红葡萄酒与相同品质类别的白葡萄酒相比具有更高的酒精含量（中位数）。然而，如果检查质量等级，我们可以看到，对于较低等级的葡萄酒（3 和 4），白葡萄酒酒精含量（中位数）大于红葡萄酒样品。否则，红葡萄酒与白葡萄酒相比似乎酒精含量（中位数）略高。 可视化 4 维数据（4-D） 基于上述讨论，我们利用图表的各个组件可视化多个维度。一种可视化 4 维数据的方法是在传统图如散点图中利用深度和色调表征特定的数据维度。 通过利用散点图以及色调和深度的概念可视化 4 维数据 wine_type 属性由上图中的色调表征得相当明显。此外，由于图的复杂性，解释这些可视化开始变得困难，但我们仍然可以看出，例如红葡萄酒的固定酸度更高，白葡萄酒的残糖更高。当然，如果酒精和固定酸度之间有某种联系，我们可能会看到一个逐渐增加或减少的数据点趋势。 另一个策略是使用二维图，但利用色调和数据点大小作为数据维度。通常情况下，这将类似于气泡图等我们先前可视化的图表。 通过利用气泡图以及色调和大小的概念可视化 4 维数据 我们用色调代表 wine_type 和数据点大小代表残糖。我们确实看到了与前面图表中观察到的相似模式，白葡萄酒气泡尺寸更大表征了白葡萄酒的残糖值更高。 如果我们有多于两个分类属性表征，可在常规的散点图描述数值数据的基础上利用色调和分面来描述这些属性。我们来看几个实例。 通过利用散点图以及色调和分面的概念可视化 4 维数据 这种可视化的有效性使得我们可以轻松识别多种模式。白葡萄酒的挥发酸度较低，同时高品质葡萄酒具有较低的酸度。也基于白葡萄酒样本，高品质的葡萄酒有更高的酒精含量和低品质的葡萄酒有最低的酒精含量！ 让我们借助一个类似实例，并建立一个 4 维数据的可视化。 通过利用散点图以及色调和分面的概念可视化 4 维数据 我们清楚地看到，高品质的葡萄酒有较低的二氧化硫含量，这是非常相关的，与葡萄酒成分的相关领域知识一致。我们也看到红葡萄酒的二氧化硫总量低于白葡萄酒。在几个数据点中，红葡萄酒的挥发性酸度水平较高。 可视化 5 维数据（5-D） 我们照旧遵从上文提出的策略，要想可视化 5 维数据，我们要利用各种绘图组件。我们使用深度、色调、大小来表征其中的三个维度。其它两维仍为常规轴。因为我们还会用到大小这个概念，并借此画出一个三维气泡图。 利用气泡图和色调、深度、大小的概念来可视化 5 维数据。 气泡图灵感来源与上文所述一致。但是，我们还可以看到以二氧化硫总量为指标的点数，发现白葡萄酒的二氧化硫含量高于红葡萄酒。 除了深度之外，我们还可以使用分面和色调来表征这五个数据维度中的多个分类属性。其中表征大小的属性可以是数值表征甚至是类别（但是我们可能要用它的数值表征来表征数据点大小）。由于缺乏类别属性，此处我们不作展示，但是你可以在自己的数据集上试试。 借助色调、分面、大小的概念和气泡图来可视化 5 维数据。 通常还有一个前文介绍的 5 维数据可视化的备选方法。当看到我们先前绘制的图时，很多人可能会对多出来的维度深度困惑。该图重复利用了分面的特性，所以仍可以在 2 维面板上绘制出来且易于说明和绘制。 我们已经领略到多位数据可视化的复杂性！如果还有人想问，为何不增加维度？让我们继续简单探索下！ 可视化 6 维数据（6-D） 目前我们画得很开心（我希望是如此！）我们继续在可视化中添加一个数据维度。我们将利用深度、色调、大小和形状及两个常规轴来描述所有 6 个数据维度。 我们将利用散点图和色调、深度、形状、大小的概念来可视化 6 维数据。 这可是在一张图上画出 6 维数据！我们用形状表征葡萄酒的质量标注，优质（用方块标记），一般（用 x 标记），差（用圆标记）：用色调表示红酒的类型，由深度和数据点大小确定的酸度表征总二氧化硫含量。 这个解释起来可能有点费劲，但是在试图理解多维数据的隐藏信息时，最好结合一些绘图组件将其可视化。 结合形状和 y 轴的表现，我们知道高中档的葡萄酒的酒精含量比低质葡萄酒更高。 结合色调和大小的表现，我们知道白葡萄酒的总二氧化硫含量比红葡萄酒更高。 结合深度和色调的表现，我们知道白葡萄酒的酸度比红葡萄酒更低。 结合色调和 x 轴的表现，我们知道红葡萄酒的残糖比白葡萄酒更低。 结合色调和形状的表现，似乎白葡萄酒的高品质产量高于红葡萄酒。（可能是由于白葡萄酒的样本量较大） 我们也可以用分面属性来代替深度构建 6 维数据可视化效果。 因此，在这种情况下，我们利用分面和色调来表征三个分类属性，并使用两个常规轴和大小来表征 6 维数据可视化的三个数值属性。 数据可视化与科学一样重要。如果你看到这，我很欣慰你能坚持看完这篇长文。我们的目的不是为了记住所有数据，也不是给出一套固定的数据可视化规则。本文的主要目的是理解并学习高效的数据可视化策略，尤其是当数据维度增大时。希望你以后可以用本文知识可视化你自己的数据集。 原文链接：https://towardsdatascience.com/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57 
248,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738028&idx=2&sn=997679eb67a62510e5de0c37f8ccf7db&chksm=871ac912b06d400477c34d9e6c2824665172e85539a410acf0e70d6ce87133b597d71694596f&scene=27,前沿 | MIT开发神经网络专用芯片：能耗降低95%，适合移动端设备,MIT News 路雪、刘晓坤 近日，MIT 研究人员开发了一种专用芯片，可以提高神经网络计算的速度，比之前的芯片速度提升三到七倍，同时将能耗降低 93% - 96%。这使得在智能手机本地运行神经网络，甚至在家用电器上嵌入神经网络变成可能。相关论文已投中 ISSCC。 人工智能系统近期的进展，如语音或人脸识别都受到神经网络的支持，简单信息处理器深度互联，通过分析大量训练数据来学习执行任务。 但是神经网络规模很大，计算能耗高，因此它们不适合用于手持设备。大部分依赖神经网络的智能手机 app 只是简单地将数据上传至网络服务器进行处理，然后服务器将处理结果返回至手机。 现在，MIT 研究人员开发了一种专用芯片，提高神经网络计算的速度，比之前的芯片速度提升三到七倍，同时将能耗降低 93% - 96%。这使得在智能手机本地运行神经网络，甚至在家用电器上嵌入神经网络变成可能。 「通用处理器模型中，芯片一部分是内存，另一部分是处理器，进行计算时必须把数据在二者之间来回传输。」该芯片开发项目的主导者、MIT 电子工程与计算机科学系研究生 Avishek Biswas 说道。 「由于这些机器学习算法需要大量计算，因此数据的来回传输占能耗的主要部分。但是这些算法执行的计算可以简化成一个特定的运算，即点积。我们的方法就是，在存储器内部实现点积运算，这样就无需将数据来回传输了。」 Biswas 和他的论文导师、MIT 工程学院院长、电子工程和计算机科学系 Vannevar Bush 教授 Anantha Chandrakasan 在本周 Biswas 投中 ISSCC 的论文中介绍了这一新型芯片。 回到模拟（analog） 神经网络通常有多个层。每层中单个处理节点通常接收来自下面一层多个节点的数据，并将数据传输至上面一层的多个节点。节点之间的每个联结都有自己的「权重」，表示该节点输出在下一个节点所执行的计算中的影响力大小。训练神经网络就是在调整这些权重。 从下面一层多个节点接收数据的每个节点将使每个输入乘对应连接的权重，然后再求和。该运算（对相乘结果求和）即是点积的定义。如果点积超过阈值，则该节点通过具备自身权重的连接将其传输至下一层的节点。 神经网络是抽象：「节点」是存储在计算机内存中的权重。计算点积通常需要从内存中获取权重，获取相关的数据项，将二者相乘，存储运算结果，然后对节点的每个输入重复该运算。鉴于神经网络具备成千上万甚至数百万个节点，因此需要计算大量数据。 但是这一系列运算只是大脑活动的数值逼近，大脑中信号沿着围绕「突触」的多个神经元游走。神经元的放电频率和突触之间的电化学信号对应数据值和权重。MIT 研究人员的新型芯片通过更准确地模仿大脑实现效率提升。 该芯片中，节点的输入值被转换成电压，然后乘适当的权重。对乘积求和就是把电压连接起来。只有连接起来的电压才会被转换回数值表示并存储，方便以后的处理。 该芯片可以计算多个节点的点积，原型中每一步每次可计算 16 个节点，而不需要每次计算都在处理器和存储器之间来回传输。 二值权重 该系统的关键在于所有的权重都是 1 或-1（1 比特，二值）。这意味着它们可以用其自身的内存作为简单开关（即关闭或打开一个电路）而实现。最近的理论研究表明训练只有两个权重值的神经网络，其准确率只有略微的降低，大约 1 到 2 个百分点。 Biswas 和 Chandrakasan 的研究证实了这个结论。在实验中，他们在传统计算机上运行了一个神经网络的完整实现，并在他们的芯片上运行了同等的二值权重神经网络。他们的芯片运行结果准确率只比传统网络降低了 2 到 3 个百分点。 「这是基于 SRAM 的内存模拟计算在深度学习应用中很有前景的实证证明，」IBM 人工智能副总裁 Dario Gil 说。「该实验结果展示了利用存储器阵列的卷积运算的高能效实现的规范。未来，它一定能为更复杂的卷积神经网络在图像和视频分类中的应用提供更多的可能性。」 原文链接：http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices-0214 
249,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737814&idx=3&sn=7b4bfea9c80d5b6faa5347e951cf70a2&chksm=871ac8e8b06d41fed5a2bf25d3b37c4f66be49b3f7ecd4a8b7fcf24580d4825e2836c6952808&scene=27,入门 | 简易指南带你启动 R 语言学习之旅,"TowardsDataScience R 语言是结合了 S 编程语言的计算环境，可用于实现对数据的编程；它有很强大的数值分析工具，对于处理线性代数、微分方程和随机学的问题非常有用。通过一系列内建函数和库，你可以用 R 语言学习数据可视化，特别是它还有很多图形前端。本文将简单介绍 R 语言的编程基础，带你逐步实现第一个可视化案例。 代码地址：https://github.com/aaqil/r-lang-fundamentals R 语言最近刚刚拥有了  ，目前最好的 R 语言版本是 R Studio。在学习之前，你需要了解一下 R 语言本身。 为什么要学习 R 语言？ R 很灵活； R 很强大； R 不仅是个统计计算工具包，它还是一门编程语言； R 可以针对问题的形式设计程序； R 可以高效地处理和存储数据。 R 语言的代码通用性较好，微调任务后只需要再微调代码就可以直接应用。 R 安装 选择你喜欢的 CRAN 镜像进行安装：https://cran.r-project.org/mirrors.html R Studio 是很高效的版本，它包括一个代码编辑器、调试和可视化工具。它是一个集成开发环境，包括一个控制台、支持直接代码执行的语法高亮显示编辑器，以及绘图、历史查询、调试和工作区管理的工具。 R Studio 下载地址：https://www.rstudio.com/products/rstud 成功安装 R 语言之后，在终端或 shell 里点击「R」启动交互式 shell。 你可以将 R shell 当成计算器使用，执行简单的数学，甚至高级的机器学习算法。 执行以下命令退出 R 程序： 你可以点击「y」保存工作区会话，如果直接点击「n」，则不保存直接返回终端/shell。 我们从在 R 交互式 shell 中输出「Hello World!」开始学习。 在 R 中我们使用 print() 函数返回参数中给定的字符串： R 语言支持命令，而这些命令会被解释器忽略。 以下是一些 R 语言的命令的例子： R 语言的变量可以保存一个原子向量（atomic vector）、一组原子向量或多个 R 对象的组合。R 语言的命名区分大小写。在为数据结构命名的时候，我们需要遵循以下规则： 以. 起始的命名是系统命名，并且使用 ls() 函数时这些命名不总是可见。 上面的代码声明了一个变量「a」并分配了值 3。 typeof() 函数返回变量的数据类型。 numeric（实数或十进制数）：十进制值在 R 语言中被称为 numeric，是默认的计算数据类型。 此外还包括 integer（整数）、charater（字符串）、logical（逻辑值）、complex（复数）等，以下是这些数据类型的定义代码示例： 使用命令「Rscript filename.R」在 shell 上执行 R 文件。 注意：R 语言的字符串是 Character 类型的。 算术运算 R 语言可以执行所有基本的算术运算，例如加、减、乘、除等。 原子向量 在 R 语言中，向量是最常用的数据结构。向量是相同类型的数据元素的序列。向量成员的正式名称是成分（component）。向量的元素的数据类型可以是 character、logical、integer 或 numeric。 我们使用 vector() 函数创建一个空向量，以下代码展示了如何声明一个向量： 列表 R 语言的列表作为容器，是包含其它对象的通用向量。和原子向量不同，列表中的变量不局限于单一的数据类型，可以包含任意的数据类型的混合。一个列表可以包含其它列表。 R 语言中的列表可以用 list() 函数创建。 矩阵 在 R 语言中，矩阵是一种特殊类型的向量。矩阵是数据元素以二维矩形排布的集合，矩阵有行和列。 现在我们创建一个 2x2 矩阵，使用 matrix 函数并以行和列作为参数。行数以 nrow 表示，列数以 ncol 表示。 数据帧（data frame） 数据帧是 R 语言里最常用的数据结构之一。数据是由带有行和列的数据表格表示的。 我们通常在数据帧里读取一个 csv 文件，使用 read.csv() 或 read.table() 函数，然后把 csv 文件的名字作为参数输入函数里来实现的。 我们也可以用 data.frame() 函数来创建一个数据帧。 这里有几个重要的函数，应用到数据帧得出其结构信息等。 head() 用来看前 6 行 tail() 用来看后 6 行 dim() 用来看维度 nrow() 行的数量 ncol() 列的数量 str() 每一列的结构 因子（factor） 因子是带标签的整数。因子看起来像字符矢量，但实际上是整数，当你把它们当成字符来对待时，需要特别谨慎。一些字符处理方法会强制把因子转换成字符，而其他的字符处理方法会报错。 因子可以用 factor() 函数创建。输入一般是字符矢量。 以下是控制函数的脚本执行流程的常用结构，包括： 1. if, else 2. for 3. while 4. repeat 5. break if-else 我们经常需要可以检查一个程序的状态和改变这个程序的行为的功能。条件语句可以提供这样的功能，最简单的形式是 if 语句。 例子 for 循环 R 语言里的 for 循环可以在任何列表或矢量中执行。 上述代码是在 R 语言里声明 for 循环的例子，for 循环让循环变量 i 在给定的范围内迭代。 几种实现 for 循环的方法。 while 一个 R 语言里的 while 循环会反复的执行目标语句，只要给定的条件一直是真。不像 for 循环，while 循环不会确定循环次数，而会一直跑直到条件不满足。 while 循环句法 这是一个例子，我们实现一个简单的 while 循环。 repeat 和 break 一个 repeat 循环是用来反复执行一段代码的，其中没有检查条件然后退出的机制。 repeat 和 break 的句法 现在，让我们用 repeat 和 break 来输出前五个数字。 在任何编程语言中函数的主要作用就是可复用性。函数是一系列声明的组合以执行特殊的任务。在 R 语言里有很多内建的函数，例如 sum()、min()、max()、mean() 等。 R 语言中声明函数的句法 现在让我们创建一个函数来输出一系列的数字。 数据可视化是对决策至关重要的参考。R 语言为了创建数据储存和可视化儿提供了最好的内置函数和库。 现在，让我们用 R Studio 里的 ggplot2 来创建一个简单的线图，我们需要安装 ggplot2 包，你会在左角找到控制台，执行命令安装包（「package_name」)： 我们现在导入一个内置的数据集（mpg），然后画一个简单的图。 关于 mpg 数据集：这是一个关于燃料经济的数据集，包含了从 1999 年到 2008 年 38 种流行车款的数据。 1. 一个 234 行和 11 个变量的数据帧； 2. displ-发动机排量，以升为单位； 3. hwy-高速公路耗油量，英里每加仑。 原文链接：https://towardsdatascience.com/r-lang-zero-to-hero-c59a9f66841c "
250,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737774&idx=3&sn=f3fbaea96715f2eec4ad5c566bd32eaa&chksm=871ac810b06d41062a87bdae0de22c5ce1dee628d0d5c515b96eab26a32e88d66bb5157bb5c5&scene=27,前沿 | UC Berkeley提出特征选择新方法：条件协方差最小化,"BAIR Blog UC Berkeley 近日提出了一种新型特征选择方法 CCM，该方法基于最小化条件协方差算子的迹来进行特征选择。研究者的实验证明该方法在多个合成和现实数据集上达到了不输当前先进方法的性能。相关论文《Kernel Feature Selection via Conditional Covariance Minimization》被 NIPS 2017 接收。 论文链接：https://papers.nips.cc/paper/7270-kernel-feature-selection-via-conditional-covariance-minimization.pdf GitHub 链接：https://github.com/Jianbo-Lab/CCM 降维可以增强模型的可解释性，特征选择则是常用的降维方法。随着大型数据集变得流行，近年来包括文本分类、微阵列数据中的基因选择、人脸识别等现实任务见证了特征选择的广泛使用。BAIR 研究了监督性特征选择的问题，监督特征选择需要寻找一个输入特征的子集来较好地解释输出结果。这个方法可以通过消除冗余或者噪声特征来降低下游学习（downstream learning）的计算成本，同时还能通过保留下来的特征来提供对数据的洞见。 特征选择算法通常可分为三个主要的类别：滤波器（filter）方法、封装（wrapper）方法以及嵌入（embedded）方法。滤波器方法基于数据的本质属性选择特征，与所用的学习算法无关。例如，我们可以计算每个特征和响应变量之间的相关性，然后选择相关性最高的变量。相比之下，封装方法就更加具体，它的目标是寻找能够使某个预测器的性能最优化的特征。例如，我们可以训练多个支持向量机，每个支持向量机使用不同的特征子集，然后选择在训练数据上损失最小的特征子集。因为特征子集的数量是指数规模的，所以封装方法通常会使用贪心算法。最后，嵌入方法是将特征选择和预测结合成一个问题的多目标技术，它通常会优化一个目标函数，这个目标函数结合了拟合优度和对参数数量的惩罚。一个例子就是构建线性模型的 LASSO 方法，它用ℓ1 penalty 来表征对参数数目的惩罚。 本文提出了条件协方差最小化（CCM）方法，这是一个统一前两个观点的特征选择方法。在以下部分中，BAIR 研究者首先描述了自己的方法。然后我们通过几个综合实验证明该方法能够捕获特征集之间的联合非线性关系。最后，一系列现实任务证明，该算法具有和其他几种特征选择算法相当或者优于它们的性能。 理解特征选择问题的一种方式是从依赖的角度来看。理想情况下，我们希望先确定一个大小为 m 的特征子集 T，这样剩下的特征独立于 T 的响应。然而，如果 m 太小，则这可能无法实现。所以我们使用某个指标来量化对剩余特征条件依赖的程度，并且在所有合适大小的特征子集 T 上优化该指标。 或者，我们希望找到一个特征子集 T，它能够在特定的学习问题上最有效地预测输出 Y。在我们的框架中，将样本标签和最佳分类器所做的预测之间的均方差定义为预测误差。 我们提出了一个可以在回归中同时描述依赖性和预测误差的标准。首先，我们分别介绍了在特征子集 X_T  的域和响应变量 Y 的域上的两个函数空间。每个函数空间都是一个完全的内积空间（希尔伯特空间），这个函数空间有可以将整个空间进行延展的核函数，且具备「再生性」。这样的函数空间被称作「再生核希尔伯特空间」（Reproducing Kernel Hilbert Space，RKHS）。然后，我们将响应变量的域上的 RKHS 算子定义为：在给定所选特征的情况下，描述输入数据上的响应变量的条件依赖。这个算子叫做「条件协方差算子」（conditional covariance operator）。我们用对应的经验分布计算得到的条件协方差算子的迹作为我们的优化标准，这也是最佳预测器在给定的输入数据域上的 RKHS 中的估计回归误差。在特征子集上直接最小化这个标准是很难计算的。相反，我们使用一个介于 0 和 1 之间的实值标量来对每个特征进行加权，通过这个方式将其表达为一个松弛问题。该松弛问题的目标可以用核矩阵来表示，而且可以很容易地使用基于梯度的方法进行优化。 我们分别在合成数据集和现实数据集上测评了我们的方法。我们比较了现有的几个强大算法，包括递归式特征消除（RFE）、最小冗余最大关联（mRMR）、BAHSIC，以及使用互信息（MI）和皮尔逊相关系数（PC）的滤波器方法。RFE 是一个很流行的封装方法，它基于从分类器收到的得分贪婪地选择特征。mRMR 选择的特征能够捕获彼此不同的信息，但是每一个都与响应变量有很大的相关性。BAHSIC 是一个核方法，它贪婪地优化所选特征和响应变量之间的依赖。最后，滤波器方法使用互信息（MI）或者皮尔逊相关系数（PC）分别贪婪地优化所选特征子集和响应之间的相应指标。 合成数据 我们使用以下的合成数据集： 二元分类（Friedman et al.）。当 Y=−1 时，10 个特征 (X_1,…,X_10) 是相互独立的标准随机变量。当 Y=1 的时候，前 4 个特征是标准正态随机变量，它们满足以下条件： 剩下的 6 个特征 (X_5,…,X_10) 也是独立的标准正态随机变量。 三维异或作为四分类。三维超立方体有 8 个顶点， 通过元组 (v1v3,v2v3) 对它们进行分组，就得到了 4 组与各自的反面组成的成对向量 对类别 i 而言，样本可以通过等概率选择 v^(i) 或者−v^(i) 和添加噪声来生成。每个样本就额外增加 7 个标准正态噪声特征，得到的特征集总共有 10 个特征。 可加非线性回归。考虑以下可加模型： 每个样本额外有 6 个噪声特征，总共 10 个特征。所有的特征和噪声 ε 都是从标准正态分布中生成的。 左：二元分类数据；右：二维异或 第一个数据集表示一个标准的非线性二分类任务。第二个数据集表示一个多类别分类任务，其中每个特征都是和 Y 独立的，但是三个特征联合起来会对 Y 产生影响。第三个数据集针对可加非线性回归模型。 每个数据集的特征维度 d=10，但是只有 m=3 或者 4 是真实特征。因为这些真实特征已经是已知的了，所以我们可以通过计算每种特征选择算法给真实特征赋予的中值等级来评估该算法的性能，较低的中值等级意味着较好的性能。 上图展示了模拟数据集上真实特征的中值等级（y 轴）随着样本量（x 轴）的变化。中值等级越低，性能越好。点画线代表最优的中值等级。 在二分类和四分类任务中，我们的方法比其他的算法有着更优的性能，我们的方法在 50 个样本以内就能够鉴别出真实特征，而其他的方法需要接近 100 个样本或者最终都无法成功收敛。在可加非线性模型中，几个算法的性能都比较好，在所有的样本大小中我们的算法都不逊色。 现实数据 现在我们将注意力转移到现实任务中，研究我们的方法和其他几种非线性方法（mRMR、BAHSIC、MI）与核 SVM 结合使用时的下游分类性能。我们在 ASU 特征选择网站和 UCI 库中的 12 个标准基准任务上进行了实验。下表是对所用数据集的总结。 数据集来自多个领域，包括基因数据、图像数据、声音数据，而且高维度和低维度的都有。 对于每一个任务，我们运行并评估每个算法，以获取所有特征的排序。然后基于前 m 个特征训练核 SVM，计算准确率，从而评估性能。下图展示了我们的结果。 上图展示了现实基准数据集中的准确率（y 轴）随所选特征数目（x 轴）的变化。准确率越高，性能越好。 与其他三种流行的非线性特征选择方法相比，我们的方法在大多数例子中都是最强大的，有时候会有特别大的差距，例如在 TOX-171 任务中。尽管我们的方法偶尔在开始时（所选特征数目比较少时）性能会不太好，但最终还是会跟其他算法的性能持平或超越它们（glass 任务除外）。 结论 在这篇文章中，我们提出了条件协方差最小化（CCM）方法，这个方法基于最小化条件协方差算子的迹来进行特征选择。这个方法的思想是选择能够最大化预测基于协变量响应依赖的特征。我们通过将很难处理的离散协变量响应依赖松弛化，以获取适合基于梯度的优化的连续近似，从而完成该方法。我们在多个合成数据集和现实数据集上进行实验，证明了该方法的有效性，发现我们的方法通常会优于目前最先进的算法，包括另一个基于希尔伯特-施密特独立性系数（Hilbert-Schmidt independence criterion）的核特征选择方法。 更多信息 关于算法的更多信息，请参考链接中的内容： 论文：https://papers.nips.cc/paper/7270-kernel-feature-selection-via-conditional-covariance-minimization 代码：https://github.com/Jianbo-Lab/CCM 论文：Kernel Feature Selection via Conditional Covariance Minimization 摘要： 我们提出了一种特征选择方法，该方法利用基于核的独立性估计找出协变量的子集，可最大化预测响应变量。我们基于之前的核降维研究构建该方法，展示了如何通过约束优化问题（涉及条件协方差算子的迹）进行特征选择。我们证实了该步骤稳定一致的结果，同时还展示了该方法在多个合成和现实数据集上与其他先进算法相比的优越性。  "
251,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737814&idx=1&sn=b3f57f84fe6243c0d36cc006178f1763&chksm=871ac8e8b06d41fe405408b6024f0e1fb8d063c1b507108eadeba2c6934a16be2be53ac562c3&scene=27,谷歌云TPU服务正式全面开放：「AlphaGo背后的芯片」进入商用化,刚刚，谷歌云博客宣布：谷歌云 TPU 机器学习加速器测试版已向外部用户开放，价格大约为每云 TPU 每小时 6.50 美元，而且数量有限。此举意味着这种曾支持了著名 AI 围棋程序 AlphaGo 的强大芯片将很快成为各家科技公司开展人工智能业务的强大资源。 2016 年 5 月，谷歌向世人发布了一款特别的机器学习专属芯片：张量处理器（Tensor Processing Unit，TPU），去年又推出了它的第二代产品（Cloud TPU）。这是一种被认为比 CPU 、甚至 GPU 更加高效的机器学习专用芯片。作为科技巨头的谷歌早已把这种高度定制化产品应用在了自己的服务器中，而本周一，谷歌宣布其他公司马上也将可以享受新型芯片带来的计算服务了。 虽然新一代 TPU 的适应性如何还有待观察，但是较之于单纯的机器学习任务加速的功能而言，TPU 确实颇具实力。据谷歌称，第一代 TPU 仅能够处理推理任务，而第二代 TPU 还可以用于机器学习模型的训练，这个机器学习过程中重要的一部分完全可在单块、强大的芯片上进行。在 2017 年 4 月，谷歌曾通过一篇论文《In-Datacenter Performance Analysis of a Tensor Processing Unit》介绍了 TPU 研究的相关技术以及第二代芯片与其它类似硬件的性能比较结果（参见： ）。 TPU 可以帮助谷歌的各类机器学习应用进行快速预测，并使产品迅速对用户需求做出回应。谷歌称，TPU 已运行在每一次搜索中；TPU 支持作为谷歌图像搜索（Google Image Search）、谷歌照片（Google Photo）和谷歌云视觉 API（Google Cloud Vision API）等产品的基础的精确视觉模型；TPU 也帮助了谷歌神经翻译质量的提升；而其强大的计算能力也在 DeepMind AlphaGo 对阵李世乭的重要胜利中发挥了作用——这是计算机首次在古老的围棋比赛中战胜人类世界冠军。 谷歌本周一的「Beta 测试」公告也宣示着这家现代科技企业正在改变自己的运营理念——它已开始涉及人工智能专属芯片的解决方案，这是一个包含数十家创业公司，以及英特尔、高通和英伟达这样的传统硬件厂商的重要市场。随着时代的发展，谷歌、亚马逊和微软已不再是纯粹的互联网企业，它们都已或多或少地开始扮演起硬件制造者的角色。 在此之前，谷歌其实也并不是 TPU 的唯一使用者，美国出行服务公司 Lyft 在去年底开始参与了谷歌新型芯片的测试。Lyft 希望通过使用 TPU 加速自动驾驶汽车系统的开发速度：TPU 在计算机视觉模型的训练速度上具有优势，可将原先耗时数日的任务缩短至几小时内完成。 谷歌在其云平台博客上宣布了 TPU 服务开放的消息： 通过谷歌云平台（GCP）提供的 Cloud TPU beta 版自 2018 年 2 月 12 日起可用，其旨在帮助机器学习专家更快地训练和运行 ML 模型。 Cloud TPU 是谷歌设计的一种硬件加速器，旨在优化以加速和扩大使用 TensorFlow 编程的机器学习工作负载。Cloud TPU 使用四个定制化 ASIC 构建，单个 Cloud TPU 的计算能力达到 180 万亿次浮点运算，具备 64 GB 的高带宽内存。这些板卡可单独使用也可通过超快的专门网络联合使用以构建数千万亿次级别的机器学习超级计算机，我们称之为「TPU pod」。今年稍后，我们将在 GCP 上提供更大型的超级计算机。 我们设计 Cloud TPU 的目的是为 TensorFlow 工作负载提供差异化的性能，使 ML 工程师和研究者实现更快迭代。例如：   你们无需费力等待调度共享计算机集群，通过谷歌计算引擎 VM，就可以独立获取交互式的网络联结 Cloud TPU。 无需花费数日或数周等待商用级机器学习模型，你可以在一系列 Cloud TPU 上训练同样模型的不同变体，而且第二天就可以将准确率最高的训练模型部署到生产过程。 使用单个 Cloud TPU，并遵循该教程（https://cloud.google.com/tpu/docs/tutorials/resnet），你可以在不到一天的时间内，训练 ResNet-50 使其在 ImageNet 基准挑战上达到期望的准确率。 让机器学习模型训练更容易 传统上，编写自定义 ASIC 和超级计算机的程序需要极高的专业度。而对于 Cloud TPU 而言，你可以使用高级 TensorFlow API 进行编程，我们开源了一系列参考高性能 Cloud TPU 模型实现，帮助大家立刻上手： ResNet-50（https://cloud.google.com/tpu/docs/tutorials/resnet）和其他常用的图像分类模型（https://github.com/tensorflow/tpu/tree/master/models/official）。 用于机器翻译和语言建模的  （https://cloud.google.com/tpu/docs/tutorials/transformer (https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)） 用于目标检测的 RetinaNet（https://github.com/tensorflow/tpu/blob/master/models/official/retinanet/README.md） 为了节约大家的时间和精力，我们持续测试这些模型实现的性能和在标准数据集上收敛至期望准确率的情况。 以后，我们还将开源其他模型实现。喜爱冒险的机器学习专家可以使用我们提供的文档（https://cloud.google.com/tpu/docs/）和工具（https://cloud.google.com/tpu/docs/cloud-tpu-tools）利用 Cloud TPU 自己优化其他 TensorFlow 模型。 现在开始使用 Cloud TPU，今年稍后我们宣布 TPU pod 时，你们将从时间-准确率的大幅改进中受益良多。正如我们在 NIPS 2017 上宣布的那样，在完整的 TPU pod 上，ResNet-50 和 Transformer 的训练时间从大半天降到不到 30 分钟，而且无需改变任何代码。 可扩展的机器学习平台 云 TPU 同样简化了对机器学习计算资源的规划和管理： 你可以为自己的团队提供顶尖的机器学习加速，随着需求的变化动态调整自己的容量； 相比于花费资金、时间和专业人才来设计、安装、维护一个实地的机器学习计算群（它还需要专门化的供能、冷却、网络和存储），你可以从谷歌多年以来优化过的大规模、高集成的机器学习基础设施受益； 不需要花费努力保证驱动对各种工作站和服务器保持更新升级，云 TPU 做了不需要驱动安装的预配置； 受谷歌云服务提供的同样复杂的安全机制和实践的保护。 在谷歌云，我们还想为客户的每个机器学习负载提供最好的云。伴随谷歌云 TPU，我们也会提供大量的高性能 CPU（包括英特尔 Skylake）和 GPU（包括英伟达的 Tesla V100）。 开始使用云 TPU 云 TPU 如今在数量受限的情况下可用，价格以秒计费，大约为每云 TPU 每小时 6.50 美元。 注册地址：https://services.google.com/fb/forms/cloud-tpu-beta-request/  
252,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737814&idx=2&sn=8bfdccb1b7b2db5097728502cf3c669a&chksm=871ac8e8b06d41fe9e210ec6c5da0ff9e1933dc884b96cce58f61c9cf96181ab94ddc505bac3&scene=27,教程 | 如何使用Keras、Redis、Flask和Apache把深度学习模型部署到生产环境？,pyimagesearch Adrian Rosebrock 本文介绍了如何使用 Keras、Redis、Flask 和 Apache 将自己的深度学习模型迁移到生产环境。文中的大多数工具可以互换，比如 Keras 可替换成 TensorFlow 或 PyTorch，Django 可代替 Flask，Nginx 可代替 Apache。作者唯一不推荐替换的工具是 Redis。同时本文还对深度学习 REST API 进行了压力测试，这种方法可以轻松扩展到添加的服务器。 将深度学习模型用迁移到生产是一项不平凡的任务。 如果你不相信，请花点时间看看亚马逊、谷歌、微软等「科技巨头」——几乎所有公司都提供了一些将机器学习/深度学习模型迁移到云端生产环境中的方法。 虽然使用模型部署的服务是完全可行且能够接受的，但是如果你想独立操作整个过程而不依赖于外部服务呢？这种情况比你想像的更常见。试想以下情况： 不能将敏感数据移到外网的内部项目 指定了整个基础架构必须留在公司内的项目 需要私有云的政府组织 处于「秘密模式」的创业公司，需要在内部对其服务/应用程序进行压力测试 在这种情况下，如何将你的深度学习模型迁移到生产环境中呢？或许最重要的是，同时使其具有可扩展性？ 本文是关于构建深度学习模型服务器 REST API 的三部分系列文章的最后一部分： 第一部分（https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html）（发布在官方 Keras.io 博客上）是一个简单的 Keras +深度学习 REST API，用于没有并发请求的单线程。如果这是你第一次构建深度学习 Web 服务器，或者这是家用/业余爱好项目，则此方法非常适合。 第二部分（https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/）中，我们演示了如何利用 Redis 和消息队列/消息代理（broker）范式有效地批处理传入的推断请求（但伴随在服务器线程的一个小警告，它可能会导致问题）。 第三部分，我将向你展示如何解决这些服务器线程问题，进一步扩展我们的方法，提供基准，并演示如何有效地利用 Keras、Redis、Flask 和 Apache。 压力测试结果将证明，我们的单机 GPU 可以轻松处理 500 个并发请求（每个之间 0.05 秒的延迟）而毫不费力——性能也在不断扩展。 想要了解如何使用 Keras、Redis、Flask 和 Apache 将自己的深度学习模型迁移到生产环境，请继续阅读。 源代码地址：https://www.getdrip.com/forms/15005528/submissions 本文的代码主要基于我之前的文章（https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/），但有一些小修改——今天指南的第一部分将一起回顾这些变化和项目架构。之后，我们将继续配置深度学习 Web 应用程序，包括安装和配置你可能需要的任何软件包（Redis、Apache 等）。最后，我们将压力测试我们的服务器并对结果进行基准对比。 想要快速了解我们的深度学习生产系统（包括演示），请观看上面的视频！ 我们的项目架构如下： 我们来回顾一下重要的 .py 文件： run_web_server.py 包含我们所有的 Flask Web 服务器代码——Apache 在启动我们的深度学习 Web 应用程序时加载它。 run_model_server.py 将会： 从磁盘加载我们的 Keras 模型 不断从 Redis 请求（poll）新图像进行分类 分类图像（为提高效率进行批处理） 将推断结果写回 Redis，以便通过 Flask 将其返回给客户端 settings.py 包含所有基于 Python 的深度学习生产服务设置，例如 Redis 主机/端口信息、图像分类设置、图像队列名称等。 helpers.py 包含 run_web_server.py 和 run_model_server.py 将使用的效用函数（即 base64 编码）。 keras_rest_api_app.wsgi 包含我们的 WSGI 设置，所以我们可以从 Apache 服务器提供对 Flask 应用程序的服务。 simple_request.py 可用于以编程方式处理深度学习 API 服务的结果。 jemma.png 是我家小猎犬的照片。我将使用它作为调用 REST API（来验证它确实正在工作）的示例图像。 最后，我们将使用 stress_test.py 来压力测试我们的服务器并衡量所有图像的分类。 如第二部分所介绍的，我们在 Flask 服务器上有一个终点（endpoint）「/predict」。此方法位于 run_web_server.py 中，并将根据需要计算输入图像的分类。图像预处理也在 run_web_server.py 中。 为了使我们的服务器为生产做好准备，我从上周的脚本中提取了 classify_process 函数，并将其放在 run_model_server.py 中。这个脚本非常重要，因为它会加载我们的 Keras 模型，并从 Redis 的图像队列中抓取图像进行分类。结果将写回 Redis（「/predict」终点和 run_web_server.py 中的相应函数会监控 Redis 将结果发送回客户端）。 但是，除非知道它的能力和限制，否则如何知道深度学习 REST API 服务器有什么好处？ 在 stress_test.py 中，我们将测试服务器。我们将通过开发 500 个并发线程来完成这个任务，这些线程将把图像发送到服务器进行并行分类。我建议启动时在服务器本地主机上运行，然后从离线的客户端运行它。 几乎在这个项目中使用的每一行代码都来自我们之前关于构建可扩展深度学习 REST API 的文章（https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/）——唯一的变化是我们将一些代码迁移到单独的文件中，以便在生产环境中实现可扩展性。 为了完整性，我将每个文件的源代码包括在本文中（以及本文的「下载」部分）。有关这些文件的详细内容，请参阅之前的文章。 设置和配置 在 settings.py 中，你可以更改服务器连接参数：图像维度 + 数据类型，以及服务器队列。 帮助文件的效用 helpers.py 文件包含两个函数——一个用于 base64 编码，另一个用于解码。 编码是必要的，以便我们可以在 Redis 中序列化 + 存储我们的图像。同样，解码是必要的，以便我们可以在预处理之前将图像反序列化（deserialize）为 NumPy 数组格式。 深度学习 web 服务器 在 run_web_server.py 中，你会看到 predict，这是与 REST API/predict 终点相关的函数。 predict 函数将编码图像推入 Redis 队列，然后不断循环/请求，直到它从模型服务器得到预测数据。然后，我们对数据进行 JSON 编码，并指示 Flask 将数据发送回客户端。 深度学习模型服务器 run_model_server.py 文件包含我们的 classify_process 函数。这个函数会加载模型并对批图像进行预测。此过程在 GPU 上运行最佳，但也可以使用 CPU。 本例中，为了简单起见，我们将使用在 ImageNet 数据集上预先训练的 ResNet50。你可以修改 classify_process 函数以适用于你自己的深度学习模型。 WSGI 配置 下一个文件 keras_rest_api_app.wsgi 是深度学习 REST API 的新组件（相比于上周）。 WSGI 配置文件将服务器目录添加到系统路径，并导入 web 应用以启动所有操作。我们在本文后面提到的 Apache 服务器设置文件 /etc/apache2/sites-available/000-default.conf 中指向这个文件。 压力测试 stress_test.py 脚本将帮助我们测试服务器并确定其局限性。我一直建议对深度学习 REST API 服务器进行压力测试，以便知道是否需要添加其它 GPU、CPU 或 RAM。此脚本启动 NUM_REQUESTS 线程并 POST 发送到 /predict 终点。这取决于 Flask web 应用。 本节将讨论如何为我们的深度学习 API 服务器安装和配置必要的先决条件。 我们将使用我的 PyImageSearch 深度学习 AMI（https://www.pyimagesearch.com/2017/09/20/pre-configured-amazon-aws-deep-learning-ami-with-python/）作为基础。在这个例子中，我选择了一个 p2.xlarge 实例和单个 GPU。 你可以修改代码以利用多个 GPU： 运行多个模型服务器进程； 为每个 GPU 维护一个图像队列以及相应的模型进程。 但请记住，你的机器仍然受到 I/O 设备限制。改用多台机器（每台机器上 1-4 个 GPU）可能是有益的，而不是将一台机器扩展到 8 或 16 个 GPU。 编译和安装 Redis Redis 是一个高效的内存数据库，将作为我们的队列/消息代理。 获取和安装 Redis 非常容易： 创建你的深度学习 Python 虚拟环境 我们来为这个项目创建一个 Python 虚拟环境。请参阅上周的教程（https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/），通过说明了解如何在不熟悉 Python 虚拟环境的情况下安装 virtualenv 和 virtualenvwrapper。 准备工作就绪后，创建虚拟环境： 在此之上，我们来安装必要的软件包： 注意：由于我们正在使用 CUDA 8，这里使用 TensorFlow 1.4.1。如果使用 CUDA 9，则应该使用 TensorFlow 1.5。 安装 Apache Web 服务器 由于我对 Apache 更加熟悉，我将在这个例子中使用 Apache，但是你也可以使用其它的网络服务器，例如 nginx。 Apache 可以通过以下方式安装： 如果你使用 Python 3 创建了一个虚拟环境，则需要安装 Python 3 WSGI + Apache 模块： 否则，Python 2.7 用户应该安装 Pytohn 2.7 WSGI + Apache 模块： 要验证是否安装了 Apache，请打开浏览器并输入 web 服务器的 IP 地址。如果看不到服务器启动画面，请确保打开端口 80 和端口 5000。 我的服务器的 IP 地址是 54.187.46.215（你的 IP 将不同）。在浏览器中输入地址后我看到： ……这是默认的 Apache 主页。 Sym-link 你的 Flask + 深度学习应用程序 默认情况下，Apache 提供来自 /var/www/html 的内容。我建议创建一个从 /var/www/html 到你的 Flask web 应用的 sym-link。 我已经将我的深度学习 + Flask 应用程序上传到我的主目录中的一个名为 keras-complete-rest-api 的目录中： 我可以通过以下方式将它 sym-link 到 /var/www/html： 为了配置 Apache 以便指向我们的 Flask 应用程序，我们需要编辑 /etc/apache2/sites-available/000-default.conf。 打开你最喜欢的文本编辑器（这里我将使用 vi）： 在文件头提供你的 WSGIPythonHome（Python bin 目录的路径）和 WSGIPythonPath（Python site-packages 目录的路径）配置： 由于在这个实例中使用了 Python 虚拟环境（我已经将我的虚拟环境命名为 keras_flask），我们提供了 Python 虚拟环境的 bin 和 site-packages 目录的路径。 然后在 <VirtualHost> 的主体中，在 ServerAdmin 和 DocumentRoot 之后添加： Sym-link CUDA 库（可选，仅限 GPU） 如果你的深度学习模型正在使用 GPU，并希望利用 CUDA，但很不幸的是 Apache 在 /usr/local/cuda/lib64 中并不知道 CUDA 的库。 我不知道告诉 Apache 这些 CUDA 库在哪里的「最正确」的方式，但「完全破解」的解决方案是将 /usr/local/cuda/lib64 中的所有文件 sym-link 到 /usr/lib： 重新启动 Apache Web 服务器 一旦你编辑了你的 Apache 配置文件，并且选择 sym-link CUDA 深度学习库，一定要通过以下方式重新启动你的 Apache 服务器： 测试你的 Apache Web 服务器 + 深度学习终点 要测试 Apache 是否已正确配置以产生你的 Flask + 深度学习应用程序，请刷新 web 浏览器： 你现在应该在浏览器中看到「欢迎使用 PyImageSearch Keras REST API！」文本。 一旦你到了这个阶段，你的 Flask 深度学习应用程序应该准备好了。 如果你遇到任何问题，请务必参考下一节…… 提示：如果遇到麻烦，请监控 Apache 错误日志 多年来，我一直在使用 Python + Web 框架（如 Flask 和 Django），却仍然在正确配置环境的过程中犯错误。 虽然我希望有一个万全的方法来确保所有事情运行顺利，但事实是，一些事情很可能将工作搞砸。 好消息是，WSGI 会将 Python 事件（包括运行失败）记录到服务器日志中。 在 Ubuntu 上，Apache 服务器日志位于 /var/log/apache2/ 中： 在调试的时候，我通常会开着运行调试的终端： ……所以我可以在错误滚入的瞬间看到它。 使用错误日志来帮助你在服务器上创建并运行 Flask。 你的 Apache 服务器应该已经在运行了。如果没有，你可以通过以下方式启动它： 然后，你将启动 Redis 商店（数据库）： 在另一个单独的终端启动 Keras 模型服务器： 之后，尝试将图像示例提交给你的深度学习 API 服务： 如果一切正常，你将从深度学习 API 模型服务器接收到带有类预测 + 概率的格式化的 JSON 输出。 当然，这只是一个例子。让我们来压力测试我们的深度学习 REST API。 打开另一个终端并执行以下命令： 在你的 run_model_server.py 输出中，你将看到以下内容被记录到终端中： 即使每隔 0.05 秒发出一个新的请求，我们的批大小也不会超过每批约 10-12 个图像。 我们的模型服务器可以轻松处理加载，而且可以轻松扩展。 如果你的加载超过了服务器承受（也许你的批大小太大，同时你的 GPU 内存用完了并开始报错），你应该停止服务器，然后使用 Redis CLI 清除队列： 之后，你可以调整 settings.py 和 /etc/apache2/sites-available/000-default.conf 中的设置。然后你可以重新启动服务器。 有关完整演示，请观看文章开头的视频。 我可以给出的最好的建议之一就是让你的数据（特别是你的 Redis 服务器）尽可能离 GPU 近。 你可能想要使用数百 GB 的 RAM 来启动一个巨大的 Redis 服务器，以处理多个图像队列并为多个 GPU 机器提供服务。 这里的问题将是 I/O 延迟和网络开销。 假设 224×224×3 的图像用 float32 数组表示，一个批大小为 32 的图像将是约 19MB 的数据。这意味着对于来自模型服务器的每个批请求，Redis 将需要提取 19MB 的数据并将其发送到服务器。 在快速交换机上，这不是什么大问题，但是你应该考虑在同一台服务器上同时运行模型服务器和 Redis，来保证数据尽可能离 GPU 近。 在本文中，我们学习了如何使用 Keras、Redis、Flask 和 Apache 将深度学习模型部署到生产。 我们这里使用的大多数工具是可以互换的。你可以将 Keras 替换成 TensorFlow 或 PyTorch。Django 可以用来代替 Flask。Nginx 可以代替 Apache。 我唯一不推荐替换的工具是 Redis。Redis 可以说是内存数据存储的最佳解决方案。除非你有特殊原因不使用 Redis，否则我建议你使用 Redis 进行队列操作。 最后，我们压力测试了我们的深度学习 REST API。 我们向服务器提交了 500 个图像分类请求，每个请求之间有 0.05 秒的延迟——我们的服务器没有分阶段（phase）（CNN 的批大小不超过满负载的 37％ 左右）。 此外，这种方法可以轻松扩展到添加的服务器。如果为这些服务器设置加载平衡器，你可以轻松地进一步扩展此方法。 原文链接：https://www.pyimagesearch.com/2018/02/05/deep-learning-production-keras-redis-flask-apache/ 
253,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737774&idx=4&sn=8b9715e93c6ba1865c2f9f874b00e7bd&chksm=871ac810b06d41064389fe508f9b65e5f84fa4a5e10ea5fceff074422d0b21592a42427168e1&scene=27,AAAI 2018 | 港中文-商汤联合论文：自监督语义分割的混合与匹配调节,"作者：Xiaohang Zhan、Ziwei Liu、Ping Luo、Xiaoou Tang、Chen Change Loy 这篇文章介绍了香港中大-商汤科技联合实验室的新论文「Mix-and-Match Tuning for Self-supervised Semantic Segmentation」，该论文被 AAAI 2018 录用为 Spotlight。 用于语义分割的卷积神经网络通常需要大量的标注数据来进行预训练，例如 ImageNet 和 MS-COCO。自监督学习最近被提出来，主要用来减少标注工作量，它的预训练过程中无需任何人工标注。此项研究已经在图像分割领域中得到了应用（Zhang, Isola, and Efros 2016a;Larsson, Maire, and Shakhnarovich 2016; 2017）。其关键在于，在此过程中引入了一个无监督的「预训练任务」，这个任务可以在无标注数据上执行，用来学习图像的描述。然而，很多预训练任务缺乏能够对目标任务进行有效区分的监督信号，导致最终的结果比有监督的初始化要低很多。在本文的研究中，我们通过引入「混合与匹配」的机制来解决这个局限性。这个机制可以兼容很多自监督与训练方法，相比于原来的方案无需更多的标注数据。利用了「混合与匹配」机制的自监督初始化模型，其最终结果可以匹敌甚至超越全监督的初始化模型。 自监督学习通常分为两个步骤：无监督的预训练和对目标任务的微调。预训练步骤不只需要数据本身，而无需数据的标注结果。它需要设计一个预训练任务，利用从数据本身到的的监督信号来训练。例如，图像上色任务（Larsson, Maire, and Shakhnarovich 2017），利用了图像可分解成亮度和色彩这样的特点，通过输入图像亮度，来预测图像的颜色。在此过程中学习到带有某种程度语义的图像描述子，并将之通过微调应用在最终的语义分割任务上。自监督学习展现出其强大的学习能力，即使在没有标注的数据上，也能获得不错的初始化效果，大大超越了随机初始化的模型。 尽管自监督学习很有前景，但目前其表现结果还远不如有监督的初始化。例如，使用 VGG-16 网络，用图像上色来进行预训练，在 PASCAL VOC2012 上能得到 56.0% mIoU 的结果，大大超过随机初始化的 35.0% mIoU 的结果。但用 ImageNet 分类任务来初始化，则可以达到 64.2%。这说明自监督预训练和有监督预训练之间还有较大的差距。 图一：（a）展示了来自「汽车」和「巴士」两个类别的图像块，它们具有非常相近的色彩分布。（b）展示了这两个类别的深层特征分布，执行过「混合与匹配」的特征对于这两个类别有更好的区分性。 图二：我们提出的方法的整体流程图。我们的方法在自监督预训练任务与最终目标任务之间插入了「混合与匹配」过程。 我们认为这个差距的主要原因在于，自监督预训练任务和最终的目标任务在语义层次上有较大的区别。例如对于图像上色来说，这个任务中网络能学到对颜色分布比较敏感的图像描述子，这样的描述子对图像上色很有帮助，但对更高的语义层次，其作用会弱很多。例如，在图一（a）中，汽车和巴士的颜色分布非常接近，因此基于图像上色得到的图像描述子，对于汽车和巴士会有大范围的重合，难以区分，见图一（b）。 要提升自监督图像分割的表现，需要让图像描述子对目标任务有更好的区分能力。然而这个目标并不容易，因为图像分割的数据集通常很小，其标注数量很少，一般来说只有几千张图片。现有的方法一般用」Softmax」损失函数来利用这些标注，对于初始化得很好的有监督初始化网络，它是足够有效的，但对于初始化较差的自监督初始化网络，「Softmax」是不够的。我们认为，利用以像素为单位的图像分割的标注，」Softmax」并不是唯一的方式。 在这项研究中，我们提出了一个新的策略，叫「混合与匹配」(Mix-and-Match)，来更好地利用有限的标注信息，从而提升自监督初始化网络的性能。见图二，「混合与匹配」紧接自监督预训练任务之后，作为一个中间过程用来弥补预训练任务和目标任务之间的差距。值得注意的是，此过程只使用了目标任务的数据和标注，并不需要额外的数据或标注。 此过程分为「混合」与「匹配」两个步骤。在「混合」步骤中，我们从目标任务图像中随机抽样了大量的局部图像块并混合在一起。这些图像块跨越了多张图像，因而能够减弱图像内部的相关性，从而无偏地反映目标图像的分布。由这些图像块组成的大量的三元组也能够为优化提供稳定的梯度。在「匹配」过程中，我们构建了一个无向图，它的节点即图像块的深层特征。它有两种类型的边，如果两个图像块属于同一类，那么我们定义为「相吸边」，反之，则定义为「相斥边」。我们通过迭代的方式构建的图可以确保同类的节点组成一个连通子图，见图三。在此方式构建的图中，我们可以获得更加鲁棒的三元组，能够让网络学会将同类的图像块映射到同一个点上，或者说优化的过程中使得它们的描述子在欧几里得空间中构成单个中心，而非多个中心，并且使得不同类之间具有较大的距离。我们抽样三元组的方式和以往的工作具有非常大的不同。 图三：此图展示了不同的构建三元组的方式。节点的颜色代表其类别。蓝色和红色的边分别代表「相吸边」和「相斥边」。（a）是随机选择的三元组（Schroff, Kalenichenko, and Philbin 2015），其中来自相同类的节点不能构成一个连通图。（b-i）和（b-ii）是我们通过构建无向图来构建三元组的方法。我们构建的无向图中，每一类都分别构成一个连通子图。 我们的工作的贡献主要有：1. 我们提出了「混合与匹配」的调节机制，首次让自监督预训练的模型超过了有监督预训练的模型。具体来说，在 PASCAL VOC2012 数据集上，在 VGG-16 网络上，使用图像上色作为预训练任务，我们的方法获得了 64.5% mIoU 的性能，超过了 ImageNet 分类作为预训练任务的模型，64.2%。在 CityScapes 数据集上，我们得到了 66.4% 的性能，匹敌 ImageNet 预训练的结果，67.9%。此提升极具显著性，考虑到我们的方法是基于无监督预训练的。2. 除了利用图像上色作为预训练任务，我们还利用基于图像内容的自监督方法——Jigsaw Puzzles，获得了较大的提升。3. 使用随机初始化，在不同网络结构和不同数据集上，我们的方法也获得了显著的提升。这使得随机初始化训练语义分割成为可能。4. 我们提出的一种新的基于类内连通图的三元组抽样方案，相比于传统的三元组抽样方案更加鲁棒。 表一：在此数据集是公认的语义分割数据集，PASCAL VOC2012 数据集上的结果对比。我们的方法同时在 VGG-16 和 AlexNet 上大幅度超过了目前最好的基于自监督初始化的方法，并且在 VGG-16 上超过了 ImageNet 初始化的方法。 表二：PASCAL VOC2012 上单类语义分割结果。 图四：此图展示了使用了我们的方法之后，图像特征分布的变化。 表三：此表格展示了不同数据集下，用不同网络结构，和不同预训练任务（包括随机初始化），我们的方法获得的提升效果。 图五：结果可视化。 "
254,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737814&idx=4&sn=341ee8f57e6f89a8cdcac6ac6987a836&chksm=871ac8e8b06d41fe03bc4bbad0d6d21fc9c29661a1393ecb699353c1a883aab3d4ea1c6f058e&scene=27,ICLR 2018 | 彩云科技提出结合组合子抽象的神经编程器-解释器，提升通用性和可学习性,"penReview 近日，ICLR 2018 接收论文公布，国内人工智能公司彩云科技的一篇论文被此大会接收。在此论文中，作者们通过引入组合子抽象的方法，可以建立一种新的架构 CNPI，使得核心控制器需要解释的程序显著减少且程序复杂度显著降低，从而克服神经编程器-解释器（NPI）在通用性和可学习性存在局限性的问题。 论文：IMPROVING THE UNIVERSALITY AND LEARNABILITY OF NEURAL PROGRAMMER-INTERPRETERS WITH COMBINATOR ABSTRACTION 论文地址：https://openreview.net/pdf?id=rJlMAAeC- 摘要： 为了克服神经编程器-解释器（NPI）在通用性和可学习性方面的局限，本文提出在神经编程中引入组合子抽象（combinator abstraction）方法，并为此提出了 NPI 的一种新架构——组合神经编程器-解释器（CNPI）。有了组合子抽象，CNPI 的核心控制器需要解释的程序显著减少且程序复杂度显著降低，同时在核心控制器与其他组件的协同作用下，CNPI 仍然可以表示和解释任意复杂度的程序。本文提出用四个组合子来表征最常见的四种编程模式。由于组合子数量有限，形式简单，并且减少了核心控制器的解释工作量，我们可以构造出一个 CNPI，使之适用于所有可「组合子化」（即可用组合子描述）的程序，这样足以解决大多数算法任务。此外，通过恰当地设计课程，可能直接用策略梯度增强学习来训练 CNPI，无需提供程序执行轨迹。 教机器学程序是个有挑战的任务。研究者提出了很多模型，例如神经图灵机（Graves et al., 2014）、可微分神经计算机（Graves et al., 2016）、神经 GPU（Kaiser & Sutskever, 2015）、神经编程器（Neelakantan et al., 2015）、神经随机存取机（Kurach et al., 2015），还有神经编程器-解释器（Reed & de Freitas, 2016）。这些模型通常含有可微分存取的存储组件。大多数模型以程序输入-输出对作为训练数据，训练后的神经网络可以成为特定的目标程序，模仿一种特定的图灵机。 这些模型中的一个特例是神经编程器-解释器（NPI）（Reed & de Freitas, 2016）及其支持递归的扩展形式（Cai et al., 2017，在该论文中被称为 RNPI）。NPI 包含三个组件：一个核心控制器，通常由递归神经网络（RNN）实现；一个程序存储器，用于存储已学得程序的嵌入向量；还有多个特定域编码器，让单个 NPI 在多种环境中均可运行。核心控制器不学习任何特定的程序，而是学习解释任何一个用向量表示的程序，即模仿一个通用图灵机。核心控制器（即解释器）与已学得程序的存储器（即编程器）相结合，使 NPI 能够通过合子程序来学习新的程序，其架构更灵活也更具可组合性。然而，NPI 在理论和实际应用方面依然存在一些局限。 NPI 模型之所以可能适用于多任务学习、迁移学习和终身学习，是因为研究者假设 NPI 理论上具有通用性，即能够表达和解释任何程序。由于 NPI 仅靠核心控制器来解释程序，通用性意味着一个固定的核能够解释潜在的大量程序。想要持续地学习新程序并复用已学得的程序，一个普适且固定的核至关重要，因为一旦核的权重改变，学习了新程序就可能没法解释学过的旧程序了。尽管最早提出 NPI 的论文（Reed & de Freitas, 2016）实验显示单个共享的核可以解释 21 个程序，涵盖五种任务，并且训练过的固定核 NPI 还可以学习一个简单的新程序 MAX，但尚不清楚普适的 NPI 是否存在、究竟能够多普适。具体而言，考虑到所有可能的程序是个无限集，能够被一个固定核解释的程序子集并没有被显式定义。就算存在普适的 NPI，用 Cai et al. (2017) 论文中提出的验证方法也很难保证其通用性，因为需要验证无穷个程序。 实际应用时，Reed & de Freitas (2016) 的 NPI 模型需要强监督训练，也就是以程序的示例执行轨迹为训练数据。获取这种形式的训练数据，其成本往往高于获取程序输入-输出示例。相比之下，弱监督训练才是充分发挥 NPI 潜力的理想选择。 为了克服 NPI 的上述局限，本文提出将组合子抽象方法引入 NPI 模型，并为此设计了必要的组件和机制来增强原有的 NPI 架构。新架构被称为组合神经编程器-解释器（CNPI）。组合子，即高阶函数，是函数式编程中一种重要的抽象技巧，本文借用组合子的概念来表示不同程序共有的一些编程模式。 2.1 综述 NPI 及其局限 本节将概述 Reed & de Freitas (2016) 和 Cai et al. (2017) 论文中的 NPI 架构，分析架构的局限，这也是本文提出组合子抽象的动机。NPI 模型包含三个具有学习能力的组件：一个与任务无关的核心控制器、一个程序存储器，以及让 NPI 适用于多种环境的多个特定域编码器。核心控制器是一个长短期记忆（LSTM）网络（Hochreiter & Schmidhuber, 1997），作为各个程序之间的路由。在每个时间步上，核心控制器可以选择以特定的参数调用其他程序，也可以终止调用当前程序。当程序返回后，控制权将返给主调程序，即主调程序的 LSTM 隐藏单元和程序嵌入向量弹出程序调用栈，从中断处继续执行主调函数。   NPI 的推理过程如下（详见论文 Reed & de Freitas (2016) 第 3.1 节和算法 1）。在时间步 t 上，编码器 f_enc 接收环境观测量 e_t 和参数 a_t，输出状态 s_t。核心 LSTM 网络 f_lstm 接收状态 s_t、程序嵌入向量 p_t∈ R^P 和上一个隐状态 h_t-1，从而更新自身的隐状态 h_t。根据最顶层 LSTM 的隐状态，多个解码器可产生如下输出：返回概率 r_t、下一个程序的键嵌入向量 k_t+1，以及下一个程序的参数 a_t+1。通过对比键嵌入向量 k_t+1 与键存储器 M_key 中的每一行，可得到下一个程序的 ID。于是，从存储了 N 个程序的程序存储器 M_prog 中可以获取下一个程序的嵌入向量： 以上描述的 NPI 架构有两个局限。第一，由上式可知，程序存储器中保存着已学得的全部 N 个程序，在每个时间步上，核心控制器都要从这 N 个程序里选择一个调用。随着数量 N 增大到成百上千，让单个核心控制器正确解释程序会变得越来越困难。更难的是，核心控制器在学习新程序的同时，还不能忘记已学得的旧程序。第二，不同名称、不同功能的程序往往有一些共同的潜在编程模式。以 Reed & de Freitas (2016) 和 Cai et al. (2017) 论文中的两个程序为例（见图 1）：ADD1 程序执行多位数十进制加法，BSTEP 程序执行冒泡排序中的一次冒泡。这里采用 Cai et al. (2017) 论文中的递归形式。两个程序的循环模式非常相似，然而核心控制器需要分别学习这两个程序，无法利用他们共有的模式来提高学习效率。于是，以普适为目标的核心控制器需要学习无穷个程序。由于存在以上两个局限，我们认为即使可以构造普适的 NPI，挑战也非常大。 2.2 本文的组合子抽象方法 为了克服 NPI 的局限，本文提出在 NPI 架构中引入组合子抽象。函数式编程中，作为有力的抽象手段，组合子是一种特殊高阶函数，可以增强编程语言的表达力。本文将组合子概念引入到神经编程中，使之成为提升 NPI 通用性的关键角色。 组合子定义为一个「程序模板」，形参作为要填的空，可以是要调用的子程序。用另一个称为应用子（applier）的程序来包裹组合子，即可表示实际的程序。当执行组合子的时候，应用子调用组合子，将实参传递给组合子。组合子的实参可以是一组实际的程序，也可以是包裹结构的程序（即应用子），这样就能反复组合，构造出越来越复杂的程序了。与原始 NPI 相似的是，核心控制器对组合子的解释取决于一个轻量级的特定域编码器的输出。这个编码器我们称为探测子（detector），也由应用子动态地提供。图 1 为引入组合子抽象的 NPI 新架构。 在 CNPI 架构中，组合子是唯一需要被核心控制器解释的程序类型。由于限定了组合子只能调用传递给它的参数（程序），在每个时间步上可调用程序的选择范围就从不断增加的 N 个缩减到常数 K 个，K 是组合子可调用参数的最大数量（本文模型中 ≤ 9）。同时，与无限个潜在程序相比，有用的组合子数量有限而且通常很少。本文构造的组合子集合仅包含四个，用于表征最常见的四种编程模式。我们将证明，一个相当小的核与其他组件协同作用，就足以构造出普适的 CNPI。 3.1 组合子和组合程序 本文提出用四个组合子来表征算法任务中最常见的四种编程模式：顺序模式、条件模式、线性递归和树形递归（即多重递归）。四个组合子的伪代码见图 2。 3.2 CNPI 架构及算法 5.1 监督学习结果 5.2 增强学习的课程 5.3 增强学习结果 6 结论 通过在函数式编程中结合组合子抽象，本文提出的 CNPI 首次解决了 NPI 提升通用性和可学习性的问题。分析和实验结果表明 CNPI 对于所有「可组合子化」的程序都是通用的，并且无论用强监督还是弱监督都可以训练。研究者认为这个方法足够通用，除了解决算法任务之外，还存在潜在的应用。其中一个可能的应用场景是用强化学习训练智能体以服从指令和进行泛化（例如, Oh et al. (2017), Andreas et al. (2017), Denil et al. (2017)）。自然语言中包含了「高阶」词，例如「然后」、「两次」等，这些词在语言表达中很重要，但普通的序列到序列模型（Lake & Baroni, 2017）可能很难理解其意义。通过将这类词表示成组合子，并将智能体结合类似 CNPI 的组件，就有可能构造出性能更强的智能体，不仅可以根据简明的指令做出更复杂、更结构化的行为，而且由于抽象程度的提升，泛化性能也会提升。此类探索将在未来的研究中进行。 "
255,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650738083&idx=2&sn=7854e011dc0779768617f82e43773f04&chksm=871ac9ddb06d40cb29d5b3ce5612a7e5c0b28c754af4a9d513be6e8d6d8ad15642a841ab8bea&scene=27,业界 | 通过分析视网膜图像诊断心脑血管疾病：谷歌大脑团队取得医疗深度学习新进展,"近日，Jeff Dean 在其推特上说：「谷歌大脑团队在机器学习+医疗上再次取得新进展；我们有了令人鼓舞的新发现：非侵入式视网膜图像上含有机器学习模型可以介入的确诊心脑血管疾病的微妙指标，之前医生都不知道有这种迹象。」 心脏病、中风和其他心脑血管疾病一直是人类健康的头号杀手。评估这类风险是未来降低患者罹患心脑血管疾病的关键性第一步。为此，医生要考虑多种风险因子—比如遗传（性别或年龄）、生活习惯（抽不抽烟，血压等）。大部分因素可通过询问病人得到，其他因素比如胆固醇则要通过验血。医生也会考虑病人有没有其他疾病，比如糖尿病，其患病风险与心脑血管疾病成正相关。 最近，诸多实例 [1-4] 已展示深度学习如何增加医学图像确诊的概率，尤其是糖尿病引起的眼病。在「Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning」(详见： ）一文中，我们展示了在检测眼病之外，眼睛图像也可以被用来准确预测其他心脑血管疾病的指标。这个发现非常令人激动，因为它暗示我们也许会发现更多的用视网膜图像来确诊疾病的办法。 通过 284335 名病患数据训练深度学习模型，我们可在两个独立的数据库（一个 12026 病例，另一个 999 病例）上用视网膜图像以极高的正确率预测心脑血管风险因素。比如，我们的算法可从视网膜图像上分辨吸烟者与不吸烟者，正确率是 71%。此外，当医生可从视网膜图像上分辨病人有没有高血压时，我们的算法可以更深入地预测心脏收缩血压，在所有病患身上平均误差为 11 mmHg，包括那些有或没有高血压的患者。 左图：黑色部分的眼球显示了斑点（中间深色的部分、视神经盘（右边的亮点）、血管（从亮点向外扩展的深色红弧线）。右图：灰色视网膜图像，用绿色突出的（热图）是用来训练深度学习模型预测血压的像素。我们发现早期心脑血管风险因素预测使用了一个独特的模式，然后用视神经盘预测其他疾病。 在利用视网膜图像预测不同的风险因子（年龄、性别、烟史、血压等）之外，我们的算法可以相当准确地直接预测心脑血管疾病的风险。该算法借助整个图像来量化图像和心脏病或中风之间的关系。给出两张视网膜图像，一张来自罹患心脑血管疾病（比如心脏病）5 年以上的患者，另一张来自无心脑血管病史的患者，我们的算法区分两者的成功率是 70%。这个表现接近了其他需要抽血测量胆固醇的心脑血管风险计算器的正确率。 更重要的是，我们借助注意力机制查看算法如何做出预测而打开了「黑箱」。这些技术允许我们生成一张热图，以显示哪些像素对于预测特定的心脑血管疾病最为重要。比如，更加注重血管的算法将来预测血压，如上图所示。解释算法如何做出预测会使医生更加信任算法。此外，这项技术有助于针对未来的心脑血管风险和视网膜的调查提出假设。 在最广泛的层面上，我们对这项工作深感兴奋，因为也许它是一种新的科学发现方法。传统上，医学发现通常是借助复杂的假设与测试的形式而达成——通过观察做出假设，接着设计和做实验以验证假设。然而，面对医学图像之时，由于实际图像中存在的各种特征、图案、颜色、值和形状，观察和量化其间的关联非常困难。我们的方法借助深度学习来获取人体解剖学与病变之间变化的关系，类似于医生学习如何通过症状诊断新的疾病。这有助于科学家提出更有针对性的假设，并推动未来的广泛研究。 通过这些有希望的结果，很多科技工作得以继续开展。我们数据集之中的很多图像标注有吸烟状况、心脏收缩血压、年龄、性别及其他变量，但只包含数百个心脑血管实例。我们希望可以在更大更全面的数据集上发展和测试我们的算法。为了使其对病患有帮助，我们将试图理解干预措施比如生活方式的改变或药物对风险预测的影响，并且我们将产生新的假设和理论来测试。 参考 [1] Gulshan, V. et al. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf). JAMA 316, 2402–2410 (2016). [2] Ting, D. S. W. et al.Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes (https://jamanetwork.com/journals/jama/article-abstract/2665775?redirect=true). JAMA 318, 2211–2223 (2017). [3] Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks (https://www.nature.com/articles/nature21056). Nature (2017). doi:10.1038/nature21056 [4] Ehteshami Bejnordi, B. et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer (https://www.nature.com/articles/nature21056). JAMA 318, 2199–2210 (2017). "
256,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737774&idx=1&sn=6aa899e9de5e8363f91cff6dfb676a9c&chksm=871ac810b06d4106dd98c8c63e8ceac61caff9b3491bda5c4abd94b046c82706c03db63d056a&scene=27,数据科学家必须了解的六大聚类算法：带你发现数据之美,"TowardsDataScience 在机器学习中，无监督学习一直是我们追求的方向，而其中的聚类算法更是发现隐藏数据结构与知识的有效手段。目前如谷歌新闻等很多应用都将聚类算法作为主要的实现手段，它们能利用大量的未标注数据构建强大的主题聚类。本文从最基础的 K 均值聚类到基于密度的强大方法介绍了 6 类主流方法，它们各有擅长领域与情景，且基本思想并不一定限于聚类方法。 本文将从简单高效的 K 均值聚类开始，依次介绍均值漂移聚类、基于密度的聚类、利用高斯混合和最大期望方法聚类、层次聚类和适用于结构化数据的图团体检测。我们不仅会分析基本的实现概念，同时还会给出每种算法的优缺点以明确实际的应用场景。 聚类是一种包括数据点分组的机器学习技术。给定一组数据点，我们可以用聚类算法将每个数据点分到特定的组中。理论上，属于同一组的数据点应该有相似的属性和/或特征，而属于不同组的数据点应该有非常不同的属性和/或特征。聚类是一种无监督学习的方法，是一种在许多领域常用的统计数据分析技术。 K-Means 可能是最知名的聚类算法。它是很多入门级数据科学和机器学习课程的内容。在代码中很容易理解和实现！请看下面的图。 首先，我们选择一些类/组，并随机初始化它们各自的中心点。为了算出要使用的类的数量，最好快速查看一下数据，并尝试识别不同的组。中心点是与每个数据点向量长度相同的位置，在上图中是「X」。 通过计算数据点与每个组中心之间的距离来对每个点进行分类，然后将该点归类于组中心与其最接近的组中。 根据这些分类点，我们利用组中所有向量的均值来重新计算组中心。 重复这些步骤来进行一定数量的迭代，或者直到组中心在每次迭代后的变化不大。你也可以选择随机初始化组中心几次，然后选择看起来提供了最佳结果的运行。 K-Means 的优势在于速度快，因为我们真正在做的是计算点和组中心之间的距离：非常少的计算！因此它具有线性复杂度 O(n)。 另一方面，K-Means 有一些缺点。首先，你必须选择有多少组/类。这并不总是仔细的，并且理想情况下，我们希望聚类算法能够帮我们解决分多少类的问题，因为它的目的是从数据中获得一些见解。K-means 也从随机选择的聚类中心开始，所以它可能在不同的算法中产生不同的聚类结果。因此，结果可能不可重复并缺乏一致性。其他聚类方法更加一致。 K-Medians 是与 K-Means 有关的另一个聚类算法，除了不是用均值而是用组的中值向量来重新计算组中心。这种方法对异常值不敏感（因为使用中值），但对于较大的数据集要慢得多，因为在计算中值向量时，每次迭代都需要进行排序。 均值漂移聚类是基于滑动窗口的算法，它试图找到数据点的密集区域。这是一个基于质心的算法，这意味着它的目标是定位每个组/类的中心点，通过将中心点的候选点更新为滑动窗口内点的均值来完成。然后，在后处理阶段对这些候选窗口进行过滤以消除近似重复，形成最终的中心点集及其相应的组。请看下面的图例。 为了解释均值漂移，我们将考虑二维空间中的一组点，如上图所示。我们从一个以 C 点（随机选择）为中心，以半径 r 为核心的圆形滑动窗口开始。均值漂移是一种爬山算法，它包括在每一步中迭代地向更高密度区域移动，直到收敛。 在每次迭代中，滑动窗口通过将中心点移向窗口内点的均值（因此而得名）来移向更高密度区域。滑动窗口内的密度与其内部点的数量成正比。自然地，通过向窗口内点的均值移动，它会逐渐移向点密度更高的区域。 我们继续按照均值移动滑动窗口直到没有方向在核内可以容纳更多的点。请看上面的图；我们一直移动这个圆直到密度不再增加（即窗口中的点数）。 步骤 1 到 3 的过程是通过许多滑动窗口完成的，直到所有的点位于一个窗口内。当多个滑动窗口重叠时，保留包含最多点的窗口。然后根据数据点所在的滑动窗口进行聚类。 下面显示了所有滑动窗口从头到尾的整个过程。每个黑点代表滑动窗口的质心，每个灰点代表一个数据点。 与 K-means 聚类相比，这种方法不需要选择簇数量，因为均值漂移自动发现这一点。这是一个巨大的优势。聚类中心朝最大点密度聚集的事实也是非常令人满意的，因为理解和适应自然数据驱动的意义是非常直观的。它的缺点是窗口大小/半径「r」的选择可能是不重要的。 基于密度的聚类方法（DBSCAN） DBSCAN 是一种基于密度的聚类算法，它类似于均值漂移，但具有一些显著的优点。请看下面的另一个有趣的图形，让我们开始吧！ DBSCAN 聚类 DBSCAN 从一个没有被访问过的任意起始数据点开始。这个点的邻域是用距离 ε（ε 距离内的所有点都是邻域点）提取的。 如果在这个邻域内有足够数量的点（根据 minPoints），则聚类过程开始，并且当前数据点成为新簇的第一个点。否则，该点将会被标记为噪声（稍后这个噪声点可能仍会成为聚类的一部分）。在这两种情况下，该点都被标记为「已访问」。 对于新簇中的第一个点，其 ε 距离邻域内的点也成为该簇的一部分。这个使所有 ε 邻域内的点都属于同一个簇的过程将对所有刚刚添加到簇中的新点进行重复。 重复步骤 2 和 3，直到簇中所有的点都被确定，即簇的 ε 邻域内的所有点都被访问和标记过。 一旦我们完成了当前的簇，一个新的未访问点将被检索和处理，导致发现另一个簇或噪声。重复这个过程直到所有的点被标记为已访问。由于所有点都已经被访问，所以每个点都属于某个簇或噪声。 DBSCAN 与其他聚类算法相比有很多优点。首先，它根本不需要固定数量的簇。它也会将异常值识别为噪声，而不像均值漂移，即使数据点非常不同，也会简单地将它们分入簇中。另外，它能够很好地找到任意大小和任意形状的簇。 DBSCAN 的主要缺点是当簇的密度不同时，它的表现不如其他聚类算法。这是因为当密度变化时，用于识别邻域点的距离阈值 ε 和 minPoints 的设置将会随着簇而变化。这个缺点也会在非常高维度的数据中出现，因为距离阈值 ε 再次变得难以估计。 K-Means 的一个主要缺点是它对于聚类中心均值的简单使用。通过下面的图，我们可以明白为什么这不是最佳方法。在左侧，可以非常清楚的看到有两个具有不同半径的圆形簇，以相同的均值作为中心。K-Means 不能处理这种情况，因为这些簇的均值是非常接近的。K-Means 在簇不是圆形的情况下也失败了，同样是由于使用均值作为聚类中心。 高斯混合模型（GMMs）比 K-Means 给了我们更多的灵活性。对于 GMMs，我们假设数据点是高斯分布的；相对于使用均值来假设它们是圆形的，这是一个限制较少的假设。这样，我们有两个参数来描述簇的形状：均值和标准差！以二维为例，这意味着，这些簇可以采取任何类型的椭圆形（因为我们在 x 和 y 方向都有标准差）。因此，每个高斯分布被分配给单个簇。 为了找到每个簇的高斯参数（例如均值和标准差），我们将用一个叫做最大期望（EM）的优化算法。请看下面的图表，这是一个高斯适合于簇的例子。然后我们可以使用 GMMs 继续进行最大期望聚类的过程。 使用 GMMs 的 EM 聚类 我们首先选择簇的数量（如 K-Means 所做的），并随机初始化每个簇的高斯分布参数。也可以通过快速查看数据来尝试为初始参数提供一个好的猜测。但是请注意，正如上图所看到的，这不是 100% 必要的，因为高斯开始时我们很穷，但是很快就得到了优化。 给定每个簇的高斯分布，计算每个数据点属于一个特定簇的概率。一个点越靠近高斯的中心，它就越可能属于该簇。这应该是很直观的，因为对于高斯分布我们假设大部分数据更靠近簇的中心。 基于这些概率，我们计算一组新的高斯分布参数使得簇内的数据点的概率最大化。我们使用数据点位置的加权和来计算这些新参数，其中权重是数据点属于该特定簇的概率。为了用可视化的方式解释它，我们可以看一下上面的图，特别是黄色的簇，我们以它来作为例子。分布在第一次迭代时随即开始，但是我们可以看到大部分的黄点都在分布的右侧。当我们计算一个概率加权和时，即使中心附近有一些点，但它们大部分都在右侧。因此，分布的均值自然会接近这些点。我们也可以看到大部分的点分布在「从右上到左下」。因此改变标准差来创建更适合这些点的椭圆，以便最大化概率加权和。 重复步骤 2 和 3 直到收敛，其中分布在迭代中的变化不大。 使用 GMMs 有两个关键的优势。首先，GMMs 比 K-Means 在簇协方差方面更灵活；因为标准差参数，簇可以呈现任何椭圆形状，而不是被限制为圆形。K-Means 实际上是 GMM 的一个特殊情况，这种情况下每个簇的协方差在所有维度都接近 0。第二，因为 GMMs 使用概率，所以每个数据点可以有很多簇。因此如果一个数据点在两个重叠的簇的中间，我们可以简单地通过说它百分之 X 属于类 1，百分之 Y 属于类 2 来定义它的类。即 GMMs 支持混合资格。 层次聚类算法实际上分为两类：自上而下或自下而上。自下而上的算法首先将每个数据点视为一个单一的簇，然后连续地合并（或聚合）两个簇，直到所有的簇都合并成一个包含所有数据点的簇。因此，自下而上层次聚类被称为凝聚式层次聚类或 HAC。这个簇的层次用树（或树状图）表示。树的根是收集所有样本的唯一簇，叶是仅仅具有一个样本的簇。在进入算法步骤前，请看下面的图例。 凝聚式层次聚类 我们首先将每个数据点视为一个单一的簇，即如果我们的数据集中有 X 个数据点，那么我们就有 X 个簇。然后，我们选择一个测量两个簇之间距离的距离度量标准。作为例子，我们将用 average linkage，它将两个簇之间的距离定义为第一个簇中的数据点与第二个簇中的数据点之间的平均距离。 在每次迭代中，我们将两个簇合并成一个。这两个要合并的簇应具有最小的 average linkage。即根据我们选择的距离度量标准，这两个簇之间的距离最小，因此是最相似的，应该合并在一起。 重复步骤 2 直到我们到达树根，即我们只有一个包含所有数据点的簇。这样我们只需要选择何时停止合并簇，即何时停止构建树，来选择最终需要多少个簇！ 层次聚类不需要我们指定簇的数量，我们甚至可以选择哪个数量的簇看起来最好，因为我们正在构建一棵树。另外，该算法对于距离度量标准的选择并不敏感；他们都同样表现很好，而对于其他聚类算法，距离度量标准的选择是至关重要的。层次聚类方法的一个特别好的例子是当基础数据具有层次结构，并且你想要恢复层次时；其他聚类算法不能做到这一点。与 K-Means 和 GMM 的线性复杂度不同，层次聚类的这些优点是以较低的效率为代价的，因为它具有 O(n³) 的时间复杂度。 当我们的数据可以被表示为一个网络或图（graph）时，我们可以使用图团体检测方法完成聚类。在这个算法中，图团体（graph community）通常被定义为一种顶点（vertice）的子集，其中的顶点相对于网络的其它部分要连接得更加紧密。 也许最直观的案例就是社交网络。其中的顶点表示人，连接顶点的边表示他们是朋友或互粉的用户。但是，若要将一个系统建模成一个网络，我们就必须要找到一种有效连接各个不同组件的方式。将图论用于聚类的一些创新应用包括：对图像数据的特征提取、分析基因调控网络（gene regulatory networks）等。 下面是一个简单的图，展示了最近浏览过的 8 个网站，根据他们的维基百科页面中的链接进行了连接。 这些顶点的颜色表示了它们的团体关系，大小是根据它们的中心度（centrality）确定的。这些聚类在现实生活中也很有意义，其中黄色顶点通常是参考/搜索网站，蓝色顶点全部是在线发布网站（文章、微博或代码）。 假设我们已经将该网络聚类成了一些团体。我们就可以使用该模块性分数来评估聚类的质量。分数更高表示我们将该网络分割成了「准确的（accurate）」团体，而低分则表示我们的聚类更接近随机。如下图所示： 模块性可以使用以下公式进行计算： 其中 L 代表网络中边的数量，k_i 和 k_j 是指每个顶点的 degree，它可以通过将每一行和每一列的项加起来而得到。两者相乘再除以 2L 表示当该网络是随机分配的时候顶点 i 和 j 之间的预期边数。 整体而言，括号中的项表示了该网络的真实结构和随机组合时的预期结构之间的差。研究它的值可以发现，当 A_ij = 1 且 ( k_i k_j ) / 2L 很小时，其返回的值最高。这意味着，当在定点 i 和 j 之间存在一个「非预期」的边时，得到的值更高。 最后的 δc_i, c_j 就是大名鼎鼎的克罗内克 δ 函数（Kronecker-delta function）。下面是其 Python 解释： 通过以上公式可以计算图的模块性，且模块性越高，该网络聚类成不同团体的程度就越好。因此通过最优化方法寻找最大模块性就能发现聚类该网络的最佳方法。 组合学（combinatorics）告诉我们对于一个仅有 8 个顶点的网络，就存在 4140 种不同的聚类方式。16 个顶点的网络的聚类方式将超过 100 亿种。32 个顶点的网络的可能聚类方式更是将超过 128 septillion（10^21）种；如果你的网络有 80 个顶点，那么其可聚类的方式的数量就已经超过了可观测宇宙中的原子数量。 因此，我们必须求助于一种启发式的方法，该方法在评估可以产生最高模块性分数的聚类上效果良好，而且并不需要尝试每一种可能性。这是一种被称为 Fast-Greedy Modularity-Maximization（快速贪婪模块性最大化）的算法，这种算法在一定程度上类似于上面描述的 agglomerative hierarchical clustering algorithm（集聚层次聚类算法）。只是 Mod-Max 并不根据距离（distance）来融合团体，而是根据模块性的改变来对团体进行融合。 下面是其工作方式： 首先初始分配每个顶点到其自己的团体，然后计算整个网络的模块性 M。 第 1 步要求每个团体对（community pair）至少被一条单边链接，如果有两个团体融合到了一起，该算法就计算由此造成的模块性改变 ΔM。 第 2 步是取 ΔM 出现了最大增长的团体对，然后融合。然后为这个聚类计算新的模块性 M，并记录下来。 重复第 1 步和 第 2 步——每一次都融合团体对，这样最后得到 ΔM 的最大增益，然后记录新的聚类模式及其相应的模块性分数 M。 当所有的顶点都被分组成了一个巨型聚类时，就可以停止了。然后该算法会检查这个过程中的记录，然后找到其中返回了最高 M 值的聚类模式。这就是返回的团体结构。 团体检测（community detection）是现在图论中一个热门的研究领域，它的局限性主要体现在会忽略一些小的集群，且只适用于结构化的图模型。但这一类算法在典型的结构化数据中和现实网状数据都有非常好的性能。 以上就是数据科学家应该知道的 6 大聚类算法！我们将以展示各类算法的可视化效果结束本文！  原文链接：https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68 "
257,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737774&idx=2&sn=d960f2200e06c3b7e4fecbcd658ef860&chksm=871ac810b06d4106cfc370fdff6dff3966e18a2cedb817c65aa542523cbc5b90c192d1507e24&scene=27,业界 | 深度学习计算哪家强？最新云端&单机GPU横评,Medium 近日，Vincent Chu 在 Medium 上发文介绍自己对新一代 GPU 在各类深度学习任务上的测评结果，作者对比了 Paperspace Volta Tesla V100、Google Cloud P100、Amazon EC2 p3.2xlarge（Tesla V100）等云端计算平台，以及 Nvidia GeForce 1080Ti 单卡的成绩，具体测评结果详见全文。 随着机器学习（ML）研究人员和实践者们不断探索深度学习的范围，人们对于强大 GPU 计算能力的需求正在变得愈发强烈。面向目标检测、图像分割和语音转录等各种任务的新模型正在不断发展，并被应用于从自动驾驶到家庭助理等多个行业。 为了满足这样的 GPU 计算需求，亚马逊和谷歌等云服务提供商近期及时在服务项目中加入了 Volta 架构的 V100 GPU 和 Pascal 架构的 P100 GPU。另一家云 GPU 提供商 Paperspace 也在服务项目中加入了 Volta 系列 GPU。P100 和 V100 GPU 是当前市面上最好的 GPU，为机器学习应用实现最优的性能。这些 GPU 的性能优于之前的 Kepler 架构的 K80 GPU，同时它们还具备 16GB 的内存，保证更具表达性的 ML 模型和更大的训练小批量大小。 为了测试现代 GPU 在典型机器学习任务上的性能，我用英伟达最近发布的 GPU 训练了一个 Faster R-CNN/resnet101 目标检测模型。该模型在 TensorFlow 上实现，输入为 300x300px 的图像，训练小批量大小为 10、15、20 个图像。 测试所用 GPU/云 GPU： Paperspace Volta (https://www.paperspace.com/volta-gpu) (16GB—$2.30/hour) Google Cloud P100 (https://cloud.google.com/gpu/) (16GB—$1.73/hour) Amazon EC2 p3.2xlarge Volta (https://aws.amazon.com/ec2/instance-types/p3/) (16GB—$3.06/hour) Nvidia 1080Ti (https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti/) (11GB—Personal Machine) 注：该测试主要关注新型 GPU，因此没有测试 K80 和 Quadro GPU，它们的相关测评详见：https://medium.com/initialized-capital/benchmarking-tensorflow-performance-and-cost-across-different-gpu-options-69bd85fe5d58。 结果 从性能来看，Volta 毫无疑问是目前最强大的 GPU，性能显著优于 Nvidia 1080Ti（约 1.1-1.3 倍）和 P100（约 1.2-1.5 倍），尽管 1080Ti 才面世 9 个多月。这反映了英伟达发布强大 GPU 的一贯快节奏。 值得注意的是，在同样的训练任务上，Amazon Volta 实例性能不如 Paperspace Volta。我简单调查后，认为原因在于实例和 GPU 之间的缓慢输入／输出。只对比 Amazon 和 Paperspace 的 GPU 基准的结果展示了类似的性能。 从成本来看，Paperspace Volta 性价比高。同等性能条件下，Google P100 比 Paperspace Volta 贵大约 10%，亚马逊比 Paperspace Volta 贵 40% 以上。 Paperspace 和 Google 性价比较高 应该用哪种？ 重度用户当然应该购买自己的 GPU。从云提供商处租 GPU 时间长了比较昂贵，而购买自己的 GPU，你可以以最低的成本获取最好的硬件，当然前提是你一直使用它们，不让钱白花（特别是在近期 GPU 价格飞涨的情况下）。 Paperspace Volta 适合不打算购买 GPU 的用户。对于只需要单个 GPU 的用户来说，使用 Volta 将带来较大的性能提升。 Google P100 使用起来最为灵活，它允许用户在任意实例上使用 1、2、4 个 P100 GPU（或最多 8 个 K80 GPU），允许用户自定义 CPU 和 GPU 配置来满足计算需求。尽管由于架构所限，Tesla P100 的性能略显落后，但从成本角度考虑，其性价比很有优势。 Amazon Volta 的性能优于 Google P100，也可以连接 1、4 或 8 个 GPU。但是，用户无法自定义基础实例类型。此外，它们性价比比较低。如果你迫切需要用 8 个 GPU 或在 EC2 上搭建模型，那么目前仍推荐使用 Amazon Volta。 原文链接：https://medium.com/initialized-capital/benchmarking-tensorflow-performance-on-next-generation-gpus-e68c8dd3d0d4 
258,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737732&idx=3&sn=07d7b5897c476878dbe2ff8cecfee9f8&chksm=871ac83ab06d412cb736a35dbfed3c0abd26fcdf8e4e2126a145b1106e2e4eb429fbefdf3a79&scene=27,教程 | 如何在TensorFlow中高效使用数据集,处理并使用数据集是深度学习任务非常重要的组成部分。在本文中，作者 Francesco Zuppichini 将教你使用 TensorFlow 的内建管道向模型传递数据的方法，从此远离「feed-dict」。本文内容已更新至最新的 TensorFlow 1.5 版本。 相关代码地址：https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb 经常使用神经网络框架的人都会知道，feed-dict 是向 TensorFlow 传递信息最慢的方式，应该尽量避免使用。向模型提供数据的正确方式是使用输入管道，这样才能保证 GPU 在工作时永远无需等待新的数据。 幸运的是，TensorFlow 拥有一个名为 Dataset 的内建 API，它可以让我们的工作更加简单。在本教程中，我们将介绍搭建内建管道，让数据高效传递给模型的方法。 本文将解释 Dataset 的基本原理，包含大多数常用案例。 使用 Dataset 需要遵循三个步骤： 载入数据：为数据创建一个数据集实例。 创建一个迭代器：通过使用创建的数据集构建一个迭代器来对数据集进行迭代。 使用数据：通过使用创建的迭代器，我们可以找到可传输给模型的数据集元素。 我们首先需要一些可以放入数据集的数据。 从 Numpy 导入 这是一种常见情况：我们拥有一个 numpy 数组，想把它传递给 TensorFlow。 我们当然也可以传递多个 numpy 数组，一个典型的例子是：当我们已有被分配多个特征和标签的数据时…… 从张量导入 当然，我们也可以从张量中初始化自己的数据集。 从占位符导入 当我们希望动态地修改 Dataset 中的数据时，这就会很有用，稍后会有详述。 从生成器导入 我们还可以从生成器中初始化 Dataset，这种方式在拥有不同长度的元素的数组时有意义（例如一个序列）。 def generator () 在这种情况下，你还需要告诉 Dataset 数据的类型和形状以创建正确的张量。 我们已经学会创建数据集了，但如何从中获取数据呢？我们必须使用迭代器（Iterator），它会帮助我们遍历数据集中的内容并找到真值。有四种类型的迭代器。 One Shot 迭代器 这是最简单的迭代器，使用第一个示例： 随后你需要调用 get_next() 来获取包含这些数据的张量 我们可以运行 el 来查看它们的值。 可初始化的迭代器 如果我们想要创建一个动态的数据集，在其中可以实时更改数据源，我们可以用占位符创建一个数据集。随后我们可以使用通常的 feed-dict 机制来初始化占位符。这一过程可用「可初始化迭代器（initializable iterator）」来完成。使用上一节中的第三个例子： 这次我们调用 make_initializable_iterator。然后，我们在 sess 中运行 initializer 操作，以传递数据，这种情况下数据是随机的 numpy 数组。 假设现在我们有了训练数据集和测试数据集，那么常见的代码如下： 然后，我们训练该模型，并在测试数据集上对其进行测试，测试可以通过训练后再次初始化迭代器来完成。 可重新初始化的迭代器 这个概念和之前的类似，即在数据之间动态地转换。但并不是将新数据馈送到相同的数据集，而是在数据集之间转换。如前，我们需要一个训练集和一个测试集。 我们可以创建两个数据集： 接下来是要展示的技巧，即创建一个通用的迭代器： 以及两个初始化运算： 和之前一样，我们得到了下一个元素： 现在，我们可以直接使用会话运行这两个初始化运算。总结起来我们得到： 可馈送的迭代器 老实说，我并不认为这个有什么用。基本上，它是用迭代器之间的转换取代了数据集之间的转换，从而得到如一个来自 make_one_shot_iterator() 的迭代器，以及一个来自 make_initializable_iterator() 的迭代器。 在前述例子中，我们利用会话输出 Dataset 中下一个元素的值。 为了将数据传递给模型，我们只需要传递从 get_next() 生成的张量。 在下面的代码中，我们有一个包含了两个 numpy 数组的 Dataset，这里用了和第一节一样的例子。 注意，我们需要将.random.sample 封装到另一个 numpy 数组，以增加一个维度，从而将数据进行分批。 然后，和往常一样，我们创建一个迭代器： 创建一个模型，即一个简单的神经网络： 我们直接使用来自 iter.get_next() 的张量作为第一层的输入和损失函数的标签。总结起来我们得到： 输出： 数据分批 通常数据分批是一件令人痛苦的事情，但通过 Dataset API，我们可以利用 batch(BATCH_SIZE) 方法自动地将数据集按设定的批量大小进行分批。默认批量大小为 1。在下面的示例代码中，我们使用的批量大小为 4。 输出： repeat 使用.repeat()，我们可以指定数据集被迭代的次数。如果不传输任何参数，循环将永久进行。通常来说，永久运行循环和在标准循环中直接控制 epoch 的数量可以得到不错的结果。 shuffle 我们可以利用 shuffle() 进行数据集 shuffle，默认是在每一个 epoch 中将数据集 shuffle 一次。 记住：数据集 shuffle 是避免过拟合的重要方法。 我们还可以设置参数 buffer_size，下一个元素将从该固定大小的缓存中均匀地选取。例如： 第一次运行的输出： 第二次运行的输出： 这样，数据集 shuffle 就完成了。你还可以设置 seed 参数。 你可以使用 map 方法对数据集中的所有成员应用定制化函数。下列示例中，我们把每个元素乘 2： 输出： TensorFlow 数据集教程：https://www.tensorflow.org/programmers_guide/datasets 数据集文档：https://www.tensorflow.org/api_docs/python/tf/data/Dataset 该数据集 API 使我们快速、稳健地创建优化输入流程来训练、评估和测试我们的模型。本文中，我们了解了很多可以常见操作。 
259,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737732&idx=2&sn=908afe2ef8f5689a7490ce3275e20bde&chksm=871ac83ab06d412c128d639f7f857337939c855984c25d4f28acc17c29066dca4a21eec44825&scene=27,资源 | 李沐《动手学深度学习》课程视频汇总,9 月份，亚马逊AWS首席科学家李沐博士开始了《动手学深度学习》 ，想要结合实践来介绍深度学习。历时 5 个月，这门课程的第一季已经结束，共讲了 19 课。李沐博士对此系列视频进行了整理，有需求的同学可通过以下视频学习。 视频链接：https://discuss.gluon.ai/t/topic/753 课程简介 该系列课程是 MXNet 团队联合将门创投，开设的一门零基础、着重动手实践的深度学习在线直播课程。 课程内容将基于李沐等人的开源教程《动手学深度学习》(英文版为 Deep Learning - The Straight Dope)，并使用 Apache MXNet 的最新前端 Gluon 作为开发工具，在动手实践的过程中学会使用简单易读的代码写出产品级的应用。 《动手学深度学习》中文文档地址：http://zh.gluon.ai/ 课程大纲 第一课：从上手到多类分类 第二课：过拟合、多层感知机、GPU和卷积神经网络 第三课：深度卷积网络，如何使用Gluon，以及核武器购买指南 第四课：BatchNorm，更深的卷积神经网络，图片增强和新的Kaggle练习 第五课：Gluon高级和优化算法基础 第六课：优化算法高级和计算机视觉 第七课：物体检测 第八课：物体检测·续 第九课：物体检测·再续 第十课：语义分割 第十一课：样式迁移 第十二课：循环神经网络 第十三课：正向传播、反向传播和通过时间反向传播 第十四课：实现、训练和应用循环神经网络 第十五课：门控循环单元（GRU）、长短期记忆（LSTM）、多层循环神经网络以及Gluon实现 第十六课：词向量（word2vec） 第十七课：GloVe、fastText和使用预训练的词向量 第十八课：seq2seq（编码器和解码器）和注意力机制 [第一季完结] 第十九课：应用seq2seq和注意力机制：机器翻译  
260,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737732&idx=4&sn=3cff9045d0832a6fe9c2351c77e2b04f&chksm=871ac83ab06d412c2bfc08efe2a718549981287637fc1513bc6e169f2d4149bf97a2e76de2ee&scene=27,学界 | 谷歌大脑提出通过多文档摘要方法生成维基百科，可处理较长序列,"arXiv 近日，谷歌大脑发布论文，提出一种通过提取多文档摘要来生成英文维基百科文章的方法，该方法可以处理长序列。 序列到序列框架已被证明在自然语言序列转导任务（如机器翻译）中取得了成功。最近，神经技术被应用于提取新闻文章中的单文档、抽象（释义）文本摘要（Rush et al. (2015), Nallapati et al. (2016)）。之前的研究以端到端的方式训练监督模型的输入——从一篇文章的第一句到整个文本——来预测参考摘要。进行端到端的处理需要大量相关的文章-摘要对，因此语言理解是生成流畅摘要的首要条件。 而谷歌大脑这篇论文考虑的是多文档摘要的任务，输入是提炼过摘要的相关文档的集合。之前的研究主要是提取摘要（从输入中选择句子或词组来形成摘要），而不是生成新文本。抽象神经模型的应用有限，一个可能的原因是缺少大型标注数据集。 在这篇论文中，研究者把英语维基百科看成是一个多文档摘要的监督式机器学习任务，输入是维基百科的主题（文章标题）和非维基百科参考文献的集合，目标是维基百科文章的文本。研究者首先描述了基于参考文本抽象生成维基百科文章的第一部分或主要部分。除了在任务中运行强大的基线模型以外，研究者还将 Transformer 结构（Vaswani et al., 2017）修改为只包含一个解码器的结构，与 RNN 和传统的编码器-解码器模型相比，这种结构在长输入序列中表现更好。最后，研究者展示了可生成整个维基百科文章的优化模型。 表 1：摘要数据集输入/输出的数量级和一元回调（unigram recall）。 表 2：WikiSum 数据集不同属性的百分比，大小以单词数量为单位。 将英文维基百科作为一个多文档摘要数据集 作为百科全书的维基百科（Wikipedia）可以被看作是给定不同标题的各种主题摘要的集合，如「加拿大（Canada）」和「机器学习（Machine Learning）」。用于提取摘要的原始材料可以是网上或书中各种有良好声誉的文件，然而，为了使问题更加容易处理，研究者考虑所有文档的以下子集 D： 1. 引用资料：一篇符合体例指南的维基百科文章在「References」（参考文献）部分应该有引用资料。对于每篇文章，对于一篇文章 a_i，研究者从可抓取的引用文档中提取无标记的所有文本 C_i（C_i ⊂ D）作为模型的输入。 2. 网页搜索结果：为了扩展参考文档的集合，研究者使用文章标题作为搜索内容，在谷歌搜索引擎中搜索结果。每次查询收集 10 个结果页面。在此集合中，去掉维基百科文章自身（往往在最上面），同时也去掉「克隆」的结果（与维基百科文章高度重叠的结果）（A.2.1 中有详细介绍）。研究者将文章 a_i 精炼后的搜索结果表示为 S_i（S_i ⊂ D）。类似于 C_i，研究者仅提取文本作为输入。 表 2 描述了 WikiSum 数据集的整体属性。许多文章的引用资料很少，因此研究者使用网页搜索结果作为源文档的补充。不过，引用资料往往质量更高。统计数据集中的总单词数时，我们会发现它比之前的摘要数据集大一个数量集。 为了在语料库比较实验（corpus-comparison experiment）中使训练/开发/测试数据保持一致，研究者将文章的范围限制为至少具备一个可抓取引用资料的维基百科文章。研究者将文章按 80/10/10 的比例大致分成训练/开发/测试子集，分别得到了 1865750、233252 和 232998 个样本。 方法和模型 由于输入参考文档（C_i，S_i）中的文本数量会非常大（参见表 2），考虑到当前硬件的内存限制，训练端到端的抽象模型并不可行。因此，研究者首先通过抽取摘要粗略地选择输入的子集，然后基于此训练一个生成维基百科文本的抽象模型。这两步受到人们从多个长文档中提取摘要的启发：首先突出显著信息，然后基于此生成摘要。 图 1：T-DMCA 模型中使用的自注意力层架构。每个注意力层都将一个符号序列作为输入，并产生一个相似长度的输出序列。左图：transformer-解码器中的原始自注意力。中图：内存压缩的注意力，减少了密钥/数值的数量。右图：将序列分割为单个较小子序列的局部注意力。之后子序列合并在一起得到最终输出序列。 图 4：相同样本在不同模型中产生的预测结果。模型输入样本可在附录 A.4 中找到。 图 4 展示了三个不同模型（使用 tf-idf 提取和组合语料库）的预测结果和维基百科原文本（输入样本）。随着复杂度的降低，我们可以看到模型输出在流畅性、事实准确性和叙述复杂性方面有所改善。特别是，T-DMCA 模型提供了维基百科版本的一个可替代性选择，并且更为简洁，同时提到了关键事实，例如律师事务所的位置、成立方式和时间以及企业的兴衰。 在模型输出的手动检查中，研究者注意到一个意想不到的副作用：模型尝试学习将英文名称翻译为多种语言，例如将 Rohit Viswanath 翻译成印地语（见图 5）。尽管研究者没有系统地评估这些翻译，但他们发现译文往往是正确的，而且在维基百科文章里找不到。研究者还证实了译文通常不是从内容源中复制的，例如目标语言不正确的示例（例如把英文名称翻译为乌克兰语）。 表 7：与 Sauper 和 Barzilay（2009）的论文中的结果比较。请注意，谷歌大脑这篇论文中的的结果是维基百科主要部分的报告，而 Sauper 和 Barzilay 的是文章的报告。 相关代码链接：https://gist.github.com/peterjliu/f0dc9152a630520dc604c783db963aa7 论文：Generating Wikipedia by Summarizing Long Sequences 论文链接：https://arxiv.org/abs/1801.10198 我们展示了可以通过对源文档提取多文档摘要来生成英文维基百科文章。我们通过提取摘要来粗略地识别显著信息，通过神经抽象模型生成文章。对于抽象模型，我们引入了只含一个解码器的结构，它可以处理很长的序列，比序列转导中传统的编码器-解码器架构处理的序列长得多。我们展示了这个模型可以生成流畅、连贯的多句段落，甚至生成整个维基百科文章。在给出参考文档时，我们证明了该模型可以提取相关的事实信息，以复杂度、ROUGE 分数和人类评估结果的形式呈现。 )  "
261,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737699&idx=2&sn=eea6c09824fd5ef5f430dfafad5177bc&chksm=871ac85db06d414b767ddd0e8605c146040677104782884e4439119076ead75dc0cb2e92f1ee&scene=27,独家 | 村子里到处在宣传智能养猪，但阿里云的AI养猪梦真的靠谱吗？,特驱集团信息中心总监独家讲述用阿里云人工智能养猪到底是怎么一回事。 「养了十几年猪，现在我们开始学习智能养猪。」 「今年打工不出门，智能养猪到家门。」  …… 近日，几幅超级接地气的农村标语，将人工智能与养猪这两个看似风马牛不相及的事情结合在一起。 实际上，这并不是恶搞。2 月 6 日，阿里云与德康集团、四川特驱集团宣布达成合作，将对 ET 大脑进行针对性训练与研发，将人工智能用于养猪。 基于机器视觉技术的视频分析，ET 大脑将为每头猪建立档案，包括猪的品种、日龄、体重、进食情况、运动强度、频次、轨迹等。这些数据可以用于对猪的行为特征、进食特征、料肉比等进行分析，并将这些分析贯穿于整个生猪的养殖过程，以此全面实现人工智能养猪。 该项目总投入达数亿元，首期落地包括各类猪只数量识别、猪群行为特征分析、疾病识别和预警、无人过磅等十项功能。 「传统养殖以人工为主，少量生产设备半自动化，但养殖过程生产管理、疾病防治等没有做到智能化。」特驱集团信息中心总监张海峰在接受机器之能的采访时说。 特驱集团和德康集团隶属于希望集团下属的华西希望集团，2020 年，特驱集团年饲料销量预计将突破 1000 万吨，德康集团生猪养殖将突破 1000 万头，年家禽养殖将突破 2.5 亿只，是生猪养殖领域的龙头企业。在这个项目中，特驱集团、德康集团希望借由人工智能打造农业板块的工业 4.0，从传统养殖进化到智慧养殖。 「我们希望驱动养殖企业成本下降，通过人工智能和数据，驱动农业的变革。」阿里云大数据专家以及项目负责人张盛说，「另一方面，从社会责任的角度讲，我们希望所有人能吃上我们小时候吃的那种安全肉和放心肉。这是我们特别想做的事情。」 建立生猪可追溯系统，使传统养殖进化到智慧养殖 过去一年，阿里云 ET 大脑已在航空、交通、工业、环境、医疗等多个领域落地，ET 城市大脑更是成为了国家四大人工智能开放平台之一。 「企业要想发展，就必须承担社会责任。」张盛认为，将 ET 大脑的应用场景拓宽至农业，尤其是食品行业，让民众吃上放心肉是他们寄托社会责任感的一个途径。 「无论是城市治理，还是工业发展以及农业领域。我们都希望通过数据为环境治理、食品安全等国计民生的问题，做出一些贡献。」基于这样的想法，阿里云开始寻找合适的合作伙伴。 据张盛介绍，特驱集团和德康集团付出了巨大成本和大量精力，在部署物联网和 ERP 系统。他们对 IT 和人工智能重视的程度，表明对前瞻技术的敏感以及对新技术的开放心态。 「和他们的合作，也希望通过这样的龙头企业，为整个行业赋能。」张盛介绍。 另一方面，对特驱集团、德康集团来说，在进一步发展的路上，也急需新的技术。作为生猪养殖企业，仔猪从出生开始到最终猪肉到达消费者手中，整个流程中，猪是如何喂养的，患过何种疾病，用过哪些药，建立这样的可追溯系统，是保证食品安全的有效方式。 德康集团正在通过物联网布局可追溯系统。但要建立可追溯系统，首先需要建立猪的身份识别，使每头猪都有自己唯一的身份，以此对每头猪进行过程管理。仔猪在出生之后，给它们打上二维码或 RFID 耳标，是常用的方式。 德康集团采用了 RFID 耳标，然而，实施起来，并非易事。 普通的 RFID 耳标扫描距离短，感应效率低，不能快速完成区域性数据读取，同时耗用成本较高，比如每头猪要佩戴两个耳标，以便掉落后进行补充，因此，每出栏一百万头猪，需要耗用一千万的 RFID 成本，导致无形中增加养殖成本，减少养户利润。如果采用有源 RFID，成本还将大幅增加，同时不便于挂在猪的耳朵上，推广应用的难度较大，失去了普遍性应用的价值。 从出生到出栏的唯一身份识别是个难题，但要建立生猪养殖的可追溯系统，还需要在猪的生长过程中，对猪的喂养过程、疾病治疗等进行全程跟踪记录，而这些基础数据的收集以往都是通过人工填写纸制表单，或者让工作人员手持相应设备，通过手工输入到 App 中，进行数据录入。而所有的数据整理、数据分析，都只有在生产过程操作完成之后，才能进行。 但在特驱集团、德康集团，除了自有养猪场之外，还有大量农户经营的家庭养殖场，「对这些老百姓来说，他们养猪还停留在传统模式，既不熟悉新的 IT 技术，也不重视业务数据。因此，采集的数据既不及时也不准确而且效率低下。」张海峰说。 如何建立猪的唯一身份，以及如何快速、准确收集生猪养殖过程中的日常数据，建立完整的生物资产管理体系和业务可追溯体系，就成了他们急需解决的问题。 起初，集团并没有考虑人工智能，因为觉得人工智能在农业离落地还很远。但在一个偶然的机会，张海峰看到阿里云 ET 大脑应用于工业领域和交通治理上的业务场景。 张海峰认为，可以找到 ET 大脑在工业领域、交通治理与农业养殖的共同点，再针对农业制定有针对性的业务解决方案。 在城市交通领域，应用了大量摄像头，相比 RFID 以及配套的设备，摄像头更易安装，而实际上，养猪场本身也需安装监控系统，在这个基础上进行视频采集和分析，成本不仅会下降，同时大量日常业务也无需再让工作人员进行录入操作，解决了数据收集必须依赖一线工作人员的难题。通过视频的实时传输，数据收集的及时性和准确性也会得到保证。 「采用摄像头和视频分析，是在现有环境的基础上，自动化地全程搜集数据，这是 RFID 和视频分析两个方案中最本质的区别。」张盛说。而在 ET 大脑中，视频和摄像头占比很大，因此也是一个成熟的解决方案。 配种、生产、称重、疾病防控等，人工智能贯穿于养猪领域每一个环节 对特驱集团、德康集团而言，要建立全程可追溯体系，打造安全食品生态链，首先要做的就是，使每头猪拥有唯一 ID，以便进行身份识别。 既然之前采用的 RFID 方式并不现实，那么，阿里云 ET 大脑如何对猪进行身份识别？ 对人来说，人脸识别是有效的方式，识别猪是否可以应用猪脸识别？实际上，阿里云最初考虑过此种方案。但最终发现依靠单纯的猪脸识别并不实用。因为从特征上来说，猪脸的特征点在技术上更难掌控、有效识别率不高。而从工程实施来说，也并不适合使用猪脸识别。 因此，张盛介绍，阿里云和特驱集团、德康 采取的做法是，沿用传统的身份识别方法，在猪身上进行标记，建立唯一识别码，再结合视频分析等技术，共同对猪进行识别。而目前，阿里云和特驱集团正在探索摄像头通过何种方式来拍摄猪身上的唯一识别码。 实际上，基于摄像头的视频分析，不仅能够实时收集可追溯系统中需要的过程数据，还能对养猪场进行精细化管理，降低成本，提升效率。 比如，对养猪企业而言，每头母猪每年所能提供的断奶仔猪头数，也就是说一头母猪，每年能生产多少仔猪以及存活下来的仔猪能有多少，是关键的指标，因为这直接关系着养猪场的经济利益。 那么，从猪的配种开始，就可以通过 ET 大脑对母猪的行为进行分析，获知母猪的发情状况及交配时机，从而恰当安排配种，因为正确的交配时机和交配方法，能够提升母猪的受胎率以及产仔数。 然而，母猪并不是细心的妈妈，尤其是每胎都会生产 10 多只小猪，因此在生产过程中，一些仔猪很容易被母猪压在身下而夭折。对于大规模的养殖场来说，每天有大量的母猪在生产，如果全部依靠人工来照料，显然很难有效保障。 通过语音识别则能自动判断仔猪是否被压住，以便及时解救，以此提高仔猪的存活率。据介绍，经由人工智能的智能分析，在前期的理论验证阶段，母猪的年生产数量将提升 3 头，死亡淘汰率降低 3% 左右。 除了配种和生产，在生猪养殖过程中，还需要随时掌握猪的体重情况，进而调整喂养计划。通过视频分析，再结合该猪的品种、喂养标准和增重标准，综合判断之后，以此对猪的体重进行测算，代替以往传统的称重方法。 「用视频测算猪的体重，原理还是通过视频来识别猪的体型和猪的体重之间的关系。但在实际的工程方案中，会结合一头猪，几头猪，一群猪的特征，对群体的特征进行分析，再综合生猪标准饲喂体系，得到比较准确的数据。」张盛说。 配种、分娩、哺乳、育肥等形态变化，饲料耗用、体重检测，这些都是养猪场必不可少的饲养管理工作。但对养猪场来说，最关键的还是疾病防治和疫情监测。阿里云则将通过对猪的行为分析，结合声学特征和红外测温技术，判断猪的健康状态，有针对性地进行疫情预警，并进行专项防疫处理。 但张盛也坦言，疾病防治并不容易，「我们这个项目刚成立，还要做很多期，尤其是在疾控领域。到底通过什么方式去判断这头猪有没有潜在的疾病，或者在什么时候会发病，或者在发病之前，我们如何处理。在这个领域，我们还需要花很多时间和精力，进行技术上的突破。」 实际上，在这背后，除了特驱集团有过程数据收集的需求之外，对于阿里云 ET 大脑来说，同样需要大量的数据进行模型训练。比如，用语音识别技术判断仔猪的生存状况以及生猪的健康状态，就需要大量的语音数据。 对于基础业务数据如何采集？张盛认为，可以通过智能语音的方式，让操作人员佩戴方便使用的设备，在猪场操作的同时，实时收集猪的声音，将数据传到系统，结合自然语言处理技术以及操作人员在数据收集时的标准话术，将这些数据转化为结构化的数据，从而进行模型的训练。 而为了完成这个项目，阿里云将投入 PD、算法工程师、产品和开发团队以及达摩院视频分析的团队。特驱集团则会组建由养殖专家、一线业务骨干以及 IT 专家组成的专家团队。 张海峰表示，在投入人力之外，集团还要投入专门用于人工智能养殖的示范基地，通过这个基地探索业务方案是否可行以及建立可行性推广标准，尽量做到效率最高，成本最低。 「这跟工业不同，工业可能对成本的考虑相对较少，但养殖业的附加值没有工业高，所以会在养殖基地完成所有成本和效率的验证，然后才会在集团内部进行推广。」张海峰说。 「从这个案例来说，我们的 ET 大脑实际上就是把他们的业务承揽下来，然后通过数据，去分析和解释，最后服务于对方的业务。」张盛表示，「从成本、操作和效益来说，我们现在核心的解决方案是通过视频的自动分析，无论是对农户的依赖，还是对设备的依赖，都会降低。」 张盛认为，这样的方案，推广性和可复制性也很强，而借由与特驱集团、德康集团的合作，阿里云 ET 大脑也可以将此案例复制到其他的养殖企业。 「我们做这个人工智能养殖示范基地，一，探索能否节约人力成本，以及节约到何种程度，跟工业 4.0 相比，能否做到无人养殖场。二，通过人工智能改善我们整个生产业务模式。三，无论是对猪的图像识别、行为分析还是疾控分析，我们都是为了最终的目标，做到养殖过程全程可追溯，为老百姓提供更放心的安全食品。」张海峰总结道。 
262,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737732&idx=1&sn=97cd0bdfad21b7d358b94c21c167bf24&chksm=871ac83ab06d412c54129d3d9f37fdb0c4c860b662948cd1e6ba7f1cd275349ead8816c146dc&scene=27,从语义上理解卷积核行为，UCLA朱松纯等人使用决策树量化解释CNN,"arXiv 近日，加州大学洛杉矶分校的朱松纯教授等人发布了一篇使用决策树对 CNN 的表征和预测进行解释的论文。该论文借助决策树在语义层面上解释 CNN 做出的每一个特定预测，即哪个卷积核（或物体部位）被用于预测最终的类别，以及其在预测中贡献了多少。此前， 解释如何用决策树解释深度网络，由此可见可解释性的决策树在理解深度网络的进程中将发挥重要的作用。 卷积神经网络在许多视觉任务上取得了惊人的表现，例如物体分类和检测。然而，除了辨别能力，模型可解释性仍旧是神经网络的一大挑战。许多研究提出对 CNN 中隐藏的特征表征进行可视化、分析或者语义化，从而获得对网络表征的理解。 在此论文中，朱松纯等研究者提出了一种新任务，也就是使用决策树在语义层次上来量化解释 CNN 预测的逻辑。注意，这里的决策树泛指对不同图像的 CNN 预测生成先验解释的「一般性」树模型。 CNN 记忆多少类模式？ 对每个输入图像，哪种物体-部位模式被用于预测？ 如何量化测量每个物体-部位模式对预测的贡献度？ 解决上面三个问题需要：1）确定卷积层特征图中每个神经激活值的语义含义；2）量化测量不同神经激活值的贡献，这对当前最优算法是重大挑战。 在此论文中，研究者通过略微修正 CNN 而解开表征，并学习一种决策树来解释 CNN 的预测。给定特定领域的物体图像以及随机图像作为正例和反例样本，同时作为学习 CNN 和决策树的输入。在此过程中，我们并未标记任何部分或者结构作为附加的监督。首先，我们向 CNN 添加了论文《Interpretable convolutional neural networks》中提出的卷积核损失函数。其次，我们创造了一种决策树来量化解释对输入图像的决策模式，也就是哪个物体部位（卷积核）被用在预测中，贡献度有多大。 如下图 1 所示，决策树中的每个节点表示特定的决策模式，且该决策树以由粗到细的方式组织所有的决策模式。接近顶部根节点表征许多样本共享的通用决策模式。接近叶节点对应少数样本的细粒模式。特别是每个叶节点对应于 CNN 的输出关于图像中不同物体部位的梯度。 图 1. 在语义层次上解释 CNN 预测的决策树。我们学习到一种分类物体的 CNN，带有顶部卷积层的解开表征，其中每个过滤层表征一个特定的物体部位。以一种由粗到精的方式，决策树解码 CNN 全连接层中隐藏的各种决策模式。给定一张输入图像，我们推断出一种解析树（红线）来量化分析 CNN 预测的基本原理，例如，哪些物体部位（或者过滤层）被用于预测，且其对预测的贡献度是多少。我们对总结低层节点并提供 CNN 预测紧密逻辑的高层决策模式更感兴趣。 因此如上所述，在这篇论文中，研究者们关注了一项新任务，即解开 CNN 的表征内容，并学习一个决策树以量化地解释每一个 CNN 预测的逻辑。他们提出了一种简单但高效的方法以在不使用标注的信息下学习一个决策树，因此可以在不使用物体部位作为额外的监督而实现学习 CNN 的过程。从理论上来说，研究者的方法是一种修正 CNN 的广泛技术，它能学到紧密耦合的 CNN 和决策树。实践上也证明了这种基于 VGG 网络方法的高效性。 论文：Interpreting CNNs via Decision Trees 论文地址：https://arxiv.org/abs/1802.00121 本文提出的方法可通过学习决策树从而量化地解释预训练卷积神经网络（CNNs）每一预测的内在逻辑。我们的方法从两个方面提升了神经网络的可解释性。1) 在 CNN 中，高层卷积层之中的每一个过滤层必须表征一个特定的物体部位，而不是描述无明确含义的混合模式。2) 人们可以借助决策树在语义层面上解释 CNN 做出的每一个特定预测，即哪个过滤层（或物体部位）被用于预测，以及其在预测中贡献了多少。为了对 CNN 做出量化解释，我们的方法学习 CNN 高层卷积层中物体部位的明确表征，并挖掘存储在全连接层之中的潜在决策模式。决策树按照由粗到细的方式组织这些潜在的决策模式。最后，我们的实验表明了这一方法的有效性。 3.1. 准备工作：学习带有解开（disentangled）表征的 CNN 通过在顶部卷积层为每一个过滤层添加损失，[30] 获得了已学习的 CNN 解开表征，从而把过滤层的表征推向了一个特定的物体部位。注意人们无需为监督标注物体部位。CNN 在端到端学习期间自动为每一个过滤层分配一个特定部位。 如图 2 所示，人们为正样本设计 L^2 正模板 T^+ = {T_1,1, T_1,2, . . . , T_L,L} 以表征当滤波器 f 的物体部位出现在 x^(d) 上的 L^2 个不同位置候选处时的理想激活形状。负模板 T^−同样用于描述负样本上的特征图。过滤层 f 的损失是作为所有特征图和所有模板之间的负互信息给出的。 其中 X 表征所有训练样本上过滤层 f 的特征图的集合，T 表征所有的 L^2 + 1 模板。先验概率 p(T) 被定义为一个常量。 图 2：过滤层 f 的特征图的正模板。每个模板表征当过滤层 f 的物体部位出现在特定的图位置时的理想激活形状。 3.2. 学习一个决策树 过滤层中的部位概念：等式 (1) 中的损失确保每个过滤层表征一个特定的物体部位。让我们聚焦在有特定过滤层 f 所产生的特征图 xi , x ^(d)_ i ∈ R^L×L 的第 d 个通道，该通道表征一个解开的物体部位。我们可以把等式 (1) 中的过滤层损失重写为 如图 3 所示，这一损失确保第 d 个过滤层 f 表征目标物体的一个部位。 图 3：普通 CNN 特征图与本研究中使用的解开特征图之间的对比。我们可视化对应于每一特征图的图像区域。 CNN 预测的内在逻辑：如 [21] 所述，全连接层中为 I_i 编码的决策模式可以大致为分段线性表征所描述： 其中⊗表示卷积。关于特征图的梯度 partial y_i 除以 partial x_i 可通过梯度反向传播计算。 树：如图 4 所示，我们提取编码在 CNN 全连接层之中的决策模式，并构建一个决策树以组织决策模式的层级。从顶部节点到终端节点，决策树通过由粗到细的方式编码决策模式。 图 4：决策树的学习过程。P_3 中的红线表示解析树以解释给定图像 I 的原理。 学习：决策树学习的基本思想是从不同样本的特定决策模式中总结常见的决策模式，从而表征 CNN 预测的基本原理。 开始时，通过设定 和α = 1，我们把每一个正样本 I_i 的梯度 g_i 初始化为一个终端节点。因此如图 4 所示，我们构建了一个初始树 Q，其中顶部节点把所有正样本的梯度作为后代。接着，在每一步中，我们在第二个层中选择并合并两个节点 v, v0 ∈ V（即，顶部节点的后代）以获得一个新节点 u，其中 V 表征第二层中的节点集合。v 和 v^'成为了 u 的后代，并且 u 替代 v 和 v^'成为了顶部节点的新后代。通过这种方式，在 T 合并操作之后我们逐渐把初始树 P_0 = Q 修改为最后的决策树： 整体如下： 3.3 解释 CNN 给定一个测试图像 I_i，我们使用 CNN 以预测 y_i。我们使用决策树对预测的基本原理进行莲花的计算。在推断过程中，我们能自上而下构建一棵解析树，图 4 中的红线展示了这样的解析树。如果读者希望了解更多解释 CNN 预测类别的过程，请查看原论文的该章节。 表 1：在决策树第 2、5、10、50 和 100 层的平均节点数。 表 2：在决策树第 2、5、10、50、100 和底层上的平均分类准确度。 表 3：在决策树第 2、5、10、50、100 和底层节点上的平均预测误差。 表 4：在决策树第 2、5、10、50、100 和底层节点上对卷积核拟合度的平均贡献。 上表 1 展示了决策树的结构，其余展示了各层级的特性。下图 5 可视化了决策树中的决策模式，而图 6 展示了物体部位的分布对 CNN 预测的贡献，该贡献通过使用决策树第二层节点进行估计。 一般来说，当我们使用更加细粒度的决策模式来解释预测逻辑时，该解释将更好地拟合 CNN 中的实际逻辑，并且使用决策模式预测 y_i 的误差也会降低。然而，更加细粒度的决策模式并不会展现更高的分类准确率，因为我们方法的目标是总结预训练 CNN 模型的决策模式，而不是提升分类准确度。 图 5：可视化决策树第二层节点所对应的决策模式，我们展示了每一个决策模式中的典型样本。 图 6：对 CNN 预测的物体-部位贡献。饼图展示了不同部位的贡献比例，它们都通过第二层节点进行估计。热力图表示顶部卷积层的神经元激活值分布，该热力图并不代表「贡献」的分布，因为神经元激活值并没有被 g_i 加权。右图为不同卷积核的图像感受野。基于这些感受卷积核，我们可以通过不同的物体部位分配卷积核，因而能计算物体部位的贡献。 本论文再最后的结语中表明，决策树理论上只为 CNN 的预测提供近似的解释，它不会对 CNN 的表征细节进行准确的重构。首先，我们并没有准确的物体-部位标注以监督 CNN 的学习，[30] 只能粗略地令每个卷积核表征一个物体部位。而卷积核在一些困难的样本中也可能生成不正确的激活值。其次，每一个节点的决策模式忽略非显著性的物体-部位模式（卷积核）以确保决策模式的稀疏表征。 "
263,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737699&idx=3&sn=ab18c895c48401c1e397df75479a4dcb&chksm=871ac85db06d414b31e9a175a9bb952c3b2339c20de2e1414edb5dd311c62ab8c47757eea3a7&scene=27,业界 | 自动驾驶机密之争戛然而止：Waymo与Uber达成2.45亿美元庭外和解,Reuters Alexandria Sage、Dan Levine、Heather Somerville 在为期数月的交锋之后，Uber 最终与谷歌旗下的 Waymo 在自动驾驶汽车技术诉讼上达成了庭外和解，Uber 将向 Alphabet 支付约合 2.45 亿美元的自有股权以解决这桩有关商业机密的法律纠纷。 Anthony Levandowski 是 Uber 自动驾驶商业机密事件的中心 本周五，Uber 突然宣布了这项和解——仅在旧金山联邦法院开始审判前五天，这一引发众多关注的案件戛然而止。 这一诉讼是 2017 年 1 月由 Waymo 提出的，谷歌旗下的这家公司称，曾任 Uber 自动驾驶汽车项目主管的一名工程师 Anthony Levandowski 在离开谷歌时带走了数千份机密技术文件。 该案对于 Uber 的自动驾驶汽车项目进程造成了极大的阻碍，并影响了它的长期利润率。在 Waymo 提出诉讼之后，Uber 开除了它的自动驾驶项目负责人。Uber 部署自动驾驶汽车「舰队」的计划已经远远落后于其它公司，而这是硅谷目前利润最丰厚的竞赛之一。 这个解决方案使 Uber 的首席执行官 Dara Khosrowshahi 得以将另一项公司的丑闻抛诸脑后，并继续进行自动驾驶技术的开发。前 CEO Travis Kalanick 在星期二和星期三的审判中出庭作证。 作为解决方案的一部分，Waymo 将得到 Uber 0.34% 的股份，约值 2 亿 4500 万美元（基于 Uber 目前 720 亿美元的估值），一个 Waymo 代表说道。该解决方案包含了一个协议，确保 Waymo 的机密信息不会被纳入 Uber 的技术中，Waymo 称这也是他们提出这次诉讼的主要目的。 据路透社报道，在去年的和解协商中，Waymo 曾向 Uber 要求至少 10 亿美元的赔偿，并希望拥有独立的监管人，以确保 Uber 在未来不会使用 Waymo 的技术。Waymo 还要求 Uber 做出道歉。Uber 以不切实际为理由拒绝了这些要求。 Waymo 在本周早期曾经同意了一个价值 5 亿美元的解决方案，Khosrowshahi 将该协议带到了 Uber 的董事会。 但 Uber 的董事会在星期二拒绝了这个协议，Khosrowshahi 和 首席法律官 Tony West 只得重新展开谈判。 在案件审理期间，以好斗出名的 Kalanick 出庭作证，他以冷静的态度回答了关于 Uber 和 Alphabet 之间的僵硬关系的问题，并对这场诉讼的罪魁祸首即 Uber 的自驾汽车工程师 Anthony Levandowski 表达了「钦佩」之情。 在为期四天的作证之中，Waymo 几乎没有提出任何有关 Uber 窃取了自己商业机密的有力证据。 一名消息人士称：在周四晚间，Waymo 同意了这笔价值 2.45 亿美元的交易。 Khosrowshahi 在周五的一份声明中表示，他们对于 Uber 曾经的行为表示「遗憾」。 「虽然我并不相信有任何商业机密从 Waymo 转向了 Uber，也不相信 Uber 在自动驾驶汽车中使用了任何 Waymo 的专有信息，我们正与 Waymo 合作，确保 Lidar 及其软件能够在我们自己的努力之下工作良好，」Khosrowshahi 在声明中说道。 小型激光雷达 Lidar 是自动驾驶汽车中至关重要的轻型传感器。 两家公司均未披露本次和解达成的细节，本周五，Alphabet 的股票价格上升了 3.8%。 Elizabeth Rowe，弗罗里达大学的法律学院教授，在分析了大概 150 个 2014 年的商业秘密诉讼裁决后将这次价值 2.45 亿美元的和解排在了第二高。在这个基础上，同时谷歌母公司 Alphabet 的 CEO 拉里·佩奇应该会在下周提供证词，她说 Waymo 要求的和解是可以被理解的。 「他们的重大风险终于解除了。」Rowe 说。 Waymo 的诉讼说 Levandowski 在 2015 年 12 月份到 2016 年在 Uber 工作的时候下载了超过 1 万 4 千分包含无人驾驶车设计的保密文件。 美国司法部正在进行一个独立的商业秘密犯罪调查。Levandowski 从来没有公开承认任何指控，执法部门在此过程中也没有以偷窃为名指控任何其他人。Levandowski 在本案中不是被告。 Waymo 诉讼是 Uber 面临的最有压力的法律战争，但在去年长长的争辩列表里 Uber 只摆脱了一件。 关于 Uber 的性骚扰和有毒工作环境的内部调查的公开审讯让超过 20 人被开除，同时公司也面临多个联邦犯罪探查。公司也在为 6 月份高层驱逐 Kalanick 和痛苦的懂事会争吵所受罪。 在法庭上的文件里可以看到，Uber 已经计划在 2018 年末在 20 个城市部署无人驾驶车，2019 年要有 50 个城市，2020 年要有 150 个城市。但这还有很远的路要走。 Uber 现在已经在亚利桑那州的坦佩和宾夕法尼亚州的匹兹堡开始了小型测试项目。尽管他们已经在加利福尼亚得到了必要的许可，他们还是不能用自动驾驶车来运送乘客。 该和解增长了 Alphabet 在 Uber 原来的 2.58 亿美元投资，这也是当时 Uber 融资路上最大的一笔。 Uber 在上一个月已经接受了软银的 140 亿美元投资，软银现在有 Uber 17.5% 的股权。其中，软银大部分购入的股票都按照估值的 7 折购买的。 自动驾驶汽车给 Uber 提供了一个百万级的市场机会来重新定义交通系统，当然苹果等其他公司也有着同样的机会。通用汽车集团等车企和很多初创公司都在发展这方面的技术。 若想在本案中获得胜利，Waymo 必须提供 Uber 收取了 Waymo 的交易秘密，而且 Uber 把 Waymo 的技术据为己有的有力证明。 Kalanick 周五的时候在一份声明中说到了他们必胜，Uber 自己客观的目标是雇佣最有才华的科学家和工程师，所以他们永远不用购买别人的机密。 「该案审理以这种方式结束，Uber 很显然已经获胜了，」Kalanick 说道。 
264,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737657&idx=2&sn=472ce44a915aead8798cd95f0c321d4f&chksm=871acf87b06d469177e410c116135498a79d20f5d1355281f673efd738a6dea76f808c83c6d4&scene=27,资源 | R语言也能使用TensorFlow了！RStudio发布全新接口,日前，RStudio 博客发文称其已开发出适合 R 语言用户的 TensorFlow 接口，R 语言的用户也可以方便地使用 TensorFlow 了。博客还介绍了接口中的包和工具、学习资源等。以下，机器之心对本文进行了编译介绍。 链接：https://tensorflow.rstudio.com/ 在过去一年中，RStudio 的开发者们一直在努力为 R 语言构建 TensorFlow 的接口。几天前，开发小组终于宣布大部分工作已经完成，现在，R 语言的用户也可以方便地使用 TensorFlow 了。 TensorFlow 是谷歌推动的开源深度学习框架，自两年前发布以来，TensorFlow 很快就成为了机器学习从业者与研究者的首选框架。上周六，RStudio 首席执行官 J.J. Allaire 在一次活动中正式展示了如何在 TensorFlow 中使用 R 语言。 TensorFlow 的 R 接口包括一套 R 语言包，该包提供多种 TensorFlow R 接口，适用于不同的任务和抽象级别，包括： Keras：神经网络的高级接口，致力于促使快速实验。（https://tensorflow.rstudio.com/keras/） TensorFlow Estimators：常见模型类别的实现，如回归器和分类器。（https://tensorflow.rstudio.com/tfestimators/） Core TensorFlow API：TensorFlow 计算图的低级接口。（https://tensorflow.rstudio.com/tensorflow/） TensorFlow Dataset API：TensorFlow 模型的可扩展输入管道。（https://tensorflow.rstudio.com/tools/tfdatasets/） 除了多种 TensorFlow R 接口以外，还有多种工具有助于训练工作流，包括 RStudio IDE 中训练指标的实时反馈： tfruns 工具包（https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html）提供工具来追踪和管理 TensorFlow 训练运行和实验： 训练卷积或循环神经网络往往需要大量算力，而使用近期新推出的高端英伟达 GPU 可以带来很大帮助。但是，大部分用户没有此类本地硬件。为了解决这个问题，RStudio 提供了多种云端使用 GPU 的方式，包括： Google CloudML（https://tensorflow.rstudio.com/tools/cloudml/）； 适用于 AWS Amazon EC2 的 RStudio 云服务器 TensorFlow GPU（https://tensorflow.rstudio.com/tools/cloud_server_gpu.html#amazon-ec2）（Amazon EC2 图像，预先配置了英伟达 CUDA 驱动、TensorFlow、TensorFlow R 接口和 RStudio 服务器）； 利用 Paperspace 服务设置 Ubuntu 16.04 Cloud Desktop GPU。 如果你具备需要的 NVIDIA GPU 硬件，可以查看设置 GPU 的相关文档：https://tensorflow.rstudio.com/tools/local_gpu.html RStudio 在学习资源方面也进行了大量投资，所有 TensorFlow R 接口的资源可在该网站获取：https://tensorflow.rstudio.com 学习资源包括但不限于： 《Deep Learning with R》 地址：https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X 适合统计学家、分析师、工程师和学生，需要具备一定 R 语言使用经验，但不必精通机器学习和深度学习。你将学习 30 多个代码示例，包括详细的注释和详尽的介绍。读者不必具备机器学习和深度学习知识，这本书涵盖所有必备基础知识。读者也无需深厚的数学背景，高中数学水平就足够了。 Deep Learning with Keras Cheatsheet  地址：https://github.com/rstudio/cheatsheets/raw/master/keras.pdf Keras R 接口概念和可用函数的快速参考指南，涵盖不同种类的 Keras 层、数据预处理、训练工作流和预训练模型。 Gallery 地址：https://tensorflow.rstudio.com/learn/gallery.html TensorFlow R 接口的深入使用案例，包括详细的解释，同时覆盖多种辅助任务，如数据预处理和可视化。 示例 地址：https://tensorflow.rstudio.com/learn/examples.html TensorFlow R 接口的介绍性案例，包括使用 Keras、tfestimators 和 TensorFlow 包训练模型的基础知识。 RStudio 将继续构建适合 R 语言用户的 TensorFlow 包和工具，帮助学习、生产和解决该领域的难题，也将继续添加深入案例。若想持续获得最新信息，请访问 TensorFlow for R 博客：https://tensorflow.rstudio.com/blog.html 虽然 TensorFlow 和深度学习在图像识别、语音识别等领域里已经有了令人瞩目的成果，但它在一些其他领域：如生物医疗和时序分析中仍然没有得到广泛应用。随着 TensorFlow 中 R 语言接口的全面推出，更多的可能性已经出现，现在，是时候进行更多探索了。  原文链接：https://blog.rstudio.com/2018/02/06/tensorflow-for-r/ 
265,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737699&idx=4&sn=ad199469275a0696fb1f33a3ee55917f&chksm=871ac85db06d414b683508f55ce968277355accab0c1c91a43da5bca031bd4896c28b779e6a2&scene=27,教程 | 概率编程：使用贝叶斯神经网络预测金融市场价格,"Medium 随着人工智能技术的普及，用机器学习预测市场价格波动的方法最近层出不穷。本文中，Alex Honchar 介绍了利用概率编程和 Pyro 进行价格预测的方法，相较于常规神经网络，新方法对于数据的依赖程度更小，结果更准确。在实验中，作者选择了最近流行的虚拟货币「以太币」作为实例进行价格预测。 去年我曾发表过几篇有关使用神经网络进行金融价格预测的教程，我认为其中有一部分结果至少还挺有意思，并且值得在实际交易中加以应用。如果你阅读过这些文章，你一定注意到一个现象：当你试图将一些机器学习模型应用于「随机」数据并希望从中找到隐藏规律的时候，训练过程往往会产生严重的过拟合。我们曾使用不同的正则化技术和附加数据应对这个问题，但是这不仅很费时，还有种盲目搜索的感觉。 今天，我想介绍一个略微有些不同的方法对同样的算法进行拟合。使用概率的观点看待这个问题能够让我们从数据本身学习正则化、估计预测结果的确定性、使用更少的数据进行训练，还能在模型中引入额外的概率依赖关系。我不会过多深入贝叶斯模型或变分原理的数学、技术细节，而是会给出一些概述，也更多地将讨论集中在应用场景当中。文中所用的代码可以在以下链接中找到：https://github.com/Rachnog/Deep-Trading/tree/master/bayesian 与此同时，我也推荐大家查阅我此前发布的基于神经网络的财务预测教程： 1. 简单时间序列预测（错误纠正完毕） 2. 正确一维时间序列预测+回测 3. 多元时间序列预测 4. 波动预测和自定义损失 5. 多任务和多模式学习 6. 超参数优化 7.  为了更深入地了解概率规划、贝叶斯模型以及它们的应用，我推荐你在以下资源网站中查看： 模式识别和机器学习 黑客贝叶斯方法 下面即将提到的库文件 另外，你还可能会用到下列 Python 库： PyMC3 (https://github.com/pymc-devs/pymc3) Edward (http://edwardlib.org/) Pyro (http://pyro.ai/) 这个「概率」指的是什么？我们为什么称其为「编程」呢？首先，让我们回忆一下我们所谓「正常的」神经网络指的是什么、以及我们能从中得到什么。神经网络有着以矩阵形式表达的参数（权重），而其输出通常是一些标量或者向量（例如在分类问题的情况下）。当我们用诸如 SGD 的方法训练这个模型后，这些矩阵会获得固定值。与此同时，对于同一个输入样本，输出向量应该相同，就是这样！但是，如果我们将所有的参数和输出视为相互依赖的分布，会发生什么？神经网络的权重将与输出一样，是一个来自网络并取决于参数的样本——如果是这样，它能为我们带来什么？ 让我们从基础讲起。如果我们认为网络是一个取决于其他分布的数集，这首先就构成了联合概率分布 p(y, z|x)，其中有着输出 y 和一些模型 z 的「内部」隐变量，它们都取决于输入 x（这与常规的神经网络完全相同）。我们感兴趣的是找到这样神经网络的分布，这样一来就可以对 y ~ p(y|x) 进行采样，并获得一个形式为分布的输出，该分布中抽取的样本的期望通常是输出，和标准差（对不确定性的估计）——尾部越大，则输出置信度越小。 这种设定可能不是很明确，但我们只需要记住：现在开始，模型中所有的参数、输入及输出都是分布，并且在训练时对这些分布进行拟合，以便在实际应用中获得更高的准确率。我们也需要注意自己设定的参数分布的形状（例如，所有的初识权重 w 服从正态分布 Normal（0,1），之后我们将学习正确的均值和方差）。初始分布即所谓的先验知识，在训练集上训练过的分布即为后验知识。我们使用后者进行抽样并得出结果。 图源：http://www.indiana.edu/~kruschke/BMLR/ 模型要拟合到什么程度才有用？通用结构被称为变分推理（variational inference）。无需细想，我们可以假设，我们希望找到一个可以得到最大对数似然函数 p_w（z | x）的模型，其中 w 是模型的参数（分布参数），z 是我们的隐变量（隐藏层的神经元输出，从参数 w 的分布采样得到），x 是输入数据样本。这就是我们的模型了。我们在 Pyro 中引入了一个实例来介绍这个模型，该简单实例包含所有隐变量 q_（z）的一些分布，其中 ф 被称为变分参数。这种分布必须近似于训练最好的模型参数的「实际」分布。 训练目标是使得 [log(p_w(z|x))—log(q_ф(z))] 的期望值相对于有指导的输入数据和样本最小化。在这里我们不探讨训练的细节，因为这里面的知识量太大了，此处就先当它是一个可以优化的黑箱吧。 对了，为什么需要编程呢？因为我们通常将这种概率模型（如神经网络）定义为变量相互关联的有向图，这样我们就可以直接显示变量间的依赖关系： 图源：http://kentonmurray.com/ 而且，概率编程语言起初就被用于定义此类模型并在模型上做推理。 为什么选择概率编程？ 不同于在模型中使用 dropout 或 L1 正则化，你可以把它当作你数据中的隐变量。考虑到所有的权重其实是分布，你可以从中抽样 N 次得到输出的分布，通过计算该分布的标准差，你就知道能模型有多靠谱。作为成果，我们可以只用少量的数据来训练这些模型，而且我们可以灵活地在变量之间添加不同的依赖关系。 概率编程的不足 我还没有太多关于贝叶斯建模的经验，但是我从 Pyro 和 PyMC3 中了解到，这类模型的训练过程十分漫长且很难定义正确的先验分布。而且，处理从分布中抽取的样本会导致误解和歧义。 数据准备 我已经从 http://bitinfocharts.com/ 抓取了每日 Ethereum（以太坊）的价格数据。其中包括典型的 OHLCV（高开低走），另外还有关于 Ethereum 的每日推特量。我们将使用七日的价格、开盘及推特量数据来预测次日的价格变动情况。 价格、推特数、大盘变化 上图是一些数据样本——蓝线对应价格变化，黄线对应推特数变化，绿色对应大盘变化。它们之间存在某种正相关（0.1—0.2）。因此我们希望能利用好这些数据中的模式对模型进行训练。 首先，我想验证简单线性分类器在任务中的表现结果（并且我想直接使用 Pyro tutorial——http://pyro.ai/examples/bayesian_regression.html——的结果）。我们按照以下操作在 PyTorch 上定义我们的模型（详情参阅官方指南：http://pyro.ai/examples/bayesian_regression.html）。 以上是我们以前用过的简单确定性模型，下面是用 Pyro 定义的概率模型： 从上面的代码可知，参数 W 和 b 均定义为一般线性回归模型分布，两者都服从正态分布 Normal（0,1）。我们称之为先验，创建 Pyro 的随机函数（在我们的例子中是 PyTorch 中的 RegressionModel），为它添加先验 ({『linear.weight』: w_prior, 『linear.bias』: b_prior})，并根据输入数据 x 从这个模型 p(y|x) 中抽样。 这个模型的 guide 部分可能像下面这样： 我们定义了想要「训练」的分布的可变分布。如你所见，我们为 W 和 b 定义了相同的分布，目的是让它们更接近实际情况（据我们假设）。这个例子中，我让分布图更窄一些（服从正态分布 Normal(0, 0.1)） 然后，我们用这种方式对模型进行训练： 在模型拟合后，我们想从中抽样出 y。我们循环 100 次并计算每一步的预测值的均值和标准差（标准差越高，预测置信度就越低）。 现在有很多经典的经济预测度量方法，例如 MSE、MAE 或 MAPE，它们都可能会让人困惑——错误率低并不意味着你的模型表现得好，验证它在测试集上的表现也十分重要，而这就是我们做的工作。 使用贝叶斯模型进行为期 30 天的预测 从图中我们可以看到，预测效果并不够好。但是预测图中最后的几个跳变的形状很不错，这给了我们一线希望。继续加油！ 在这个非常简单的模型进行实验后，我们想要尝试一些更有趣的神经网络。首先让我们利用 25 个带有线性激活的神经元的单隐层网络训练一个简单 MLP： 训练 100 个 epoch： 其结果如下： 使用 Keras 神经网络进行为期 30 天的预测 我觉得这比简单的贝叶斯回归效果更差，此外这个模型不能得到确定性的估计，更重要的是，这个模型甚至没有正则化。 现在我们用 PyTorch 来定义上文在 Keras 上训练的模型： 相比于贝叶斯回归模型，我们现在有两个参数集（从输入层到隐藏层的参数和隐藏层到输出层的参数），所以我们需要对分布和先验知识稍加改动，以适应我们的模型： 以及 guide 部分： 请不要忘记为模型中的每一个分布起一个不同的名字，因为模型中不应存在任何歧义和重复。更多代码细节请参见源代码：https://github.com/Rachnog/Deep-Trading/tree/master/bayesian 训练之后，让我们看看最后的结果： 使用 Pyro 神经网络进行为期 30 天的预测 它看起来比之前的结果都好得多！ 比起常规贝叶斯模型，考虑到贝叶斯模型所中习得的权重特征或正则化，我还希望看到权重的数据。我按照以下方法查看 Pyro 模型的参数： 这是我在 Keras 模型中所写的代码： 例如，Keras 模型最后一层的权重的均值和标准差分别为 -0.0025901748 和 0.30395043，Pyro 模型对应值为 0.0005974418 和 0.0005974418。数字小了很多，但效果真的不错！其实这就是 L2 或 Dropout 这种正则化算法要做的——把参数逼近到零，而我们可以用变分推理来实现它！隐藏层的权重变化更有趣。我们将一些权重向量绘制成图，蓝线是 Keras 模型的权重，橙线是 Pyro 模型的权重： 输入层与隐藏层之间的部分权重 真正有意思的不止是权重的均值与标准差变得小，还有一点是权重变得稀疏，所以基本上在训练中完成了第一个权重集的稀疏表示，以及第二个权重集的 L2 正则化，多么神奇！别忘了自己跑跑代码感受一下：https://github.com/Rachnog/Deep-Trading/tree/master/bayesian 我们在文中使用了新颖的方法对神经网络进行训练。不同于顺序更新静态权重，我们是更新的是权重的分布。因此，我们可能获得有趣又有用的结果。我想强调的是，贝叶斯方法让我们在调整神经网络时不需要手动添加正则化，了解模型的不确定性，并尽可能使用更少的数据来获得更好的结果。感谢阅读：)  原文链接：https://medium.com/@alexrachnog/financial-forecasting-with-probabilistic-programming-and-pyro-db68ab1a1dba "
266,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737699&idx=1&sn=3337a0de7ebc9bdb778ba50e2c386da0&chksm=871ac85db06d414bf429a8dc21dbbf3c0bc2af5a9e431708409a6fd47e265ad0cd3b6ed8835b&scene=27,想要实现深度神经网络？一张 Excel 表格就够了,Medium Blake West 卷积神经网络（CNN）经常被用于图像识别、语音处理等领域，是人工智能近年来快速发展的重要组成部分。然而，对于入门人士来说，我们似乎难以理解其中的原理。实际上「卷积」等概念并非遥不可及，本文作者 Blake West 向我们介绍了使用 Excel、Google Sheets 等电子表格实现卷积神经网络的方法。 事实上，深度卷积神经网络根本不像它听起来这么可怕。本文将在谷歌表格中实现一次来证明给你看。进入演示地址，下载为 Excel 表格，然后你就可以随意编辑，看看不同层是怎样影响模型最后的预测结果的。 演示地址：https://docs.google.com/spreadsheets/d/1SwfVctd4TjdN2S8BL09ktpQN_41sARYzD3NEHyr-8Z0/edit?usp=sharing 下面我们会从较高角度来做一个简短的卷积神经网络原理的介绍。 在我们开始前，我要介绍一下 FastAI，我最近完成了他们的深度学习课程，所有的灵感和功劳都归他们。Jeremy Hdward 是一个非常棒的导师，和他的联合创始人 Rachel Thomas 在课上展示了怎样用 excel 做卷积神经网络。但就现在的状况来说，这个表在网上找不到，而且我不认为这个表格是完整的网络。我做了一点扩展，然后把这个放到了谷歌表格里，这样大家都能随意地耍了。 我用 MNIST 数据集训练了一个非常简单的卷积神经网络，这个数据集是用来预测有手写数字图片里的数字的。每一个图片都是 28×28 像素大小。每一个像素用 0（空白）到 1（深色）之间来表示。MNIST 是一个非常经典的数据集，常被用于各种新技术的研究，如 Geoffrey Hinton 等人的 Capsule。这个数据集非常的小所以训练起来很快，但这个数据集又有足够多的数据来展示机器学习的复杂性。这个模型的工作是预测图片里的数字是多少。每一个图片都明确的是 0-9 之间的一个数。 MNIST 数据集的一个例子，28×28 像素大小。注意：我加了有条件的格式，这样有更大数字的像素会显得更红。 我用了一个非常著名的深度学习库 Keras，然后把我模型中训练好的权重放到表里。训练好的权重只是数字。把它们放到表格里就是说从模型里复制然后粘贴到表格里。最后一步是在表格里加上复制模型功能的公式，就用传统的加法、乘法等。让我再说一次：用来重现一个深度学习模型的数学就是乘法和加法。 Keras 文档：https://keras.io 下图是模型每一层的权重/参数。权重是机器学习模型自动学来的。这个模型有差不多 100 个权重。更复杂的模型很容易就有几亿个参数。在下图你可以看到这模型的所有的 1000 个参数。 卷积神经网络的工作方式是在序列数据中找寻模式，这个模式可能难以用语言表达，或用简单的规则来表达。卷积神经网络假设序列的顺序是很重要的。 举个例子，图片分类是卷积神经网络主要的用途之一，因为像素是逻辑上有序的，而且对于所有人类来说图片里充满了模式。但是，只要试着用语言来描述到底什么区分开了一只猫和吉娃娃，你就知道为什么卷积神经网络有用了（下意识感知过程）。 另一方面，如果你收集了两支棒球队的各种数据，并希望从中预测出两者对决的结果，在这种情况下卷积神经网络就是一个不太合适的选择。你所拥有的数据（如联赛获胜&失败次数，球队击球平均数）并无固有顺序性。在这里顺序其实并不重要，而我们也已提取了自己所认为有用的特征。在这里，卷积神经网络并不适用。 为了理解 CNN 背后的工作原理，我们将深度卷积神经网络拆分成「深度」、「卷积」和「神经网络」并分别解释。 卷积 想象你正闭着眼睛，试图识别手写图像上的数字。你可以和看着图像的人交谈，但他们并不知道这个数字是什么。所以你只能问一些简单的问题，该怎么办？ 有一些可行的问题，例如，「它在顶部是基本呈直线吗？」「它有对角线吗？」等。问了足够多的问题以后，你就能很好的猜测该数字到底是 7 还是 2，或任何其它数字。 直观上，这正是卷积运算的工作方式。计算机是盲目的，所以它会按照自己的方式询问大量的关于小区域模式的问题。 为了询问这些问题，图像中的每一个像素需要运行一个函数（即卷积运算），生成相应的像素值，以回答对应的关于小区域模式的问题。卷积运算利用卷积核寻找模式。例如，上图方框 2 中的数字在右侧的红色更深（值更大），左侧更浅（值更小）。则该卷积核的作用就是寻找竖直的颜色从左向右变深的边缘（简称左边缘）。 这可能不够直观，但只要在该表格中尝试交互你就能发现卷积核的工作方式。卷积核寻找的是和自己相似的模式。并且一个 CNN 通常有数百个卷积核，你可以对所有像素捕捉不同类型的模式，例如左边缘、上边缘、对角线、角等。 深度 寻找边缘是很基本的要素，那对于更复杂的形状要怎么处理呢？这正是「深度」即多层的用武之地。通过底部卷积层的运算中，我们已经有了图像的「左边缘」、「上边缘」和其它简单的卷积模式的分布。现在添加更多的层，对前面得到的所有模式分布再次进行卷积操作，然后组合起来。所以结合一个左边缘和一个上边缘各 50% 可以给你一个任意的左角，怎么样，很强大吧。 第二个卷积层取上一个卷积层得到的像素输出为输入，并用自己的卷积核对其进行卷积运算。和之前一样，我们得到了第二个卷积层的新的对应像素值。 实际应用的 CNN 会有很多个层，使得网络能构建越来越抽象和复杂的形状。即使仅仅经过 4 到 5 层的卷积，模型也能开始寻找关于脸部、动物等物体的关键特征。 神经网络 现在你或许会问自己，「这都很好，但是把所有的卷积结果一起提出来听起来很乏味？怎样才能把所有这些卷积核给出的结果结合起来变成有意义的东西呢。 首先，在一个更高的层面上，我们的卷积网络可以分成两个部分。第一个是卷积，卷积在图片数据里找出有用的特征。 第二部分是，在表格靠后的」密集层「（即全连接层，这么命名是因为这里每一个神经元都有好多参数）是用来分类的。一但你有了这些纹理，这些密集层的作用其实就是对每一个可能的数计算一堆线性回归，然后给一个分数。最高的分数就是模型的最终预测。 矩阵 1 是我们的卷积输出。每个矩阵 1 中的像素乘以矩阵 2 中的数字，求和之后的结果生成方框 3 里的数字。接下来对绿色框里的矩阵重复同样的运算。在这里，我们最终得到了 8 个输出，在深度学习里它们被称为「神经元」。 想要最终找出这些卷积核和密集层中正确的权重，是非常繁杂的一项工作。幸运的是，自动更新权重是神经网络工作的一部分，所以我们不必担心这一点。如果你对此比较好奇，请查找「反向传播」的有关内容（可参阅： ）。 每个卷积神经网络都包含两大部分：卷积，它总是先开始行动，寻找图片中有用的特征；而分层在其后，经常被称为「密集」层，它们会根据特征对事物进行分类。 为了对这些概念有一个清晰的认识，我推荐你使用电子表格对它们进行操作。 在这里，你可以从头到尾跟踪一个像素，看着它通过过滤器，最终会发生些什么。我还在电子表格的评论中加入了更多技术细节的解释。 资源 想要学习更多？我推荐这些资源： 交互式卷积——一个强大的交互式卷积神经网络教程（只包括卷积部分，不包括神经网络部分），由 Victor Powell 编写：http://setosa.io/ev/image-kernels/。 深度学习实践——我已从这份 Fast.AI 课程中学到了很多，它是在线课程，完全免费：http://course.fast.ai/。 卷积神经网络视频展示——这份包含 20 分钟 CNN 讲解的教程由 Jeremy Howard（FastAI 创始人）授课，请跳至视频的第 21 分钟开始：https://www.usfca.edu/data-institute/certificates/deep-learning-part-one。 Notes 训练卷积神经网络需要的数学基础包括微积分，这样才能自动调整权重。但随着模型训练完成，它实际上只需要乘法与加法进行预测。在实践中，微积分部分的内容是由你使用的深度学习库来处理的。 
267,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737657&idx=4&sn=78c2483eede09ec00d6e4653efcd212a&chksm=871acf87b06d4691a75b316343fb91f8a999826f0741181d50b1e0f9d0e104bf84e4d1743fab&scene=27,AAAI 2018 | 从哈希到CNN：中科院自动化所提出高精度&低功耗训练方法,"在美国新奥尔良刚刚落幕的人工智能国际会议 AAAI 2018 上，来自中科院自动化所程健研究员团队的胡庆浩等人报告了一种基于哈希的深度神经网络二值化训练方法 [1]，揭示了哈希与二值权重的神经网络之间的紧密关系，表明了网络模型的参数二值化问题可以转化为哈希学习问题，从而大幅提高了二值化深度神经网络模型的性能，使其能在资源受限场景下能兼顾性能和功耗。 近年来，深度卷积神经网络已经深入了计算机视觉的各个任务中，并在图像识别、目标跟踪、语义分割等领域中取得了重大突破。在一些场景下，当前深度卷积网络性能已经足以部署到实际应用中，这也鼓舞着人们将深度学习落地到更多的应用中。 然而，深度卷积网络在实际部署时面临着参数量和时间复杂度等两方面的问题，一方面是深度网络巨大的参数量会占用大量的硬盘存储和运行内存，这些硬件资源在一些移动和嵌入式设备中往往是很有限的；另外一方面就是深度网络的计算复杂度较高，这会使得网络推理速度很慢，同时会增加移动设备的电量消耗。 为了解决此类问题，人们提出了很多网络加速和压缩方法，其中网络参数二值化是一种将网络参数表示为二值参数的方法。由于二值网络中参数只有+1 和-1 两种值，乘法运算就可以被加法运算替代。由于乘法运算比加法运算需要更多的硬件资源和计算周期，使用加法运算替代乘法运算能够实现网络加速的目的。另一方面，原始网络参数的存储格式是 32 位浮点数，二值参数网络只使用 1 位来表示+1 或者-1, 达到了 32 倍的压缩目的。但是将参数从 32 位量化到 1 位会导致较大的量化损失，当前的二值网络训练方法往往会导致较大的网络精度下降，如何学习二值的网络参数同时又不带来较大的精度下降是一个问题。 自动化所程健研究员团队的胡庆浩等人最近提出了一种基于哈希的二值网络训练方法，揭示了保持内积哈希（Innerproduct Preserving Hashing）和二值权重网络之间的紧密关系，表明了网络参数二值化本质上可以转化为哈希问题。 给定训练好的全精度浮点 32 位网络参数 W，二值权重网络（BWN）的目的是学习二值网络参数 B 并维持原始网络精度。学习二值参数 B 的最朴素的方式就是最小化 B 与二值参数 B 之间的量化误差，但是这种量化误差和网络精度之间存在着一定的差距，最小化量化误差并不会直接提高网络精度，因为每一层的量化误差会逐层积累，而且量化误差会受到输入数据的增幅。 一种更好的学习二值参数 B 的方式是最小化内积相似性之差。假设网络某一层输入为 X，X^TW是原始的内积相似性，则 X^TB 是量化之后的内积相似性，最小化 X^TW 与 X^TB 之间的误差可以学习到更好的二值参数 B。从哈希的角度来讲，X^TW 代表着数据在原始空间中的相似性或者近邻关系，X^TB 则代表着数据投影到汉明空间之后的内积相似性。而哈希的作用就是将数据投影到汉明空间，且在汉明空间中保持数据在原始空间中的近邻关系。至此，学习二值参数 B 的问题就转化成了一个在内积相似性下的哈希问题，该哈希主要是将数据投影到汉明空间并保持其在原始空间中的内积相似性。 团队首先在 VGG9 小网络上对方法进行验证，并且在 AlexNet 和 ResNet-18 上超过当前的二值权重网络。特别的，在 ResNet-18 上，该方法比当前最好方法的精度提高了 3 个百分点。获得了较好的实验结果。     Dasgupta 等人 [2] 在 2017 年 11 月份的《Science》上撰文揭示了果蝇嗅觉神经回路其实是一种特殊的哈希，其哈希投影是稀疏的二值连接。对比二值权重网络（BWN）, 我们可以发现二者之间有着密切的关系，首先，二者的网络都是二值连接，这意味着二值权重在生物神经回路中有存在的生物基础，这也为二值权重网络的潜在机理提供了启发；其次，二者都是为了保持近邻关系，并且可以描述为哈希问题，这种现象表明神经网络的某些连接是为了保持近邻关系。最后，果蝇嗅觉回路中的稀疏连接和卷积层的参数共享机制有着异曲同工之处，都是对输入的部分区域进行连接。 参考资源： [1] Qinghao Hu, Peisong Wang, Jian Cheng. From Hashing to CNNs: Training Binary Weight Networks via Hashing. AAAI 2018 [2]Dasgupta S, Stevens C F, Navlakha S. A neural algorithm for a fundamental computing problem. Science, 2017, 358(6364): 793-796.  团队介绍 中国科学院自动化研究所程健研究员团队主要从事深度神经网络优化计算、人工智能芯片设计等方面的研究和应用，在深度神经网络的加速和压缩、定点量化、低比特量化、加速器设计等方面取得了系列进展，相关成果发表在 JMLR、TNNLS、ToMM、CVPR、ICCV、ICML、AAAI、MM、DATE 等国际杂志和会议上，部分成果已经在企业得到应用。  论文链接：https://pan.baidu.com/s/1jJJTv6Y  （提取密码：r76f） "
268,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737657&idx=3&sn=cd5ca734b19db541116227cb960e69b4&chksm=871acf87b06d4691d6f77c29fa1d71b024068a2d54c9287d3a2a3d43f56e3e668fbee323f0dc&scene=27,业界 | OpenAI提出新型神经网络：自动计算词对象，实现实体消岐,本文通过让神经网络决策一个词是否属于 100 个自动发现的「类别」（非专属范畴）之一，从而建立了一个可以自动计算单词指称对象的神经网络。该方法在若干个实体消岐（entity disambiguation）数据集上实现了当前最优的提升。 通过让神经网络决策一个词是否属于 100 个自动发现的「类别」（非专属范畴）之一，我们已建立一个神经网络，可以自动计算一个词所指称的对象。比如对于语句「猎物看到美洲虎（Jaguar）穿过丛林（Jungle）」，系统会执行「20 个问题」来分析美洲虎属于哪个预分的类别，而不是去试图直接把美洲虎归类到车、动物或者其他类里。这一方法在若干个实体消岐（entity disambiguation）数据集上实现了当前最优的巨大提升。 论文链接：https://arxiv.org/abs/1802.01021 代码链接：https://github.com/openai/deeptype 在我们的训练数据中，jaguar 指称汽车的概率是 70%，指称动物的概率是 29%，余下 1% 的概率是飞行器。按照我们的类别，第一个实例的歧义不会改变很大，这个模型明显认可了这张图像是美洲虎（动物）跑在高速公路上——但是第二个实例中歧义改变很多，模型不认可图像是美洲虎（汽车）在丛林里穿梭。 我们在 CoNLL(YAGO) 上达到了 94.88% 的准确率（之前的最高是 91.5% 和 91.7%），并在 TAC KBP 2010 挑战赛中达到了 90.85%。先前方法使用分布式表征。类别几乎可以在这些任务上完成，完美的类别预测也只能达到 98.6%-99% 的准确率。 我们的系统采取以下步骤： 1. 提出每一个维基百科内部链接来确定对于每一个词所属的类别。举例，当在维基百科页面上遇到这个链接：[jaguar](https://en.wikipedia.org/wiki/Jaguar)，我们可以总结 https://en.wikipedia.org/wiki/Jaguar 是 jaguar 的意思之一。 2. 用维基百科类别树来确定每一个实体属于的类别。举例，在 https://en.wikipedia.org/wiki/Jaguar_Cars 这个维基百科页面的底部是这些类别（它们有其专属类别，比如汽车）。 3. 选一个有 100 种类别的列表当作你的「类系统」，然后最大化这个分类从而使其可紧凑地表征任何实体。因为我们懂得映射实体与类别，所以给出一个类系统，我们可以表示每一个实体为一个 100 维度的矢量，这个矢量表示与每一类别的所属关系。 4. 使用每一个维基百科内部的链接及其内容，生产训练数据映射一个词+内容与那个 100 维度的对应相应类别的二进制表达式，然后训练一个神经网络来预测映射。这个系统连接起了之前的步骤：维基百科的链接联系起了一个词与一个实体，我们知道第二步里的每一个实体的类别，第三步在我们的种类里选择了类别。 5. 最后，给出一个词和附带的内容，我们的神经网络的输出就可以被理解成输入词所属于每一类别的概率。如果知道确切的每一类别的所属关系，我们会把类别范围缩小到一个（假设完美分类）。但是我们必须要问大约 20 个问题，才能用贝叶斯理论去计算这个词所属类别的概率。 更多实例 以下是我们系统的其他实例： 维基百科的知识图可转为训练数据源，从而把细粒度实体映射到类别。我们应用它的实例关系递归的去确定一个给定实例的类。举例，任何人类节点派生的节点都是人类类别。维基百科也可以通过它的类别链接提供实例到类别的映射。 维基百科内部链接统计提供了一个有关特定词组指称实体的概率的好的评估。但是，这个有噪声的，因为维基百科经常链接到一个具体的实物而不是连接到类别本身，代指：（国王--这个词被连接到 Charles I of England）或者链接有时候会链接到外号（转喻）。这会造成一次关联探索（比如国王这个词有 974 个关联实体）并且总有被扭曲的链接（举例：女王链接到女王乐队 4920 次，伊丽莎白二世 1430 次，还有 32 次君主）。 最简单的方法是删除少见的链接，但这会丢失信息。我们做的是用维基百科属性图来排序这些链接，如下图所示： 在这个过程之后，国王从 974 个降到了 14 个关联实体，同时女王到君主的链接数量从 32 增长到 3553 次。 我们需要选择最佳的类别系统和参数，从而最大化消歧精确度。这里有大量的可能的类别集，所以让一个准确的答案变得很棘手。相反，我们通过启发式搜索或者随机优化（进化算法）选择一个类系统，然后借助剃度下降训练一个类别分类器来预测类系统的行为。 我们需要去选择有歧义的类别，（这样可以快速削减可能的实体集），同时轻松学习（所以周边的文字是用来让一个神经网络知道这里有这样一个类别）。我们通过两种启发法为我们的搜索提供信息：可学习性（预测分类器的训练结果的曲线面下平均面积数）；预测正确性（如果我们能完美地预测所有类别，就可以完美地消除歧义）。 在给定语境窗口的情况下，我们训练一个二值分类器来预测我们数据库里 150000 个类别中的每一个的所属性。这个分类器的曲线下面积就变成了这一类的「可学习性分数」。高的曲线下面积意味着这一类是容易被从内容中预测的。表现差可能意味着我们的训练数据很少，或者词窗口几乎没有任何用处（这种情况一般对于非自然的类比如 ISBN 是正确的）。训练我们的整个模型需要数天，因次我们使用一个更小的模型作为我们的「可学习性分数」的代理，同时训练时间只需 2.5 秒。 我们现在可用这些可学习性分数和计数统计来估计给定类型子集的性能作为我们的类型系统。下面你可以跑交叉熵方法去发现你浏览器里的类别。注意改变取样大小和惩罚是如何影响解决方案的。 为了更好地可视化类别系统设计的哪一部分简单，哪一部分又比较难，我们希望你可以亲自试手设计。选择一个高级域名后，你可以开始查看歧义实例。有可能的回答别圈起来显示在顶端，正确答案是有颜色的圈（hover 以查看其名称）。最下面的一排包含了你可以用的类别。连接顶端与底端的线显示继承关系。选择你想要的关系。一旦你有了足够的关系去区分正确的答案，这个实例就是有歧义的。 从我们的类系统优化中选用最好的答案，然后我们可以用类系统生产出来的标签来标注维基百科的数据。用这个数据（在我们的实验里有英语和法语的 400M 个标记），现在我们可以训练双向 LSTM 来独立地预测每个词的所有类别属性。在维基百科源文本中，我们只监督了内部的维基链接，但是这也足够训练出了一个在 F1 上预测类别准确率为超过 91% 的深度神经网络。 预测文档中的实体通常依赖于不同实体之间的「一致性」度量，比如：在一个长度为 O(N^2) 的文档里，测量每一实体相互之间的契合程度。但我们的运行时间是 O(N)，因为我们只需要在字典树（trie）中查找每个短语，将短语映射到其可能的含义。我们根据维基百科里的链接频率，排序所有可能的实体，接着按照这些链接是否仍属于该类而更精确地改变排序。可通过指定它们的类别归属（人、动物、国家、周期等）添加新实体。 在解决这一问题上，我们的方法与之前有很多不同。我们感兴趣的是分布式表征的端到端学习与这里开发的基于类别的推理相比表现如何。这里的类型系统是使用一个小的维基百科子集发现的，将其扩展到整个维基百科能够带来一个应用更广泛的类别系统。希望我们的代码对你有帮助！  原文链接：https://blog.openai.com/discovering-types-for-entity-disambiguation/ 
269,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737699&idx=5&sn=085b1f079222c5547f05ea6892802f3f&chksm=871ac85db06d414b51f2f7c35bd2fa3124d558e8091ac5f0bfc5ac72bb14c4b390685773bc45&scene=27,学界 | Facebook提出DensePose数据集和网络架构：可实现实时的人体姿态估计,"实现从 2D 图像到 3D 表面的对应在很多方面都有极具价值的应用前景。近日，FAIR 发布了一篇研究论文，介绍了他们通过人工方式标注的图像到表面密集对应数据集 DensePose-COCO 以及基于此训练的 DensePose-RCNN 架构，得到了一个能实时地得到高准确度结果的系统。该研究发布后得到了广泛的关注，机器之心在此对该论文进行了摘要介绍，更多详情请参阅原论文和项目网站。 论文地址：https://arxiv.org/abs/1802.00434 项目网站：http://densepose.org 本研究的目标是通过建立从人体的 2D 图像到基于表面的 3D 表征的密集对应（dense correspondence）来进一步推进人类对图像的理解。我们可以认为这个任务涉及到一些其它问题，比如物体检测、姿态估计、作为特例或前提的部位和实例分割。在图形处理、增强现实或人机交互等不只需要平面关键特征位置标记的问题中，这一任务的解决将能实现很多应用，并且还能助力实现通用型的基于 3D 的物体理解。 建立从图像到基于表面的模型的密集对应的任务已经在可使用深度传感器的设置中基本得到了解决，比如在 [41] 的 Vitruvian 流形中、指标回归森林 [33] 或最近 [44] 提出的密集点云对应。相对而言，我们的情况则是考虑使用单张 RGB 图像作为输入，然后我们基于此来构建表面点和图像像素之间的对应。 最近也有一些其它研究想要以无监督的方式恢复 RGB 图像配对 [3] 或集合 [48,10] 之间的密集对应。最近，[42] 使用了同变性原理（equivariance principle）来将图像集对齐到一个共同坐标系，同时也遵循了分组图像对齐的一般思想，比如 [23,21]。 尽管这些研究都针对的是一般类别，但我们的研究关注的可以说是最重要的视觉类别——人类。对于人类而言，可以通过使用参数可变形表面模型（parametric deformable surface model）来简化这一任务，比如 [2] 的 Skinned Multi-Person Linear（SMPL）模型或最近的 [14] 中通过精心控制 3D 表面获取而得到的 Adam 模型。对于图像到表面映射的任务，[2] 中的作者提出了一种两阶段方法：首先通过一个 CNN 检测人类关键特征位置，然后通过迭代式最小化为该图像拟合一个参数可变形表面模型。与我们的研究同时进行的 [20] 对 [2] 的方法进行了发展，使之能以端到端的方式工作，其在用于恢复 3D 相机姿态和低维身体参数化的深度网络中整合了一个模块——迭代式重投射误差最小化（iterative reprojection error minimization）。 我们的方法与这些研究都不一样，我们采用了一种全面的监督学习方法并收集了人体的图像与详细准确的参数表面模型 [27] 之间的真实对应数据：我们没有在测试时间使用 SMPL 模型，而是将其用作在训练阶段定义我们的问题的一种方法。我们的方法可以被理解成是 [26, 1, 19, 7, 40, 18, 28] 中用于人类的标准的下一步延伸工作。Fashionista [46]、PASCAL-Parts [6] 和 Look-Into-People (LIP) [12] 数据集中已经提供了人体部位分割掩码；这些可以被看作是提供了图像到表面对应的粗糙版本，其中没有连续的坐标，而是可以预测离散的部位标签。在表面层面的监督直到最近才被 [43] 引入合成图像，同时 [22] 中一个包含 8515 张图像的数据集标注上了 3D 模型到图像的关键点和半自动拟合。本研究没有损伤我们的训练集的范围和真实性，而是引入了一种全新的标注流程，让我们可以为 COCO 数据集的 5 万张图像收集真实的对应，进而得到了我们新的 DensePose-COCO 数据集。 我们的工作在思想上最接近于近期的 DenseReg 框架 [13]，其中训练的 CNN 能成功构建自然场景中的 3D 模型和图像之间的密集对应关系。那项工作主要关注的是人脸，并且只在姿态变化适中的数据集上评估了他们的结果。但是，由于人体具有更高的复杂度和灵活性，同时姿态也存在更大的变化，所以我们这里还面临着新的难题。我们采用了合适的架构设计来解决这些难题，详见第 3 节；该架构相比于 DenseReg 类型的全卷积架构有显著的提升。通过将我们的方法与近期的 Mask-RCNN 系统 [15] 相结合，我们表明通过鉴别式方法训练的模型能实时地为涉及数十人的复杂场景恢复高准确度的对应场：我们的系统在一个 GTX 1080 GPU 上能以每秒 20-26 帧的速度处理 240×320 图像或以每秒 4-5 帧的速度处理 800×1100 图像。 我们的贡献可以总结为三点。首先，如第 2 节所述，我们通过收集 SMPL 模型 [27] 和 COCO 数据集中的人物外观之间的密集对应而为该任务引入了第一个人工收集的真实数据集。这是通过在标注过程中使用一种利用了 3D 表面信息的全新标注流程实现的。 第二，如第 3 节所述，通过在任何图像像素对人体表面坐标进行回归，我们使用所得到的数据集训练了可以得到自然环境中密集对应的基于 CNN 的系统。我们实验了依赖于 Deeplab [4] 的全卷积架构和依赖于 Mask-RCNN [15] 的基于区域的系统，并观察到了基于区域的模型相比于全卷积网络的优越性。我们还考虑了我们的方法的级联变体，并在已有的架构上实现了进一步提升。 我们探索了利用我们构建的真实信息的不同方法。我们的监督信号是在每个训练样本中随机选择的图像像素子集上定义的。我们使用了这些稀疏对应来训练一个「教师（teacher）」网络，其可以「修补（inpaint）」图像其余区域的监督信号。不管是与稀疏点相比还是与其它任何已有的数据集相比，使用这种修复后的信号能够得到明显更好的表现，第 4 节通过实验证明了这一点。 我们的实验表明密集的人体姿态估计在很大程度上是可以实现的，但仍还有改善的空间。我们使用一些定性结果和表明该方法发展潜力的方向而对我们的论文进行了总结。我们将通过我们的项目网站公开提供代码和数据：http://densepose.org。 COCO-DensePose 数据集  图 1：密集姿态估计的目标是将 RGB 图像上的所有人类像素映射成 3D 的人体表面。我们引入了一个大规模真实数据集 DensePose-COCO，其中包含人工标注的 5 万张 COCO 图像的图像到表面对应数据；我们还训练了 DensePose-RCNN，能以每秒多帧的速度在每个人体区域内密集回归特定部位的 UV 坐标。左图：图像及通过 DensePose-RCNN 所得到的回归后的对应。中图：DensePose-COCO 数据集标注。右图：身体表面的分割和 UV 参数化。 图 2：通过让标注者将图像分割成形义区域然后再在任何渲染的部位图像上为每个被采样的点定位其对应的表面点，我们标注了图像和 3D 表面模型的密集对应关系。红色叉号表示当前被标注的点。渲染后视图的表面坐标在 3D 模型上定位收集到的 2D 点。 图 3：用于收集每个部位的对应标注的用户界面：我们向标注者提供了人体部位的 6 个预渲染的视角，这样整个部位表面都是可见的。一旦标注了目标点，该点就会同时显示在所有渲染过的图像上。 图 4：标注的可视化：图像（左）、收集到的点的 U 值（中）和 V 值（右） 学习密集人体姿态估计 图 7：DensePose-RCNN 架构：我们使用了区域提议生成和特征池化的级联，之后跟着一个全卷积网络，用于密集地预测离散部位标签和连续表面坐标。 图 8：交叉级联架构：图 7 中 RoIAlign 模块的输出送入 DensePose 网络以及用于其它任务（掩码、关键点）的辅助网络。一旦从所有任务获得了第一阶段的预测，它们就将被组合起来送入每个分支的第二阶段细化。 实验 图 11：使用不同类型的监督信号进行训练的单人表现：DensePose 得到了比其它数据集显著更准确的结果。DensePose∗在训练和测试时都使用了 figure-ground oracle 图 12：多人密集对应标注的结果。这里我们在包含多人的真实 COCO 数据集图像上比较了我们提出的 DensePose-RCNN 系统与全卷积方法的表现，其中这些图像在尺寸、姿态和背景上具有较高的多样性。 图 14：用于纹理迁移的定性结果：在上面一行中所提供的纹理根据估计的对应映射成了图像像素。完整视频请访问：http://densepose.org。 论文：DensePose：自然环境中的密集人体姿态估计（DensePose: Dense Human Pose Estimation In The Wild） 摘要： 在本研究中，我们构建了人体的 RGB 图像与基于表面的表征之间的密集对应，我们将这个任务称为密集人体姿态估计。首先，我们通过引入一种有效的标注流程而收集了 COCO 数据集中 5 万张人类外观的密集对应。然后我们使用我们的数据集训练了能够在自然环境中（in the wild）得到密集对应的基于 CNN 系统，也就是说环境中存在背景、遮挡和尺度变化等情况。通过训练一个可以填补缺失真实值的「修补」网络，我们提升了我们的训练集的有效性；并且相比于过去所能实现的最好结果有明显的提升。我们使用全卷积网络和基于区域的模型进行了实验，并观察到了后者的优越性；我们通过级联进一步提升了准确度，得到了一个能实时地得到高准确度结果的系统。我们的项目网站还提供了补充材料和视频：http: //densepose.org。 "
270,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737657&idx=5&sn=8a03f44b0ea2e81d3d24eab6afdc0f08&chksm=871acf87b06d46915d8f6bd71f934bf5afae8d86606cbea5ebe91fa1b14f81ddd74e934c7afa&scene=27,机器之心「AI00」一月榜单：前谷歌自动驾驶首席工程师创立Nuro,"We believe AI should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible. －OpenAI 这不仅是一份榜单，更是一个开源项目，主要基于以下几点： 人工智能是一个复杂庞大的体系，涉及众多学科，也关乎技术、产品、行业和资本等众多要素，报告的写作团队只代表他们的专业观点，有自己的局限性，需要更多行业专家参与进来加以修正和完善。 人工智能技术和行业的发展瞬息万变，而报告的制作周期较长，其中的内容和数据势必会落后于行业的最新进展，无法同时满足时效性和高质量的要求。而领域内参与者的及时更新可以解决这个问题。 我们深刻地理解在没有专业用户反馈的情况下所做出报告的质量局限性，所以希望用工程界「Agile Development」的理念来对待我们的报告，不断收集专业反馈来持续提升报告质量。 人工智能是一个永恒命题，我们不仅会把「100 家公司」这个主题持续做下去，还会陆续开展其他主题。这个过程需要人工智能领域不同的参与者加入进来。 向 OpenAI 、「斯坦福人工智能百年研究」和「Open Source」致敬。 为此，我们邀请人工智能领域的科学家、技术专家、产业专家、专业投资人和读者加入进来，共同完成这项人工智能的长期研究。 如果你对「AI00」感兴趣，可在公众号对话框回复「AI00」（注：字母 AI 加数字 00）查看本开源项目的具体参与方式。 「AI00」第一期的榜单得到了来自投资界、人工智能产业界和学界的众多反馈，我们也依此对榜单的信息作出了一些订正和调整。在前几期的榜单中，我们先后增加了 Waymo、Neurala、Graphcore、云从科技、Citadel 与 Petuum 、竹间智能、Gamalon 和 DataRobot 等公司，同时也移除了一些公司。 在最新一期的 AI00 榜单中，我们加入了把自动驾驶用于配送的硅谷机器人公司Nuro，一月底该公司 。 公司官网：https://nuro.ai/ 在人工智能热潮下，无数的自动驾驶领域创业公司涌现。近日，硅谷机器人公司 Nuro 引起了业内极大的关注，该公司发布了其第一款产品——Level 4 无人配送车。Nuro 由谷歌自动驾驶团队的前首席工程师朱佳俊和 Dave Ferguson 于 2016 年创立。朱佳俊是谷歌自动驾驶汽车团队首席工程师之一，也是最早的项目创始团队成员之一，负责带领环境感知技术和仿真模拟的研发。Dave 是谷歌自动驾驶汽车团队机器学习和机器视觉技术的负责人，还是自动驾驶汽车业界最早的开拓者之一。 融资情况上，Nuro 已经完成了 A 轮融资。A 轮分为两次，第一次是高榕资本领投，其他参与的国内投资人有网易创始人丁磊、真格基金等。第二次是美国的 Greylock Partners 领投，原有投资机构继续参与。两次一共融资 9200 万美元。 以下为 AI00 一月榜单： 搜狗 中国 人工智能综合研究 搜索引擎、知识图谱、输入法、语音交互产品等 估值约50亿美元 美国 自然语言处理 个性化智能助理 三轮融资共获得 3.4 亿美元 中国 语音技术和自然语言处理 智能家居、车载、电信等行业解决方案 市值约 393 亿人民币 中国 智能语音交互和自然对话 车载、智能家居和智能机器人等智能硬件的语音交互服务 B 轮：2 亿人民币 中 / 美 声源分离、声音增强、声纹识别、麦克风阵列 会议转录，通讯，机器人，智能家居，虚拟现实，增强现实，混合现实 未透露 中国 情感对话机器人、语音情感技术、多模态情感识别 竹间个人助理机器人小影、金融机器人、客服机器人 2500万美元融资 美国 语音识别技术、自然语言处理技术（NLP） 电话语言反馈、预测销售结果、自动信息检索 1400万美元的A轮融资 美国 智能客服 理解和分类用户请求 1200万美元的A轮融资 美国 计算机视觉 图像及视频识别 API B 轮： 3000 万美元 美国 计算机视觉和深度学习 实时面部表情分析和情绪识别解决方案 四轮融资共获得 3372 万美元 新加坡 计算机视觉、视觉搜索、图像识别 电子商务、移动商务、 在线广告等图像识别解决方案 两轮融资共获得 1400 万美元 美国 计算机视觉、深度学习和数据科学 分析海量卫星图像，用于经济趋势分析和公益研究 C轮融资共 5千万美元 美国 计算机视觉和数据科学 将卫星图像识别用于农业、城市规划和灾害响应等 2015 年 5 月融资 1830 万美元 美国 计算机视觉和深度学习 通过 DLFP 平台为农业提供数据分析和预测的解决方案 B轮 3千万美元 中国 计算机视觉和深度学习 人脸识别、危险品识别、行为检测、车辆检测等的安防监控系统 4.1亿美元最新融资 中国 计算机视觉和深度学习 Face++ 人脸识别云服务平台、Image++ 图像识别平台、VisionHacker 移动游戏工作室 C轮4.6亿美元融资 中国 计算机视觉和深度学习 基于图像理解的信息获取和人机交互服务 3.8亿元C轮融资 中国 计算机视觉和深度学习 图像识别、视频鉴黄、智能审核、图片增值等云服务 新一轮千万美元融资 波士顿 深度学习、计算机视觉 帮助机器人和智能设备学习和适应环境的软件 A 轮融资约1400 万美元 中国 人脸识别、计算机视觉 金融机构人脸识别应用、公安系统实时布控、追逃等 B 轮 5亿人民币 公司 美国 深度学习芯片架构 DPU (Dataflow Processing Unit) 3轮融资共5900万美元 美国 人工智能芯片技术 用于机器学习的第二代神经网络软件框架 CDNN2 纳斯达克上市，市值 9.12 亿美元 美国 基于 FPGA，针对于服务器端的高性能深度学习平台 被移动设备直接嵌入的深度学习模块 未透露 中国 深度学习 中国首款神经网络处理器 1亿美元 A 轮融资 中国 深度学习 DPU 平台 深度学习 DPU 平台 A+轮约4000万美元融资 英国 深度学习硬件和软件开发 开源软件框架 Poplar 和「智能处理器」IPU A 轮融资 3000 万美元 Groq 美国 深度学习硬件、芯片 暂无信息 1030 万美元融资 瑞士 机器人及自动化技术 工业机器人、智能设备 468.95 亿美元市值 日本 机器人及自动化技术 工业机器人 4.4兆日元市值 德国 机器人及自动化技术 工业机器人 美的 272 亿美元拿下库卡 94.55% 的股份 美国 机器人、人机交互 智能机器人 1800万美元新一轮融资 丹麦 机器人及自动化技术 工业机器人 未透露 美国 计算机视觉、机器人技术 无人机，软件服务 五轮融资超 1 亿 2 千 600 万美元 日本 可穿戴设备 医疗助理机器人 2083.56 亿日元市值 美国 计算机视觉、机器人技术 电子产品，家用机器人 27亿美元市值 德国 计算机视觉、机器人技术 代步机器人，残障专用智能设备 2000万美元市值 英国 智能机器，自动视觉定位及室内地图构建 清洁机器人 未透露 中国 机器人及自动化技术 工业机器人和行业解决方案 约 289 亿市值 中国 机器人 工业机器人、智能装备和行业解决方案 未透露 中国 计算机视觉、无人机控制、环境及障碍感知、视觉跟随、自动寻路 无人机航拍和图像传输 估值约100亿美元 美国 可以接入机器人的智能模块，工厂、仓库等自动化技术 智能设备 种子轮 700 万美元融资 Arterys 美国 深度学习系统生成医疗图像 深度学习分析系统 Arterys System 1200 万美元 A 轮融资 美国 深度学习、大数据、图像检测 癌症检测系统 三轮融资共 1500 万美元 VoxelCloud（体素科技） 美国 深度学习 医疗影像分析云服务 千万美金 A 轮融资 美国 深度学习和大数据 癌症诊疗 1000万美元最新融资 美国 大数据和机器学习技术 通过数据分析为放射肿瘤学家提供临床决策支持，用于个性化医疗。 两轮融资共 132 万美元 美国 深度学习 药物发现 种子轮 630 万美元 美国 深度学习 通过药物研发平台 DUMA™来评估大型公共和私有数据集，以迅速识别药物，并对药物和疾病的匹配度按照概率进行排序。 种子轮 340 万美元 美国 深度学习 Interrogative Biology® 平台结合病人生物学和人工智能分析来进行药物发现、开发和诊断等。 未透露 美国 机器学习、自然语言处理 拥有 MedxExchange 、MedxInsights 和 MedxCare 三款服务产品的医疗人工智能平台，提供数据、医疗洞见和健康管理服务。 融资 660 万美元 加拿大 深度学习、基因生物学 精准医疗 种子轮 370 万美元 中国 大数据、人工智能 通过数据挖掘和机器分析提供个人性健康指数分析和预测。 A 轮融资近 10 亿人民币 美国 数据挖掘、预测分析 本地分析软件和云服务 五轮融资 4430 万美元 美国 机器学习、自然语言处理、数据挖掘 金融分析辅助决策系统 B 轮融资 5000 万美元（2017 年 2 月）年2 美国 数据挖掘、机器学习 信用服务 四轮共 1.12 亿美元 英国 信用评级 金融产品的信用评级 四轮共 700 万美元 美国 数据挖掘、机器学习 AppBank、金融业务自动化 市值 867.10 亿美元 中国 人工智能、数据挖掘 智能助理、信用评级和风险管理等应用 估值 600 亿美元 Citadel 美国 人工智能、数据挖掘 对冲基金 目前掌管至少 260 亿美元资产 美国 自动驾驶 自动驾驶汽车 谷歌无人驾驶项目开始以独立公司的身份运营 美国 自动驾驶 电动汽车 498.35 亿美元市值 美国 深度学习 自动驾驶汽车 5000 万美元B轮融资 新加坡 / 美国 3D 自动驾驶技术 城市自动驾驶的算法和软件 2轮融资公1960万美元 以色列 智能 3D 传感、传感器融合和精准地图和定位等核心自动驾驶技术 物美价廉的高清晰度固态激光雷达 完成了 A 轮融资，金额高达 900 万美元 美国 雷达和专用短程通信安全系统 自动驾驶卡车 最新一轮6000万美元融资（2017 年 4 月） 美国 计算机视觉、数据挖掘 交通安全和表现的智能解决方案 六轮融资共 1.8 亿美元 美国 全自动驾驶 全新的自动驾驶汽车 3 轮融资共 2.9 亿美元 中国 计算机视觉和深度学习 自动驾驶 5500 万美金的 C 轮融资 中国 计算机视觉、智能驾驶 辅助驾驶系统 A 轮数千万人民币 Argo AI 美国 人工智能和自动驾驶技术发 自动驾驶汽车 福特将持续注资 10 亿美元 美国 人工智能和自动驾驶技术 自动驾驶汽车Level 4 无人配送车 A 轮融资 9200 万美元 英国 利用机器学习和独家算法来检测和响应以前未识别的网络威胁 Darktrace 的核心产品为「企业免疫系统」(EIS) 三轮共融资 1.045 亿美元 美国 自动驾驶、机器学习、数据挖掘 自动驾驶汽车、智能交通和智能出行应用 12 轮融资 87.1 亿美元，估值 660 亿美元 美国 机器学习 开源 AeroSolve 机器学习框架、智能助手、智能推荐、定价 9 轮共融资 20 亿 9 千万美元 美国 云计算、深度学习、数据处理 CRM 解决方案 市值约 638.37 亿美元 美国 机器学习 企业通讯应用，bots 平台 总融资 5.4 亿美元，估值约 38 亿美元 美国 人工智能、大规模分布式计算 解决复杂商业问题的综合智能系统 1.03 亿美元 C 轮融资，三轮共 1.3578 亿美元 美国 数据挖掘 基于社交网络的数据分析服务 1.3 亿美元 D 轮融资。五轮共 1.83 亿美元 美国 认知计算、深度学习、自然语言处理 法务研究智能辅助工具 未透露 中国 自动驾驶、机器学习、数据挖掘 自动驾驶汽车、智能交通和智能出行应用 超55亿美元新一轮融资 （2017 年 4 月） 中国 深度学习、自然语言处理、图像识别 媒体产品的应用 估值约 120 亿美元 中国 基于云端的深度神经网络算法、图像、语音、自然语言理解和运动控制、技术集成 智能机器解决方案 A+ 轮近亿美元 美国 机器学习、数学科学 使用人工智能来预防网络攻击 已融资 1.77 亿美元 美国 机器学习 利用人工智能/机器学习来开发网络安全应用的公司 4 轮共 5360 万美元融资 SparkCognition 美国 机器学习、人工智能、数据分析 使用机器学习和人工智能技术来分析预测网络安全漏洞与系统故障 新一轮3250万美元融资 英国 人工智能基础研究 AlphaGo、医疗健康、谷歌内部产品应用。 以 4 亿英镑（约 5.32 亿美元）被谷歌收购 美国 人工智能基础研究 新的计算机视觉系统，机器人视觉 五轮获得 7200 万美元融资 美国 深度学习 Inkling 脚本语言和集成开发环境 Mastermind 760 万美元A轮融资 日本 深度学习 深度学习操作系统 Chainer，机器学习在物联网的应用 三轮融资共 1730 万美元 美国 深度学习 深度学习企业应用包 SKIL、开源框架 Deeplearning4j 种子轮融资 300 万美元 美国 机器学习 开源机器学习平台和商业化支持 四轮融资 3360 万美元 美国 数据挖掘、机器学习 为铁路、建筑等大行业提供数据预测分析 SaaS 服务 B轮融资5000万美元（2017 年 4 月） 美国 机器学习 为数据科学家提供图像、文本的识别和分析的工具 4轮融资共 438 万美元 中国 机器学习 金融应用和「先知」平台 三大国有银行联合投资金额未公开 美国 机器学习 Bayesian Program Synthesis 可以自行编写代码，用最优的方法解释收集到的数据 来自 DARPA 的 770 万美元投资、来自 Felicis Ventures 的 450 万美元种子轮融资 美国 机器学习 机器学习平台公司，DataRobot 平台上有数百个开源机器学习算法 5400 万美元 C 轮融资 美国 机器学习与人工智能平台 PetuumOS、Poseidon 框架、Petuum Healthcare Solutions 9300万美元B轮融资 美国 算法 类似于苹果 App Store 的「算法应用」商店 1050 万美元 A 轮融资 美国 人工智能综合研究 TensorFlow 等开源框架，Google Photos、Now、Inbox 和搜索等多项产品和服务、硬件 市值 6701 亿美元 美国 人工智能综合研究 多个开源框架和硬件平台，Messenger、社交网络和定向广告等多项产品和服务 市值 4296 亿美元 美国 人工智能综合研究 云服务、Echo 等智能家居、机器人、电商产品应用 市值 4696 亿美元 美国 人工智能综合研究 CNTK 等开源框架，Cortana、小冰等多项产业和服务，硬件 市值 5362 亿美元 美国 人工智能综合研究 Watson、行业认知计算解决方案、量子计算机等 市值 1434 亿美元 美国 人工智能综合研究 基于智能手机等硬件的多项产品和硬件、智能助手、智能家居、医疗等 市值 8067 亿美元 中国 人工智能综合研究 开源框架 PaddlePaddle、百度大脑、自动驾驶、互联网应用 市值 849.78 亿美元 中国 人工智能综合研究 云服务、人工智能平台 DT PAI、电商产品应用 市值3079 亿美元 中国 人工智能综合研究 互联网应用 25683.98亿人名币 美国 人工智能硬件 GPU、深度学习超级计算机 DGX-1、自动驾驶超级计算机 Xavier 市值 832.07 亿美元 美国 人工智能硬件 CPU、Xeon Phi、Nervana 市值 1706.30 亿美元 美国 人工智能硬件 移动智能设备芯片 市值约 899.36 亿美元 美国 全可编程技术和器件 All Programmable FPGA、SoC 和 3D IC 提供商 160.70 亿美元 华为 中国 人工智能综合研究、硬件 人机交互设备应用、芯片等 2017 年以 785.108 亿美元营业收入首次打入《财富》前百强 京东 中国 人工智能综合研究 市值约 649.90 亿美元 「 AI00 开源项目」参与方式： "
271,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737603&idx=5&sn=5f89b7df086f998faec6900c34775194&chksm=871acfbdb06d46ab3960ebf92fd911141dc368150aeaf02f1bb687c21f9c3ef66dec4bc4ab04&scene=27,AAAI 2018 | 腾讯AI Lab提出降秩线性动态系统：可处理有噪声计数值短数据,"AAAI 2018 于 2 月 7 日在美国新奥尔良闭幕，本次会议收录了腾讯 AI Lab 共 12 篇论文。这里我们编译介绍了其中的《降秩线性动态系统（Reduced-Rank Linear Dynamical Systems）》；研究结果表明该方法可以稳健地从长度较短的、有噪声的、有计数值的数据中学习隐含空间。此外，作者也已经在 GitHub 上公布了他们用 Matlab 实现 RRLDS 的代码。 论文地址：http://yuan-gao.net/pdf/AAAI2018.pdf 代码地址：https:// github.com/sheqi/RRLDS 解析高维时间序列的隐含结构是人工智能领域的基本问题之一，并且已经在从社会、经济到生物科学等各种领域得到了广泛的应用（Linderman, Stock, and Adams 2014; She, So, and Chan 2015; She, Chen, and Chan 2016; So et al. 2016; Hein et al. 2016）。在这样的情况中，很多研究和理论都认为高维时间序列是某些基本的、低维的、随时间变化的相关信号的有噪声观察结果（Pfau, Pnevmatikakis, and Paninski 2013; Archer et al. 2014; Sussillo et al. 2016）。人们已经将线性动态系统（LDS）用于从被观察的多元时间序列数据中提取低维的隐含网络结构（Archer et al. 2014; Lakshmanan et al. 2015; Linderman et al. 2017），这能得到观察结果的变化性质，不管是空间上的还是时间上的。 但是，在使用 LDS 来检索最优的隐含网络时，还存在两个主要的难题。第一，已有的模型需要一个预定义的隐含维度。为了保证该模型的能力，它通常会被设置成一个很大的值，这会因为过拟合而导致模型难以建模长度较短的高维时间序列数据。由于很多现实世界场景中都存在长度较短的时间序列数据，所以这个建模问题会带来很多麻烦。比如，在神经科学领域，由于 (i) 某些神经元的寿命短、(ii) 记录材料的有效时间有限以及 (iii) 在动物活动过程中记录电极的微运动，我们无法在实验中得到长序列的高质量神经数据（Spira and Hai 2013）。在临床领域，病人的临床数据的长度通常在 50 以下，因为大多数病人的住院时间都少于两周（Banaee, Ahmed, and Loutfi 2013）。在经济领域，比如国内生产总值和居民消费价格指数等计量经济学的多元时间序列是按季度或年度测量的，这会导致数据长度较短。 第二，真实世界的时间序列数据往往是计数值（而非实值）。标准 LDS 假设观察遵循高斯分布，所以应用标准 LDS 是不可行的（She, So, and Chan 2016）。举几个例子：在神经群中记录到多个尖峰队列（Paninski et al. 2010）、S&P 100 指数上的交易数据（Linderman and Adams 2014）。所以需要对模型进行扩展，以处理这些数据的计数本质。最近，有研究者提出了泊松线性动态系统（PLDS）（Buesing et al. 2014）来进行计数数据建模。但是，泊松假设说明观察结果是均匀离散的，即条件均值和方差是相等的。这就限制了 PLDS 在描述神经尖峰计数上的应用，因为通常观察到的神经尖峰计数要么是过离散的，要么就是欠离散的（方差大于或小于平均）（Churchland et al. 2010; She, Jelfs, and Chan 2016）。如果没有合适的分布能取得计数数据的离散性，那就不能学习到该数据的变化性质，由此就无法推理得到最优的隐含网络。 鉴于存在这些限制，我们提出了一种全新方案，可用于根据长度短的有噪声计数数据推理隐含网络。我们重点关注的是 LDS 的动态矩阵，它表示一个隐含节点对另一个节点的后续活动所施加的影响。换句话说，该动态矩阵可被用于控制隐含网络的节点演进。我们观察到的关键结果是该动态矩阵的秩中的信息包含这些节点的状态空间的固有维度。为了防止 LDS 与给定的长度较短的数据发生过拟合，我们的目标是学习一个紧凑的低秩动态矩阵。 具体来说，我们为动态矩阵构造了两个不同的低秩先验（low-rank prior），即多元拉普拉斯（multivariate Laplacian）和核范数（nuclear norm），它们在检索固有维度上具有相似的表现，并且被广泛应用于不同的场景（Gao and Yuille 2016; Gao, Ma, and Yuille 2017）。此外，为了促进基于计数数据的降秩动态矩阵学习，我们还引入了离散性自适应（DA：dispersion-adaptive）分布并开发了一种全新的、可灵活参数化的观察模型   图 1：根据 (a) 无约束的动态矩阵和 (b) 降秩动态矩阵所重建的隐含轨迹（不同的颜色表示不同的模拟试验）。(b) 中的低维流形更加平滑且构建得更好。 图 1 给出了带有 DA 分布的降秩动态矩阵在根据长度较短的、有噪声计数值时间序列数据恢复低维流形上的优势。观察数据是 40 维（即 40D）的时间序列数据，这是用一个 10D 的动态矩阵建模的（同样的初始状态）。结果表明我们的方法能够从该动态矩阵中成功检索三个固有的维度，从而得到由这个三维曲线表示的更平滑的且构建得更好的流形，而使用无约束动态矩阵的方法则会失败。总的来说，我们的研究有四大贡献： 我们提出通过在动态矩阵上施加两个降秩结构来检索多元时间序列的固有维度。 我们引入了一种计数值的指数族分布（称为 DA 分布）来求取计数数据的离散本质，并且得到了各种常用的分布作为特例。 我们利用了一种隐含的降秩线性动态模型来调节 DA 观测分布的期望，由此构建了一种全新的线性动态系统模型。 通过将当前最佳的方法延展成全新的模型，我们开发了一种变分贝叶斯期望最大化算法（VBEM：Variational Bayes Expectation Maximization）。 我们在模拟数据和真实世界数据的基准方法上对我们的框架进行了评估。出色的表现说明我们的方法：(1) 能够自动减少隐含状态空间的冗余维度，从而防止与大量预定义隐含状态过拟合；(2) 相比于基准方法，能显著提升预测有噪声神经尖峰活动的表现；(3) 能稳健且有效地检索来自两个实验数据集的基础复杂神经系统的固有维度。 降秩结构 为了通过动态矩阵 A 的秩而从 MTS 数据集恢复固有的维度，我们应该选择能够诱导出预期的低秩性质的特定先验。我们有两种引导低秩动态矩阵的选择：(1) 多元拉普拉斯先验，(2) 核范数先验。如表 1 所示：   表 1：动态矩阵的先验选择 离散性自适应（DA）分布   图 2：(a) 函数 w(·) 的不同选择的 DA 分布的均值和方差。当 log w(·) 固定时，增大θ会使均值和方差更大（更黑的点）；(b) 通过参数化 θ 和 w(·) 而得到的 DA 分布的常见计数分布特例。 降秩线性动态系统（RRLDS） 有了两种降秩结构和 DA 分布，现在我们可以将它们与一个隐含的线性动态系统耦合起来。我们将这个系统称为 RRLDS，该系统有利于建模有限的计数数据来检索固有维度。我们将其用于建模在大脑神经元上记录到的时间序列数据（尖峰计数），而且也可以简单直接地将其用于描述和解读其它计数过程的观察值。   图 3：RRLDS 的两个阶段 图 3 展示了 RRLDS 并给出了其两阶段的模型结构：第一个阶段包含在动态矩阵 A 上构造的降秩结构，其控制了隐含状态 xt 的演进。第二个阶段是通过 DA 观察模型将隐含状态 xt 映射到响应 yt 上，这可以学习其离散性质。 推理（E 步骤） 学习（M 步骤）   算法 1：推理和学习的框架（VBEM） 为了证明 DA 的泛化性并验证我们的算法实现，我们首先在广泛的模拟数据上测试了我们的推理和学习方法。然后我们通过在两个神经科学数据集上与当前最佳方法的比较而对预测表现进行了评估。最后我们验证表明我们的方法可以从这些多元时间序列中检索其固有维度，并且比已有的研究成果都强大得多。表 2 列出了在「结果」部分比较的方法的缩写。   表 2：我们的方法与多个基准方法的缩写。ML 表示多元拉普拉斯，NN 表示核范数。alternative LDS 方法包含单纯的 LDS（Ghahramani and Hinton 1996）、PLDS（Buesing, Macke, and Sahani 2012）、SubspaceID（Van Overschee and De Moor 2012）和 StableLDS（Boots, Gordon, and Siddiqi 2007）   图 5：使用 RRLDS-ML（红色三角形）、RRLDS-NN（黄色正方形）和带有泊松观察模型的 LDS（PLDS，紫色叉号）评估的动态矩阵的谱（spectrum）。真实的复杂特征值谱用蓝色圆圈表示。RRLDS-ML 和 RRLDS-NN 方法（没有 DA）的接近于真实特征值，而 PLDS 无法消除冗余的维度。(b) 预测得到的和真实的计数数据的平稳协方差矩阵中的元素的散点图。   图 6：(a) 使用不同的动态矩阵真实秩学习到的 4 个 LDS 的预测对数似然，(b) 使用不同长度的训练数据学习到 4 个秩为 5 的 LDS 的预测对数似然。(a) 和 (b) 中预定义的隐含状态数量都是 10。RRLDS-ML 和 RRLDS-NN 都显著（p<0.001，配对 t 检验）超越了其它方法。   图 7：在 Task #1 中的实验尖峰活动的预测对数似然   图 8：五个模型在神经元尖峰计数（Task #2）上的预测表现。每张小图中的行表示神经元的尖峰序列。颜色表示每个时间步骤所记录/预测的计数值。 论文：降秩线性动态系统（Reduced-Rank Linear Dynamical Systems） 摘要： 线性动态系统（LDS）在研究多变量时间序列的基本模式方面有广泛的应用。这些模型的一个基本假设是高维时间序列可以使用一些基本的、低维的和随时间变化的隐含状态来表征。但是，已有的 LDS 建模方法基本上是学习一个规定了维度的隐含空间。当处理长度较短的高维时间序列数据时，这样的模型会很容易过拟合。我们提出了降秩线性动态系统（RRLDS），可以在模型学习过程中自动检索隐含空间的固有维度。我们观察到的关键是 LDS 的动态矩阵的秩中包含了固有的维度信息，而使用降秩正则化的变分推理最终会得到一个简明的、结构化的且可解释的隐含空间。为了让我们的方法能处理有计数值的数据，我们引入了离散性自适应分布（dispersion-adaptive distribution）来适应这些数据本身具备的过离散性/等离散性/欠离散性。在模拟数据和实验数据上的结果表明我们的模型可以稳健地从长度较短的、有噪声的、有计数值的数据中学习隐含空间，并且还显著超越了当前最佳的方法的预测表现。 "
272,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737657&idx=1&sn=82dc3c5a415156481d8b927e464da91a&chksm=871acf87b06d46911a64d008713edabedbc27b585fd49563e80515435a687647b716c639c60c&scene=27,让AI自行编写程序：神经程序合成近期研究进展综述,"让计算机程序自动生成新的程序看起来是一个非常「人工智能」的概念，事实上，它也是众多 AI 研究者们努力的方向。2017 年以来，学界就出现了 、Bloomberg 和英特尔的  、谷歌大脑的 等程序生成方法。近日，来自 UC Berkeley 的 Neel Kant 对于近期神经网络程序生成的发展进行了概述。 论文链接：https://arxiv.org/abs/1802.02353 作为一个总体领域，程序合成可以被定义为发展一个满足一系列要求与限制的算法。因为限制条件是被用来确定正确性的标准，所以限制条件是用来定义这个算法的。这些条件可能包括了如速度，空间复杂性还有输入输出正确与否等等运行时间内性能。对于计算机科学系的学生来说，这个概念很熟悉，考试中会经常需要直接写下三元快速排列的代码或者填充不全一段算法的代码。 程序合成有很大的市场。成功的程序在未来可以操控现在人的工作：计算机编程。想象一个可以不用人来调试，重构，转化及整合的世界。甚至可能会出现电脑程序本身不能直接解决问题，但可以提出编程问题并加以解决的世界。在数学和物理学科里，理论证明是需要人来根据已经存在的定理来产生新视角的例子。一个完整的程序合成系统可以跑一个程序来证明或推翻同样的预测，然后这个任务可以被看成非常创新。当计算机视觉瞄准自动化一个生物复杂精细的感知系统，系统合成就是一个瞄准解决问题，逻辑和自动化它自己的领域。 这些非常强大的程序合成系统的应用使这一领域变得非常让人着迷，但是它可能会花费几十年的研究才能达到我们之前所展望的程度。相似的，深度学习已经得到了非常大的关注，而且已经被当成一种重要工具在每一种认知任务中被尝试使用。这篇文章会总结一下这两个领域最近的突破。我们可以看到，这里成就及挑战并存，我们应该谦虚的去追求我们的目标。 摘要： 近年来，深度学习在很多领域中取得了巨大成功，这使得研究者开始考虑智能系统是否能够解决人类近期才开始考虑的问题：程序合成。这项挑战与目标识别和语音翻译不同，因为其抽象本质和对严谨性的高要求对人类来说都很困难。尽管神经程序合成技术距离解决该问题还很遥远，或者与大多数现有方法相比也不具备太大竞争力，但是它发展很快，深入了解之后就会发现其具备巨大前景。本论文首先探讨程序合成的问题陈述和挑战。然后，我们回顾程序归纳模型的发展历程，以及它们的成败和再发展。最后，我们对比了程序合成的不同研究，并介绍了该领域未来可能有潜力的推荐研究方向。 研究者提出多种模型范式和架构来解决神经程序归纳中的多种挑战。尽管模型倾向于共享使之易于分类的某些基本属性，但它们在细节方面还有更复杂的结构。这种特殊性可用于解释为什么某些模型可以成功处理一项任务，而其他模型不行。这些细节还提示哪些属性是通用的、哪些属性可以高效解决神经程序归纳任务。 2.1 简介：循环模型 循环神经网络（RNN）因其与序列数据的直观匹配而独树一帜，它们还天然匹配编程任务，因为程序归纳中的输入和输出的规模是可变的。如果任务是程序合成，则输出规模无法根据输入来推断，因此一种自然的计算方法就是一次生成一个输出 token，输出过程中不断更新内部状态。一些任务还可以建立在自然语言输入之上，这样多个 RNN 就可以一起工作。正如我们所见，一些模型使用外部存储资源得到增强，这样 RNN 就可以在不同的时间步发送请求。 2.2 卷积循环 一种非常成功的神经程序归纳模型是神经 GPU[15]，它是一种环路，但在每一个「时间步」中都涉及一个门控卷积运算。输入数据首先嵌入到被称为模型「内部状态」的 3D 张量中，它是 RNN 的隐藏状态的模拟。在时间步 t 上，模型的门控卷积运算通过卷积门控循环单元（Convolutional Gated Recurrent Unit，CGRU）运转。这种机制与 GRU 几乎相同，除了向量上的矩阵乘法被 3D 张量的卷积运算取代。 重要的是，这里有一个「更新」和「重置」门，可用于保存长期记忆信息，就像 LSTM 一样。CGRU 的输出与输入的形态总是相同的。在时间步 t+1 上，神经 GPI 使用的是与 CGRU（t）相同的 CGRU 操作，所以既有卷积也有循环。重点在于，对于某些输入规模 n，神经 GPU 只简单地应用了 n 次 CGRU 操作。在这些运算之后，模型的内部状态被解码以生成程序输出。如果模型在小 n 上能成功运行，则我们希望它在更大的问题规模上通过重复迭代运算也能有效。 卷积循环是一个聪明的想法，但神经 GPU 在实现上会有一些困难。在时间步 t 上，有关问题在所有时间步 1、2……t-1 的信息都会存储在内部状态中，并且我们无法分别观察之前的内部状态。这意味着模型的内部信息包含了处理下一时间步的所有必要信息。 2.3 注意力和指针网络 上一节讨论的问题可以用注意力机制解决。注意力机制很有用，它可以令解码器没有任何瓶颈的情况下访问每个编码器状态的信息。文献 [7] 中的神经机器翻译技术在构成输出时采用这种机制独立地处理每个输入标记。然而，更广义地说，注意力分布可以看成生成非负的归一化分布，例如标准的概率分布。在编码器-解码器范式中，这意味着解码器可以选择性地访问编码器状态，并选择其中最有用的进行处理。在程序归纳和合成中，注意力还可以用于放宽离散运算，例如选择。 指针网络（Pointer Networks，Ptr-Nets）[9] 可能是对注意力机制的最直接的非平凡应用，其将注意力机制的 RNN 应用到了神经程序合成中。Ptr-Nets 中涉及的运算总结如下： 按序馈送输入到模型中，作为编码器。编码器的隐藏状态 e_1 . . . e_k 被全部记录。 一旦输入完成，模型将按序对 {e_1 . . . e_k, d_t} 生成注意力分布，其中 d_t 是当前的解码器隐藏状态。这些分布在训练过程中作为软选择，并且所有分布都可以用于损失函数和梯度下降中。 重要的是，Ptr-Nets 可以求解比训练时使用的输入规模更大的问题。这是泛化能力的直接标志，但很容易推出，随着输入规模量级的增加，模型的性能将大幅降低。这归咎于神经程序归纳模型的存在症结。虽然这些模型可以归纳和训练时使用的输入规模相当的程序，但几乎不能保证其泛化到更大规模程序以及极端情况的性能。 2.4 注意力结合记忆机制 神经图灵机（Neural Turing Machine，NTM）[4] 引入了用外部记忆增强神经网络的概念。实际上，此时神经网络将作为一个控制器，执行记忆的读取和写入命令，而不是用其自身的参数作为记忆的主体。外部记忆的用矩阵 M∈ R^m×n 表示： 基于内容求解：对一个读取的向量 v∈ R^n，返回结果 u∈ R^m，其中 u 表示每个内存槽内容和读取向量的相似度。 基于定位求解：读取/写入向量 v∈ R^m 表示被读取/写入的关于记忆索引的注意力分布。 这两个机制非常简单优雅。在每个时间步上，NTM 采用输入并生成读取和写入的命令，将 RNN 控制器的输出与记忆进行交互。然而，对比 Ptr-Nets，该论文中展示的结果是非常初级的，特别是在问题规模方面。而任务本身对于算法也不复杂。尽管更加灵活，且表达力更强，但是 NTM 的训练难度要高得多。 Graves 等人采用了 NTM 的思想，进一步提出了可微神经计算机（Differentiable Neural Computer，DNC）[14]。DNC 可以用多个读写头训练，并有额外关于它的记忆的数据。它有两种特别性质： 时间连接（Temporal Linkages）：关于记忆写入顺序的相关信息。它能允许控制器推理数据之间的关系，因为时间连接在算法中是很常用的。 内存利用（Memory Usage）：关于记忆的索引是否包含有用信息的信息。理论上，它应该能通知和简化控制器对读取和写入的选择。 DNC 仍然使用 RNN 控制器作为其主核心，使用基于注意力的寻址技术。但是，和 NTM 类似，与更简单的模型如 Pointer Nets 相比，DNC 似乎更难执行高效训练。 2.5 内存和预定义基元 之前的所有模型从设计上看都是联结主义。现在，我们将尝试两个模型：神经编程器 [17] 和 Neural RAM [16]，仅使用明确定义的数据变换。具体来说，控制器不向内存发送直接可读／写的指令，而是对数据执行一种可能的一元／二元运算，如基本算术（如加、乘）、逻辑运算符（如相等比较、小于）和聚合运算符（最小、最大）。这两种模型可以能够运行的时间步可多于仅描述问题的必要时间步数。这使得模型能够组合这些基本运算，创建复杂程序。 具体来说，典型的组件有： RNN 控制器：使用来自（a）控制器外部和／或（b）存储单元的序列输入，自动以嵌入格式出现。 习得的函数可以在（a）可执行的预定义运算和（b）可执行运算的数据上生成注意力分布。 可读写数据的存储单元。这还可以作为指定输出位置（Neural RAM）。 两种模型存在一些有趣的差异。神经编程器设计为使用自然语言输入。当然，它们通过 RNN 控制器变成向量化的嵌入格式，但是该模型仍然需要学习英语的语义。类似地，神经编程器具备使之在存储上执行数据库类型运算的模块，该模型可从数据库中返回多个元素。从这个层面上看，神经编程器是为了成为一个自动问答系统，学习回答问题所需的潜在程序。因此，解决方案可能是综合的，但是比问题中存在表征的情况需要的步骤要少一些。 而 Neural RAM 专门创建高度综合的程序。由于该模型的 14 个预定义模块都是原子性的，因此这些程序可以使用数百个时间步来完成。 好消息是与 NTM 和 DNC 相比，Neural RAM 可创建连贯的程序，可运行更多的时间步。这可能是因为运算并不模糊，即使使用变化比重的注意力来选择运算。当使用误差反向传播进行监督时，该模型理论上知道其中一个基元运算是正确的，无需仅依赖于写入的存储（NTM、DNC 的情况）。该模型还可以选择何时使用 sigmoid 的末尾单元终止程序，何时超出阈值，何时完全停止模型。需要注意的是，与 NTM 相比，任务本身没有那么复杂，Neural RAM 甚至无法尝试 DNC 比较擅长的图形问题。 2.6 函数层级（Function Hierarchy） 在某些情况下，监督信号会非常弱，因此训练数据很容易过拟合。神经编程解释器（Neural Programmer-Interpreter，NPI）[19] 试图通过增加模型的复杂度和监督强度解决这些问题。其中非常重要的进步是令函数在新的堆栈帧中灵活调用子函数。这可以在新的帧中通过将 RNN 控制器的隐藏状态重置为零，将给定嵌入程序、参数和环境作为输入来实现。 这些堆栈帧可以终止，并通过 一个 Sigmoid 末端控制返回调用帧（caller frame），和 Neural RAM 一样。但子函数结束时，调用子函数之前的控制器隐藏状态会被储存起来。当顶层函数结束时，整个模型将结束执行。 核心概念是 NPI 可以抽象和更高阶地控制该程序。当新的函数和参数被调用时，它可以被表达成一个嵌入格式的向量。这个嵌入格式的向量通过联合在一起的参数嵌入、整体环境和通知子函数未来调用的语境构建而成。 3 归纳任务上的模型性能分析 这部分介绍了四个难度递增的例子。难度来自于所需的控制流级别、目标程序运行时复杂度（如果存在一个人造程序），和训练数据的可用性。 3.1 算术 我们可以比较一下 Neural GPU、NPI 与 NPL，这些模型已经用加法任务测试过了，并且在同样的问题大小环境下完美地进行小数加法。这些模型也实现了函数层级，因此它们的泛化性能更加可靠。 3.2 表逻辑 NTM 是第一个试着搞定这一问题的新型神经程序归纳模型。它能完成复制并排序长度为 20 的列表，这一成就在程序合成社区中非常引人瞩目。 Neural RAM 可以复制、融合、交换、倒置一个长度为 50 的列表。 3.3 组合优化 组合优化是一种需要用离散目标组合起来的优化函数问题，并且解决方案是有限制的。该问题首先用指针网络进行测试，测试表明指针网络中的注意力机制对于该问题设置很有用。 3.4 语义查询解析 最后，该论文还展示了一些可能重塑神经编程归纳模型的策略，若读者希望了解这一部分的内容，请详细查阅原论文中的第四部分。 结构化注意力机制 分层记忆 允许递归（Enabling Recursion） 贪婪算法  "
273,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737603&idx=3&sn=c4ef7909d0e5ebb80dc63871d5b2e245&chksm=871acfbdb06d46ab10d59fcb95e4c13788e5719febf67a137efb3e76f747cd5e871358945fb7&scene=27,业界 | Udacity飞行车课程报名启动，我们和负责人聊了聊飞行车的现在和未来,"为什么 Udacity 选择在这个时间点推出飞行车课程？拿到飞行车纳米学位的人才会有什么样的出路？目前飞行车行业的发展状况究竟如何？这篇文章给你答案。 自动驾驶技术还没有大规模普及，飞行车就已经迫不及待地要进入到公众的视野。1 月 23 日，Udacity 推出的飞行车纳米学位正式上线，成为了全球第一门针对自主飞行和飞行汽车领域的网络教学课程。 这门课程将会推出两个学期：第一学期围绕基本的空中机器人学（Aerial Robotics），会传授设计和开发飞行机器人所需的核心概念。入学者将使用四旋翼测试平台和定制飞行模拟器来实施规划，控制和估算解决方案。第二学期是基于前一个学期的进阶版，包括整个智能空气系统的全栈。入学者将了解固定翼飞机并优化「飞行长度」任务，协调整个飞行车队。最后，完成一个完整的「飞行城市」测试。 由于是首次开设这样的课程，Udacity 除了让创始人、无人车之父 Sebastian Thrun 亲自把关以外，还邀请了包括来自麻省理工大学航空和航天学院教授 Nicholas Roy（同样也是以及计算机科学和人工智能学院的副教授）、多伦多大学航空航天学院教授 Angela Schoellig、苏黎世联邦理工学院动力系统和控制系统教授 Raffaello D'Andrea 等学术大牛亲自教学。 两门课程目前的价格分别是 899 美元和 1200 美元，每个学期有三个月时间，每周学习时间预计为 15 个小时。Udacity 对入学者有一定要求，需要良好的编程能力和基本的数学物理功底，但是过去进修过 Udacity 自动驾驶和机器人课程的毕业生可以直接通过。 随着近五年来人工智能技术、无人机（Drone）技术和自动驾驶技术的迅猛发展，飞行车已经成为行业内热议的未来出行方式之一。但是，相比于业内公认的无人车将在 2020 年左右出现大规模的商用产品和服务，飞行车在商业上的承诺可能还要再往后推，这也引起了不少人的好奇：为什么 Udacity 选择在这个时间点推出飞行车课程？拿到飞行车纳米学位的人才会有什么样的出路？目前飞行车行业的发展状况究竟如何？ 带着这些问题，机器之能采访了 Udacity 飞行车课程的项目负责人、同时也是多个飞行车公司的咨询顾问 Jake Lussier，探讨有关飞行车课程和整个行业的商业化问题。 关于课程 上完这门课程的学生最终可以学会什么技能？ 你所看到的课程分为两个学期：第一个学期是空中机器人（ ），第二个学期我们称之为智能空中系统（Intelligent Air Sytem），所以，实质上我们是培养我们的学生成为自主飞行的任何领域以及飞行汽车领域都非常杰出的软件工程师。 第一个学期的空中机器人可以教会学生开发任何一种机器人所需的基础知识，但是内容会聚焦在空中，有规划、控制、预测这些环节。这些是你可能想到的出现在机器人课程中的概念，只不过会在无人机上进行模拟。学生将有机会将他们的代码移植到实际的无人驾驶飞机上，这些内容为无人飞行奠定了基础。 第二学期，我们更多地进入具体的飞行汽车领域。在第二学期的第一部分，我们将讨论固定翼飞行，同时会谈论一些混合动力设计。飞行汽车的设计并没有真正地集中在一个设计点上，有四轴飞行器、飞机元素以及其他的元素。我们首先介绍这些固定翼设计，以及它与第一学期中学到的内容的关系。 接下来我们再来看看这种飞行车的系统，比如控制问题如何变化，稳定性如何，然后我们进入飞行车队。如果你想在多个地方提供服务，就要考虑如何优化这些服务、如何有效地实现这一目标、以及如何整合船队、如何协调这些服务，以便他们能够高效地运作。要知道，协调包括哪怕程序崩溃还要使一切安全地工作，并与中央控制相连接，这就是课程的总体轨迹。所以在每一个阶段，学生不仅仅是在学习课堂，而且还要在课堂用 Python 进行编码，并得到即时的反馈，然后将这些代码翻译成 C++。写代码一开始只是为了理解这个概念，之后为飞行车准备的代码就是你实际上可能放在无人机上的东西。 回答你的问题，学生实际上从中学到了什么？他们理解所有这些概念以及实施这些概念的能力。对于现在这个行业来说，他们可以进入并带来自己的技术和自主权，而且不单单是飞行车，而是在整个飞速发展的飞行车行业，成为真正的领导者。 我认为你可以看到不同程度的特异性。在最高层次上，这些学生将非常熟练地掌握技巧和经验，建立不仅智能而且非常可靠且鲁棒的自主系统。 如果你在网上进行图像分类，你将熟悉训练算法来做预测。如果你弄错了，这也不是一个大问题。但是在飞行中，如果你弄错了，这是一个很大的问题。因为这不只是一个简单的算法，你必须有一个全局考量。我们所有的毕业生都应该有这个技能，这有着相当广泛的适用。 他们可以通过这些技能找到很多角色，我认为他们甚至可以在金融行业找到工作，在那里，你必须真正了解你的预测指标。但在具体的飞行车中，我认为现有的行业就已经有很多机会，大多数飞机在天空上的大部分时间都是自主飞行的。真正有人类参与的部分只有起飞和着陆，即使在这些部分，技术也相当有用，有很多公司需要这些技能。 那么正如我之前提到的那样，我们认为规模较小但迅速增长的市场是他们有机会担任领导职位并真正建立这些体系的地方。无人机已经是一个蓬勃发展的行业，增长非常迅速，我们认为飞行车领域虽然现在有点小，但也会有类似的增长模式。 这个课程的前提条件是，它需要一些在编程语言上相当好的经验。所以你需要习惯编写代码，最好是使用 Python 、 C++，但是如果你对另一种语言非常熟悉，而且你愿意学习 Python 或 C++，那么你也可以学习这门课程。你应该对一些更高级的数学有一个基本的了解，对线性代数概率统计有一个基本的了解，稍微知道一下物理学，因为我们有覆盖空气动力学。这些都是一些先决条件。所以如果你还没有从其他课程毕业，但你有这些技能，然后在我们的录取过程中，你仔细说明了你的资格，我们会审查每个入学申请，并确保你有必要的技能。 申请人只需要填写表格里的相关信息就可以，我们会进行核实。 是的，他们必须依次上，虽然第二个学期更多的是关于这些系统更多的飞行汽车，第一个学习将涉及所有的基础。这也将让你熟悉我们的模拟环境。它会教你如何把你的代码应用在飞行车上。所以，如果你跳过第一学期，你将不会有这个基础，所以我们要求你做第一个学期。 我认为理想的目标之一是软件工程师，他们一般都会对进入自主系统感到兴奋。当然，如果有飞行爱好者是更好。所以任何真正有能力编码的人，我认为都是一个很好的目标。另外一个可能的就是在航空航天方面具有良好编码能力的人，他们也可以从空域的角度来进一步了解软件方面的问题。我认为这些都是非常有吸引力的目标。 我会说比起其他 Udacity nano-degree，比如自动驾驶，强度是差不多的。我们是真的在给我们的学生们建议，并且帮助他们拥有一定的时间完成自己的工作量。我们知道，很多学生都有工作，这个课程将在下班时间内被完成，但是我们可以尽可能地提供所有的支持。我们组织同龄人，试图让他们与社区沟通，我认为这就是在网上阅读一些文章和真正从课程编码学习以及和你的同学交流之间的差异。 Udacity 更高层次的任务是提供终身学习。我们试图提供一个学习的经验，这将有助于保持你的专业技能保持最新，你就可以继续在事业上取得成功和成长。通常在我们的所有课程中，都会有一些行业内重要的高增长领域，如果学生在这些领域获得技能，他们将在事业上取得成功。比如自动驾驶，在硅谷我们看到这是一个蓬勃发展的行业，拥有这些技能的工程师现在做得非常好，而且他们未来只会做得更好。所以一辆飞行汽车听起来可能非常遥远，但是它与现有的航空业和盛行的无人机行业已经有了相同的模式。对于现在有工作能力和自主能力的学生来说，这是一个很好的地方，我们看到现在飞行中有很多活动，而且在接下来的几年里，这个活动真的在加速。所以现在学这个东西的学生在未来几年真的会很受欢迎。 我认为思考这三个定位的方法是：机器人学是一个通用的机器人课程。自动驾驶和飞行车的专业化程度不同，我认为它们的区别在于，自动驾驶汽车在杂乱无章的随机环境中需要非常基本的感知问题，所以你会发现计算机视觉和深度学习在这里被大量使用。而在飞行车系统中，天上没有那么多障碍物，所以要侧重在控制和计划上面。 在第二学期，我们讨论的是整个协调系统。对于自动驾驶来说，你需要从 a 到 b，确保一切都很好，并保持自己的道路规划。车联技术实际上很难实现，因为你不是从头开始。而在飞行汽车领域，你有更多的机会来设计整个系统，所以有一些飞行车组件之间会相互交谈，并与集中管理系统交谈，针对非常不同的问题提出的不同解决方案。 暂时没有，但是其他的课程有很多。现有的 Slack 社区很多的学生都非常感兴趣，我们会寻求并鼓励线下合作，组织学生的事情，也鼓励他们见面协助。而在自动驾驶的课程上，我们会举办招聘活动，让 30、40 个招聘合作伙伴来面见我们的毕业生。 我不会说它必须是以美国为重点的创新和飞行车的课程。我觉得有些国家甚至有更先进的监管环境。当我想到更多现存的飞行车初创公司的时候，美国是我第一个想到的，德国也有一些，还有中国的亿航。我认为这是非常国际化的努力，世界各地的学生基本上都会有我之前描述的这样机会, 包括中国，我认为在飞行车领域已经有不少令人兴奋的成长型公司。 我们所有的课程都是为了让学生可以向期待的职业方向发展。如果他们现在是在一家公司，而且是一家航空航天公司，现在我们要去了解他们需要做什么才能让公司做得更好。如果他们是一个前端或者全栈软件开发人员，他们对自动驾驶系统感兴趣，我们将试图在那里满足他们，让他们成为工程师，去自动驾驶或飞行车或任何他们希望进入的行业。如果他们在这个领域，而且他们希望发起创业，我想这个课程也会给他们很多实际的经验和领导技能，并且与行业领导者进行联系。我们不应该规定我们认为他们应该追求哪条道路，但我们一定会尽全力帮助他们追求任何他们想要的道路。 我认为很多在课程中教授的技术在这些行业中具有相当广泛的适用性和重要性。一些主要的航空航天公司已经拥有自主的系统部门，他们已经将其用于 UAV 和无人机以及所有这些事情上。除此之外，Udacity 还有 100 多个招聘合作伙伴，其中幸运马丁已经是 Udacity 的招聘合作伙伴。飞行车领域，现在主要是 Sebastian Thrun 创立的 Kitty Hwak 作为招聘合作伙伴，我们希望在未来几个月建立更多的伙伴关系。 课程申请在 1 月 23 日开放，直到 2 月 7 日结束。我不能确切地分享有多少申请，但我可以分享的是通常我们会以几百学生的数量作为一期，第一期将在二月末开始。 现在飞行车领域的玩家主要有哪些？ 首先是初创公司。例如由 Sebastian 创立的 KittyHawk。现在在迪拜部署空中的士的 Volocopter。德国有 Lilium ，中国有亿航。Terrafugia 是另一个，还有极光飞行科学公司（Aurora Flight Sciences）。还有更多，这不是一个详尽的名单。 同时也有很多大公司正在自己内部开发或者正在开发收购。如果我没有弄错，那么极光飞行科学已经被收购了。而且我认为 Air Bus 可能也已经被收购了，沃尔沃收购了 Terrafugia。 还有就是这个行业的服务提供商，例如就在上个星期，Iris Automation 完成了 A 轮融资，他们使用计算机视觉和 AI 来避免碰撞。这只是一种技术，但是越来越多的公司并不试图从头开始建造飞行器，而是提供该系统的组件。 此外，还有 Uber Elevate 等，这是 Uber 在飞行车领域作出的努力，他们将航班作为未来城市交通的一部分，正在创建 Uber Elevate Network，让任何一个飞行汽车制造商进入他们的车辆。 得出这样一个分数是很困难的。所以我想我们可以谈论一下，市场是多么成熟，它是多少的增长市场。 我认为，一个非常类似的情况发生在汽车行业，那里有非常成熟的汽车市场，而且越来越多地使用 AI。例如，如果你是一个正在求职的工程师，现在汽车公司有一个很大的市场，尤其是一个非常快速增长的自动驾驶。虽然汽车还没有被广泛地应用自动驾驶技术，但是在这个领域里有很多的潜力。航空航天市场也是一样的，我们有一个成熟的航空航天市场，还有一个规模较小但飞速发展的无人机和飞行车行业。 另外，五年前的自动驾驶汽车正处在技术开始被认真对待的时代，虽然还是一个很垂直的行业，但是增长迅速。现在的飞行车也处于类似的位置，我们处于早期阶段，但将会是高速增长。飞行车相比自动驾驶汽车方面有着非常好的优势，而自动驾驶已经为飞行汽车的许多进步方面，在技术上和社会效应上都铺好了路。因为有了自动驾驶，人工智能，导航，制图，协调等方面技术都有了很大的进步。我们认为未来一两年，飞行车将出现大量的增长，接下来的五年将会有更多的大规模应用。 令人惊讶的是，基本的空气动力学知识、硬件组件基本上都在那里。低级软件大多在那里，所以实际上飞行汽车并不是很难。在某些方面，他们比自动驾驶更容易些，因为他们不必处理所有的障碍物或者是从路上突然冒出来的人。我认为这给了我们信心，这实际上不会花费那么长时间才能发展行业，因为技术在很大程度上存在。 剩下的挑战是那些能够胜任计算机科学领域的航空航天工程师，这就是为什么我们现在正试图提供这个课程，也就是说，我们传统上有一群航空航天工程师对计算不太舒服，习惯对产品进行较慢的迭代。另一边我们有计算机科学家，他们可能更习惯于在网络上运行机器学习，而不是在现实世界的机器人系统上运行。我知道在和 KittyHawk 交谈时，他们也在试图找到能够熟练掌握这些技能的工程师时遇到了问题。所以这个项目的一个重要目标是赋予人们这些技能，因为这实际上是这个行业发展的限制之一。一旦你有这两个厉害的工程师，不仅要让一辆车稳定地飞行，还要有一大堆车辆在天空中每小时行驶一百二十英里，还要有一个超级锁定安全协调体系。这种更高层次的系统思考和系统优化，今天仍然是一个挑战。 整个飞行汽车的问题，在某些方面更容易。但我不是说它是个简单的问题，这个问题依然很难。但是，如果你从单个车辆的角度来看待它，从 a 到 b，你不看系统。实际上，自动驾驶的难度更大，所以他们有他独特的挑战。关于飞行车的好处，他们的技术挑战是非常可解决的，随着工程技术人才的增加将能够有效地做到这一点。如果你想拥有一个高效快速的飞行车网络，这个系统没有任何固有的限制。只要我们完成了工程，我们就可以到达那里。但自动驾驶是被限制在地球表面，这就是一个天然的瓶颈，在天空中不存在这样的问题，随着交通量的增加，你会爱上一个事实：天上有着很多空间。 机器学习可以在大部分过程中使用。但因为纯黑盒子的思维。机器学习在很多飞行上是不允许的，所以在低级别的控制上，我们不能只是得到一些训练集、训练一个模型、然后希望性能足够好。我们要求有一个物理模型，车辆的性能遵循这个模型，并且以自己的方式预期。 但是，很多这些模型都有参数，可以让我们测量车辆的功效，无论何时，如果你有这样的设置、你有参数、你有一个错误的功能，这就需要用到机器学习，你可以根据对完成度的度量来优化这些参数。就算是低层次的部分，也许不是每秒运行一千次的控制器，而是更高层次的控制器，我们使用机器学习来更新参数。 当我们看到整个系统的优化时，这是一个问题，你可以把它看作是 Uber 优化问题的一种。你有一套资源，你有一些你想要最大化的目标，然后你使用你的数据来尽量减少一些错误函数。在这个飞行车的各个层面上，我们可以使用机器学习和 AI 来做比我们在简单物理模型中使用简单程序更好的方法。 也不能这么说。任何有参数的地方，我们都可以基本上估计这些参数。对于你车辆的参数，为了识别代表你的车辆的模型，当你得到大量的数据，你可以估计这些参数。 例如，你有一个飞行的汽车，你有一个参数是车辆的重量。这实际上是一个动态的参数，因为即使你测量他们，在人们进入之前，人们走出去，重量就会改变。所以你需要估计这些参数，这是当你有数据并且使用它的时候你会做得更好的东西。所以它已经是这样了，然后在整个系统优化中，也有很多模拟工作，你不只是从代码到飞机做模拟的东西。学生们可以根据模拟的性能优化车辆的实际设计，寻找新的机会。这些都是没有广泛使用的例子，我认为很多航空航天公司可能就只有数十名工程师在做这些事情，但我想在未来会看到更多的东西。 安全是我们最关心的，并且会真正影响公众对这项技术的看法。它需要实现一个几乎可能安全的方式，同时也有一个很好的过往记录。对于这个行业来说重要的是我们为客户提供真正价值的案例。当我们证明价值时，人们会以一种安全的方式去欣赏它，他们会相信它。我认为安全可能是我们现在确实需要确定的第一问题。 我认为你所说的其他方法绝对是，当你把飞行车推上天空的时候，需要有效地做到的点。如果它真的是很重的负载，那么就需要耗费大量的能量和电池。所以我们需要解决那些可以利用可用电池技术的解决方案，同时满足客户价值的问题，所以我今天就会说，如果我们有车辆比现有的方法更快速，更可靠，更安全，那么你可以用今天的电池做到这一点。 正如我刚才在前面提到的那样，一个非常重要的事情就是我们开发这种技术，以此来显示它真正的价值，所以每当你推出一个新产品的时候就有一个产品存在着市场风险，存在技术风险，所以，有了这个真正的新技术，那么社区开始建立解决方案来解决有意义的问题是非常重要的。 从技术方面来说，我认为显然是最重要的安全问题。实际上运用创新的方法，有多层次的冗余，与监管机构密切合作，努力争取公众的信任，我认为这实际上将是一个巨大的差异化，然后还有其他一些问题，车辆的不同形态，如何看待他们的设计，你的设计需要有意地被选择用于你正在处理的场景，如果你正在处理一个你不到五英里的城市交通问题，那么相比六十英里行程设计是非常不同的。 当我们谈论无人机的时候，我们可以把四旋翼当作旋翼飞机的东西，直升机就像一个单一的螺旋桨。 所以首先，我们需要就定义达成一致。我认为飞行汽车是一个术语，是一个广泛的易于使用、非常安全可靠的飞机。乘客每天都可以使用它来做非常常规的用例。说到这个问题，我认为就无人飞行而言，这又取决于你想要做什么。如果你只是运输一个物件，那么我认为很多这些小型四轴飞行器或者小型固定翼无人机很有用。一旦你开始起身运送人类，就需要考虑旋翼机与固定翼。 这完全取决于不同的场景。我觉得亿航的设计很合理，还有 Volocopter。他们都使用旋翼飞机，他们明确的目的是解决在一个极其拥挤的城市内出行的问题。我认为如果你从这里（山景城）旅行到旧金山，有固定翼和螺旋桨的话，这样做会更有意义，因为在那么长的时间里，使用旋翼机实在是效率低下。你想利用空气动力学和固定翼飞行的效率。 基本上就像飞行汽车一样，我们想让它变得非常方便和安全。这取决于乘客想要做的出行方式，然后我们再使用那种模式。 我们今天在地面上移动的大多数方式，我们都能在空中重新构想。 有一些是特别引人注目的。因为时间是非常宝贵的，或者地面上的道路要么困难，要么堵塞，要么困难，那就是飞行的重大意义。我前面提到的是城市之间的，如果你的时间真的很有价值，而且一开始如果价格更昂贵的话，你依然会更喜欢那些速度更快的解决方案，这种解决方案同样也适用于紧急援助。这是时间到了本质的情况。 还有生态旅游的机会。Kitty Hawk 就是作为休闲用，也有很大的机会，如果你想考虑移动物件，那么也许你不是在谈论在那里驾驶汽车，而是在谈论无人机。这就是我们在这门课程中必须要教的内容。所以，任何地方的任何事情，你可以想象在空中这样做，特别是当我们建立技术和电池的进步，那么我认为这将越来越引人注目。 这是一个相当困难的预测问题，我想我们在接下来的几年肯定会有很多的增长。在接下来的五年里，我们可以期待看到一些乘客采用的商业版本。就我们大部分城市之间的交通运输而言，这可能是一个更难预测的问题，我甚至不会去尝试。 "
274,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737603&idx=2&sn=d218df2c4c6c06f329cb67da4a291dd8&chksm=871acfbdb06d46aba9fbf964eab22f3439097998671ecbb33027aacef5fde9f0c66c1b63496f&scene=27,业界 | 银行AI转型之路：站在巨人的肩膀上 ——对话平安首席科学家肖京,"这是一个 AI 无处不在的时代，过去一年 AlphaGo 战胜世界第一棋手的余波还未平息，人工智能正以前所未有的速度渗透到人们生活的方方面面，并成为各国提升国家竞争力、维护国家安全的重大战略。如果用「人工智能会吃掉世界」来形容人工智能的影响力，也许在不远的将来，显得并不夸张。作为最重要的智能化场景应用之一，银行迈向智能化方向转型的道路上，究竟需要具备怎样的能力？成为不少银行既想与时俱进实现智能化转型，却又在实际建设中遭遇不少瓶颈问题。 对专门 给中小银行 提供金融科技服务的金融壹账通来说，AI 如何真正帮助中小银行实现「三提两降」？带着这些问题的思考，笔者探访了平安集团科技的领头人——肖京博士。温文尔雅的学者风范，没有深谈 AI 技术，却在如火如荼的 AI 浪潮中，保持清醒的认知。让我们跟着平安 AI 缔造者，一起探究银行智能化转型的路径。   平安集团首席科学家肖京 问：最近关注了一则新闻，工商、中行、建行共同投资一家帮助企业建立自主 AI 开发能力的企业。实际调研中发现，中小银行已经在规划和布局，但有不少还是在探索与寻找方向。银行做智能化的前提是什么？需要具备怎样的能力？ 肖京 ：关键是需要从顶层设计开始，做好规划。我们知道不同银行所处的区域、面对的人群、自身的资源禀赋有很大的差异，需要保持客观清醒的认识，每家银行具备怎样的能力，能够做什么，还有哪些方面需要进一步引入合作方。需要对自身保持深刻的认识，不是所有银行都如出一辙，这样肯定做不好。 智能化与银行现状有着很大的关联。银行本身就是数字化，有很好的基础。中小银行本身组织架构简单，只要做好相关机制，就能够创造很大效益。 问：肖总的话对银行尤其是中小银行应该说是兴奋剂，只要中小银行能够保持正确的认知，看来智能化对中小银行并不难。 肖京 ：是这样，刚刚讲了，银行做智能化有着得天独厚的优势，只要机制设置合理，就能够产生很好的经济效益。首先，数字需要标准化，底层平台的搭建非常重要。需要有自动的更新机制、修改机制、系统监控，如果有变化，系统会自动报警。所有依赖人的系统都蕴藏很大风险。其次，信息需要流通。平台搭建之后，信息需要流通。信息和数据需要完整记录，包括客户接触的所有数据，如来自于网站、电话、移动端等等。只有数据规模达到一定量级，才有可能智能化。第三，智能化需要依托银行自身发展。使用智能化技术之前，需要将成本和风险的标准定义好。根据银行发展战略和业务特色，要提前梳理好业务条线包含的重要流程，如渠道、产品、客户等。梳理清楚之后，就可以在银行内部开展做智能化的算法。 智能化是需要站在巨人肩膀上的才能实现，而这个巨人的肩膀就是银行自身的现状、底层平台、信息和数据。 问：调研中，发现很多中小银行对智能化的理解不一，究竟银行如何去做好智能化建设？比如在系统架构设计、整体业务落地，短期与长期的布局？哪些模块可以标准化，哪些需要个性化与业务的相结合？ 肖京 ：银行具体做智能化应该从获客、服务和运营等几个方面去考虑。在获客方面，借助获客工具，做到精准营销。对用户画像、产品画像、渠道画像、业务员画像进行详细分析，做到精准匹配。结合用户生命周期，初期设计钩子产品，中期向上销售和交叉销售，比如我们可以促使信贷客户转化信用卡客户，进行交叉销售。这一方面，平安拥有 SAT 工具 (社交+APP+电话三个渠道的结合) 是结合 MGM 和用户权益等级的一款拉新、促活、促交易的营销工具。获取客户之后有基于大数据的智能营销系统。一是报表实时监控。通过技术处理，可以把运营关键指标投放到监控大屏，进行实时监控，这里面还可以包括很多维度，如手机机型分布、安卓和 IOS 的占比、男女比例、地域分布，理财销售的波峰和低谷等等。二是获客优化。可以对获客质量进行监控，通过客户注册时长，地域分布、注册 IP 地址等进行质量监控，防止薅羊毛。三是精准营销。通过客户的多维度分析，包括历史交易情况、近期活跃情况、点击的产品或周边产品、画像等，精准推荐相关的产品，提高销量。四是流失唤醒。通过近期用户行为分析，及时调整和补充相应的运营策略，防止流失，提升用户活跃。 问：智能化更重要的是让个性化体验有了彰显，面对客户的众多需求，如何针对不同的客群开展个性化的服务？ 肖京 ：在服务方面，智能化能够提升客户体验。进行客户分级，如哪些客户是好客户，好客户如何服务好。服务包括产品和服务本身。一是智能客服，如文字客服和电话语音客服，文字客服可以实现精准回答客户问题。电话语音客服可以实现自动回答，满足客户需求。平安自助研发的问答算法，开发了智能机器人，提供各场景丰富的智能问答交互，降低客服中心成本和压力。能够对客户咨询的开放性问题、针对性问题、关联性问题、信息告知、简单寒暄、无意义信息进行有效的分析、识别、过滤、整理和输出。智能机器人还具有自我学习能力，对知识库进行训练，对未知问题进行收集。二是智能网点，可以通过小型智能机器人、智能终端设备解决客户的大部分问题, 释放人的生产力。平安银行 FACE-BANK 客户自助业务办理系统，可以在网点实现客户自助办理开户及签约类业务，据统计，平安某支行网点面积从原来 1000 平方米缩减到 480 平方米，业务处理时间平均压缩 70%，大大提升网点运营效率。三是高端客户，这部分客户是银行核心客户，需要随时随地帮助到这部分高端客户解决问题。如赠送家用智能机器人等提升客户体验。 问： 业务领域的风险频频曝光，智能化管理能否也体现在具体的运营管理中？ 肖京 ：这个问题很好，结合管理积累的经验，智能化手段的运用，可以降低一定的风险。在运营方面，要做好业务员的管理、风控、监控和培养四大重点工作管理。 一是业务员的管理。对业务员进行画像分析，匹配适合客户和适合渠道。在生命周期晚期阶段，分析流失的概率，决定是否需要采取保护措施，必要时启动相应的 backup 机制。二是业务员的风控。如保险业务员不方便监控，如何管理可能产生的违规售卖非本公司的产品，欺诈客户等违规行为？如何做到早期发现？针对此类问题，可以考虑通过技术来解决，对业务员的行为进行记录，结合业务员交往的客户、业务员背景、社交圈子，换工作的频率等，进行实时管理。三是业务员的监控。如通过业务员微博、微信发的文章，以及进场外出的数据等进行监控，及时发现异常情况。四是业务员的培养。针对业务员的多元化需求做出各种需求类型的设定，不同的需求类型按照不同路径去培养，培养过程中根据监控情况加大力度或开展必要的调整。 问 ：我们都知道小微金融是个世界性难题，尤其小微企业风控和业务员管理是个难点，如何进行智能化管理？ 肖京： 小微企业综合了对公和零售业务，而小微企业对应的数据又非常少，所以风控管 理面临很大挑战。可以通过对企业经营状况分析经营风险。同时，开展行业调研，分析存在的宏观风险。在业务员风控方面，结合上面提到的管理，也可以把各家银行多年积累形成的有效管理经验数据化。通过高管层的深入访谈，把有可能存在的风险点记录下来，进行系统化、标准化，设计到系统模型里。在这个基础上再做智能化，将比人的经验管理还要精准，因为系统还会结合其他的历史数据、交易数据不断深入学习等。 采访临近尾声，肖博士的工作电话接连响起，只能暂告一段落。但是采访依然给人意犹未尽的感受。在 AI 时代下，谁能提前做好战略布局，谁能更快整合内部数据平台，谁能灵活搭建智能算法？对中小银行而言，银行在智能化道路上任重道远。然而，未来已来，任何的犹豫和彷徨，都可能会失去最佳时机。这是一个最好的 AI 时代，这也是一个充满机遇和挑战的时代。银行家们，你们准备好了吗？   "
275,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737481&idx=2&sn=2b3f6df7c0c8b3d835c1a232ac4ad330&chksm=871acf37b06d4621d98d9fa88250b348f5e5cc6772764e4411cb85d32102ad303fbebdd043a6&scene=27,入门 | 数据科学家效率提升必备技巧之Jupyter Notebook篇,"本文作者参加过 fast.ai 的深度学习课程，了解到很多适用于一般软件工程的技巧，所以写作本文梳理所学，并共享给大家。 标准的 Jupyter Notebook 很不错，但还有更多的扩展，其中集成了大量的函数。 安装 Jupyter 扩展包 你可以在这里找到更多的 Jupyter theme：https://github.com/dunovank/jupyter-themes 在安装了 Configurator 之后，你可以看到一个新的「Nbextensions」标签。将这几项打钩。 1. Autopep8 2. Collapsible Headings 3. Gist-it A. Collapsible Headings 现在，你可以将 notebook 里的长代码折叠，而不用再辛苦地滚动浏览。根据我的经验，当进行探索性数据分析和画图表的时候，我需要写很长的代码，经常需要滚动查询很久才能找到我想查看的代码。现在你可以自由选择将代码折叠或展开。并且我认为你甚至可以做一个目录（我尚未尝试这种扩展）。 B. Gist-it 你可以看到上图位置中小小的 GitHub 图标，只需要点击它，就能发表你的 Gist。 Gist 是一个可分享 notebook 的地方，你可以在里面分享你遇到的 bug 和其它技术难题。 其默认发表的是匿名 Gist，如果你想要用你的 GitHub 账号发表，需要生成身份验证的标记。两者的主要区别在于，当你用自己的账号发表时，可以对你的 Gist 进行编辑。 这是我为这篇文章写的 notebook：https://gist.github.com/noklam/a0d020c17ce1715bf1d031b1cb8a9fa4 C. Autopep8 你可以用这个按钮或快捷键，up to you！使用这个按钮可以帮你写入所有的空格。PEP 8 是 Python 代码的风格设计指南。 PEP 8：https://www.python.org/dev/peps/pep-0008/ 我通常会在执行循环命令之前声明开始时间，然后用结束时间减去它以得到运行时间。这没问题，但其实可以更简单。使用内建的魔术命令（magic command）。它们可能看起来很不自然，但很好用（魔术命令以% 起始）。 以一个简单的函数为例，它计算的是小于 n 的最后一个斐波那契数。 你可以使用%time 为单次运行计时，或用%timeit 进行多次计时，然后得到平均值和标准差。因此这对于简单的函数很有用。那对于调用其它函数的函数，情况如何呢？ 你可以使用%prun，我创建了一个哑函数（dummy function），可以多次调用 fib1()。你可以看到该循环过程大多数时间消耗在 fib1() 上。 Cython 是一个工具包，可以使你在 Python 中编译 C 语言，这就是为什么 numpy 和 pandas 很快的原因。确保你已安装 Cython： pip install cython 你可以不改变任何代码而获得双倍的性能。这很棒，但一点也不惊奇。 如果你稍微改变脚本，看看你可以获得什么。如果你有 C 语言编程经验，你很可能知道当我们声明一个变量时，我们需要定义一个数据类型。脚本确实改变了一些，因为像这样的操作对 Python 来说是唯一的，C 语言并不具备这样的功能。因为我们需要分配一个临时变量以存储这个值。 a,b = b,a 从 582 ns 到 48 ns，快了 10 倍，实际上你并不需要改变太多脚本。我感到很兴奋，因为大多数时间慢代码对你来说是 okay 的。你真正关心的是一次又一次被调用的代码。通过%prun 和一些 Cython 代码，你可以获得 C 语言的运行速度而无需编译任何文件。 除了魔术命令，我发现 Jupyter 之中的 shell 命令也很有帮助。（魔术命令以% 开始，shell 命令以! 开始） "
276,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737481&idx=3&sn=fcb2a0dbcc43107399b42757e3ff07b5&chksm=871acf37b06d4621b967b65402a9aede2301b4569f6670de98756a93f1cec84738d875efff51&scene=27,业界 | Petuum提出对偶运动生成对抗网络：可合成逼真的视频未来帧和流,"对于自动驾驶系统而言，准确预测驾驶场景的未来情况对于驾驶安全而言至关重要。卡内基梅隆大学和 Petuum 的一项研究试图通过对偶对抗学习机制来解决这一问题，他们提出的对偶运动生成对抗网络在合成逼真的视频未来帧和流上都取得了很好的表现。机器之心对该研究的论文进行了编译介绍。 尽管用于监督学习的深度学习架构取得了很大的进展，但用于通用和可扩展的视觉任务的无监督视频表征学习仍然很大程度上仍未得到解决——尽管这也是一个关键的研究问题。最近，预测视频序列中的未来帧 [22,20,28] 已经成为了视频数据的无监督学习的一个很有希望的方向。 由于自然场景具有复杂的外观和运动动态，所以视频帧预测本身是一项很有挑战性的任务。直观上讲，为了预测未来帧中的实际像素值，模型必须要能理解像素层面的外观和运动变化，这样才能让之前帧的像素值流入到新的帧中。但是，大多数已有的当前最佳方法 [20,28,18,16,26,37] 都使用了生成式神经网络来直接合成未来视频帧的 RGB 像素值，无法明确建模固有的像素方面的运动轨迹，从而会导致预测结果模糊。尽管最近有些研究 [23,16,26] 试图通过设计能从之前的帧复制像素的运动场层（motion field layer）来缓解这一问题，但因为中间流不准确，预测结果往往有显著的伪影问题。 在这项工作中，我们开发了一种对偶运动生成对抗网络（dual motion Generative Adversarial Network）架构，可以使用一种对偶对抗学习机制（dual adversarial learning mechanism）来学习明确地将未来帧中的合成像素值与像素上的运动轨迹保持连贯。具体来说，它能同时根据一种共享的概率运动编码器而解决原始的未来帧预测（future-frame prediction）问题和对偶的未来流预测（future-flow prediction）问题。受 GAN [6,13] 的成功的启发，我们在两个未来帧和未来流生成器以及两个帧和流鉴别器之间建立了一种对偶对抗训练机制，以便得到与真实数据难以区分的预测结果。通过互相的彼此审查，这种基本的对偶学习机制将对未来像素的想象和流预测联系到了一起。我们的对偶运动 GAN 由如下三个完全可微分的模块构成： 概率运动编码器可以获取可能出现在不同位置的运动不确定性并为之前的帧产生隐含的运动表征，然后这些表征会被用作两个生成器的输入。 然后未来帧生成器会预测未来的帧，预测结果会在两个方面得到评估：帧鉴别器会对帧的逼真度进行评估，流鉴别器会根据之前帧和预测帧之间的估计的流而评估流的逼真度。 未来流生成器又会预测未来的流，这也会在两个方面得到评估：流鉴别器会对流的逼真度进行评估，帧鉴别器会根据推算得到的未来帧（是通过一个嵌套的流变形层（flow-warping layer）计算的）来评估帧的逼真度。 通过从两个对偶的对抗鉴别器学习对称的反馈信号，未来帧生成器和未来流生成器可以受益于彼此互补的目标，从而得到更好的识别预测。在使用了 KITTI 数据集 [5] 中车载摄像头拍摄的视频和来自 UCF-101 数据集 [27] 的消费者视频训练之后，我们的对偶运动 GAN 在合成接下来的帧以及自然场景的长期未来帧上的表现超过了所有已有的方法。我们还通过在另一个汽车摄像头拍摄的 Caltech 数据集 [3] 以及一个来自 YouTube 的行车记录仪原始视频集合上的测试证明了它的泛化能力。此外，我们还通过大量 ablation study（注：指移除模型和算法的某些功能或结构，看它们对该模型和算法的结果有何影响）表明了每个模块的设计选择的关键性。我们还在流估计、流预测和动作分类上进行了进一步的实验，结果表明了我们的模型在无监督视频表征学习上的优越性。 对偶运动 GAN 我们提出了对偶运动 GAN，这是一种用于视频预测的完全可微分的网络架构，能够联合解决原始的未来帧预测和对偶的未来流预测。图 1 给出了这种对偶运动 GAN 架构。我们的对偶运动 GAN 以视频序列为输入，通过融合未来帧预测与基于未来流的预测来预测下一帧。 网络架构 图 2 和图 3 分别给出了生成器和鉴别器网络的详情。为了简洁，图中略去了池化层、批规范化层和中间卷积层之后的 ReLU 层。 图 2：对偶运动生成器。给定序列中的每一帧都会被循环地送入概率运动编码器 E，其中包含 4 个卷积层、1 个中间 ConvLSTM 层和 2 个用于得到均值图和方差图的 ConvLSTM 层，以用于对 z 采样。接下来，未来帧生成器 GI 和未来流生成 GF 会分别解码 z 以得到未来帧 图 3：两个对偶运动鉴别器的架构。帧鉴别器和流鉴别器分别学习分类真实的和合成的帧和流。 实验 表 1：经过 KITTI 数据集的训练之后，在 Caltech 和 YouTube 剪辑上的视频帧预测表现（MSE 和 SSIM） 表 2：在 UCF-101 和 THUMOS-15 上的视频帧预测表现（PSNR 和 SSIM） 图 4：在 YouTube 剪辑上的定性结果。为了更好地比较，我们用红色框和蓝色框突出展示了两辆以相反方向前进的车辆的预测区域 图 5：在来自 Caltech 数据集的车载摄像头视频上，与 Prednet [18] 的下一帧预测结果的定性比较 图 6：在 Caltech 数据集上的多帧预测表现的比较 图 7：我们的模型在 Caltech 序列上的 5 个时间步骤的多帧预测结果 图 8：我们的模型在来自 KITTI 数据集的两个序列上得到的一些未来帧预测和未来流预测示例 表 3：在 KITTI 数据集上的流估计和预测的终点误差。这里值更低表示表现更好。 表 4：在 UCF-101 上的动作识别的分类准确度 论文：用于未来流嵌入式视频预测的对偶运动生成对抗网络（Dual Motion GAN for Future-Flow Embedded Video Prediction） 链接地址：https://arxiv.org/abs/1708.00284 视频的未来帧预测是无监督视频表征学习的一个很有前途的研究方向。视频帧是基于视频中的外观和运动动态，根据之前的帧通过固有的像素流而自然生成的。但是，已有的方法都重在直接想象像素值，从而会得到模糊的预测。在这篇论文中，我们开发了一种对偶运动生成对抗网络架构，可通过一种对偶学习机制来学习明确地强制未来帧预测与视频中像素层面的流一致。其原始的未来帧预测和对偶的未来流预测可以形成一个闭环，从而能为彼此生成信息丰富的反馈信号，进而实现更好的视频预测。为了使合成的未来帧和流都与现实情况难以区分，我们提出了一种对偶训练方法以确保未来流预测能够帮助推理逼真的未来帧，而未来帧预测又反过来能帮助得到逼真的光流。我们的对偶运动 GAN 还能使用一种新的概率运动编码器（基于变分自编码器）来处理不同像素位置的自然的运动不确定性。我们进行了大量实验，结果表明我们提出的对偶运动 GAN 在合成新视频帧和预测未来流上表现优于之前最佳的方法。我们的模型能很好地泛化到不同的视觉场景上，并且表现出了在无监督视频表征学习方面的优越性。 "
277,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737603&idx=1&sn=a8e003446dd0b3ce5a047c69330ac817&chksm=871acfbdb06d46ab8875936c608a15736f52530183102981ef2881292d914d95a168f049b54a&scene=27,302页吴恩达Deeplearning.ai课程笔记，详记基础知识与作业代码,"吴恩达的 DeepLearning.ai 已经于 1 月 31 日 。近日，来自重庆大学的 Wan Zhen 制作了一份深度学习专项课程笔记，该笔记从神经网络与深度学习基础、提升深度神经网络性能和卷积神经网络三门课程出发详细解释了关键概念与作业代码。本文概括性地介绍了这三课的主要内容，并选择每个课程主题比较有意思的知识点进行介绍。 资源链接：https://pan.baidu.com/s/1oAqpmUe 提取密码（已失效） 在这份笔记中，Wan Zhen 不仅介绍了每个课程的关键知识点，同时还详细解释了各课程的编程作业。在第一门课程《神经网络与深度学习基础》中，该课程笔记不仅提供了最基础的 Python 和 NumPy 操作笔记，同时还从最基础的 Logistic 回归推导到最一般的深度全连接网络。当然，还介绍了必要的损失函数与反向传播方法。而在第二门课程中，该笔记详细记录了提升深度网络性能所需要的技巧与基础，例如初始化、正则化和梯度检验等在实践上大大提升模型性能的方法，以及一般 SGD、动量法和适应性学习率方法等常见的最优化方法。最后，第二门课程重点介绍了 TensorFlow，包括该框架的常用函数和实际构建网络的过程等。最后一章节主要记录了卷积神经网络，包括基本的卷积运算、残差网络和目标检测框架等。 以下是该课程笔记的简要框架与一些详细的知识点。 这一部分对应的是吴恩达深度学习课程的第一课，主要介绍必要的编程语言和编程工具，并逐步进阶介绍线性网络、非线性网络、隐藏层网络到深度网络的实现方法，细节详尽，附有完整的代码。通过这一部分的学习，你将理解神经网络的结构和数据流（前向传播和反向传播），非线性激活函数和隐藏层对学习复杂函数的作用，并知道如何一步步构建完整的（任意结构的、自定义的）神经网络，体会向量化和模块化编程思想的妙处。 1.1 Python 基础和 Numpy   本章第一节介绍了如何使用 Python 的 Numpy 工具包、iPython Notebook 等基本的编程工具。然后介绍如何用这些工具构建神经网络，特别是理解神经网络计算的向量化思想和 Python 广播的使用。   1.2 logistic 回归 第 2 节介绍如何构建一个准确率为 70% 的 logistic 回归神经网络分类器（图像识别网络）来识别猫，并介绍如何进一步将准确率提高的方法，以及损失函数的偏导数更新参数的过程。其中特别强调了尽量用向量化结构而不要用循环结构，除非有必要（比如 epoch 的迭代就必须使用循环结构）。   1.2.1 介绍必要的 Python 工具包；1.2.2 介绍数据集的结构；1.2.3 介绍整个学习算法的宏观架构；1.2.4 介绍构建算法的基本步骤；1.2.5 和 1.2.6 总结前述内容进行代码实现，并进行了可视化分析；1.2.7 介绍如何用自己的数据集训练该神经网络；1.2.8 展示了 logistic 回归神经网络的完整代码。   其中 1.2.4 介绍的构建算法的基本步骤为：   定义模型结构； 初始化模型参数； 循环迭代结构：     计算当前损失函数值（前向传播）     计算当前梯度值（反向传播）     更新参数（梯度下降）   通常 1—3 部分是分开构建的，然后整合到一个函数 model() 中。   1.2.5 对 model() 进行了代码实现，并画出了损失函数和梯度的图像。 图 1.2.3：损失函数     1.3 用隐藏层分类平面数据点   第 3 节介绍如何在神经网络中添加隐藏层以对平面数据点进行分类，本节将教你理解反向传播的工作过程、隐藏层对捕捉非线性关系的作用，以及构建辅助函数的方法。 重点内容包括：用单个隐藏层实现二分类器；使用非线性激活函数；计算交叉熵损失；实现前向和反向传播。   1.3.1 介绍必要的工具包；1.3.2 介绍数据集的构成（平面上的红点和蓝点）；1.3.3 介绍无隐藏层的 logistic 回归对该数据集的分类结果；1.3.4 介绍添加了隐藏层的完整模型的实现过程和对该数据集的分类；1.3.5 展示了完整代码。   其中 1.3.3 的分类结果如下图所示：     1.3.4 中使用的神经网络的架构：   1.3.4 构建神经网络的方法和 1.2.4 基本相同，重点强调了如何定义隐藏层结构和非线性激活函数的使用，实现代码后，得到的运行结果为： 图 1.3.6：有隐藏层分类器的决策边界 其中，添加了隐藏层之后，必须使用非线性激活函数，因为不使用非线性激活函数的线性层堆叠是无意义的，无法增大模型的复杂度和容量。   1.4 一步步构建完整的深度神经网络   第 4 节介绍深度神经网络的完整架构，以及如何构建自定义的模型。完成这部分后，你将学会：使用 ReLU 激活函数提升模型的性能、构建更深的模型（隐藏层数大于 1），以及实现易用的神经网络（模块化思想）。   1.4.1 介绍必要的工具包；1.4.2 介绍任务概述；1.4.3 介绍从 2 层网络到 L 层网络的初始化过程；1.4.4 介绍前向传播模块的构建，从线性前向传播、线性+非线性激活前向传播，再到 L 层网络的前向传播；1.4.5 介绍损失函数；1.4.6 介绍反向传播模块的构建，从线性反向传播、线性+非线性激活反向传播，再到 L 层网络的反向传播；1.4.7 展示了深度神经网络的完整代码。       1.5 深度神经网络的图像分类应用   通过前面四节的学习，你已学会如何一步一步构建完整的深度神经网络。第 5 节介绍如何用深度神经网络构建猫识别分类器。此前在 logistic 回归网络中，识别准确率只能达到 68%，而在完整的深度网络中，识别准确率能达到 80%！   完成本节后，你将学会：用前面介绍的所有辅助函数构建任意结构的神经网络；试验不同结构的神经网络，并进行分析；理解构建辅助函数对构建网络的好处（对比从零开始）。   1.5.1 介绍必要的工具包；1.5.2 介绍数据集（猫 vs. 非猫）；1.5.3 介绍模型架构，其中分别构建了 2 层和 L 层的神经网络；1.5.4 介绍 2 层神经网络的训练和测试结果；1.5.5 介绍 2 层神经网络的训练和测试结果；1.5.6 对结果进行分析；1.5.7 介绍如何用你自己的图像训练分类模型；1.5.8 展示了完整代码。   其中，2 层神经网络的运行结果： 运行结果：   运行结果：     通过比较可知，更深的网络有助于提高识别准确率（0.72 vs. 0.8；2 层 vs. 5 层）。   1.5.6 简单总结了影响识别错误的因素：   猫出现在非常规的位置； 猫的颜色和背景相似； 非常规的猫毛色和品种； 拍摄角度； 照片的亮度； 猫的占图比例太小或太大。 这些识别错误可能跟全连接网络自身的局限性有关，包括参数共享、过拟合倾向（参数数量）和层级特征方面，而这些问题将在卷积神经网络里得到改善。 这一部分对应吴恩达 deeplearning.ai 的第二门课程，重点从超参数调整、随机和 Xavier 等参数初始化方法、Dropout 和 L2 范数等正则化方法、以及 1 维和 N 维梯度检验方法来描述深度学习在实践上的性能提升方法。当然，这一部分不仅包含课程知识点，还展示了课后问答与实现作业。 最优化在这一部分课程中也得到了重点讲解，Wan Zhen 的课程笔记从最基本的最速下降法到小批量随机梯度下降介绍了基本的一阶梯度法，然后再探讨动量法与适应性学习率方法来利用历史梯度获得更好的下降方向。值得注意的是，该笔记详细介绍了 Adam 最优化方法的更新过程与实现代码。 这门课程最后一部分主要展现了 TensorFlow 的基本函数与实际构建神经网络的过程。 在参数初始化、正则化和梯度检验中，比较有意思的是 He 初始化和 Dropout 的机制，下面我们将详细探讨这两个组件。 He 初始化（He Initialization，He et al., 2015）是根据第一作者的名字而确定的。如果读者了解 Xavier 初始化，那么其实它们是非常相似的，只不过 Xavier 初始化会为权重 W^l 使用标量元素 sqrt(1./layers_dim[l-1])，而 He 初始化会使用 sqrt(2./layers_dims[l-1])。以下展示了如何实现 He 初始化，这一部分是课程作业的答案： 最后，该文档还总结了三种初始化的效果。如下所示，它们都在相同迭代数、相同的超参数和相同网络架构下进行测试： Dropout 在正则化方法中，Dropout 是非常有用和成功的一种技术。虽然近来有研究者发现把它和批归一化（BN）一起使用会产生一些冲突，但仍然不影响它作为一种强大的技术来控制模型过拟合。一般来说，Dropout 会随机删除一些神经元，以在不同批量上训练不同的神经网络架构。 Bagging 是通过结合多个模型降低泛化误差的技术，主要的做法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。而 Dropout 可以被认为是集成了大量深层神经网络的 Bagging 方法，因此它提供了一种廉价的 Bagging 集成近似方法，能够训练和评估值数据数量的神经网络。 在每一个批量的前向传播与反向更新中，我们关闭每个神经元的概率为 1-keep_prob，且关闭的神经元不参与前向传播计算与参数更新。每当我们关闭一些神经元，我们实际上修改了原模型的结构，那么每次迭代都训练一个不同的架构，参数更新也更加关注激活的神经元。这种正则化方法可以看成是一种集成方法，即集成每个批量所训练的不同网络架构。 在正则化方法中，该笔记也比较了 L2 正则化和 Dropout 的效果： 在后面的最优化方法中，我们比较感兴趣的是 Adam 方法，因此下面我们也将重点描述该方法。 Adam Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计，为不同的参数设计独立的自适应性学习率。 Adam 算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即： 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率，以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很优秀的性能。 Adam 算法同时具备 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。具体来说，算法计算了梯度的指数移动均值（exponential moving average），超参数 beta1 和 beta2 控制了这些移动均值的衰减率。 移动均值的初始值和 beta1、beta2 值接近于 1（推荐值），因此矩估计的偏差接近于 0。该偏差通过首先计算带偏差的估计而后计算偏差修正后的估计而得到提升。 正如该笔记所总结的，Adam 的计算更新过程可分为三部分： 1. 计算历史梯度的指数加权平均值，并将它储存在变量 v 中（有偏估计），然后再计算 v^corrected（修正后得出的无偏估计）。 2. 计算历史梯度平方的指数加权平均值，并将它储存为变量 s（有偏估计），然后计算 s^corrected（修正的无偏估计）。 3. 然后结合前两步的信息更新参数。 这一更新过程如笔记所述可表示为： 其中 t 会计算 Adam 所迭代更新的次数、L 为层级数、β_1 和β_1 为控制指数加权平均值得超参数、α 为学习率，而 ε 为避免分母为零的小常数。 Wan Zhen 同样给出了 Adam 的实现代码或作业解读： 以下实现了上面描述的参数更新过程： 该章节同样介绍和对比了这几个最优化方法的优势： TensorFlow  最后一部分重点介绍了 TensorFlow 的函数与实践。TensorFlow 是一种采用数据流图（data flow graphs），用于数值计算的开源软件库。其中 Tensor 代表传递的数据为张量（多维数组），Flow 代表使用计算图进行运算。数据流图用「结点」（nodes）和「边」（edges）组成的有向图来描述数学运算。「结点」一般用来表示施加的数学操作，但也可以表示数据输入的起点和输出的终点，或者是读取/写入持久变量（persistent variable）的终点。边表示结点之间的输入/输出关系。这些数据边可以传送维度可动态调整的多维数据数组，即张量（tensor）。 这一部分笔记重点介绍了课程的测试和实现代码，例如以下构建了简单的占位符： 定义变量和常量的方法： 初始化参数的方法： 运行计算图： 在大神吴恩达的第四课里我们学习的是卷积神经网络也就是 CNN，这一章的习题是让你用 Numpy 实现一个卷积层和一个池化层同时还有前馈和反向传播。你所能用到的包有： Numpy：一个基本的在 Python 里用来做科学计算的包。 Matplotlib：用来画图。 列举一下你将要学习实现的函数： 1. 卷积函数，包括： 零填充（zero padding） 卷积窗（convolve window) 前向卷积（convolution forward） 反向卷积（convolution backward） 2. 池化函数，包括： 前向池化（Pooling forward) 创建掩码（create mask) 分布值（distribute value） 反向池化（Pooling backward) 第一个作业要求用 Numpy 一点点实现这些函数，下一个作业要求用 TensorFlow 里的函数建立模型。 第三章里讲了卷积神经网络、池化层和卷积神经网络的反向传播。卷积神经网络这部分讲了零填充、单步卷积和前馈卷积网络；池化层这部分讲了前向池化；卷积神经网络的反向传播这部分讲了卷积层反传和池化层反传。 3.1.3 卷积网络 虽然编程框架使卷积易于使用，但卷积仍是深度学习里最难懂的一部分。卷积网络大致上会像下图一样把输入转化成输出： 为了帮助大家进一步理解卷积，小编在这里着重写一下卷积的操作。 在这部分，我们会实现一个单步的卷积，就是你用一个滤波器（filter）在输入的单个位置上执行一下，然后整个的卷积结果就是不断地拖动这个滤波器在输入值的所有位置上执行。我们要做的： 1：拿到输入数据； 2：使用滤波器在输入数据的每一个位置上执行一下； 3：输出另一个数据（一般和输入数据大小不一样）。 在计算机视觉里，矩阵中每一个值对应的都是单个的像素值，我们用一个 3*3 大小的滤波器去卷图片，就是在每一个位置上，用滤波器里的每一个值去乘原始矩阵里对应位置上的值，然后求和之后再加上一个偏差数值，然后再拖动这个滤波器去下一个位置上，这就是卷积，每一次拖动的距离叫做步长。在第一步练习里，你将要实现一个一步的卷积，这个一步的卷积是使用一个滤波器在输入数据一个位置上卷积得到的单个的实数输出。 练习：实现 conv_single_step() 函数   代码： 再大致讲一下卷积神经网络前传（Forward pass）。 之前我们讲了怎样用一个滤波器去做卷积，在前传里是用好多个滤波器一个一个地卷积然后把结果（2D 的）一层一层地堆成一个 3D 的结构。 详细信息更有意思，请学习吴恩达的课程。 3.1.4 池化层 池化层很好玩，为了降低输入数据的大小，减少计算量，就有了池化层，同时池化层也会帮助特征检测器更独立于位置信息。这里介绍两种池化层： 最大池化层 滑动一个 (f,f) 大小的窗口到输入的数据上，在窗口里取最大的一个值为输出然后把这个值存到准备输出的数据里。 平均池化层 从名字就可以看出它需要同样地滑动一个 (f,f) 大小的窗口到输入数据上面，然后求窗口中数据的平均值，再把这个值储存到要输出的值里。 这些池化层没有参数可以被训练，但对于窗口的大小 f，你可以自己去尝试然后选择最好的。 3.2 卷积神经网络应用 在这一部分里吴老师讲了 TensorFlow 模型、创建占位符、初始参数、前向传播、计算损失及模型。 在这里我们聊一聊怎么初始化参数吧。你需要用 tf.contrib.layers.xavier_initializer(seed=0) 来初始化权重/滤波器 W1 和 W2。你不需要担心偏重值因为很快你就能发现 TensorFlow 函数已经解决了这一点。注意，你只需初始 conv2d 函数的权重/滤波器，TensorFlow 会自动初始全连接部分的层。我们会在之后的作业里看到更多。 练习：实现 initialize_parameters(). 每一组滤波器/权重的维度已经提供给大家。记住，在 TensorFlow 里初始化一个形状为 [1,2,3,4] 的参数 W 用：  More Info. 更多的信息： 3.3 Keras 教程：幸福家（Happy House） 这一部分讲了一个作业（幸福家），怎样用 Keras 建模型、总结、用自己的图片去测试其他非常有用的 Keras 函数。在这里我们着重讲一下幸福家是什么： 3.3.1 幸福家 下一次放假旅行，你决定和你的 5 个在学校认识的朋友一起度过一周。这附近有一个非常方便的房子可以做很多事情。但最重要的福利是在房子里的时候每个人都必须要快乐。所以每个想进房间的人都要提供他们现在的幸福情形。 作为一个深度学习专家，要确保「开心」这一规则被坚决的执行了，你要去建一个算法来通过前门的摄像头得到的照片来检查这个人是不是开心。 你已经收集到了你和朋友在前门照的照片，数据库已经被标注了。0 代表不开心，1 代表开心。 跑一下下面的程序让数据库更加标准化，然后学习一下它的形态。 幸福家数据库的细节： 图片数据大小为 (64, 64, 3)  训练数据：600 测试数据：150 现在去解决「快乐」挑战吧！ 3.4 残差网络 这一部分介绍了非常深的深度神经网络的问题，如何构建残差模块、残差连接、卷积块等，并组合它们而建造第一个残差网络模型以及用自己的图片去实验。 残差网络可以解决一些非常深的深度神经网络所具有的问题，我们在这里着重谈一下非常深的深度网络的问题。 近几年，神经网络变得越来越深了，最前沿的网络有几层的，也有超过一百层的。 深度神经网络最主要的优势是它可以表示非常复杂的函数。它也可以从不同的抽象等级去学习从边缘特征（浅层）到复杂的特征（深层）。但是，用一个更深的神经网络不总是有效。一个大缺点是训练模型的时候梯度会消失：非常深的网络经常会非常快地梯度下降到 0。这样会让梯度下降变得不可忍受的慢，因为每一次只会更新一点点。更具体地，在梯度下降的时候，当你反向传播从最后一层一直到第一层的时候，每一步都在乘以权重矩阵，所以梯度会以指数函数的速率下降到 0（或者在有些极少的情况里，梯度以指数函数的速率产生爆炸）。 在训练的时候，你也能看到前面层的梯度数值非常快地下降到 0。 你现在可以用残差网络解决这个问题。 3.5 用 YOLOv2 检测车辆 这一部分讲了：问题叙述、YOLO、模型细节、用一个门槛来过滤一个班的分数、非最大抑制、包装过滤器、在图片上测试 YOLO 模型、定义类、转折点和图片大小、加载一个训练好的模型、转化输出模型到一个可用的边界盒张量、过滤盒及对图片跑计算图。 在这里我们讲一下问题是什么： 你现在在做一个自动驾驶车，作为一个重要的部分，你想先建一个车辆检测系统，当你开车时候这个系统会每隔几秒照一下前面的路。 你现在已经收集了所有的这些图片到一个文件夹里然后也已经用框标注出了每一个你可以找到的车。这里是例子： p 代表了你有多自信圈出来的是什么，c 代表了你认为圈出来的是什么。 如果你想让 YOLO 认出 80 种类别, 你可以让 c 表示 1 到 80 中的一个数，或者 c 是一个 80 长度的矢量。视频课里已经用了后一种表示。在这个笔记里，我们两种都用了，这取决于哪种更好用。在这个练习里，你将要学习 YOLO 怎样工作，怎样运用这个区检测车。因为 YOLO 训练的时候非常耗费计算量，我们将会加载训练好的权重来用。 3.6 快乐家中的脸部识别 在这一张里我们可以看到：简略脸部识别，脸部图片编成一个人 128 维度的矢量，使用 ConvNet 来计算编码，三联损失，加载训练好的模型和运用训练好的模型。 我们着重介绍一下简略脸部识别： 在脸部识别里，给你两张图片然后你必须告诉我两个人是否是同一个。最简单的做法是比较两张图片中的每一个像素，如果两张原始图片的比较结果小于一个门槛值则可以说两张图片是一个人。 当然，这个算法的性能非常差，因为像素的值会随着光的改变，方向的改变，甚至镜像改变头的位置而剧烈的改变。你将看到除了用原始图片你更愿意编码一个 f(image) 来比较每一个像素这样会给你一个关于两张照片是不是一个人的问题更准确的答案。 3.7 通过神经风格迁移生成艺术 在这一章节中，我们将尝试实现神经风格迁移，并使用算法生成新颖的艺术风格图像。在神经风格迁移中，重点是我们需要优化成本函数获得像素的值。如下所示，我们使用某张图像的风格，并迁移到需要这种风格的图像中： 神经风格迁移主要使用预训练的卷积神经网络，并在它的顶部构建新的层级。这种使用预训练模型，并将其应用到新任务的方法可以称为迁移学习。卷积网络的迁移学习非常简单，一般来说，我们可以将最后几个分类层的权重随机初始化，再在新的数据集上训练而快速获得优秀的性能。 在 NST 原论文中，我们会使用 VGG-19 网络，它会预先在 ImageNet 上实现训练。因此在 Wan Zhen 的笔记中，我们可以运行以下命令下载模型及参数; 然后使用 assign 方法将图像作为模型的输入： 随后，我们就能使用以下代码获取特定层级的激活值; 在神经风格迁移中，重点是以下三个步骤： 构建需要迁移风格的图片损失函数 J_content(C, G) 构建风格损失函数 J_style(S, G) 将它们组合为最终的损失函数 J(G) =α J_content(C, G) +β J_style(S, G) 使用这样的损失函数，最终我们能以迁移学习的方法实现神经风格迁移。 "
278,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737338&idx=4&sn=564a95315892513fb85868b913f835a0&chksm=871acec4b06d47d2210c39554adcfc84d47c3959aa4910389dfd12d2564c723ad06909d15871&scene=27,学界 | 商汤联合提出基于FPGA的快速Winograd算法：实现FPGA之上最优的CNN表现与能耗,"此前，商汤科技联合北京大学等提出一种基于 FPGA 的快速 Winograd 算法，可以大幅降低算法复杂度，改善 FPGA 上的 CNN 性能。论文中的实验使用当前最优的多种 CNN 架构，从而实现了 FPGA 加速之下的最优性能和能耗。 1. 引言 深度卷积神经网络（CNN）在多个计算机视觉任务上取得了优秀的性能，包括图像分类、目标检测和语义分割 [1, 2]。CNN 的高准确率是以极大的计算复杂度为代价的，因为它需要对特征图中的所有区域进行综合评估 [3, 4]。为了解决如此巨大的计算压力，研究者使用 GPU、FPGA 和 ASIC 等硬件加速器来加速 CNN [5–17]。其中，FPGA 因其高性能、低能耗和可重配置性成为有效解决方案。更重要的是，使用 C 或 C++的高级综合（High Level Synthesis，HLS）大幅降低了 FPGA 的编程障碍，并提高了生产效率 [18–20]。 CNN 通常包含多个层，每一层的输出特征图是下一层的输入特征图。之前的研究发现当前最优 CNN 的计算主要由卷积层主导  [6, 7]。使用传统的卷积算法，则输出特征图中的每个元素要经多步乘积累加运算进行单独计算。尽管之前使用传统卷积算法的 FPGA 解决方案取得初步成功 [5–9, 11]，但是如果算法更加高效，该解决方案的效率可能会更高。本文展示了使用 Winograd 算法的卷积算法 [21] 如何大幅降低算法复杂度，改善 FPGA 上的 CNN 性能。使用 Winograd 算法，利用元素之间的结构相似性生成输出特征图中的一列元素。这减少了乘法运算的数量，从而降低算法复杂度。研究证明快速的 Winograd 算法适合为具备小型滤波器的 CNN 推导高效算法 [16]。 更重要的是，CNN 的当前趋势是带有小型滤波器的深度拓扑。例如，Alexnet 的所有卷积层（除了第一层）都使用 3 × 3 和 5 × 5 滤波器 [3]；VGG16 仅使用 3 × 3 滤波器 [22]。这为使用 Winograd 算法高效实现 CNN 创造了机会。但是，尽管在 FPGA 上使用 Winograd 算法很有吸引力，但仍然存在一些问题。首先，设计不仅要最小化内存带宽要求，而且要匹配计算引擎与内存吞吐量。其次，在 FPGA 上映射 Winograd 算法时存在很大的设计空间。很难推断哪些设计会改善性能，抑或损害性能。 本文设计了一种行缓冲结构为 Winograd 算法缓存特征图。这允许不同的 tile 在卷积运算进行时重用数据。Winograd 算法的计算涉及通用矩阵乘法（GEMM）和元素级乘法（EWMM）的混合矩阵变换。然后，我们设计了一种高效的 Winograd PE，并通过并行化启动多个 PE。最后，我们开发分析模型用于评估资源使用情况，并预测性能。我们使用这些模型探索设计空间，确定最优的设计参数。 本文的贡献如下： 提出一种架构，可在 FPGA 上使用 Winograd 算法高效实现 CNN。该架构把行缓冲结构、通用和元素级矩阵乘法用于 Winograd PE 和 PE 并行化。 开发出分析性的资源和性能模型，并使用该模型探索设计空间，确定最优参数。 使用当前最优的 CNN（如 AlexNet 和 VGG16）对该技术进行严格验证。 图 1：传统卷积算法和 Winograd 卷积算法的对比。我们假设 Winograd 算法的步幅 S 为 1。 3. 架构设计 图 2 表示在 FPGA 上基于 Winograd 算法的卷积层架构。研究者在相邻 tile 的特征图中确定数据重用机会。最后，自然而然地实现了行缓冲。输入特征图 (M) 有多个通道，如图 1 所示。行缓冲的每一行都存储所有通道中同样的一行。Winograd PE 从行缓冲中获取数据。具体来说，给出一个 n×n 输入 tile，Winograd PE 将生成一个 m × m 输出 tile。研究者通过并行化多个通道的处理来启动 PE 阵列。最后，使用双缓冲（double buffer）重叠数据迁移和计算。所有输入数据（如输入特征图、滤波器）最初都存储在外部存储器中。输入和输出特征图通过 FIFO 被迁移至 FPGA。但是，滤波器的大小随着网络深度增加而显著扩大。将所有滤波器加载到片上存储器（on-chip memory）中是不切实际的。在本论文的设计中，研究者将输入和输出通道分成多组。每个组仅包含一部分滤波器。研究者在需要时按组加载滤波器。为方便陈述，下文中假设只有一组。 4. 自动工具流程 研究者设计了一个自动工具流程将 CNN 自动映射至 FPGA，如图 5 所示。该流程包括设计空间探索引擎（DSEE）。研究者使用 Caffe prototxt 来描述 CNN 的结构 [24]。FPGA 配置参数包括内存带宽、DSP 数量、逻辑单元和片上内存容量。DSEE 的输出是最优解 {n, Tm, Tn}。在步骤 2 中，基于最优解，研究者开发了代码生成引擎（CGE），可自动生成 Winograd 卷积函数。该函数描述整个加速器结构，包括行缓冲、缓冲管理和 Winograd PE。生成的实现是 HLS 兼容的 C 代码。编译指令如内存分区因素、循环展开因素 Tn Tm 以及 FIFO 接口被插入函数中。步骤 3 中，研究者使用 Xilinx HLS 工具将代码合成为寄存器传输级别。最后，研究者使用 Xilinx SDSoC（软件定义片上系统）工具链来生成比特流。 5. 实验评估 论文：Evaluating Fast Algorithms for Convolutional Neural Networks on FPGAs  论文链接：http://ieeexplore.ieee.org/abstract/document/7966660/ 摘要： 近年来，卷积神经网络（CNN）越来越广泛地应用于计算机视觉任务。FPGA 因其高性能、低能耗和可重配置性成为 CNN 的有效硬件加速器而备受关注。但是，之前基于传统卷积算法的 FPGA 解决方案通常受限于 FPGA 的计算能力（如 DSP 的数量）。本论文展示了快速的 Winograd 算法，该算法可以大幅降低算法复杂度，改善 FPGA 上的 CNN 性能。我们首先提出了一种新型架构在 FPGA 上实现 Winograd 算法。我们的设计利用行缓冲结构（line buffer structure）来高效重用不同 tile 的特征图数据。我们还高效架构 Winograd PE 引擎，通过并行化启动多个 PE。同时存在复杂的设计空间有待探索。我们提出一种分析模型，用于预测资源使用情况、推断性能。我们使用该模型指导快速的设计空间探索。实验使用了当前最优的 CNN，结果表明其实现了在 FPGA 上的最优性能和能耗。我们在 Xilinx ZCU102 平台上达到了卷积层平均处理速度 1006.4 GOP/s，整体 AlexNet 处理速度 854.6 GOP/s，卷积层平均处理速度 3044.7 GOP/s，整体 VGG16 的处理速度 2940.7 GOP/s。 "
279,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737481&idx=1&sn=10e82e52991eb87170e22109857c3dec&chksm=871acf37b06d4621aabc409f95c72f935670c8595f274d62bf5283fdf275f052f29b73c27ab0&scene=27,机器之心最干的文章：机器学习中的矩阵、向量求导,"本文的目标读者是想快速掌握矩阵、向量求导法则的学习者，主要面向矩阵、向量求导在机器学习中的应用。因此，本教程而非一份严格的数学教材，而是希望帮助读者尽快熟悉相关的求导方法并在实践中应用。另外，本教程假定读者熟悉一元函数的求导。 本文公式太多，微信上展示会有一些问题。所以本文适合读者了解矩阵、向量求导，而详细地学习与分析请下载本文的PDF版。 PDF 下载地址：https://pan.baidu.com/s/1pKY9qht 所谓矩阵求导，本质上只不过是多元函数求导，仅仅是把把函数的自变量以及求导的结果排列成了矩阵的形式，方便表达与计算 而已。复合函数的求导法则本质上也是多元函数求导的链式法则，只是将结果整理成了矩阵的形式。只是对矩阵的每个分量逐元素 地求导太繁琐而且容易出错，因此推导并记住一些常用的结论在实践中是非常有用的。 矩阵求导本身有很多争议，例如： 对于求导结果是否需要转置? 不同教材对此处理的结果不一样，这属于不同的 Layout Convention。本文以不转置为主，即求导结果与原矩阵/向量同型，术语叫 Mixed Layout。  矩阵对向量、向量对矩阵、矩阵对矩阵求导的结果是什么? 最自然的结果当然是把结果定义成三维乃至四维张量，但是这并不好算。也有一些绕弯的解决办法 (例如把矩阵抻成一个 向量等)，但是这些方案都不完美 (例如复合函数求导的链式法则无法用矩阵乘法简洁地表达等)。在本教程中，我们认为，这三种情形下导数没有定义。凡是遇到这种情况，都通过其他手段来绕过，后面会有具体的示例。 因此，本教程的符号体系有可能与其他书籍或讲义不一致，求导结果也可能不一致 (例如相差一次矩阵转置，或者是结果矩阵是否平铺成向量等)，使用者需自行注意。另外，本教程中有很多笔者自己的评论，例如关于变形的技巧、如何记忆公式、如何理解其他的教程中给出的和本教程中形式不同的结果等。 文中如有错漏，欢迎联系 ruanchong_ruby@163.com，我会尽快订正。 符号表示 标量用普通小写字母或希腊字母表示，如  等。 向量用粗体小写字母或粗体希腊字母表示，如  x 等，其元素记作  (注意这里  没有加粗。加粗的小写字母加下标，例如  等，表示这是两个不同的常数向量)。向量默认为列向量，行向量需要用列向量的转置表示，例如  等。 矩阵用大写字母表示，如A 等，其元素记作  (注意这里 a 用的是小写字母。大写字母加下标，例如 等，表示不同 的常数矩阵)。 用字母表中靠前的字母 (如 a,b,c等) 表示常量，用 f,g,h 或字母表中靠后的字母 (如u,v等）等表示变量或函数。 有特殊说明的除外。  综上所述，本文进行如下约定： 矩阵/向量值函数对实数的导数： 要点:求导结果与函数值同型，且每个元素就是函数值的相应分量对自变量  求导  若函数  ，则  也是一个 m ×n  维矩阵，且 ，也可用劈形算子将导数记作 ，或记作 由于向量是矩阵的特殊情形，根据上面的定义也可以得到自变量为向量时的定义:若函数  是一个 m 维向量，且  。若函数值 是行向量则结果为行向量，可记作  若函数值 f 是列向量则求导结果为列向量，可记作 。 注:本文开头即说明过，变量为向量时仅仅是将其看作多个实数，无所谓行向量与列向量之分。这里用行向量或列向量的 说法仅仅为了把公式用矩阵相乘的方式表示出来方便，因为在数学公式总要指定向量是行向量或者列向量中的某一个，才能与公式里的其他部分做矩阵运算时维度相容。下同。 实值函数对矩阵/向量的导数: 要点:求导结果与自变量同型，且每个元素就是f对自变量的相应分量求导 若函数  ，则 也是一个 m ×n  维矩阵，且  也可使用劈形算子将导数记作  。 由于向量是矩阵的特殊情形，根据上面的定义也可以得到自变量为向量时的定义:若函数  是一个 m 维向量，且  。若函数值  是行向量则结果为行向量，可记作  若函数值 f 是列向量则求导结果为列向量，可记作 。 向量值函数对向量的导数(雅克比矩阵 )： 若函数  ，则  是一个 m ×n  维矩阵，且 。用劈形算子表示时可记作  。 注:如前所述，本教程仅仅是把变量都看成多个实数，无所谓行与列之分，因此在表述从向量  到  的雅克比矩阵时，不区分 x 或者 f 到底是行向量还是列向量，统一用  表示，维度也都是m-by-n。有些教程可能会区分 行对列、列对列、行对行、列对行几种不同情形的求导，认为有些结果相差一个转置，有些组合不能求导等等。本教程则认为只有一种求导结果，就是雅克比矩阵。 有一点需要注意的是，若f退化成标量 ，则 x 到 f 的雅克比矩阵 是一个行向量，是梯度 (列向量) 的转置，即 。注意这里使用的记号:左边 f 加粗，是把它看做一个长度为 1 的向量，表示求向量 x 到向量 f 的雅克比矩阵;右边  为普通字体，表示实函数  对向量 x 的导数。 劈形算子 : 在求导的变量比较明确时，可以省略劈形算子的下标写成  。 劈形算子和偏导数两种记号大体上可以认为是相同的，只不过在涉及到变量分量的推导过程 (例如用链式法则推神经网络 的 BP 算法) 中，偏导数那一套符号更加常用;而劈形算子的优势是书写简单，在对传统的机器学习模型的目标函数求导 时，劈形算子有时更常用。 对于一个实函数  ，其梯度记为  ，也可记作gradf，是一个m 维向量。Hessian 矩阵记为  其中 是一个m ×m 的矩阵。根据上述定义可以发现，Hessian 矩阵其实是 X 到  的雅克比矩阵，因此  不光是一个形式记号，而是可以用  来计算 注：某些教材区分对行向量和列向量求导，认为 Hessian 矩阵是先对行向量 求导，再对列向量X求导(或者反过来)，因此写作  (或者 )。 对于一个实函数  ，其梯度规定为 m ×n  维矩阵 ，Hessian 矩阵不作定义。 对于实值函数 f，上面的定义满足转置关系（f 对某个变量和其转置的导数互为转置）：即： （其中 x 代表任意维度的向量或矩阵）。 函数增量的线性主部与自变量增量的关系：      实值函数对矩阵/向量的导数： ，此式用到的技巧非常重要：两个同型矩阵对应元素相乘再求和时常用上面第二个等式转化为迹，从而简化表达和运算。从另一个角度讲，这是矩阵导数的另一种定义。即：对于函数  ，若存在矩阵 A，使得   时（||*|| 为任意范数），成立   ，则定义  。矩阵乘积的迹是一个线性算子。事实上，如果有两个同型矩阵 A、B，他们的内积即定义为 <A, B> = tr(A^T * B)。容易验证，向量内积也符合这个定义，因此此式可以看成是向量内积的推广。      实值函数对矩阵/向量的导数： ，此式右边是向量内积，可看做前一个式子的退化情形。      向量值函数对向量的导数： ，此式即为重积分换元时用于坐标变换的Jacobian矩阵。 规则：若在函数表达式中，某个变量出现了多次，可以单独计算函数对自变量的每一次出现的导数，再把结果加起来。 这条规则很重要，尤其是在推导某些共享变量的模型的导数时很有用，例如 antoencoder with tied weights（编码和解码部分的权重矩阵互为转置的自动编码器）和卷积神经网络（同一个 feature map 中卷积核的权重在整张图上共享）等。 举例（该规则对向量和矩阵也是成立的，这里先用标量举一个简单的例子）：假设函数表达式是  ，可以先把三个 x 看成三个不同的变量，即把 f 的表达式看成  ，然后分别计算  ，  ，最后总的导数就是这三项加起来： ，此时再把 x 的下标抹掉并化简，就得到 6x+1。熟悉这个过程之后，可以省掉添加下标再移除的过程。 如果用计算图（computation graph，描述变量间依赖关系的示意图，后面会举例）的语言来描述本条法则，就是：若变量 x 有多条影响函数 f 的值的路径，则计算   时需要对每条路经求导最后再加和。 如果想更多地了解计算图和反向传播，推荐阅读 [Colah 君的文章](http://colah.github.io/posts/2015-08-Backprop/)。其中详细讲述了计算图如何工作，不仅讲反向传播还讲了前向传播（前向传播对于目前的机器学习算法来说似乎没有太大的用处，但是对于加深计算图的理解很有帮助。RNN 有一种学习算法叫 RTRL 就是基于前向传播的，不过近年来不流行了）。 有了上面的基础，我们就可以推导 Batch normalization（以下简称 BN）的求导公式了。 BN 的计算过程为： 其中 m 是批的大小，x_1 到 x_m 分别是 m 个不同样本对于某个神经元的输入，l 是这个批的总的损失函数，所有变量都是标量。求导的第一步是画出变量依赖图，如下所示（根据左边的变量可以计算出右边的变量，如果为了强调，也可以在边上添加从左向右的箭头）： 左侧，右上，右下分别是三种不同的画法（读者也可以尝试其他的画法）：左边的图是把所有变量 x_i 都画了出来，比较清楚，如果想不清楚变量之间是如何相互依赖的，这样画可以帮助梳理思路；右上是我自创的一种方法，借鉴了概率图模型中的盘记号（plate notation），把带下标的变量用一个框框起来，在框的右下角指明重复次数；右下我只画了一个局部，只是为了说明在有些资料中，相同的变量（如本例中的  ）只出现一次，而非像左图那样出现多次，从而图中会出现环。不过要不要复制同一个变量的多个拷贝没有本质的区别。在右下这种表示法中，如果要求 partial σ^2 除以 partial x_i，需要对   和   这两条路径求导的结果做加和。（事实上，这种带下标的画法有点儿丑，因为我们现在的计算图里的变量都是标量……如果用 X 表示   组成的向量，计算图会更简洁，看起来更舒服。不过这种丑陋的表示对于我们现在的目的已经够用了。 ） BN 原论文中也给出了反向传播的公式，不过我们不妨试着自己手算一遍： x_i hat 影响损失函数只有唯一的路径  ，根据链式法则，得到：  。 λ 影响损失函数有 m 条路径：对任意一个 i，   都是一条路径，需要对这些路径分别求导再加和： 。 partial l 除以 partial β 的计算与上面类似： 。  影响损失函数的路径也有 m 条： （此处忽略中间变量 y_i，直接把 l 看成的 x_i hat 函数。）所以  。注意求导的时候把   当成一个整体，想象这就是一个字母，而不要把它想成标准差的平方。  影响损失函数共有 2m 条路径： （分别对应于右上图中较短和较长的路径）。故有： 。其中最后一步的理由是根据   的定义，后一项为零。  影响损失函数有 3 条路径： ，所以 。 易发现雅克比矩阵的传递性：若多个向量的依赖关系为 ，则： 证明：只需逐元素求导即可。 ，即 的 元等于矩阵 的 i 行 和 矩阵 的第 j 列的内积，这正是矩阵乘法的定义。 注：将两项乘积的和转化成向量内积或矩阵相乘来处理，是很常用的技巧。 雅克比矩阵的传递性可以很容易地推广到多层中间变量的情形，采用数学归纳法证明即可。 若中间变量都是向量，但最后的结果变量是一个实数，例如变量依赖关系形如 ，则： 由雅克比矩阵的传递性知： 再根据 f 退化时雅克比矩阵和函数导数的关系，有： 以上三式相结合，可以得到如下链式法则: 上面的结果显然也可以推广到任意多层复合的情形（可用于 RNN 的 BPTT 的推导）。 上面的公式是把导数视为行向量（即以  和 的形式）给出的。如果需要把导数视为列向量，只需将公式两边同时转置即可。由于实践中复合一次的情形较常用，这里只给出将变量视为列向量时复合一次的公式： 若 ，则： 或写作 这里再给出一个特例：若变量依赖关系为 ，u 和x 维度相同且 仅由 计算出而与 x 的其他分量无关，则易知 是对角阵，所以上面的公式可以化简为： 其中 表示取对角矩阵 D 的对角线上的元素组成列向量， 表示两个向量逐元素相乘。 由于最终的结果是两个向量逐元素相乘，所以也可以交换一下相乘的顺序，写成： 本条规则在神经网络中也很常用，常见的情形包括但不限于：逐元素地应用激活函数 ，以及现代 RNN 单元中的门限操作（以 LSTM 为例： 。         *   因为依赖关系简单，本公式也可以直接根据导数逐分量的定义直接推出来： 此即前述公式的分量形式。 记忆：只需记住结果是一堆雅克比矩阵的乘积，相乘的顺序根据维度相容原则调整即可（假设每个中间变量的维度都不一样，看怎么摆能把雅克比矩阵的维度摆成矩阵乘法规则允许的形式。只要把矩阵维度倒腾顺了，公式也就对了。） 注：网络上各种资料质量参差不齐，在其他教程中时常会见到向量对矩阵求导的表达式。例如介绍 RNN 的梯度消失问题的文章中，经常会见到 这种式子。如果文中出现这个式子是定性的，只是为了说明链式法则中出现了很多连乘项导致了梯度消失，那么读者也只需定性地理解即可。如果文中出现这个式子是定量的，是为了推导反向传播的公式，那么笔者建议读者用如下两种方式之一理解： 其一是把  理解成一种简写形式：先把 W 抻成一个向量，然后公式中的每一个雅克比矩阵就都可以计算了，最后再把结果向量重新整理成 W 的同型矩阵。但是这种方法非常复杂，因为把 W 抻成向量以后目标函数关于 W 的表达式就变了，很难推导  这个雅克比矩阵。一个具体的算例见《Optimizing RNN performance》（https://svail.github.io/rnn_perf/）一文中最后的推导。（如果你不打算熟练掌握这种方法，只浏览一下看看大意即可。相信我，如果你学了本文中的方法，你不会再想用这种把矩阵抻开的方法求导的。） 其二是把最后一项分母中的 W 理解成矩阵 W 中的任一个元素 w_ij，从而上述表达式中的四项分别是向量（此处看作行向量）、矩阵、矩阵、向量（列向量），从而该表达式可以顺利计算。但是这也很麻烦，因为得到的结果不是直接关于 W 的表达式，而是关于其分量的，最后还要合并起来。 其他理解方式，恕我直言，基本上都是作者自己就没弄懂瞎糊弄读者的。 实值函数对向量求导 未作特殊说明即为对变量 x 求导。 几个基本的雅克比矩阵： ，特别地， 。 向量内积的求导法则： 内积是一个实数，因此本节相当于实数对向量求导，结果是与自变量同型的向量。   这是最基本的公式，正确性是显然的，因为  。 正确性是显然的，因为 。另外，也可以用变量多次出现的求导法则结合上一条公式证明。  利用变量多次出现的求导法则以及前面的公式容易证明。另外，若 A 是对称矩阵，上式右边可以化简为 2A_x。 向量内积的求导法则：          利用变量多次出现的求导法则（x 同时在 u、v 中出现）+ 复合函数求导法则（列向量形式）易证。 向量数乘求导公式 推导： ，两边逐分量对比一下便知等式成立。 记忆：按两个标量函数相乘的求导法则记，再注意一下维度相容原理即可。向量数乘的结果还是一个向量，所以此处相当于向量对向量求导，结果是一个雅克比矩阵，形状为 f 的维度乘 x 的维度。 未作特殊说明即为对 X 求导。迹是一个实数，所以相当于实数对矩阵求导，结果是一个和 X 同型的矩阵。 先回顾一下迹的基本性质： 线性性质: 转置不变性： 轮换不变性： 特别地， 。注意，轮换不变性不等于交换性。例如： ，但是一般情况下 。 基本公式： 推导：逐元素求导验证 ：（事实上这个公式就是矩阵导数的另一种定义，前面也有叙述。） 根据此式容易得到另一个式子： 迹方法的核心公式（非常重要）： 推导：利用变量多次出现的求导法则： （X_c 表示将 X 的此次出现视作常数） 这个公式非常重要，在推导最小二乘解等问题上都会遇到。公式的名字是我瞎起的，我不知道它叫什么名字。 其他与矩阵迹有关的公式 大部分都是上述核心公式的简单推论，不必强记 推导： 注：将实数看作是 1*1 矩阵的迹是很常用的技巧。 推导：使用迹方法的核心公式。过程略。 推导：将左式的括号相乘展开，然后用上面的关于矩阵迹的公式。 推导同上，只需注意到 即可。特别地， （此式也可逐元素求导直接验证） 行列式的求导公式： 实数对矩阵求导，结果是和 X 同型的矩阵。此条证明较繁琐，大致过程是用逐元素求导+伴随矩阵的性质推导，过程可参考 math overflow。最好能直接记住。 设 ，则： ,或简写为 关于维度的说明：X 是矩阵，中间变量 U 也是矩阵（未必与 X 同型），最终结果 y 是实数。因此求导结果是和 X 同型的矩阵。 注：此式似乎用的不多，毕竟这仅仅是对 x_ij 这一个分量求导的结果，很难直接得到对 X 求导的结果。而且这个式子只是最基础的多元函数复合的链式法则而已，没有得到什么特别有趣或者重要的结论。 设 ，则： （等式右边是实数和矩阵的数乘） 关于维度的说明： X,u,y 分别是矩阵、实数、实数，因此相当于实数对矩阵求导，结果是 X 同型的矩阵。 证明是显然的，逐元素求导验证即可： 线性变换的导数（非常重要。由于线性变换很常用，记住此式可以简化很多公式的推导过程）： 设有 及线性映射 （因此 ），则： 推导： ，而 （ 是 Kronecker delta 符号：若l=j 值为 1，否则为 0），将后式代入前式，得： ，即矩阵 A^T的第 i 行 和 矩阵 的第 j 列的内积。 向量的线性变换是上式的退化情形，即： 向量的线性变换还可以求二阶导： 推导：记 ，则 记忆：同上，记住大概的形状（对线性变换来说，求一次导就是乘一个矩阵），然后根据维度相容原则摆顺了就行。 由于线性变换很常用，这里不妨把给 X 右乘一个矩阵时的公式一并给出，以便查阅：设有 及线性映射 （因此 ），则： 证明：若令 ，则变量依赖关系变为： ，且 ，根据线性变换的求导法则，知： ，所以 。 记忆：先做线性变换再求导就等于先求导再做线性变换。剩下的细节（如左乘还是右乘等）根据维度相容原则倒腾即可。 注：此式很有用，在神经网络中，经常有形如 的依赖关系。其中 x 是神经网络某一层的输入数据（不是训练神经网络时要求导的变量！不过在构造对抗样本时可能需要对 x 求导）, W,b 是该层的参数（这才是训练神经网络时要求导的变量），z 是经过变换后预备输入给下一层的值，l 是最终的损失函数。根据上述线性变换的求导公式，立即可以得到 BP 算法的核心步骤： 。（另注：标准的 BP 算法通常将 定义为变量 。） 这一部分在机器学习中遇到的不多（毕竟常见的情况是求一个标量损失函数对其他变量的导数），不是特别重要，不过偶尔在凸优化里会碰到一些。这里收集整理这几个式子主要是为了资料完整、查阅方便。以下假定 F 是可逆方阵： 自变量和函数值都是实数，求导结果也是实数。推导过程较困难，主要用到了矩阵的雅克比公式（不是雅克比矩阵）。建议记住，或者用时查表。 自变量和函数值都是实数，求导结果也是实数。 推导：根据最基本的一元函数复合的求导法则即可。令 ，则： 。 矩阵对实数求导，结果是和 F^-1 同型的矩阵（也即和 F 同型的矩阵）。 推导：对恒等式 两边同时求导，再结合 |F| 的导数易得。 常见技巧及注意事项 实数在与一堆矩阵、向量作数乘时可以随意移动位置。且实数乘行向量时，向量数乘与矩阵乘法（1x1 矩阵和 1xm 矩阵相乘）的规则是一致的。 遇到相同下标求和就联想到矩阵乘法的定义，即 。特别地，一维下标求和联想到向量内积 ，二维下标求和联想到迹 （A,B 应为同型矩阵）。 如果在一个求和式中，待求和项不是实数而是矩阵的乘积，不要想着展开求和式，而要按照上面的思路，看成分块矩阵的相乘！ 向量的模长平方（或实数的平方和）转化为内积运算： 。矩阵的 F 范数的平方转化为迹运算： 。 多个矩阵相乘时，多用矩阵迹的求导公式转化、循环移动各项。实数也可看成 1X1 矩阵的迹！ 需要用到向量（或矩阵）对矩阵求导的情形，要么把矩阵按列拆开转化成向量对向量求导（最终很有可能通过分块矩阵乘法再合并起来。本文后面的算例 PRML(3.33) 说明了这种方法怎么用），要么套用线性变换的求导公式（常见于神经网络的反向传播过程）。 算例 最小二乘法 方法一：展开括号，再使用几个常用公式化简即可： 方法二：使用线性变换的求导公式： F 范数的求导公式推导 方法一：先转化为迹，再裂项，最后通过恰当的轮换，用迹方法的核心公式处理。 方法二：用线性变换的求导公式证。（注意矩阵转置不改变其 F 范数，并且实值函数对 X 和 X_T 的导数互为转置） 方法三：根据定义逐元素地算，然后合并成向量、再合并成矩阵。（太原始，易出错，不推荐） PRML (3.33) 求导 题目： 求 关于 W 的导数。 说明：上面的  的结果应当是一个向量，但是希腊字母打不出加粗的效果。 方法一：用矩阵的 F 范数推导： 上述几步的依据分别是： 将若干个列向量拼成一个矩阵，因此它们的二范数平方和就等于大矩阵的 F 范数的平方。 矩阵转置不改变其 F 范数。 矩阵数乘 (-1) 不改变其 F 范数。 线性变换的求导公式 + F 范数的求导公式。 实数在和矩阵作数乘时位置可以任意移动。 有了导数，再另导数等于零，即得 W 的最大似然解： 。 方法二： 将向量二范数用内积代替，然后逐项展开，最后利用分块矩阵相乘消掉求和号： 最后一步的化简的思考过程是把对 n 求和视为两个分块矩阵的乘积： 第一个矩阵是分块行向量，共 1xN 个块，且第 n 个分量是 。因此第一个矩阵是 第二个矩阵是分块列向量，共 Nx1 个块，且第 n 个分量是 。因此，第二个矩阵是： ，注意第二个等号的推导过程中，前一项能够拆开是因为它被看做两个分块矩阵的乘积，两个分块矩阵分别由 Nx1和 1x1 个块组成。 这种方法虽然比较繁琐，但是更具有一般性。 RNN 的梯度消失爆炸问题 通常 RNN 的状态方程的更新定义为 （f 表示一个逐元素的激活函数，例如 等。），而这里我们采用 Pascanu 等人的论文 On the difficulty of training Recurrent Neural Networks 中的定义，即认为 （这两种方程其实是等价的，只是前一种表述把隐层状态定义成激活后的值，后一种表述把隐层状态定义成激活前的值，前述论文中的脚注里也有说明。这里采用后一种方式，是因为它稍微好算一点）。展开后的网络结构示意图参见 中的 Slide 15。以下内容建议对照这份讲义的 15-19 页一起观看（另注：建议用 Stanford 的讲义梳理大致的思路，但是按照本讲稿下述步骤进行具体的求导运算。个人认为本讲稿中的过程更加清楚）。 现在我们来计算损失函数 l 对循环连接的权重矩阵 W 的导数：假设每一时间步都有一个误差 l_t（例如建立一个语言模型，每一步都要预测下一个词的概率分布，与语料库里的真实值计算交叉熵），总的误差等于每一步的误差加起来： ，因此 （对一元函数来说，和的导数等于导数的和。根据多元函数偏导数的定义，很容易推广到多元函数上，进而推广到矩阵求导上）。 考虑到矩阵 W 出现了多次，计算 需要计算 l_t 对 W 的每一次出现的导数，然后再求和。若用 W^(k) 表示 h_k-1 与 h_k之间的转移矩阵 W，则 。其中第二个等号用到的是线性变换的求导公式（类似标准 BP 算法的核心步骤） 然后根据雅克比矩阵的运算规则计算损失函数对隐层的导数 （表示将括号里的向量变成一个对角矩阵，跟前文的 互为逆运算。）： ，再将该式带入上一步中的式子，就得到 ，这就是 vanilla RNN 的 BPTT 的公式。（中间很多个隐层之间的雅克比相乘那一部分可以用求积符号来书写，这里的写法更直观一些） 注：实践中具体计算梯度的时候，一般还是先定义一组类似于 BP 神经网络 δ_t 的变量，使用循环逐层进行求导，而不是强行直接展开。这里展开是为了理论分析方便。 另注：Stanford 的讲义和前述论文中，均认为 ，这一点应该是错的，矩阵 W 不应该被转置，根据雅克比矩阵的定义写一个梯度检查的程序即可快速验证这一点。 Autoencoder with Tied-weight 求函数  对 W 的导数，其中 是逐元素求 Sigmoid。 根据变量多次出现的求导法则计算即可： ，其中 W_c 的含义是将 W 此次出现看做常数。 上式右边第一项计算如下： 第二项计算如下： ，其中第三个等号里定义 。 最终结果就是将以上两项合并起来，并去掉所有 W_c 中的下标，从略。 "
280,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737481&idx=5&sn=88164e7207460d239282d15aa6348c2f&chksm=871acf37b06d4621b2175f725de118fec28c6940c4e9df341ac2364aae6e50ff16e526b16379&scene=27,AAAI 2018 | 中科大提出新型连续手语识别框架LS-HAN，帮助「听」懂听障人士,"中科大一篇关于手语识别的论文被 AAAI 2018 接收。该论文提出一种新型连续手语识别框架 LS-HAN，无需时间分割。LS-HAN 由三部分构成：用于视频特征表示生成的双流卷积神经网络、用于缩小语义差距的潜在空间和基于识别的潜在空间分层注意力网络。实验结果表明该框架有效。 手语识别（SLR）面临的一个重要挑战是设计能够捕捉人体动作、姿势和面部表情的视觉描述符（descriptor)。主要有两类：手动制作的特征（Sun et al. 2013; Koller, Forster, and Ney 2015）和基于卷积神经网络的特征（Tang et al. 2015; Huang et al. 2015; Pu, Zhou, and Li 2016）。受 CNN 近期成功的启发，该论文作者设计了一种双流 3D-CNN 用于视频特征提取 。 时域分割是连续手语识别的另一个难题。连续 SLR 的常见方案是将句子分解成孤立的单词识别问题，这需要进行时域分割。时域分割并不简单，因为存在多种过渡动作，很难检测。而且时域分割作为预处理步骤，如果分割不准确就会导致后续步骤中出现错误。此外，标注每个孤立的片段非常耗时。 受利用长短期记忆（LSTM）网络进行视频描述生成的启发，研究者使用分层注意力网络（HAN，LSTM 的扩展）绕过时域分割，考虑结构信息和注意力机制。该方案需要向 HAN 馈送整个视频，然后逐词输出完成的句子。但是，HAN 可以根据输入视频和前一个单词来优化生成下一个单词的概率，但忽略了视频和句子之间的关系（Pan et al. 2015）。因此，它会遇到是否稳健的问题。为了解决这个问题，研究者整合了潜在空间（LS，Latent Space）模型，以明确地利用视频和文本句子之间的关系。 这篇论文的主要贡献如下： 提出新型双流 3D-CNN，用于视频特征表示生成； 提出适合连续 SLR 的新型 LS-HAN 框架，无需进行时域分割； LS-HAN 框架对相关性和识别损失进行联合优化； 编译最大的开源中国手语（CSL）数据集（截至 2017 年 9 月）用于连续 SLR，数据集具备句子级别的标注。 论文：Video-based Sign Language Recognition without Temporal Segmentation 论文链接：https://arxiv.org/abs/1801.10111 摘要：世界上数百万听障人士通常使用手语进行交流，因此手语自动翻译很有意义，也很重要。目前，手语识别（SLR）存在两个子问题：逐词识别的孤立手语识别，翻译整个句子的连续手语识别。现有的连续手语识别方法利用孤立 SLR 作为构造块，还有额外的预处理层（时域分割）、后处理层（句子合成）。不过，时域分割并不简单，且必然会向后续步骤传播误差。更糟糕的是，孤立 SLR 方法通常需要对句子中的每个单词分别进行标注，严重限制了可获取训练数据的量。为了解决这些难题，我们提出了一种新型连续手语识别框架，带有潜在空间的分层注意力网络（Hierarchical Attention Network with Latent Space，LS-HAN），无需对时间分割进行预处理。LS-HAN 由三部分构成：用于视频特征表示生成的双流卷积神经网络、用于缩小语义差距的潜在空间（Latent Space，LS）和基于识别的潜在空间分层注意力网络（HAN）。我们在两个大型数据集上进行了实验，实验结果表明我们提出的框架是有效的。 "
281,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737481&idx=4&sn=045012f63fdf68ea07124899c092e411&chksm=871acf37b06d46211bc0dbe5a2e62e1b5135407c63a65b649cd6551b67f0b28403e5f0e53150&scene=27,教程 | 如何在Python中用scikit-learn生成测试数据集,测试数据集是小型的专用数据集，它可以让你测试一个机器学习算法或测试工具。数据集中的数据有完整的定义（例如线性或非线性）使你可以探索特定的算法行为。scikit-learn Python 库提供一套函数，用于从可配置测试问题中生成样本来进行回归和分类。 在本教程中，你将学习测试问题及如何在 Python 中使用 scikit-learn 进行测试。 完成本教程后，你将知道： 如何生成多类分类预测测试问题 如何生成二进制分类预测测试问题 如何生成线性回归预测测试问题 让我们开始吧。 本教程被分成了 3 个部分，他们分别是： 1. 测试数据集 2. 分类测试问题 3. 回归测试问题 开发和实现机器学习算法时的一个问题是如何知道你是否已经正确实现了他们——它们似乎在有 bug 时也能工作。 测试数据集是小型设计问题，它能让你测试、调试算法和测试工具。它们对于更好地理解算法响应超参数变化的行为方面也很有用。 下面是测试数据集的一些理想特性： 它们可以快速、容易地生成。 它们包含「已知」或「理解」的结果来和预测进行比较。 它们是随机的，每次生成时都允许在同一个问题上随机变化。 它们很小、容易在而二维中实现可视化。 它们可以被增大。 我建议在开始一个新的机器学习算法或开发一个新的测试工具时使用测试数据集。scikit-learn 是一个用于机器学习的 Python 库，它提供了生成一组测试问题的函数。 在本教程中，我们将看一些为分类和回归算法生成测试问题的例子。 分类是将标签分配给数据的问题。在本节中，我们将看三个分类问题：blobs、moons 和 circles。 Blobs 分类问题  make_blobs() 函数可被用于生成具有高斯分布的 blobs 点。你可以控制生成 blobs 的数量，生成样本的数量以及一系列其他属性。考虑到 blobs 的线性可分性质，该问题也适用于线性分类问题。 下面的例子是一个多类分类预测问题，它生成了一个具有三个 blobs 的 2D 样本数据集。每个数据有两个输入和 0、1 或 2 个类的值。 完整的例子如下所示。 运行该示例将生成该问题的输入和输出，然后创建一个方便的 2D 图，用不同的颜色显示不同类的点。注意，考虑到问题生成器的随机特性，你的特定数据集和结果图会有所不同。这是一个特点，而不是一个错误。 我们将会在下面的例子中使用相同的示例结构。 Moons 分类问题 make_moons() 函数用于二进制分类并且将生成一个漩涡模式，或者两个 moons。你可以控制 moon 形状中的噪声量，以及要生产的样本数量。 这个测试问题适用于能够学习非线性类边界的算法。下面的例子生成了一个中等噪音的 moon 数据集。 完整的例子如下所示。 运行该示例会生成并绘制数据集以供查阅，然后再按照指定的类对样本着色。 Circles 分类问题 make_circles() 函数生成一个数据集落入同心圆的二进制分类问题。再一次地，与 moons 测试问题一样，你可以控制形状中的噪声量。该测试问题适用于可以学习复杂的非线性流行的算法。下面的例子中生成了一个具有一定噪音的 circles 数据集。 完整例子如下所示。 运行该示例并绘制数据集以供查阅。 回归是根据观察数据预测数量的问题。make_regression() 函数将创建一个输入和输出具有线性关系的数据集。你可以配置样本数量，输入特征数量，噪声级别等等。该数据集适用于可以学习线性回归函数的算法。 下面的例子将生成 100 个示例，他们具有适度的噪声，都有一个输入特征和一个输出特征。 完整例子如下所示。 运行该示例将生成数据并绘制 x 和 y 的关系，考虑到它是线性的，所以结果是很简单的。 扩展 本节列出了一些你可能想要探讨的扩展该教程的想法。 比较算法。选择一个测试问题，并比较该问题的一系列算法并汇报性能。 放大问题。选择一个测试问题并探索将其放大，用级数法来可视化结果，也可以探索一个特定算法模型技能和问题规模。 其他问题。库提供了一套其他测试问题；为每个问题编写了一个代码示例来展示它们是如何工作的。 如果你想要更深入的了解，本节提供了关于该课题更多的资源。 scikit-learn 用户指南: Dataset loading utilities (http://scikit-learn.org/stable/datasets/index.html) scikit-learn API: sklearn.datasets: Datasets (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) 在本教程中，我们学习了测试问题及如何用 scikit-learn 在 Python 中使用他们。 具体来说，其中包括： 如何生成多类分类预测测试问题； 如何生成二进制分类预测测试问题； 如何生成线性回归预测测试问题。 
282,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737338&idx=2&sn=d1eaad6cf8b3a048b82d6b8263ace281&chksm=871acec4b06d47d24d3c25e22ac8b9ea74ef644e7348952635c2ff3ed2433f06f0f2e2526caa&scene=27,独家 | 专访AAAI 2018最佳论文作者，记忆增强蒙特卡洛树搜索细节解读,"AAAI 2018 大会已于 2 月 2 日在美国新奥尔良开幕。在此之前，大会获奖论文的结果已经放出，阿尔伯塔大学提交的论文《Memory-Augmented Monte Carlo Tree Search》获得了 AAAI 2018 大会的杰出论文奖。该论文作者分别为博士生 Chenjun Xiao、梅劲骋与教授 Martin Müller。 Chenjun Xiao 硕士与博士阶段均就读于阿尔伯塔大学，师从 Martin Müller 教授。 梅劲骋本科毕业于华南理工大学，研究生赴上海交通大学，师从计算机系吕宝粮教授。2015 年起，他来到阿尔伯塔大学攻读博士，导师为 Dale Schuurmans 教授。 该论文的导师，阿尔伯塔大学教授 Martin Müller 则因计算机围棋而闻名于世。Müller 教授所带领的团队在博弈树搜索和规划的蒙特卡洛方法、大规模并行搜索和组合博弈论方面颇有建树。围棋程序 AlphaGo 的设计研发主导人物 David Silver 和黄士杰（Aja Huang）（他们分别是 AlphaGo Nature 论文的第一作者和第二作者，也名列于最近的 AlphaGo Zero 论文中）都来自于 Müller 门下。 这篇论文提出了记忆增强的蒙特卡洛树搜索（M-MCTS）方法，M-MCTS 的核心思想是将 MCTS 结合一种记忆结构，其中每一项记录包含一个特定状态的信息。通过结合相似状态的估计，这些记忆被用于生成一个近似值估计。研究人员在围棋中评估了 M-MCTS，实验结果表明 M-MCTS 的性能优于原始蒙特卡洛方法。 在得知获奖信息后，机器之心第一时间联系到了 Martin Müller 教授，并对论文的三位作者共同对论文中的内容、未来研究方向以及一些感兴趣的问题进行了交流。 对于论文的两名中国作者而言，得知获奖后的第一反应是惊讶和幸运。不过，在国际人工智能重要会议的最佳论文奖项上，中国人名字的出现早已成为常态，华人正在 AI 领域扮演着越来越重要的角色。阿尔伯塔大学里，Martin Müller 教授带领的博士生中有很多来自国内。「在阿尔伯塔大学，我们幸运地拥有很多世界级的学生前来攻读学位。」Müller 介绍道，「经我指导已经毕业的中国博士包括 Fan Xie（现为谷歌软件工程师），正在带的博士生有 Gaojian Fan、Chao Gao 和 Chenjun Xiao。他们都是在中国接受本科或研究生教育之后来到阿尔伯塔的，他们在理论背景上训练有素，同时也具备相关领域工作的实践经验。」 作为阿尔伯塔大学的博士生，Chenjun Xiao 等人可以说和 David Silver 和黄士杰师出同门。他们也对 DeepMind 最新的 AlphaGo Zero 发表了一番看法。 「这是我们目前已知的最佳启发式方法了。」Chenjun Xiao 说道。 Martin Müller 教授则认为 AlphaGo Zero 还未到达算法的极限：「但它仍然是一个启发式方法，非常强大，但并不完美……」 梅劲骋也指出了 AlphaGo 目前存在的限制：「当状态、模型和转换是完美已知的时候，这种方法才能展现能力。」 随着人工智能技术逐渐走向实用化，越来越多的科技巨头开始参与其中，业界的学术影响力也在日益提升。在  中，来自谷歌的被接收论文数量高达四十余篇，是第二名 UC Berkeley 的四倍之多。目前大学中的人工智能研究或许正因为计算资源的不足而逐渐落后于科技巨头。但 Martin Müller 认为，在大学环境中，学者们仍然可以进行有意义的研究。最佳论文也对这种观点给出了有力的证明。 在围棋之外，阿尔伯塔大学的研究者们也把蒙特卡洛方法应用在了六贯棋上（Hex，一种在六边形格棋盘上进行的桌游），Martin Müller 博士生 Chao Gao 和 Ryan Hayward 教授正在共同研究这一方向。此外，研究人员们已经在把眼光投向了更为复杂的强化学习任务，如即时战略游戏上。 深度学习作为近期人工智能发展的标志性技术，引出了无数的新方法和新应用，却也因为使用场景受限而遭到越来越多人的诟病。近日，Gary Marcus、Yann LeCun 等人对深度学习的局限性展开了很多探讨。Martin Müller 也对此表达了自己的态度：「深度学习对于学习非常复杂的函数而言非常有用，但搜索会始终在这一过程中扮演重要的角色。搜索永远不会被「纯粹的知识」取代。AlphaGo Zero 就是最好的例子，神经网络加上搜索的 Elo 得分超过了单独的神经网络高达 2000 分！这是一个非常大的差距，随着机器获取的知识越来越多，这个差距只会越来越大。」   因当时最佳论文还未公开，在文章《 》中我们无法介绍更多技术细节。如今，该论文已经放出，机器之心编译介绍如下： 蒙特卡洛树搜索（MCTS）的核心思想是构建一个搜索树，且搜索树的状态由快速蒙特卡洛模拟（Coulom 2006）评估。若从给定博弈状态开始，并通过随机 Self-play 在观察到最终结果前模拟成千上万次博弈，然后我们就可以将模拟的平均输出作为状态值的估计。同时，MCTS 在模拟中会维护一个搜索树，因而借助它指导模拟的方向，其中我们可以使用老虎机算法（bandit algorithm）来权衡利用（exploitation）和探索（exploration）（Kocsis and Szepesvari 2006）。然而，MCTS 不能有效保证「大型状态空间」的价值估计准确度，因为在相对有限的搜索时间内，状态的平均值作为估计会有较高的方差。因此，不准确的估计会误导搜索树的构建，并严重降低程序的性能。 最近，已经有学者提出几种机器学习方法来克服 MCTS 的这种缺点。例如深度神经网络可以用来学习领域知识和逼近状态值的函数。这些方法和与 MCTS 相结合可以提供启发式的方法以提高搜索样本的效率（Silver et al. 2016; Tian and Zhu 2015）。 机器学习方法的成功可以很大程度上归因于模型的泛化性能，即类似的状态共享相似的信息。泛化空间的领域知识一般由函数近似表征，例如深度网络通过一般数据集或自生成的模拟数据集来离线训练（Silver et al. 2016）。 与从离线学习过程中探索泛化的研究相比，在线实时搜索并没有过多关注利用泛化的优势。本论文提出和评估了一种增强记忆的 MCTS 算法，它提供了一种利用在线泛化优势的替代型方法。我们设计了一种记忆，其中每个元素（entry）都包含特定状态的信息，并可作为构建在线值近似的基础。我们利用围棋的实验证明这种基于记忆的框架对于提升 MCTS 的性能十分有效，不论是在理论还是实践中。 论文：Memory-Augmented Monte Carlo Tree Search   论文链接：https://webdocs.cs.ualberta.ca/~mmueller/ps/2018/Chenjun-Xiao-M-MCTS-aaai18-final.pdf 摘要： 我们在本文中提出记忆增强的蒙特卡洛树搜索（Memory-Augmented Monte Carlo Tree Search，M-MCTS）并对其进行了评估，提供了利用在线实时搜索的泛化能力的新方法。M-MCTS 的核心思想是将 MCTS 结合一种记忆结构，其中每一项记录包含一个特定状态的信息。通过结合相似状态的估计，这些记忆被用于生成一个近似值估计。我们在本文中表明基于记忆的值逼近在温和条件下高概率地优于原始的蒙特卡洛评估方法。我们在围棋中评估了 M-MCTS。实验结果表明 M-MCTS 在相同的模拟次数下优于原始的 MCTS。 蒙特卡洛树搜索 MCTS 构建树以评估状态并进行快速模拟（Coulom 2006）。树中的每个节点对应一个具体的状态 s∈S，并包含模拟统计 V (s) hat 和 N(s)。在算法的每一次迭代中，一个模拟从一个初始状态 s_0 状态开始，之后进入两个阶段： in-tree 和 rollout。在当前的搜索树表征了状态 s_t 时，会应用树策略选择一个动作，以达到下一个状态。树策略的最常用选择是使用老虎机算法，例如 UCB1（Kocsis and Szepesvari 2006）。对于树之外的策略，树将应用 roll-out 策略模拟一场博弈直到结束，其中被访问的状态的轨迹为 T = {s_0, s_1, . . . , s_T }，并在最后获得返回值 R。树中的 s∈T 的统计根据下式进行更新：   此外，树也同时在生长。在最简单的方案中，第一个被访问的尚未在树中的节点会被添加到树上。 MCTS 结合记忆 我们现在介绍记忆增强 MCTS（M-MCTS）算法。图（1）提供了简要的图示。M-MCTS 和常规的 MCTS 的主要区别在于，M-MCTS 搜索树的每一个节点都会存储统计的一个扩展集合：   这里，N_M 是近似记忆值 V_M(s) hat 的估计次数。在 MCTS 的 in-tree 树搜索期间，我们使用  取代 V(s) hat 作为状态 s 的值，用于 in-tree 选择，例如在 UCB 公式中。λ_s 是一个延迟参数，确保不存在非对称的偏差。 图 1：M-MCTS 的简要图示。当搜索到一个叶状态时，会生成一个特征表征φ（s），然后其被用于询问（query）基于记忆的值近似 V_M(s) hat。V_M(s) hat 被用于根据下式更新 s 和 s 的所有过去状态，如图中的红色箭头所示。   我们在围棋游戏中评估了 M-MCTS，我们的基线结果是基于开源的围棋程序 Fuego（Enzenberger and Muller 2008 2017），但是添加了 DCNN 以提升性能。下图展示了实验结果：   我们每次落子从 {1000, 5000, 10000} 使用不同的模拟次数，实验结果展示在上图 2(a)-(c) 中。在我们使用设定 {M = 50, τ = 0.1} 时获得了最好的结果，它以每次落子进行 10000 次模拟对抗基线算法并实现了 71% 的胜率。此外，我们也探索了不同记忆大小 {1000, 5000, 10000} 的影响。M 和 τ 分别设置为 50 和 0.1，实验结果在上图的 2(d) 中展示。直观上，我们会认为较大的记忆会有更好的性能，因为 query 会包含更多的候选状态，以上的实验结果也证明了这一点。 在本论文中，我们提出了一个有效的方法来利用实时搜索的在线泛化。我们的方法，记忆增强的蒙特卡洛树搜索（M-MCTS），将原始的 MCTS 算法与存储框架相结合，来提供基于存储的在线数值近似。未来，我们计划探索以下两个方向。首先，我们想探索是否可以通过结合离线学习的数值近似来让我们的在线存储框架获得更好的泛化性能；其次，让 M-MCTS 的特征表示重用一个神经网络来预测下一步。 "
283,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737338&idx=1&sn=34c5cab8287b2138b7f80ff730fe2cd0&chksm=871acec4b06d47d299d3a72ec955d0353b690f8734c61ae864d1054a90c5f07a6cdc0f9fe380&scene=27,快速开启你的第一个项目：TensorFlow项目架构模板,"作为最为流行的深度学习资源库，TensorFlow 是帮助深度学习新方法走向实现的强大工具。它为大多数深度学习领域中使用的常用语言提供了大量应用程序接口。对于开发者和研究人员来说，在开启新的项目前首先面临的问题是：如何构建一个简单明了的结构，本文或许可以为你带来帮助。 项目链接：https://github.com/Mrgemy95/Tensorflow-Project-Template 简洁而精密的结构对于深度学习项目来说是必不可少的，在经过多次练习和 TensorFlow 项目开发之后，本文作者提出了一个结合简便性、优化文件结构和良好 OOP 设计的 TensorFlow 项目模板。该模板可以帮助你快速启动自己的 TensorFlow 项目，直接从实现自己的核心思想开始。 这个简单的模板可以帮助你直接从构建模型、训练等任务开始工作。 概述 详述 项目架构 文件夹结构 主要组件 模型 训练器 数据加载器 记录器 配置 Main 未来工作 简言之，本文介绍的是这一模板的使用方法，例如，如果你希望实现 VGG 模型，那么你应该： 在模型文件夹中创建一个名为 VGG 的类，由它继承「base_model」类 覆写这两个函数 ""build_model""，在其中执行你的 VGG 模型；以及定义 TensorFlow 保存的「init_saver」，随后在 initalizer 中调用它们。 在 trainers 文件夹中创建 VGG 训练器，继承「base_train」类。 覆写这两个函数「train_step」、「train_epoch」，在其中写入训练过程的逻辑。 在主文件中创建会话，创建以下对象：「Model」、「Logger」、「Data_Generator」、「Trainer」与配置： 向所有这些对象传递训练器对象，通过调用「trainer.train()」开始训练。 你会看到模板文件、一个示例模型和训练文件夹，向你展示如何快速开始你的第一个模型。 模型架构 文件夹结构 主要组件 模型 基础模型 基础模型是一个必须由你所创建的模型继承的抽象类，其背后的思路是：绝大多数模型之间都有很多东西是可以共享的。基础模型包含： Save-此函数可保存 checkpoint 至桌面。 Load-此函数可加载桌面上的 checkpoint。 Cur-epoch、Global_step counters-这些变量会跟踪训练 epoch 和全局步。 Init_Saver-一个抽象函数，用于初始化保存和加载 checkpoint 的操作，注意：请在要实现的模型中覆盖此函数。 Build_model-是一个定义模型的抽象函数，注意：请在要实现的模型中覆盖此函数。 你的模型 以下是你在模型中执行的地方。因此，你应该： 创建你的模型类并继承 base_model 类。 覆写 ""build_model""，在其中写入你想要的 tensorflow 模型。 覆写""init_save""，在其中你创建 tensorflow 保存器，以用它保存和加载检查点。 在 initalizer 中调用""build_model"" 和 ""init_saver"" 训练器 基础训练器 基础训练器（Base trainer）是一个只包装训练过程的抽象的类。 你的训练器 以下是你应该在训练器中执行的。 创建你的训练器类，并继承 base_trainer 类。 覆写这两个函数，在其中你执行每一步和每一 epoch 的训练过程。 数据加载器 这些类负责所有的数据操作和处理，并提供一个可被训练器使用的易用接口。 记录器（Logger） 这个类负责 tensorboard 总结。在你的训练器中创建一个有关所有你想要的 tensorflow 变量的词典，并将其传递给 logger.summarize()。 配置 我使用 Json 作为配置方法，接着解析它，因此写入所有你想要的配置，然后用""utils/config/process_config""解析它，并把这个配置对象传递给所有其他对象。 Main 以下是你整合的所有之前的部分。 1. 解析配置文件。 2. 创建一个 TensorFlow 会话。 3. 创建 ""Model""、""Data_Generator"" 和 ""Logger""实例，并解析所有它们的配置。 4. 创建一个""Trainer""实例，并把之前所有的对象传递给它。 5. 现在你可通过调用""Trainer.train()""训练你的模型。 未来，该项目计划通过新的 TensorFlow 数据集 API 替代数据加载器。 "
284,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737338&idx=3&sn=8cf39da67f8da55444a7ea60ec6fd0d4&chksm=871acec4b06d47d2361ab084bc08e6cda9108eb6aa17869d51d54e90014c8552cbda3450db8c&scene=27,资源 | 从搜索到智能客服：阿里开放强化学习技术演进与实践书籍,下载地址：http://techforum-img.cn-hangzhou.oss-pub.aliyun-inc.com/1517812754285/reinforcement_learning.pdf 强化学习（RL）是关于序列决策的一种工具，它可以用来解决科学研究、工程文理等学科的一系列问题，它也是围棋程序 AlphaGo 的重要组成部分。在 Richard Sutton 的描述中，交互式学习几乎是所有学习与智能理论的基石，而强化学习就是这样的一种理想条件下实现交互式学习的方法。 在探讨阿里的强化学习实践书籍前，我们需要明确几个基本概念。首先，监督学习和强化学习之间的主要区别在于收到的反馈是评估性的还是指导性的。指导性反馈提示如何达到目标，而评估性反馈告诉你达到目标的程度。监督学习一般是基于指导性反馈来解决问题，而强化学习则基于评估性反馈解决问题。因此在很多情景中，强化学习这种评估性的反馈使其具有格外的优势与强大的性能。 因为存在这些差别，阿里表明基于监督学习方式的信息提供手段，缺少有效的探索能力，系统倾向于给消费者推送曾经发生过行为的信息单元（商品、店铺或问题答案）。而强化学习作为⼀种有效的基于用户与系统交互过程建模和最大化过程累积收益的学习方法，在⼀些阿里具体的业务场景中进行了很好的实践并得到⼤规模应用。 实际上正如阿里的这本书所述，如果把搜索引擎看作智能体（Agent）、把用户看作环境（Environment），则商品的搜索问题可以被视为典型的顺序决策问题（Sequential Decision making Problem）： (1) 用户每次请求 PV 时，Agent 做出相应的排序决策，将商品展示给用户； (2) 用户根据 Agent 的排序结果，给出点击、翻页等反馈信号； (3) Agent 接收反馈信号，在新的 PV 请求时做出新的排序决策； (4) 这样的过程将⼀直持续下去，直到用户购买商品或者退出搜索。 在以上问题的形式化中，Agent 每⼀次策略的选择可以看成⼀次试错（Trial-and-Error），在这种反复不断地试错过程中，Agent 将逐步学习到最优的排序策略。而这种在与环境交互的过程中进行试错的学习，正是强化学习（Reinforcement Learning，RL）的根本思想。 除了上述所述基于强化学习的实时搜索排序，阿里在很多任务或功能上都采用了强化学习的解决方案。以下展示了该书籍的主要目录，读者可以了解到底阿里在哪些业务或实践上借助强化学习而实现更好的性能。 第一章 基于强化学习的实时搜索排序策略调控 1.1 背景 1.2 问题建模 1.2.1 强化学习简介 1.2.2 状态定义 1.2.3 奖赏函数设定 1.3 算法设计 1.3.1 策略函数 1.3.2 策略梯度 1.3.3 值函数的学习 1.4 奖赏塑形 1.5 实验效果 1.6 DDPG 与梯度融合 1.7 总结与展望 第二章 延迟奖赏在搜索排序场景中的作用分析 2.1 背景 2.2 搜索排序问题回顾 2.3 数据统计分析 2.4 搜索排序问题形式化 2.5 理论分析 2.5.1 马尔可夫性质 2.5.2 折扣率 2.6 实验分析 第三章 基于多智能体强化学习的多场景联合优化  3.1 背景 3.2 问题建模 3.2.1 相关背景简介 3.2.2 建模方法 3.3 应用 3.3.1 搜索与电商平台 3.3.2 多排序场景协同优化 3.4 实验 3.4.1 实验设置  3.4.2 对比基准 3.4.3 实验结果 3.4.4 在线⽰例 3.5 总结与展望 第四章 强化学习在淘宝锦囊推荐系统中的应用 4.1 背景 4.1.1 淘宝锦囊 4.1.2 锦囊的类型调控 4.1.3 ⼯作摘要 4.2 系统框架及问题建模 4.2.1 系统框架 4.2.2 问题建模 4.3 算法及模型设计 4.3.1 主体框架 4.3.2 分层采样池 4.3.3 基准约减 4.3.4 算法流程 4.4 实验与总结 第五章 基于强化学习的引擎性能优化  5.1 背景 5.2 问题建模 5.2.1 状态定义 5.2.2 动作空间设计 5.2.3 状态转移函数 5.2.4 奖赏函数的设计 5.3 算法设计 5.3.1 Loss Function 5.3.2 Actor-crtitic 方法 5.4 理论分析 5.5 实验效果  5.6 总结 第六章 基于强化学习分层流量调控 6.1 背景 6.2 问题建模 6.2.1 Dynamic Action Boundary by CEM 6.3 实验效果 6.4 总结与展望 第七章 风险商品流量调控 7.1 背景 7.1.1 为什么进行风险商品流量调控 7.1.2 为什么使用强化学习调控 7.2 基于强化学习的问题建模 7.2.1 状态空间的定义 7.2.2 动作空间的定义 7.2.3 奖赏函数的定义 7.2.4 模型选择 7.2.5 奖赏函数 scale 7.3 流量调控系统架构 7.4 线上效果 第八章 虚拟淘宝 8.1 背景  8.1.1 强化学习⾯临的问题 8.1.2 虚拟淘宝 8.2 学习用户行为：监督学习 8.3 学习用户意图：逆强化学习 8.3.1 逆强化学习概述 8.3.2 学习用户意图 8.3.3 生成对抗式模仿学习 8.4 构建用户行为模拟器 8.4.1 问题建模 8.4.2 算法设计 8.4.3 实验结果 第九章 组合优化视角下基于强化学习的精准定向广告 OCPC 业务优化 9.1 背景 9.2 问题建模 9.2.1 奖赏 9.2.2 动作 9.2.3 状态定义 9.3 建模粒度 9.4 模型选择 9.5 探索学习 9.6 业务实战 9.6.1 系统设计 9.6.2 奖赏设计 9.6.3 实验效果 9.7 总结与展望 第十章 策略优化方法在搜索广告排序和竞价机制中的应用 10.1 业务背景 10.2 ⼴告排序和竞价的数学模型和优化方法 10.3 ⾯向⼴告商、⽤户和平台收益的排序公式设计 10.4 系统简介 10.4.1 离线仿真模块 10.4.2 离线强化学习进⾏排序策略模型初始化 10.5 在线排序策略模型优化 10.6 实验分析 10.7 总结 第十一章 TaskBot －阿里小蜜的任务型问答技术 11.1 背景和问题建模 11.2 模型设计 11.2.1 Intent Network 11.2.2 Belief Tracker 11.2.3 Policy Network 11.2.4 模型 11.3 业务实战 11.4 总结 第十二章 DRL 导购－阿里小蜜的多轮标签推荐技术 12.1 背景 12.2 算法框架 12.3 深度强化学习模型 12.3.1 强化学习模块 12.3.2 最终模型 12.4 业务实战 12.5 总结和展望 最后，强化学习在阿里巴巴内部的实践远不止于此，这本电子书只介绍了其中的⼀部分。我们希望这本书能有助于读者了解强化学习在业界的应用，并从实践和业务的角度了解阿里在商业化技术的能力。 
285,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737298&idx=1&sn=43dd580c0456592948d56618623193e2&chksm=871aceecb06d47fa6b378706960d935c096ea1faef83a4b4450b49040de2056192bad61c00a3&scene=27,一段Python代码，告诉你机器之心今天的秘密,"机器之心今天的情绪，是怎样一段待你解开的 Python 代码： 将上图输入以下卷积网络，你想要的都在这~ def inference (input_tensor) def synced (image) def main (argv=None) 「Life is short, you need Python」 "
286,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737603&idx=4&sn=216d3980555ab9e6e8365b6e2f83cad4&chksm=871acfbdb06d46abe8c47f0cb2b7190e765d66fb0104e0beeafd1db156b2a36ac3698409b46d&scene=27,前沿 | DeepMind提出新型架构IMPALA：帮助实现单智能体的多任务强化学习,DeepMind 深度强化学习（DeepRL）在很多任务中取得了成功，从机器人的连续控制问题到围棋、Atari 等游戏。不过这些领域中的进步还限制在单个任务，即在单个任务中对智能体进行调整和训练。DeepMind 最近提出的 IMPALA 开始尝试利用单智能体 同时 处理多个任务，其架构性能超越此前方法数倍，具有强大的可扩展性，同时也展示了积极的迁移性质。与新架构同时提出的还有任务集合 DMLab-30。 项目 GitHub：https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30 DeepMind 近期的研究尝试探索在多个任务上训练单个智能体。 今天，DeepMind 发布了新的任务集合 DMLab-30，其包含公共动作空间的视觉统一环境中的多种挑战性任务。想训练在多个任务中表现良好的智能体，我们需要高吞吐量和高效利用每个数据点的算法架构。因此，DeepMind 开发了一种可用于分布式训练的具备高扩展性的新型智能体架构 IMPALA（Importances Weighted Actor-Learner Architectures），该架构使用一种新型离策略修正算法 V-trace。 DMLab-30 DMLab-30 是使用 DeepMind 的开源强化学习环境 DeepMind Lab 设计出的新型任务集合。任意 DeepRL 研究者都可以使用这些环境在大量有趣的任务或多任务设置中测试系统。 设计者使这些任务尽可能多样化。它们具备不同的目标，从学习、记忆到导航等等。从视觉上来看它们也是不同的——从亮度、现代风格的纹理，从绿到浅棕色等等。它们包括不同的物理设置，从开放的多山地带到直角迷宫，再到开放的圆形房间。 此外，一些环境包括「bots」，可以执行内部的目标导向动作。同样重要的是，不同级别的任务目标和奖励也不同，从遵循语言命令、使用钥匙开门、采蘑菇，到绘画、沿着一条复杂、不可逆的路径行走。 但是，从动作和观察空间来看，这些环境基本上是一样的，允许单个智能体在该高度变化的任务集合的每一个环境中进行训练。关于环境的更多细节，详见：https://github.com/deepmind/lab。 IMPALA：Importance-Weighted Actor-Learner Architectures 为了解决智能体在 DMLab-30 中进行训练的问题，DeepMind 开发了一种新型分布式智能体 IMPALA，它利用高效的 TensorFlow 分布式架构最大化数据吞吐量。 IMPALA 受流行的 A3C 架构的启发，A3C 架构使用多个分布式 actor 来学习智能体的参数。在此类模型中，每个 actor 使用策略参数在环境中动作。actor 周期性地暂停探索，和中央参数服务器共享它们计算出的梯度，用于梯度更新（见下图）。 IMPALA 的 actor 不用于计算梯度，而是用于收集经验，然后传输至可计算梯度的中央学习器，生成一个具备完全独立的 actor 和 learner 的模型。为了利用现代计算系统，IMPALA 可使用单个学习器或执行同步更新的多个学习器来实现。用这种方式分离学习和动作可以有效地提高整个系统的吞吐量，因为 actor 不再需要等待学习步（像 batched A2C 架构中那样）。这使得我们可以在多个有趣的环境中训练 IMPALA，无需经历帧渲染时间的变动或耗时的任务重启。 但是，决策与学习的分离会导致 actor 中的策略落后于学习器。为了弥补这一差异，DeepMind 引入了一种原则性离策略优势 actor critic 算法——V-trace，它通过 actor 的离策略弥补了轨迹。关于 V-trace 的具体细节可参考论文《IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures》（见文末）。 由于 IMPALA 的优化模型相对于类似智能体而言，可以处理一到两个数量级的更多经验，让复杂环境中的学习变为可能。DeepMind 比较了 IMPALA 与几种流行的 actor-critic 方法，发现新方法的速度有显著提高。此外，使用 IMPALA 的吞吐量增长与 actor 和 learner 的增加呈线性关系，这意味着分布式智能体模型和 V-trace 算法可以处理大规模实验，上千台机器都没有问题。 在 DMLab-30 的测试中，IMPALA 处理数据的效率是分布式 A3C 的 10 倍，最终得分是后者的 2 倍。另外，IMPALA 在多任务设置的训练中，相比单任务训练还展示了正向迁移的性质。 论文：IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures 论文链接：https://arxiv.org/abs/1802.01561 摘要： 在本研究中，我们专注于使用单一强化学习智能体与单一参数集解决大型任务集合的问题。在这样的条件下，最主要的挑战是处理越来越大的数据量和不断增加的训练时间——这在单一任务学习中已经是一个挑战。我们开发了一种新的分布式智能体 IMPALA（Importance-Weighted Actor Learner Architecture），它可以扩展到数千台机器上，每秒吞吐量高达 25 万帧。我们通过结合决策和学习分离与新型离策略修正方法 V-trace，达到了很高的吞吐量，实现了稳定学习，这对于学习的稳定性至关重要。我们展示了 IMPALA 在 DMLab-30（DeepMind Lab 环境中一组 30 个任务的集合）和 Atari-57（Arcade Learning Environment 中所有可用的 Atari 游戏）中进行多任务强化学习的有效性。我们的结果展示了 IMPALA 的性能优于之前的智能体，使用的数据更少，更重要的是新方法可以在多任务中展现出积极的迁移性质。 原文链接： 
287,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737297&idx=4&sn=075ed17c1fa9ec09309c1bea0f72785e&chksm=871aceefb06d47f9afd7fa28706f660c2dcaec3193575b7a637701ceb5620dcf7173975dcec9&scene=27,学界 | Nested LSTM：一种能处理更长期信息的新型LSTM扩展,"arXiv 近日，CMU 和蒙特利尔大学联合提出一种新型的多级记忆的 RNN 架构——嵌套 LSTM。在访问内部记忆时，嵌套 LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为嵌套 LSTM 有潜力直接取代堆栈 LSTM。 虽然在层级记忆上已有一些研究，LSTM 及其变体仍旧是处理时序任务最流行的深度学习模型，例如字符级的语言建模。特别是默认的堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。在此论文中，研究者们提出并探索了一种全新的嵌套 LSTM 架构（Nested LSTM，NLSTM)，并认为其有潜力直接取代堆栈 LSTM。 在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆，使用标准的 LSTM 门选择性地进行读取、编写。相比于传统的堆栈 LSTM，这一关键特征使得模型能实现更有效的时间层级。在 NLSTM 中，(外部）记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在堆栈 LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，堆栈 LSTM 与嵌套 LSTM 之间的主要不同是，NLSTM 可以选择性地访问内部记忆。这使得内部记忆免于记住、处理更长时间规模上的事件，即使这些事件与当前事件不相关。 在此论文中，作者们的可视化图证明了，相比于堆栈 LSTM 中的高层级记忆，NLSTM 的内部记忆确实能在更长的时间规模上操作。实验也表明，NLSTM 在多种任务上都超越了堆栈 LSTM。 直观上，LSTM 中的输出门会编码仍旧值得记忆的信息，这些记忆可能与当前的时间步骤不相关。嵌套 LSTM 根据这一直观理解来创造一种记忆的时间层级。访问内部记忆以同样的方式被门控，以便于长期信息只有在情景相关的条件下才能选择性地访问。 架构 在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式： 这些方程式与 Graves (2013) 等人定义的是非常相似的，但不包括 peephole 连接。Nested LSTM 使用已学习的状态函数 c_t = m_t(f_t⊙c_t−1, i_t⊙g_t) 来替代 LSTM 中计算 c_t 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），我们会调用该函数以计算 c_t 和 m_t+1。我们可以使用另一个 LSTM 单元来实现该记忆函数，因此如上图 1 所示就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。 给定以上所述的架构特性，NLSTM 中记忆函数的输入和隐藏状态为： 特别的，注意如果记忆函数是加性的，那么整个系统将退化到经典的 LSTM，因此记忆单元的状态更新为： 图 2：LSTM、堆叠 LSTM 和嵌套 LSTM 的计算图。隐藏状态、外部记忆单元和内部记忆单元分别由 h、c 和 d 表示。当前隐藏状态可以直接影响下一个内部记忆单元的内容，而内部记忆单元只通过外部记忆单元才影响隐藏状态。 在本论文提出的 Nested LSTM 变体架构中，我们会使用 LSTM 作为记忆函数，且内部 LSTM 的运算方式由以下一组方程式控制： 现在，外部 LSTM 的单元状态更新方式为： 可视化 图 3：关于内部单元（图左）和外部单元（图右）的输入特征的单元激活的可视化。红色表示负单元状态值，蓝色表示正单元状态值。更深的颜色表示更大的值。对于内部 LSTM 的状态，对 tanh（c_t tilde）进行了可视化（因为 c_t tilde 未约束），而对于外部 LSTM 的状态，则直接可视化了 c_t。 图 4：tanh（c^n_t）的可视化，表征第一（图右）和第二（图左）堆栈层的输入字符的单元激活。红色表示负单元状态值，蓝色表示正单元状态值。更深的颜色表示更大的值。 Penn Treebank 字符级语言建模 图 5：在 PTB 的测试和验证集上的 BPC（bits per character）vs. Epoch 曲线。 表 1：嵌套 LSTM 和多个基线模型的 BPC 损失的对比。测试（test）的 BPC 损失分别和各个模型在最小验证（valid）BPC 值的 epoch 的损失相关。 中文诗歌生成 表 2：嵌套 LSTM 和多个基线模型在中文诗歌生成数据集（Chinese Poetry Generation dataset）上的困惑度的对比。 图 6：在中文诗歌生成的测试和验证集上进行字符级预测的困惑度 vs. Epoch 曲线。 MNIST Glimpses 表 3：嵌套 LSTM 和多个基线模型在 MNIST Glimpses 任务上的 NLL（负对数似然度）和准确率的对比。其中采用的 epoch 是每个模型在验证集上有最高准确率的 epoch。和 NLL 相似，模型的验证 NLL 被用于确定测试 NLL 的 epoch。 图 7：在 MNIST Glimpses 的训练集和验证集上的 NLL（图左）和误差率（图右）vs. Epoch 的曲线图。 论文：Nested LSTMs  论文地址：https://arxiv.org/pdf/1801.10308.pdf 我们在本文中提出嵌套 LSTM（Nested LSTM，NLSTM），这是一种新型的多级记忆的 RNN 架构。NLSTM 通过嵌套（和堆栈相对）为 LSTM 增加深度。NLSTM 的一个记忆单元的值由一个 LSTM 单元（有自身的内部记忆单元）计算。具体来说，NLSTM 记忆单元不会如经典 LSTM 那样计算（外部的）记忆单元的值： 而是利用级联：  作为内部 LSTM（或 NLSTM）记忆单元的输入，并设定  我们的实验表明，在相似的参数数量下，嵌套 LSTM 在多种字符级语言建模任务中的表现都超越了堆栈和单层 LSTM，并且和堆栈 LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。 "
288,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737297&idx=2&sn=b2e08057950c7d429097d7f004cd99f1&chksm=871aceefb06d47f994e16493652a4ca84f1f322171c6f97cd0dcc5072b23a3e3a59b01ce66b8&scene=27,教程 | 使用Gym和CNN构建多智能体自动驾驶马里奥赛车,"Medium 本文描述的卷积神经网络超出了简单模式识别的范畴，能够学习到控制一辆自动汽车所需的所有过程。作者介绍了如何利用 CNN 和 OpenAI Gym，创建一个多智能体的系统，这些模型可以自动驾驶马里奥赛车，并且彼此竞争。 对机器学习感兴趣的人都知道基于人工智能的强化学习的能力。过去的纪念见证了很多使用强化学习（RL）做出的突破。DeepMind 将强化学习与机器学习相结合，在很多 Atari 游戏中达到了超越人类的结果，并且在 2016 年 3 月的时候以 4:1 的成绩击败了围棋冠军李世石。尽管强化学习目前在很多游戏环境中超越了人类，但是用它来解决一些需要最优决策和效率的问题还是比较新颖的，而且强化学习也会在未来的机器智能方面起到重要的作用。 简单地解释，强化学习就是智能体通过采取行动与环境交互以尝试最大化所得的积累奖励的计算方法。下面是一张简图（智能体—环境循环），图来自于强化学习简介（第二版）（Reinforcement Learning: An Introduction 2nd Edition，http://incompleteideas.net/sutton/book/the-book-2nd.html）。 在当前状态下 St 的智能体采取行动 At，环境对这个动作进行交互和回应，返回一个新的状态 S(t+1)，并给智能体一个 R(t+1) 的奖励。然后智能体选择下一个行动，然后这个循环一直重复，直到问题被解决了或者被中断了。 最近强化学习被用来解决一些挑战性问题，从游戏到机器人学。在工业应用中，强化学习也开始作为一个实际组件出现，例如数据中心冷却。而强化学习的绝大多数成功则来自于单智能体领域，在单智能体领域中不需要对其他行动者的行为进行建模和预测。 然而，也存在很多涉及多智能体交互的重要应用，其中会出现共同进化的智能体的新兴行为和复杂度。例如，多机器人控制，通信和语言的发现，多个玩家参与的游戏，以及对社会困境的分析都会涉及多智能体领域。相关的问题也可以以不同的级别和水平来等同于多智能体问题，例如分层强化学习的变体也可以被看做多智能体系统。此外，多智能体自我模拟最近也被证明是一个有用的训练范式。在构建能够与人类有效交互的人工智能系统时，将强化学习成功地扩展到多智能体问题中是很关键的。 不幸的是，Q-learning 和策略梯度等传统的强化学习方法不能很好地适应于多智能体环境。一个问题是，每个智能体的策略都是随训练过程发生变化的，并且在任意单独的智能体的角度看来，环境会变得不稳定（以一种在智能体自己的策略中没有解释解释得方式）。这就带来了学习稳定性的挑战，以及避免了对过去经验回放（experience replay）的直接利用，经验回放对稳定深度 Q 学习是很关键的。另一方面，当需要多智能体协作的时候，策略梯度方法会表现出较高的方差。 或者，还可以使用基于模型的策略优化，这种优化可以通过反向传播学到最佳策略，但是这需要一个不同的关于环境变化的模型以及关于智能体之间交互的假设。从优化角度来看，将这些方法应用在竞争环境中也是有挑战的，其在对抗训练方法中已被证明存在高度的不稳定性。 卷积神经网络 卷积神经网络革新了模式识别。在卷积神经网络广泛使用之前，绝大多数模式识别任务都是用初始阶段的人工特征加一个分类器执行的。 卷积神经网络的突破就是：所有的特征都是从样本自动学习到的。卷积神经网络在图下个识别任务上特别强大，因为卷积运算能够捕获图像的二维本质特点。此外，通过使用卷积核来扫描整个图片，与所有的运算次数相比，最终学到的参数会相对少一些。 尽管卷积神经网络在商业应用中已经有超过 20 年的历史了，但是对它们的应用在最近几年才开始爆发（这是因为以下两个进展）： 有了便于使用的大规模的标签数据集，例如大规模视觉识别挑战（Large Scale Visual Recognition Challenge，ILSVRC）。 在大规模并行的 GPU 上实现了卷积神经网络，这极大的加快了学习和推理过程的速度。 在这篇文章中，我们描述的卷积神经网络已经超出了简单模式识别的范畴。它能够学习到控制一辆自动汽车所需的所有过程。 使用卷积神经网络和 OpenAI Gym，我们可以创建一个多智能体的系统，这些模型可以自动驾驶马里奥赛车，并且彼此竞争。 必要条件和安装 Ubuntu 最新支持 CUDA 的 NVIDIA GPU mupen64plus N64 模拟器 MarioKart（马里奥赛车）N64 ROM OpenAI Gym Mupen64Plus Gym 环境 tensorflow-gpu OpenAI Gym OpenAI Gym 是用来开发和对比强化学习算法的工具箱 在强化学习中有两个基本的概念：环境（也就是智能体所处的外部世界）和智能体（也就是所开发的算法）。智能体向环境发送行动，环境回应一个观察和奖励（也就是一个分数）。 核心 gym 接口是一个 Env(https://github.com/openai/gym/blob/master/gym/core.py)。这里没有提供智能体的接口，需要你去开发。下面是你应该知道的 Env 的方法： reset(self):重设环境状态。返回状态观察。 step(self, action)：将环境推进一个时间步长。返回观察值、奖励、done 和 info。 render(self, mode=』human』, close=False)：提交一帧环境。默认的模式会做一些友好的事情，例如弹出一个窗口。将最近的标志信号传送到这种窗口。 录制和训练 我们开发了一个 python 脚本来捕捉模拟器的屏幕，以及游戏手柄和操纵杆的位置。 这个脚本可以让我们选择用来存储训练数据的文件夹。它也能够实时的绘制出被触发的游戏手柄的命令。 这个脚本主要使用输入 python 模块来记录被按下去的按钮以及操纵杆的位置。我使用 PS4 DualShock 4 手柄来训练这个模型。记录部分是按照这样配置的：我们每隔 200ms 对模拟器进行一次截屏操作。 为了开始记录，你必须遵循以下的步骤： 1. 启动你的模拟器程序（mupen64plus），运行 Mario Kart 64。 2. 保证你将操纵杆连接好了，并且 t mupen64plus 在使用简易直控媒体层（SDL）插件。 3. 运行 record.py 脚本 4. 确保图形返回相应操纵杆的输入操作 5. 将模拟器置于合适的位置（左上角），以保证程序能够截取到图像。 6. 开始记录，并且将某一个级别的游戏玩遍。 训练数据 record.py 脚本会将你所玩过的一个级别的所有级别的截图保存下来，同时还保存了一个 data.csv 的文件。 data.csv 包含与玩家所使用的一系列控制操作相关的图片的路径链接。 当我们截取到了足够多的训练数据之后，下一步就是开始实际的训练。开始训练之前，需要准备一下我们的数据。 运行 utils.py 来准备样本：用一个由样本的路径组成的数组来构建用来训练模型的矩阵 X 和 y。zsh 会扩展到所有的样本路径。传递一个全局路径也是可以的。 X 是三维图像矩阵。 y 是期望的操纵杆输出数组 模型 我们训练一个卷积神经网络模型，作为从原始单个前置摄像头直接到控制命令的映射。最终结果证明这个端到端的方法识特别强大的。使用最少的人类玩家的训练数据，这个系统就能够学会在拥挤的道路上驾驶马里奥赛车，无论是在有分道标志还是没有分道标志的高速路上。它也能够在没有明显视觉引导的区域进行操作，例如停车区或者没有铺砌的道路上。 系统自动地学会了必要处理步骤的内部表征，例如仅仅使用人类操纵的角度作为训练信号来检测有用道路特征。我们从未显式地训练它去做这种检测，例如，使用道路轮廓。 与这个问题的显式分解（例如道路标志检测、路径规划好和控制）相比，我们的端到端系统对这些处理步骤做了同时优化。我们认为这种处理机制会带来更好的性能和更小规模的系统。会得到更好性能的原因是内部组件自优化到更好的全局系统性，而不是去优化由人类选择的中间判断标准，例如道路检测。这种标准当然是便于人类理解的，但是它不能自动化地保证最大化系统的性能。而最终网络规模会比较小的原因可能是系统在更少数量的步骤内学会了解决这个问题。 网络架构 我们的网络结构是对 NVIDIA 这篇论文中所描述的结构的实现 (https://arxiv.org/pdf/1604.07316.pdf)。 我们训练网络权重参数，以最小化网络控制命令的结果和其他人类驾驶员的控制输出之间的均方差，或者为偏离中心或者发生旋转的图片调整命令。我们的网络结构如图 4 所示。网络包含 9 层，包括一个正则化层，5 个卷积层和 3 个全连接层。输入被分割成 YUV 色彩空间的平面，然后被传递到网络中。网络的第一层执行图像正则化的操作。正则化是硬编码处理，它在学习的过程中不会被调整。在网络中执行正则化可以使得正则化方案随着网络的结构而改变，并且可以通过 GPU 处理来加速。卷积层是被设计用来进行特征提取的，是根据经验从一系列的层配置中选择的。我们在前三个卷积层中使用卷积核为 5×5，步进为 2×2 的步进卷积，在最后两层使用卷积核为 3×3 的非步进卷积。在 5 个卷积层之后是三个全连接层，最终的输出值是反转半径。全连接层被设计用来进行转向控制，但是我们要注意，通过以端到端的形式训练网络，所以不太可能明确地区别网络中的哪一部分属于控制器，哪一部分属于特征提取器。 训练 train.py 会基于 google 的 TensorFlow 训练一个模型，同时会采用 cuDNN 为 GPU 加速。训练会持续一段时间（大约 1 小时），具体耗时会因所用的训练数据和系统环境相关。当训练结束的时候，程序会将模型存储在硬盘上。 单智能体的自主驾驶 play.py 会使用 gym-mupen64plus 环境来执行在马里奥赛车环境中对智能体的训练。这个环境会捕获模拟器的截图。这些图像会被送到模型中以获取将要发送的操纵杆命令。人工智能操纵杆命令可以通过「LB」按钮来重写。 模型的训练使用了以下的环境 Luigi 赛道上的 4 次竞赛 Kalimari 沙漠赛道中的 2 次竞赛 Mario 赛道上的两次竞赛 即使在小的数据集上训练，模型有时候也能够泛化到一个新赛道上（例如上述的 Royal Raceway）。 多智能体的自主驾驶 为了让智能体自主驾驶，OpenAI mupen64plus gym 环境需要连接到一个「自动程序」输入插件。因为我们对多智能体环境感兴趣，所以我们需要一种能够使用多智能体输入程序的方式。因此，基于 mupen64plus-input-bot 和官方插件 API（https://github.com/mupen64plus/mupen64plus-core/blob/master/doc/emuwiki-api-doc/Mupen64Plus-v2.0-Plugin-API.mediawiki#input-plugin-api），我们创建了 4 个玩家输入自动程序。自动输入程序后面的主要思想就是一个 python 服务器。它能够发送 JSON 命令，并把这些命令转译到较低水平的指令。 此外，为了支持多智能体的 gym 环境，我们必须更新 gym-mupen64plus。因此，我们 fork 了官方的代码库并创建了我们自己的 gym-mupen64plus。主要的区别都是跟步进/观察/奖励函数相关。我们需要一种能够仅仅查看一部分截屏，为了知道下一步应该采取什么行动来获得所需的信息。 为了启动多智能体的马里奥赛车，只需要运行： 这个脚本会根据智能体的数目来创建 python 服务器，并给每个智能体分配端口。然后，使用 mupen64plus 环境 ttps://github.com/bzier/gym-mupen64plus），这个脚本将会选择随机玩家来开始竞赛。 现在，第一个智能体得到了上述的 CNN 模型，同时第二个智能体也得到了一个非常通用的 CNN，它包含 3 个卷积层和 2 个全连接层。每个模型都得到了一定比例的屏幕，然后预测操纵杆的位置和速度按钮。 值得一提的是，为了让游戏看起来比较平滑，我们为每个玩家创建了一个新线程（https://github.com/bzier/gym-mupen64plus）。每个线程必须提供基于全局状态的行动。 哪个模型更好一些？ 我们假设能够让智能体赢得竞赛的模型是更准确的，是性能更好的。 结论 我们创建了一个含有使用不同模型的多智能体的系统，这些智能体可以为了赢得比赛而相互竞争。当结合强化学习时，系统回答了一个关键问题：为了得到奖励，什么样的模型性能最好。我们的系统可以被用作了解一个模型比其他模型好的基准工具。 "
289,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737297&idx=3&sn=cc16437f343cac2d2eabdd8a9205c87a&chksm=871aceefb06d47f90233a703711385db2e1dc807876740bf10d50b785f59ec42a3199c83be86&scene=27,入门 | 极简Python带你探索分类与回归的奥秘,"TowardsDataScience 本文从分类和回归两个方面介绍了基本的监督学习方法，并用Scikit-Learn做了实例演示。 为何使用人工智能和机器学习？ 地球的未来在于人工智能和机器学习。如果对这些技术一无所知，人们很快会发现自己落伍了。世界发展日新月异，每天都发生着不可思议的变化。在人工智能和机器学习中，有许多实现和技术能够解决实时问题。其中，监督学习是最常用的方法之一。 「人工智能的关键在于表示。」——Jeff Hawkins 什么是监督学习？ 在监督学习中，我们首先导入包含训练属性和目标属性的数据集。监督学习算法将学习训练样本和其目标变量之间的关系，然后应用习得的关系对无目标属性的全新输入进行分类。 为了阐明监督学习如何工作，让我们考虑一个案例：根据学生的学习时长预测学生的成绩。 数学公式如下： Y = f(X)+ C 其中，F 代表学生准备考试的时长与考试分数之间的关系。X 是输入（学习时长），Y 是输出（学生在考试中的得分）。C 代表随机误差。 监督学习算法的最终目标是：以最大的准确率预测给定新输入 X 的 Y 值。有几种方法都可以实现监督学习，我们将探索其中一些最常用的方法。 基于给定的数据集，机器学习问题将分为两类：分类和回归。如果给定数据同时具有输入（训练）值和输出（目标）值，那么它属于分类问题。如果数据集有着连续数值属性而没有任何目标标签，那么它属于回归问题。 让我们来举例说明。一名医学研究者希望通过分析乳腺癌数据来预测患者应该接受三种治疗方式中的哪一种。这个数据分析任务属于分类，其中构建的模型或分类器需要预测类别的标签，比如「疗法 1」、「疗法 2」、「疗法 3」。 分类问题预测离散且无序的类别标签。这个过程分两个阶段：学习阶段、分类阶段。 分类方法以及如何选择最合适的方法 最常用的算法包括： 1. K 近邻 2. 决策树 3. 朴素贝叶斯 4. 支持向量机 在学习阶段，分类模型通过分析训练集来构建分类器。在分类阶段，模型会预测出给定数据的类别标签。被分析的数据集元组及其相关类别标签被分隔成训练集和测试集。我们从要分析的数据集中随机抽取部分元组构成训练集。剩下的数据自然就是测试集了，且二者相互独立，也就是说测试集不参与训练过程。 测试集用于评估分类器的预测准确率。分类器的准确率指分类器在测试集中作出正确预测的百分比。为了达到更高的准确率，最好的方法是测试不同的算法并针对每一种算法进行调参。最后通过交叉验证可以找出最佳分类器。 为了给任务选择一个好的算法，我们必须考虑不同算法的准确率、训练时间、线性度、参数数量及特殊情况。 运用 Scikit-Learn 在 IRIS 数据集上实现 KNN 算法，根据给定输入预测花的种类。 首先，我们需要深入理解、探索给定数据集，这样才能应用机器学习算法。在本例中，我们使用了从 scikit-learn 导入的 IRIS 数据集。接下来我们边看代码边分析数据集。 请确保你的电脑上已经安装了 Python。然后，请使用 PIP 安装如下程序包： 在下面的代码片段中，我们调用几个 Pandas 中的方法来了解 IRIS 数据集的属性。 输出： class sklearn datasets base Bunch dict_keys ([‘data’, ‘target’, ‘target_names’, ‘DESCR’, ‘feature_names’]) class numpy ndarray class numpy ndarray ( 150 ,  4 ) setosa versicolor virginica sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 如果一个算法只保存训练集的元组，待收到测试元组后再进行处理，那么它就是懒惰学习算法。该算法只有收到测试数据时才执行泛化，基于测试数据与已保存的训练数据的相似性进行分类。 K 近邻分类器就是一种懒惰学习算法。 KNN 基于类比学习。所谓类比学习，就是通过比较给定的测试元组和与其相似的训练元组来学习。训练元组由 n 个属性来描述。每一个元组表示 n 维空间中的一个点。如此一来，所有的训练元组都保存在 n 维模式空间中。当输入未知元组时，k 近邻分类器在模式空间中搜索最接近未知元组的 k 个训练元组。这 k 个训练元组就是未知元组的 k 个「最近邻」。 「亲密度」由距离度量定义，例如欧式距离。合适的 K 值根据实验而定。 在下面的代码片段中，我们从 sklearn 中导入 KNN 分类器，将其用于我们的输入数据，之后用于对花进行分类。 输出： 这里， 0 对应 Versicolor（杂色鸢尾） 1 对应 Virginica（维吉尼亚鸢尾） 2 对应 Setosa（山鸢尾） 基于给定输入，使用 KNN 分类器，两张图中的花都被预测为 Versicolor。 用于 IRIS 数据集分类的 KNN 算法直观图 我们通常将确定两个或多个变量之间关系的过程叫做回归。例如，通过给定的输入数据 X 来预测某人的收入。 这里的目标变量是我们要预测的未知变量，连续性指的是 Y 值之间不存在间隙（间断）。 预测收入是一个经典的回归问题。你的输入数据应包括所有可用于预测收入的信息（也叫特征），例如工作时长、教育程度、职位、住所等。 回归模型 最常用的回归模型如下： 线性回归 Logistic 回归 多项式回归 线性回归使用最佳拟合直线（即回归线）在因变量 Y 和一或多个自变量 X 之间建立关联。 数学公式如下： h(xi) = βo + β1 * xi + e 其中 βo 代表截距，β1 代表回归线的斜率，e 是误差项。 图形表示如下： Logistic 回归算法应用在因变量属于某一类别的情况。Logistic 回归的思想是找出特征与特定输出概率之间的关系。 数学公式如下： p(X) = βo + β1 * X 其中， p(x) = p(y = 1 | x) 图形表示如下： 多项式回归是回归分析的一种形式。以 x 的 n 次多项式形式对自变量 x 和因变量 y 之间的关系进行建模。 解决线性回归问题 对于数据集 X 及对应的目标值 Y，我们使用普通最小二乘法训练一个线性模型。通过这个模型，我们可以以尽可能小的误差来预测给定未知输入 x 的输出值 y。 给定数据被分隔成训练集和测试集。训练集是有标注的（已加载特征值），因此该算法可以从这些标注样本中学习。测试集没有标注，即你不知道要预测的值。 我们以要训练的一个特征为例，运用线性回归拟合训练集，然后使用测试集进行预测。 在 scikit-learn 中实现线性回归 输出： (diabetes_X_test, diabetes_y_pred) 预测图是线性且连续的。 "
290,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737297&idx=1&sn=d2175e48854928687c1545cf2a114fc1&chksm=871aceefb06d47f98fd1ce838b23251b76e11cd40cea5ff936e7bf920274a5b89993625584b8&scene=27,「量子霸权」提出者John Preskill展望NISQ新时代下量子计算的11大应用前景,"「我们正进入一个量子技术新时代，我称其为『NISQ』。」加州理工学院理论物理学家、「量子霸权（Quantum Supremacy）」概念提出者 John Preskill 说道。在近期位于加州山景城 NASA Ames 研究中心举办的商用量子计算会议（Quantum Computing for Business）上，Preskill 认为，人类在即将实现 50-100 量子比特的中型量子计算机后，便可将其应用于探索更多现有经典计算机无法进行但更具开拓性的研究领域，也意为着人类即将进入一个量子技术发展的关键新时代，因此称其为「NISQ（Noisy Intermediate-Scale Quantum，含噪声的中型量子）」时代。Preskill 就该主题进行了主题报告，详细介绍了近期量子技术的应用前景，并表示学术界、产业界、投资界都不能对量子计算有过高的期待。会后 Preskill 将其演讲的内容整理成一篇论文并公布在 arXiv 上。机器之心重点编译介绍了该论文的第 6 部分「量子加速」的内容，包含 11 个应用方向，希望帮助读者了解在这个特殊的量子时代下量子计算即将创造怎样的应用，又将如何影响这个世界。 论文：Quantum Computing in the NISQ era and beyond 论文地址：https://arxiv.org/pdf/1801.00862.pdf 摘要： 不久的将来我们就能用上含噪声的中型量子（NISQ）技术。具备 50-100 个量子比特的量子计算机也许就能执行超越当前经典数字计算机能力范围的任务，但量子门中的噪声会限制可被可靠执行的量子回路的规模。使用 NISQ 技术的设备将成为探索多体量子物理学的有用工具，且可能还有其它更广泛的应用，但具备 100 个量子比特的量子计算机并不会瞬间为世界带来天翻地覆的变化——我们应该将其看作是向未来更强大的量子技术迈出的重要一步。量子技术学家应该继续为更准确的量子门而努力，并最终实现完全容错的量子计算。 我想重点关注量子计算是否会有能被广泛使用的应用，尤其是能在相对近期实现的应用。关键问题是：量子计算机将在何时具备比经典计算机更快的速度求解问题的能力，以及针对什么样的问题？ 至少在近期内，量子计算机很可能仍是大多数用户只能通过云端进行访问的专用设备。当我们谈论量子加速时，我们通常指量子计算机求解问题的速度比经典计算机更快，即使该经典计算机使用了可用范围内最好的硬件并且运行了可解决同一任务的最佳算法。（但可以说，即使经典超级计算机的运行速度更快，只要有例如量子硬件具有低成本和低功耗等优势，那么人们可能还是更倾向于使用量子技术。）无论如何，我们都应该意识到经典计算机的计算能力还会继续增长，预计未来几年内就会出现可用的超大规模系统（速度超过 10^18 每秒浮点计算（FLOPS））。量子计算机追赶的是一个正在前进的目标，因为经典硬件和经典算法也都在继续变得更好。 几年之前，我充满激情地谈到「量子霸权（Quantum Supremacy）」是人类文明即将迎来的里程碑 [20]。我建议使用这个术语来描述这个情况：量子设备可以执行任何已有的（或可预见的）经典设备都无法执行的计算任务，且不管这个任务是否具有任何用途。我曾经试图重点强调现在是我们星球粗糙的技术史上一个非常特殊的时期，而且我不后悔这么做过。但从商业角度看，我们显然应该关注这些任务是否有用！量子霸权是一个值得追求的目标，但值得企业家和投资者关注的并不特别在于其本身的重要性，而是因为这是一个未来将近一步实现更有价值的应用的进展标志。 我们也应该记住，由于 NISQ 时代的技术的不完美表现，我们可能难以验证一台量子计算机给出的答案是否正确。对于物理学家尤其感兴趣的量子模拟问题而言尤其如此。所以继续寻找验证量子计算机输出的更好方法对于研究者而言是非常重要的。 6.1 量子优化器 我已经强调过，我们并不期望量子计算机能够有效求解 NP-hard 问题（比如组合优化问题）的最糟糕情况；但仍然可以相信（虽然无法确保）量子设备能够找到更优的近似解或更快地找到这样的近似解。比如，我们可能在 n 个比特上指定 m 个约束，然后求解尽可能多地满足这 m 个约束的 n 比特串。如果可以找到同时能被满足的约束的最大数量 k，那么也许可以说我们确实已经求解了这个优化问题；而且如果可以保证可满足的约束的最大数量 k，至少满足 k'（k'<k），那么我们可得到一个近似解；近似率 k'/k 是衡量该近似解是否优质的方法。 对于某些优化问题，比如在近似率足够接近于 1 [21] 的情况下求出一个近似解也是 NP-hard 问题，而且在这些情况下，我们预计量子计算机也无法有效地为该问题的困难实例找到近似解。但对于很多问题来说，在我们使用目前已知的最优经典算法所得到的解的近似效果，距离解决 NP-hard 问题还存在很大的差距。所以不难发现：对于寻找近似解的任务而言，量子计算机相对于经典计算机而言存在优势——更多使用者也许会认为这个优势很有价值。 这个量子优势是否真的存在目前还悬而未决，但我们很快就将有机会使用量子硬件用实验方法来探索这一问题了。当然，就算在处理近似优化问题上量子优势确实存在，NISQ 时代的量子技术可能也并不足够用于展示这一优势；尽管如此，尝试一下，看看效果如何，也会很有意思。 使用近期的量子技术来求解优化问题已逐步形成趋势，这一新兴范式是一种量子与经典的混合算法。在这种方案中，我们使用量子处理器来制备一个 n 个量子比特的量子态，然后再使用经典优化器来测量所有的量子比特以及处理测量结果；这个经典优化器可以指示量子处理器微调 n 个量子比特量子态的制备方式。这个过程将重复很多次，直到其收敛到一个可以提取出近似解的量子态。当被应用于经典组合优化问题时，这个流程被称为量子近似优化算法（QAOA：Quantum Approximate Optimization Algorithm）[22]。但该流程也可被应用于量子问题，比如寻找多粒子量子系统（比如大分子）的低能态。当被应用于量子问题时，这个量子与经典的混合流程被称为可变量子本征求解（VQE：Variation Quantum Eigensolver）[23]。 所以 NISQ 技术在有运行 QAOA 或 VQE 算法下，能否超越寻找同样问题的近似解的经典算法？没人知道，但我们正打算尝试，然后看看我们能做到多好。这是个宏伟目标，因为我们用于求解这些问题的经典方法已经历了数十年的打磨发展，效果已经相当好了。另一方面，即使早期 NISQ 时代的量子设备还无法与最好的经典计算机媲美，实验结果也可能会激励我们期待看到 QAOA 和/或 VQE 在未来超越经典方法，从而近一步推动技术发展。 6.2 量子硬件测试平台的意义 经典计算的发展历史告诉我们：当硬件变得可用时，会刺激和加速新算法的发展。有很多启发式研究成果都是通过实验发现的，这些效果反而更优于理论研究者最初可解释的想法。我们可以预见，同样的情况也会在量子计算机上发生。 比如说，理论研究者最终解释了用于线性规划的单纯形法（simplex method）在实践中效果良好的原因 [24]，但很久之前人们已经通过实验发现它很有用。深度学习也是现今一个较具代表性的例子 [25]；对于深度学习效果良好的原因，我们目前还缺乏一个足够好的理论解释。在量子方面也是如此，实验也许能够验证启发式算法的表现，而我们并不理解它们有效的原因。对于我们正计划测试的量子优化算法，这种情况尤其可能出现。 但我要再次强调，NISQ 系统中不完美的量子门会严重限制它们的计算能力，因为带有许多量子门的回路可能会有太多噪声，以至于无法给出有用的结果。近期的实验将会探索我们能用 100 个量子比特和少于 100 层回路深度（即时间步骤的数量）的量子计算机做什么，也许使用的量子比特位数更少、回路深度更浅。量子算法设计者和应用用户之间的热烈讨论可能有助于指明有潜力的实验方向；类似于这次会议的一大重要目标就是促进和激励量子计算的商用前景。 6.3 量子退火 我一直都在强调：50 至 100 个量子比特是量子技术即将迎来的里程碑，但事实上我们现在已经有一种 2000 量子比特的量子设备了——D-Wave 2000Q。这个机器并非一种基于回路的量子计算机，而是一种我们称为量子退火机（quantum annealer）的设备，它使用了一种不同于量子回路的运行方法来求解优化问题，而且通常能成功求解这些问题。 但就目前而言，我们还没有令人信服的理论论证或可信的实验证据来证明量子退火机确实能够加速求解——相比于针对同样问题的运行最优算法的最优经典硬件算力 [26,27]。这个情况有些微妙。量子退火机是我们所说的绝热量子计算的有噪声版本（比如对于 D-Wave 机器，使用的也是质量相当差的量子比特）。对于无噪声量子比特的情况，我们确实有一项理论论证表明绝热量子计算和基于回路的量子计算一样强大 [29]。但是，这个论证表明，只有当绝热方法以一种需要付出更多代价的方式，即需要在具有额外的物理量子比特的环境中执行时，绝热量子计算和基于回路的量子计算之间的等效性才会成立；这与当前量子退火机所使用的方法相当不同。无论如何，这个形式论证仅适用于无噪声量子比特，而且（和基于回路的量子计算不同），我们没有很好的理论论证表明量子退火机是可扩展的。也就是说，在量子比特有噪声的情况下，就算我们愿意花费相当大的成本来增强有噪声下量子计算机性能的稳健性，我们也不知道如何证明量子退火机在问题规模扩大时还能继续成功地工作。 目前而言，量子退火机基本都应用于当退火过程处于 stoquastic 模式（哈密顿体系的一种，更常见于现实场景）的情况——也就意味着经典计算机能相对容易地模拟量子退火机所做的事情 [30]。非 stoquastic 模式下的量子退火机很快就将出现，这在实现相对于最优经典算法的加速上可能有更大的潜力。 因为理论研究者仍未确定量子退火是否强大，所以还需要近一步的实验。未来几年使用量子退火机进行的实验很可能将提供丰富的信息。特别要指出，除了量子退火机在经典优化问题上的应用之外，在量子模拟问题上的应用也应该得到探索 [31]。 6.4 抗噪量子回路 我已经着重指出最终我们将使用量子纠错（quantum error correction）来扩展使用有噪声量子计算机可靠执行的计算规模。但由于量子纠错机制具有很高的开销成本，NISQ 设备在近期还不会使用它。不过，寻找减轻噪声影响的方法可能是 NISQ 时代一个重要的研究方向。 人们可能会天真地认为，对于一个具有 G 个门的通用回路，回路中任何位置的单个错误都会导致计算失败；如果真是这样，那么如果每个门的错误率远大于 1/G 时，我们就不能可靠地执行这个回路。但是，至少对于某些问题和算法而言，这个结论可能过于悲观了。 具体来说，对于物理学家感兴趣的某些量子模拟算法，仅在相对少数的回路位置，错误门才会导致计算严重出错。其原因是回路的深度较浅，而且我们可以允许最终观测结果中每个量子比特有一个恒定的错误概率；或者是因为早期在回路中由某个错误导致的损害会在之后衰减。某些张量网络结构就具有这样的抗噪能力，可以借助这种结构使用 VQE 来求解量子优化问题 [32,33]。 未来几年对于实验主义者和理论研究者来说存在一个合作的重大机会，可以努力寻找实现量子回路抗噪的更好方法，并由此增强 NISQ 技术的计算能力。但我们也应该审慎对待这样一种潜在的权衡——让量子回路抗噪能力更强可能也会让通过经典方式模拟它更容易。 6.5 量子深度学习 机器学习正在实现技术变革，而且也对科学有很大的影响，所以我们很自然就会思考机器学习与量子技术结合起来的潜力。量子机器学习（quantum machine learning）有一些不同的概念。这一主题的大多数文献都是关于开发能够加速线性代数及相关任务的量子算法 [34，35]，我会在随后的小节中指出这些应用。 但首先我想评价一下量子深度学习（quantum deep learning）[36] 的潜力。深度学习本身已经变成了一个宽泛的主题 [25]，但为了具体论述，让我们思考一下受限玻尔兹曼机。这也许可以被看作是一个处于低温但非零温度条件下热平衡环境的自旋系统，其中有很多分隔输入和输出的自旋隐藏层。（「受限」一词意味着某个给定层内的自旋没有耦合；只有相继的层中的自旋是耦合的。）该系统可能具有数百万个耦合参数，这些参数会在训练阶段得到优化，从而为输入和输出得到一个所需的联合概率分布。这个训练过程可能是监督式的（比如可能是教该网络学习识别有标签的照片集），也可能是无监督式的（其目标是识别无标签训练数据集中的模式）。这种机器的量子模式可以具有相似的结构和功能，但这里的自旋是量子比特而非经典比特。相比于经典的深度学习网络，量子深度学习网络可能具有优势；比如它们针对某些目的的训练可能更容易。但我们实际上并不真正清楚——这是另一个需要尝试并了解结果的机会。 对于量子机器学习的潜力报以期待的一个可能原因在其被称为 QRAM 的概念，即量子随机存取存储器（quantum random access memory）。通过使用 QRAM，我们可以将一个具有 N 个分量的向量编码进仅仅 log N 个量子比特中，从而能以非常简明的方式表示大量经典数据。但量子机器学习应用的典型提议都受严重的输入/输出瓶颈限制。对于使用大量经典数据集的应用，人们应该考虑将输入数据编码进 QRAM 中的成本，这会削弱其算法的潜在优势。此外，量子网络的输出本身是一个量子态；为了得到有用的信息，我们需要对该状态进行观测，而在对 log N 个量子比特的单次观测中，我们得到的信息不会超过 log N 个比特的经典信息。 当我们试图训练一个量子网络来学习经典数据中的相关性时，这些瓶颈就会出现。也许在输入和输出都是量子态的情境下思考量子机器学习要更为自然。因此，比如在控制复杂量子系统方面，我们也许可以预见量子网络比经典网络更优。更宽泛而言，在试图学习量子纠缠具有重要影响的概率分布时，可以相信量子深度学习会比经典深度学习更强大。也就是说，量子深度学习网络可能非常适合量子任务，但对于我们目前广泛研发的深度学习应用，还不确定量子网络是否具有优势。 从积极的一面看，值得注意的是，量子深度学习机器并不非得是通用的基于回路的量子计算机，也可能是一种专用设备——也许是不含太多噪声的量子退火机。 6.6 量子矩阵求逆 QRAM 对量子算法还有更近一步的影响。特别是量子技术能给矩阵求逆任务带来指数级的加速，这可能具有很多应用。 我们称为 HHL [37] 的算法（得名于九年前第一次讨论该算法的三位作者）的输入包含一个非常大的 N×N 矩阵 A 的简明表示（该矩阵必须足够稀疏且是良态的）和一个包含 N 个分量的向量 |b>（在 QRAM 中被编码成 log N 个量子比特的量子态）。这个量子算法的输出是量子态  （带有很小的误差），这是将矩阵 A 的逆应用到输入向量 |b> 上所得到的结果。该算法的运行时间为 O(log N)，相比于经典的矩阵求逆实现了指数级的加速（对于固定的误差与 A 的固定稀疏度和条件数）。 但是，除了矩阵 A 是稀疏的并且是良态的之外，还要注意输入向量 |b> 和输出向量  都是量子态。一旦我们有了输出态，我们就可以执行观测来了解输出向量  的特征；通过多次重复这一方案，我们可以了解到该输出向量的更多细节。但如果我们试图将该矩阵求逆算法应用到经典的输入数据上，我们就需要考虑将经典数据载入 QRAM 的潜在成本，这可能会抵消其指数级的加速。或者，通过在量子计算机上计算 |b>，而不是从数据库导入，也许可以避免将经典数据载入 QRAM 的过程。 我们确实有很好的理由相信这种量子矩阵求逆算法很强大，因为它能解决所谓的 BQP-complete 问题。也就是说，任何可以使用量子计算机有效解决的问题都可以被编码成这种矩阵求逆问题的一个实例。而且人们已经提出了一些潜在有用的应用。比如说，我们可以使用这种矩阵求逆算法来寻找经典的线性场方程 [38] 的近似解，在对空间格（spatial lattice）上的场进行离散化之后，再应用一个合适的预条件处理。这个过程可被用于，比如，求解在复杂三维几何中的电磁学方程，从而可以优化天线的性能。 我预计 HHL 最终会有具重大影响的应用，但在 NISQ 时代恐怕无法实现。对于不使用纠错机制的量子计算机来说，这个算法成功执行的成本可能会非常高。 6.7 量子推荐系统 最近有研究者提出了另一个量子算法 [39]，该算法在做出高价值推荐的任务上能相比于当前已知的最优经典算法实现指数级的加速。该任务的目标是：基于对某个客户和其他客户的偏好的理解有限的情况下，向该客户推荐他可能会喜欢的产品。 我们将该问题的简化版本形式化表示一下，假设有 m 个客户和 n 件产品，设 P 为一个二值的 m×n 的偏好矩阵；如果客户 a 喜欢产品 i，则  ；如果客户 a 不喜欢产品 i，则  。实际情况下，我们可能有 m≈10^8 个用户和 n≈10^6 件产品，但该矩阵的秩 k 相对较小，也许为 k≈100。秩小说明客户类型的数量有限；因此，一旦我们知道了一个新客户的一些偏好，我们就可以准确地推荐该客户会喜欢的其它产品。 推荐系统的算法有两个阶段。第一个阶段是离线执行，会构建偏好矩阵 P 的一个低秩近似。第二个阶段是在线执行，新客户会揭示出一些偏好，然后该系统输出一个该客户有很高概率会喜欢的推荐。量子能够带来加速的是在第二个阶段（在线执行阶段）。量子运行时间为 O(poly(k)polylog(mn))，而已知的最优经典算法需要 poly(mn) 时间才能返回一个高价值的推荐。关键在于，与经典算法不同，量子算法不会尝试重建完整的推荐矩阵，这样做会花费太多时间。相反，量子算法是从 P 的低秩近似上进行有效的采样。 这能为真实世界的机器学习应用提供显著的量子加速，能够激励人们发现其它类似的加速。但是，我们目前还没有一个有说服力的理论论证表明由量子推荐系统所执行的任务（在 polylog(mn) 时间内返回一个高质量推荐）无法通过经典方法实现。进行这样的论证有助于强化我们对量子计算机对机器学习会有重大影响的信心。但是，在 NISQ 时代，同样因为该算法的成本可能太高，从而无法得到让人信服的验证。 6.8 量子半定规划 半定规划（semidefinite programming）是指优化服从矩阵不等式约束的线性函数的任务。具体来说，给定输入 m+1 个 N×N 埃尔米特矩阵（Hermitian Matrix） 和实数 ，目标是找到一个半正定的 N×N 矩阵 X，使之能最大化满足 m 个约束  ( A i X )  ≤ b i  的 tr(CX)。这种类型的凸优化问题有广泛的应用，使用经典方法可以在 poly(m,N) 时间内解决。 最近发现的一个量子算法 [40,41] 能在 polylog(N) 时间内找到该问题的一个近似解，即能实现指数级的速度提升。在该量子算法中，输出是一个量子态，一个近似最优矩阵 X 的密度算符ρ。然后可以对这个量子态进行观测，以提取出有关 X 的信息，而且可以对这个过程多次重复，从而得到有关 X 的更多细节。 但是，在分析量子算法时，人们会假设存在某种特定形式的初始量子态——与一个哈密顿量关联的热吉布斯态（thermal Gibbs state），该哈密顿量是这个半定规划的输入矩阵的一种线性组合。这种指数级加速能否真正实现取决于能否使用量子计算机有效地制备出这个吉布斯态。 对于结构正确的半定规划，吉布斯态的有效制备是可能的，因此指数级加速实际上是可以实现的。具体而言，如果该半定规划的所有输入矩阵都有低秩，或如果对应的哈密顿系统能快速热化，那么该量子算法就能实现指数级加速。我们目前还不清楚在实践中这些性质能在半定规划上应用到何种程度；搞清楚这一问题是未来的一个重要研究目标。 有趣的是，该量子算法中的关键步骤是在非零温度下制备一个热态。这说明该算法有一些固有的热噪声稳健性，或者该算法的热化阶段可以通过量子退火来执行。可以相信，能够通过 NISQ 技术实现半定规划的量子求解器。 6.9 量子模拟 正如我已经强调过的，我们预计量子计算机非常适用于研究许多粒子构成的高度纠缠系统的性质。正如 Laughlin 和 Pines [8] 在我之前引述过的那项声明中强调的那样，我们认为模拟那种「强相关」物质是一个艰难的计算问题。因此，正如费曼指出的那样 [9]，对高度纠缠的量子物质的模拟显然是量子计算机相比于经典计算机有明显优势的领域。 长远来看，使用量子计算机进行量子模拟有广泛影响世界的巨大潜力 [42,43,44]。由量子计算助力实现的量子化学进展也许能够促进新药物的设计以及带来提升固氮或碳捕获效率的新型催化剂。由量子模拟得到的新材料有望带来更高效的电力传输方式或改进太阳能的收集方法。我们可能难以想象更强大的量子模拟器所能带来的最具变革力量的发现。但在 NISQ 时代，这个愿景不太可能实现，因为在没有纠错的情况下，实现能成功准确模拟大分子和特殊材料的量子算法的成本实在太高了。 在模拟量子动力学方面（即预测高度纠缠的量子态如何随时间变化），经典计算机的能力是非常差的。量子计算机在这一任务上具有很大的优势，而且物理学家希望能在相对近期使用 NISQ 技术了解有关量子动力学的有趣地方。有一个事件可做参考。上世纪 60 和 70 年代，在有可能使用经典计算机模拟混沌动力学系统之后，经典混沌理论（在经典动态系统中对初始条件极其敏感的性质，这让我们没有能力预测两周之后的天气）得到快速发展。我们可以预见，模拟混沌量子系统（在这样的系统中，纠缠的传播速度非常快）能力的出现将能促进我们对量子混沌的理解。使用具有 100 个量子比特的有噪声设备也许就已经能搜集到有价值的信息了。 6.10 数字型量子模拟与模拟型量子模拟 我已经强调过模拟量子动力学的潜在价值，我还应该谈谈模拟型（analog）量子模拟和数字型（digital）量子模拟之间的差别。当我们谈论模拟型量子模拟器时，指的是一种带有很多量子比特的系统，其动力学类似于我们试图研究和理解的模型系统的动力学。相对而言，数字型量子模拟器是一种基于门的通用量子计算机，当被合适地程序化后，该系统可以用来模拟任何相关的物理系统，而且也可被用于其它目的。 近 15 年以来，模拟型量子模拟一直都是一个非常活跃的研究领域，而使用通用目的的基于回路的量子计算机的数字量子模拟才刚刚起步。但囚禁离子和超导电路等实验平台在这两方面都可应用。值得注意的是，模拟型量子模拟器明显变得越来越复杂了，而且也已经在超出经典模拟器能力范围的量子动力学研究领域得到了应用 [45,46]。但模拟型量子模拟因为不能完美地控制而受到了阻碍——实验室中的实际量子系统只能粗略地与相关模型系统近似。由于这个原因，模拟型量子模拟器最适用于研究物理学家所说「广义」特性，即在引入少量误差源时能保持相对稳健的特性。使用模拟型量子模拟器进行研究的一个主要挑战是确定量子系统对误差稳健同时又难以通过经典方法模拟的可用特性。 我们可以预见，模拟型量子模拟器最终将会过时。因为它们难以控制，所以未来某天它们会被可使用量子纠错机制可靠地控制的数字型量子模拟器超越。但因为量子纠错具有极大的开销成本，所以模拟型量子模拟器的主导地位可能还会持续很多年。因此，在探寻量子技术的近期应用时，我们不应该忽视模拟型量子模拟器的潜在力量。 6.11 量子游戏 经典计算的进步带来了一个电子游戏新世界，触及了数百万人的生活，创造了数十亿美元的收入。量子计算机也能做到这一点吗？ 物理学家常说量子世界是反直觉的，因为它与常规经验相差太大了。就现在而言，这是实情，但未来可能会不一样吧？玩量子游戏长大的孩子也许会具备我们这一代人缺乏的对量子现象的深刻理解。此外，量子游戏可以为量子机器学习方法开辟一个新场地，这些方法可能会抓住机会在量子纠缠扮演重要角色的场景中提升游戏玩法。 现在让我总结一下我试图传达的主要观点。 现在是科学技术史上的一个特殊时期，我们正见证着 NISQ 时代的启幕（NISQ=含噪声中型量子技术）。我们很快就将有机会使用 NISQ 技术进行实验，看看它们能做到什么。也许在不久的将来，NISQ 能让我们加速求解人们广泛关注的问题，但我们并不知道这是否将会发生。 我们有一些想要尝试的特定想法，比如用量子-经典混合算法求解优化问题，这种算法既是经典的，也是量子的。 可以预见，一旦我们有了可以进行实验的量子计算机，量子算法的发展就会加速。也许人们会发现用于求解有用问题的新的启发式方法，尽管我们可能无法很好地解释这些启发式方法的工作方式——至少不能很快解释。 尽管我们可能无法使用充分的量子纠错来保护 NISQ 时代的设备免受噪声干扰，但在近期我们在设计量子算法时应该将抗噪考虑进来。精妙的抗噪算法设计可以提升 NISQ 设备的计算能力。 经典计算机模拟高度纠缠的多粒子量子系统的能力非常差；因此量子动力学是一个尤其有前景的领域，量子计算机在这一领域可以有超越经典计算机的重大优势。 我们应该继续重点开发具有更低门错误率的量子硬件。量子门准确度的提升能够降低最终实现时量子纠错的开销成本。在更近期，更准确的门让我们可以执行更大规模的量子回路，进而提升 NISQ 技术的能力。 只有 NISQ 本身可能无法改变这个世界。相反，量子平台近期的主要目标应该是为更大的回报奠定基础——这些回报将会由未来更先进的量子设备实现。 未来真正变革性的量子技术可能必须具备容错能力，而由于量子纠错具有极其高的开销成本，所以容错型量子计算的时代可能仍然还相当遥远。没人知道我们还要多久才能到达这一时代。在量子社区急切地想要抓住机会实验即将到来的 NISQ 设备的同时，我们也不能忽视至关重要的更长期目标：加速容错时代的到来。   "
291,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737254&idx=2&sn=569bd5cde3006bbcef2f635247a9f66b&chksm=871ace18b06d470e27d59c56e4692864cb0aa93f9bdc16f5fd6c9f58a2baf53e1a701c10d419&scene=27,入门 | 玩转词向量：用fastText预训练向量做个智能小程序,"Martin Konicek 越来越多的软件工程师开始学习和涉足机器学习开发。近日，伦敦的软件工程师 Martin Konicek 在 Medium 上介绍了他使用 fastText 预训练过的词向量创建一个智能回答小程序的过程，相信能给仍不理解词向量的初学者提供一些帮助。此外，这个程序的代码也已经在 GitHub 上公开，感兴趣的读者不妨自己动手实现一下。更多有关 fastText 的介绍，可参阅机器之心专栏文章《 》。 过去几年，人工智能领域发展非常快，相关的文章也层出不穷，你很有可能已经听说过自然语言处理领域内的一些出色成果了。   比如，一个程序可以通过阅读维基百科完成对这个问题的分析：男人对于国王就相当于女人对于___？（女王/王后）   我自己也写过一个简单程序来完成这一任务。我很好奇：它能够回答多难的问题？   这篇博文是对这一主题的非常轻量的介绍。我没有训练任何机器学习模型，而是下载了使用 fastText 库创造的预训练英语词向量：https://fasttext.cc/docs/en/english-vectors.html   首先先看数据   fastText 通过阅读维基百科学习到了什么？让我们打开这个 2GB 的文件一探究竟： 很好，这个格式应该很容易操作。每一行都包含一个词，表示成了 300 维空间中的一个向量。如果这是二维的，我们可以想象会是这样： 唯一的区别是每个词所具有的坐标的数量不是 2，而是 300.   这个输入文件中的词是按照频率排序的，这很方便。对于我的实验来说，使用最常见的 10 万个英语词就足够了，所以我将前 10 万行复制成了一个单独的文件。   使用 Python 3   为了让我的第一个项目足够简单，我决定使用普通的 Python 3，不带任何附加依赖包。我也在使用 Mypy，这是一个用于 Python 的静态类型检查器（http://mypy-lang.org）。   首先，我们定义一个类来表示每个词： 接下来，我们将数据加载到内存中： 我们已经将该文件解析成了一个 List[Word]。这部分非常简单，我把细节放在本文后面，现在直接来看有意思的东西。   余弦相似度   将这些向量放入内存之后，我们就可以回答各种关于它们的问题了。我的第一个问题是：   在这个向量空间中，哪个词与给定的词最近？   我们如何计算两个词向量 a 和 b 之间的距离？你可能会说「欧几里得距离」，但对我们的用例而言，余弦相似度的效果要好得多。其背后的思想是：向量的绝对长度并不重要，重要的是两个向量之间的角度。   根据高中所学习到的内容（或根据维基百科），余弦相似度为： 用 Python 表示： 现在我们可以得到一个词与给定词的相似度了： 我们只需要一些简单的实用函数来显示相关词： 现在试试看： 结果非常好！看起来余弦相似度高的词确实是彼此相关的——要么是句法上相关（roots 和 rooted），要么是语义上相关（roots 和 grass、arms 和 legs）。   完成句子：巴黎对于法国就相当于罗马对于___   来试试更难的任务。给定的两个词「巴黎」和「法国」之间存在语义关系（巴黎是法国的首都）；对于第三个词「罗马」，我们能推理得到「意大利」吗？   事实证明我们可以直接通过加减向量来做到这一点！这是因为这些词的向量在空间中具有特定的关系： 事实证明这两个红色的向量非常相似！我们可以想象它们表示「首都」这个关系。   惊人的来了： 因此： 我们会寻找接近于答案向量的词；这个答案就算不是「意大利」，也应该与之相近。   让我们实现它： 接下来问一些问题： 成功了！通过阅读维基百科，fastText 学会了关于首都、性别、不规则变化的动词和形容词的关系。再试试其它的：   还有： 这个模型并不能正确分析一切 你会怎样完成下列分析？ 1. 寿司-米饭就像是披萨-___ 2. 寿司-米饭就像是牛排-___ 3. 衬衫-衣服就像是电话-___ 4. 衬衫-衣服就像是碗-___ 5. 书-阅读就像是电视-___ 寿司是用米饭和其它配料做成的，披萨是用面团、肉肠、芝士等材料做成的。牛排是肉制品。衬衫是一种衣服，电话是一种电子设备，碗是一种餐具。书是用来阅读的，电视是用来观看的。 让我们看看答案： 还有： 可以看到，fastText 的分析并不都是正确的。它的结果很出色，但错起来也很离谱。   如果我们看看建议列表，而不只是第一个，会有更好的答案吗？ 其中第二个建议是非常好的。但是，看起来「衬衫」和「衣服」之间的关系仍然是一个谜。   结论   当我第一次在我的笔记本电脑上看到这些结果时，我完全震惊了。这个仅由少量 Python 代码组成的程序能让你感到它是智能的并且能真正理解你询问的东西。   在我尝试过一些困难的问题之后，我意识到这个程序也可能「离题千里」——任何人类都不会犯这样的错。这可能会让你认为这个模型根本就不聪明。   但是，想想这一点：这些向量是在英语文本上训练的，但和人类不一样，这个学习算法没有任何预先的英语知识。在阅读维基百科几个小时之后，它很好地学习到了英语语法以及很多真实世界概念之间的语义关系。只要文本足够，它也能使用德语、泰语、汉语或其它任何语言做到这一点。   对于一个人类孩童来说，需要多少时间才能发展出足以回答上述问题所需的逻辑思考能力？成年人学习一门外语的效果呢？或者说两门乃至三门外语呢？人类需要数年时间才能做到的事情 Word2vec 或 fastText 仅需数小时就能办到。人类的学习方式与这些算法有很大的差异。人类可以使用远远更少的数据而更好地学习到概念，但所需的时间更长。   本文中的所有代码都已公布在 GitHub 上。你只需要 Python 3 和预训练的向量来运行该代码，然后就能自己寻找词之间的有趣关系了。   代码：https://github.com/mkonicek/nlp 预训练的向量：https://fasttext.cc/docs/en/english-vectors.html 附录   我想尽可能地缩短正文的篇幅，同时涵盖最重要的基础和相关结果。下面给出了更多细节。   一点简单的开发工作   我第一次实现该算法时，得到的结果是错误的，比如： 这是因为通过「向量（国王）- 向量（男人）+ 向量（女人）」所得到的答案向量与「国王」这个词非常接近。也就是说，减去「男人」向量再加上「女人」向量对「国王」向量的影响很小，这可能是因为「男人」与「女人」本身是相关的。   实际上，我得到的几乎所有答案都只是对某个输入词的简单重复。我做了一些开发来跳过建议答案中的这些多余的词，然后才开始得到上面给出的相关答案。这部分开发在代码中被称为 is_redundant。   更新：fastText 的作者 Tomas Mikolov 在 Facebook 上回复说我所做的实际上是一个众所周知的操作，而且是正确的。 向量是如何产生的？   我只写了少量代码就得到了这些惊人的结果。这是因为所有的神奇之处都在向量之中——使用 fastText 在数千兆字节的维基百科英语文本和其它来源上进行了训练。另外还有一些与 fastText 类似的库，比如 Word2vec 和 GloVe。这些库是如何工作的？这个主题值得再写一篇文章，但背后的思想是：出现在相似语境中的词应该具有相似的向量。比如如果词「披萨」通常出现在「吃」、「餐厅」和「意大利」等词的附近，那么「披萨」的向量就会与那些词的向量具有较高的余弦相似度。很少同时出现的词会有较低的余弦相似度，可能能够达到 -1。   历史   n-gram 和将词表示为向量等思想已经存在了很长时间，但直到 2013 年那篇 Word2vec 的论文和实现发表之后（https://arxiv.org/abs/1301.3781），才表明这些方法「能以远远更低的计算成本实现准确度的极大提升」。Tomas Mikolov 为 Word2Vec 和 fastText 这两个项目都立下过汗马功劳。 我现在才开始学习，但好事不嫌晚。 为什么在 Python 中使用类型（type）？   我同意 Michael Bolin 的说法：「在为大型软件项目选择语言时，静态类型是一项关键特性」。类型有助于我们让代码更具可读性和减少漏洞。即使是对于这样的小项目，如果在运行代码之前就能在 Atom 中看到错误，也能带来极大的帮助，节省很多时间。   Python 3 runtime 接受类型注释，这一点非常棒。任何人都无需任何额外设置就能运行我的代码。在运行这些代码之前无法对其进行转换，就像使用 JavaScript 的 Flow 一样。   优化   这个代码完全没有进行优化，而且也没有进行太多错误处理（error handling）。至少我们可以归一化所有的向量以使余弦相似度的计算更快（在 10 万词的情况下，调用一次 sorted_by_similarity 来回答一个问题，在我的 MacBook Pro 上耗时 7 秒）。我们也许应该使用 numpy。这个 Python 进程要使用近 1GB 的内存。我们可以使用一个已有的实现，可以直接加载文件然后问答问题。所有这些都超出了本文的范畴。我只是想知道我能不能从头开始写一些非常简单的代码来回答一些有趣的问题。 加载和清理数据   这是 load_words 函数所做的工作： load_words_raw 只是逐行读取文件，然后将每一行解析成一个词。我们加载了 10 万词（每个词 300 维），而这个 Python 进程就用了近 1GB 的内存！这很糟糕，但可以忍受。   remove_stop_words 会将起始或结尾字符不是字母的词移除，比如「inter-」、「thanks.」、「--redrose64」。现在剩下 98,648 个词。   remove_duplicates 会忽略标点符号，所以「U.K」、「U.K.」和「UK」是一样的词，只存储一次。剩下 97,190 个词。但仍然还有一些重复，比如「years」和「Years」。我们也可以忽略大小写，但这样我们就无法区分「us」（我们）和「US」（美国）这样的词了。   原文链接：https://medium.com/swlh/playing-with-word-vectors-308ab2faa519 "
292,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737254&idx=3&sn=650e8f9f185c4e72fa78c873ebcf5f16&chksm=871ace18b06d470ebd6bb3f006a623bd77bd57d5133aca8687aeceef16fd29b2eab9cea37df8&scene=27,教程 | 如何使用贪婪搜索和束搜索解码算法进行自然语言处理,"Jason Brownlee 本文介绍了贪婪搜索解码算法和束搜索解码算法的定义及其 Python 实现。 自然语言处理任务如图像描述生成和机器翻译，涉及生成一系列的单词。通常，针对这些问题开发的模型的工作方式是生成在输出词汇表上的概率分布，并通过解码算法对概率分布进行采样以生成可能性最大的单词序列。在本教程中，你将学习可用于文本生成问题的贪婪搜索和束搜索解码算法。 完成本教程，你将了解： 文本生成问题中的解码问题； 贪婪搜索解码算法及其在 Python 中的实现； 束搜索解码算法及其在 Python 中的实现。 文本生成解码器 在自然语言处理任务中，如图像描述生成、文本摘要和机器翻译等，需要预测的是一连串的单词。通常，针对此类问题开发的模型会输出每个单词在对应的输出序列词汇表上的概率分布，然后解码器将其转化为最终的单词序列。 当你使用循环神经网络解决以文本作为输出的 NLP 任务时，你很可能会遇到这种情况。神经网络模型的最后一层对输出词汇表中的每个单词都有对应的一个神经元，同时 softmax 激活函数被用来输出词汇表中每个单词成为序列中下一个单词的可能性。 解码最有可能的输出序列包括根据它们的可能性搜索所有可能的输出序列。词汇表中通常含有成千上万个单词，甚至上百万个单词。因此，搜索问题根据输出序列的长度呈指数级变化，并且很难做到完全搜索（NP-complete）。 实际上，对于给定的预测，可以用启发式搜索方法返回一或多个逼近或「足够好」的解码输出序列。 由于搜索图的范围是根据源语句长度呈指数级的，所以我们必须使用近似来有效地找到解决方案。 — Page 272, Handbook of Natural Language Processing and Machine Translation, 2011. 候选单词序列的分数是根据它们的可能性评定的。通常，使用贪婪搜索或束搜索定位文本的候选序列。本文将研究这两种解码算法。 每个单独的预测都有一个关联的分数（或概率），我们对最大分数（或最大概率）的输出序列感兴趣。一种流行的近似方法是使用贪婪预测，即在每个阶段采用得分最高的项。虽然这种方法通常是有效的，但显然不是最佳的。实际上，用束搜索作为近似搜索通常比用贪婪搜索要好得多。 — Page 227, Neural Network Methods in Natural Language Processing, 2017. 贪婪搜索解码器 一个简单的近似方法是使用贪婪搜索，即在输出序列的每一步中选择最有可能的单词。该方法的优点是非常快，但最终输出序列的质量可能远非最佳。 我们可以用 Python 中的一个小例子来展示贪婪搜索的解码方式。我们从一个包含 10 个单词的序列的预测问题开始。每个单词的预测是其在五个单词组成的词汇表上的概率分布。 我们假定单词是整数编码的，这样，列索引就可以用来查找词汇表中的相关单词。因此，解码任务就变成从概率分布中选择整数序列的任务。argmax() 数学函数可用于选择具有最大值的数组的索引。我们可以用该函数选择在序列每个步骤中最有可能的单词索引。这个函数是直接在 numpy 中提供的。 下面的 greedy_decoder() 函数用 argmax 函数实现了这个解码思路。 下面展示了贪婪解码器的完整示例。 运行这个示例会输出一系列整数，然后这些整数可以映射回词汇表中的单词。 束搜索解码器 另一种流行的启发式算法是在贪婪搜索的基础扩展而来的束搜索，它返回的是可能性最大的输出序列列表。相对于在构建序列时就贪婪地选择最有可能的下一步，束搜索选择扩展所有可能的下一步，并保持 k 是最有可能的，k 是用户指定的参数，它通过一系列概率控制束或并行搜索的数量。 本地束搜索算法跟踪 k 个状态，而不仅仅只跟踪一个。它从 k 个随机生成的状态开始，在每一步中都生成所有 k 个状态的所有后继者。如果这其中的任何一个后继者是目标，那么算法就会停止。否则，它将从完整列表中选择k个最佳后继者并不断重复。 — Pages 125-126, Artificial Intelligence: A Modern Approach (3rd Edition), 2009. 我们不需要从随机状态开始；相反 ，我们以k个最可能的单词开始，作为序列的第一步。对于贪婪搜索，常见的束宽度值为 1，对于机器翻译中常见的基准问题，它的值为 5 或 10。由于多个候选序列增加了更好地匹配目标序列的可能性，所以较大的束宽度会使模型性能提高。性能的提高会导致解码速度降低。 在 NMT 中，新的句子通过一个简单的束搜索解码器被翻译，该解码器可以找到一个近似最大化已训练 NMT 模型的条件概率的译文。束搜索从左到右逐词完成翻译，同时在每一步中都保持固定数目（束）的活跃候选者。增大束尺寸可以提高翻译性能，但代价是解码器的速度显著降低。 — Beam Search Strategies for Neural Machine Translation, 2017. 搜索过程可以通过达到最大长度、到达序列结束标记或到达阈值可能性来分别停止每个候选项。 让我们用一个例子来具体说明这个问题。 我们可以定义一个函数来执行给定序列概率和束宽度参数k的束搜索。在每一步中，每个候选序列都被扩展为所有可能的后续步骤。每个候选步骤的分数通过概率相乘得到。选择具有最大概率的k个序列，并删去其他候选项。然后重复该过程直到序列结束。 概率是很小的数，而把小的数相乘就会得到更小的数。为了避免浮点数的下溢，可将概率的自然对数相乘，这样使得到的数字更大、更易于管理。此外，通过最小化分数来进行搜索也是很常见的，因此，可以将概率的负对数相乘。这个最后的调整使我们能够按照分数对所有候选序列进行升序排序，并选择前k个序列作为可能性最大的候选序列。 下面的 beam_search_decoder() 函数实现了束搜索解码器。 我们可以将它与上一节的样本数据结合在一起，这次返回的是 3 个可能性最大的序列。 运行该示例将输出整数序列及其对数似然函数值。 试用不同的 k 值。 扩展阅读 如果你想更深入的了解，本节将提供更多的关于该主题的资源。 Argmax on Wikipedia（https://en.wikipedia.org/wiki/Arg_max） Numpy argmax API（https://docs.scipy.org/doc/numpy-1.9.3/reference/generated/numpy.argmax.html） Beam search on Wikipedia（https://en.wikipedia.org/wiki/Beam_search） Beam Search Strategies for Neural Machine Translation , 2017.（https://arxiv.org/abs/1702.01806） Artificial Intelligence: A Modern Approach (3rd Edition) , 2009.（http://amzn.to/2x7ynhW） Neural Network Methods in Natural Language Processing , 2017.（http://amzn.to/2fC1sH1） Handbook of Natural Language Processing and Machine Translation , 2011.（http://amzn.to/2xQzTnt） Pharaoh: a beam search decoder for phrase-based statistical machine translation models , 2004.（https://link.springer.com/chapter/10.1007%2F978-3-540-30194-3_13?LI=true） "
293,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737254&idx=5&sn=2279637df9e4ca443f1b13b80aa8e542&chksm=871ace18b06d470e867cb3272173dc2f85d2b6fe11426aa79ff5d19b90d83e9281b5916239f9&scene=27,「2017年度学术公众号」入围榜单出炉，老铁们请投机器之心一票,                                                                         《环球科学》杂志 《环球科学》官网 微信平台“环球科学ScientificAmerican” 微信平台“科研圈” www.linkresearcher.com 
294,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737203&idx=3&sn=4a00cbf336a716bfc9472d4eb5aba298&chksm=871ace4db06d475b120c3e2da40b6ff93d65e8428d0d8fbad1b27ce86628e1d23f81bed61a9e&scene=27,教程 | 如何通过距离度量学习解决Street-to-Shop问题,"Medium 本文将向你介绍用机器学习解决街道到商店 (street-to-shop) 问题的流程：如何从用户图像中识别衣服，并从网上商店找到它。你可曾有过这样的经历，在大街上看到某个人，然后不禁感叹，「哇哦，多么漂亮的衣服，我在哪里能买到它呢？」本文作者虽然没有过这样的经历，但是对其而言，尝试使用距离度量学习技术是一项很酷的任务。希望你也会觉得这很有趣。 数据集 首先，我们需要数据集。实际上，当我发现 Aliexpress 上有很多用户图像的时候我就有了这个想法。我心里想，「哇，我可以通过使用这种数据来搜索，当然是仅仅为了有趣」。为了简便，我决定重点关注女装。 下面是我所使用的类别： 连衣裙 衬衣&衬衫 卫衣和运动衫 夹克和外套 我使用 requests（https://pypi.python.org/pypi/requests）和 beautifualSoup（https://pypi.python.org/pypi/beautifulsoup4）来爬取图像。卖家的图像可从商品页面上获得，但是为了得到用户图像，我们需要浏览反馈页面。在商品页面上有一个叫做「colors」的属性，指的是另一种不同的颜色或者甚至是另一件商品，所以我们需要将不同颜色的衣服视为不同的商品。 商品页面上的「颜色」 你可以在 github 上找到我用来得到关于一件服饰的所有信息的代码（https://github.com/movchan74/street_to_shop_experiments/blob/master/get_item_info.py）（它爬取到的信息甚至比我们任务中需要的信息还要多）。 我们需要做的就是通过每一个类别取浏览对应的页面，拿到所有商品的 URL，然后使用上面提到的函数取得有关每个商品的信息。 最终，我们得到了每个商品的两个图片集合：来自于卖家的图片（商品页面上每个「colors」对应的元素）以及来自于用户的图片（商品页面上每个「feedbacks」对应的元素）。 对于每个 color，我们只有一张卖家图片，但是可能具有多于一个的用户图像 (然而有时候根本没有用户图像)。 很棒现在我们有数据了。然而得到的数据集有噪声： 用户图像中含有噪声（包裹箱子的照片，商品中一些无关区域的照片，以及刚拆开包装的照片）。 用户数据中的噪声示例 为了减轻这个问题，我们给 5000 张图像打了两种不同类别的标签：好图片和噪声图片。起初，我计划训练一个分类器来清洗数据集。但是后来我决定将数据清洗分类器这项工作留在后面，仅仅将干净的数据用在测试集和验证集中。 第二个问题是，一些商品有好几个卖家。这些卖家有时候甚至用的是相同图像（经过轻微编辑）。那么如何处理这个问题呢？最简单的方法就是对数据不做任何处理，使用一个鲁棒的距离度量学习算法。但是这会影响到验证，因为在这种情况下，我们在验证数据和训练数据中有相同的商品。因此这就造成了数据泄露。另一种方式就是使用某种方法来寻找相似的（甚至完全相同的）商品，并将其合并。我们可以使用感知哈希来寻找相同的图像（例如 phash 和 whash）。或者我们可以在噪声数据集上训练一个模型来寻找相似的图像。我选择了后者，因为这种方法可以合并经过轻微编辑的图像。 距离度量学习 最常用的距离度量算法之一就是 triplet loss： 其中，max(x, 0) 是 hinge 函数，d(x, y) 是 x 和 y 之间的距离函数。F(x) 是一个深度神经网络，M 是边际，a 是 anchor，p 是正例点，n 是反例点。 F(a), F(p), F(n) 都是由深度神经网络产生的高维空间中的向量。值得提及的是，为了让模型应对对照变化的时候更加鲁棒以及训练过程中具有更好的稳定性，这些向量需进行正则化处理，以拥有相同的长度，例如||x|| = 1。anchor 和正例样本属于同一类别，反例点属于其他类别。 所以 triplet loss 的主要思想就是使用一个距离边际 M 来区分正例对（anchor 和 positive）的向量。 但是如何选择元组 (a, p, n) 呢？我们可以随机选择一个 triplet，但是这样会导致以下问题。首先，存在 N³种可能的 triplet。这意味着我们需要花费很多时间来遍历所有可能的 triplet。但是实际上我们没必要这么做，因为经过少数几次的训练迭代之后，很多元 triplet 已经符合 triplet 限制（例如 0 损失）。这意味着这些 triplet 在训练中是没用的。 最常用的 triplet 选择的方式就是 hard negative mining： 实际上，选择最严格的负样本会在训练早期导致糟糕的局部最小值。尤其是，它能够导致一个收缩的模型（例如 F(x) = 0)），为了缓解这个问题，我们使用 semi-hard negative mining（半严格负样本最小化）。  半严格负样本要比 anchor 离正样本更加远，但是它们仍然是严格的（违背了 triplet 限制），因为它们在边际 M 内部。 半严格负样本的 triplet 的限制条件 下面是生成半严格和严格负样本的两种方式：在线和离线。 在线方式意味着我们从数据集中随机地选择样本作为一个 mini-batch，并从这个 Mini-batch 中选择 triplet。然而，在线方法需要一个较大的 mini-batch。在我的情况中是不可能的，因为我只有一块具有 8GB 显存的 GTX 1070。 在离线方式中，我们需要在一段时间之后停止训练，为一定数量的样本预测向量，从中选择 triplet 并为这些 triplet 训练模型。这意味着我们要进行两次，然而这就是离线方法的代价。 好了，现在我们可以开始用 triplet loss 和离线半严格负样本最小化来训练模型了。但是，为了成功地解决 street-to-shop 问题，我们还需要一个技巧。我们面临的任务是找到与用户照片最相似的卖家图像。然而，通常卖家的图像具有更高的质量，所以我们有两个域：卖家图像和用户图像。为了得到更有效的模型，我们需要减小这两个域之间的差距。这个问题就叫做域适应。 左边是用户的图像；右边是卖家的图像 我想出了一个非常简单的方法来减少这种域差距：我们在卖家图像中选择 anchor，从用户图像中选择正例样本和负例样本。这个方法简单有效。 实现 为了实现我的想法并快速实验，我使用了基于 TensorFlow 的 Keras 库。 我选择 inception V3 作为模型的基本卷积网络。像往常一样我使用 ImageNet 权重初始化卷积神经网络。我在使用 L2 正则化的全局池化之后又加了两个全连接层，向量的维度是 128。 我们还需要实现 triple 损失函数。我们传递一个 anchor，正样本和负样本作为一个 mini-batch，在损失函数中将其分为三个张量。距离函数就是欧氏距离的平方。 并优化模型： 实验结果 结果：第一列-查询（用户图像），后五列-最相似的卖家图像。 性能衡量指标是 R@K。 我们来看一下 R@K 是如何计算的。验证集里面的每张用户图像作为一次查询，我们需要找到对应的卖家图像。我们不仅使用了验证集中的卖家图像，还使用了训练集中的图像，因为这样可以使我们增加干扰数量，使得我们的任务更加具有挑战性。 所以我们就有一张查询图像，以及一些很相似的卖家图像。如果在 K 个最相似的图像中有对应的卖家数据，我们就返回 1，否则就返回 0。现在我们需要为验证集中的每一次查询返回这么一个结果，然后找到每次查询的平均得分。这就是 R@K。 正如我之前讲到的，我从噪声图像中清洗了一部分用户图像。所以我在两个验证集上衡量了一下模型的质量：完整的验证集只包含干净数据的子验证集。 验证数据中的 R@K 结果距离理想情况还很远，还有很多需要做： 从噪声数据中清洗用户图像。我在第一步中已经做了一部分。 更加准确地合并相似的图像（至少在验证集中）。 降低域差距。假设可以通过特定域增强的方法（例如亮度增强）以及其他特定的方法完成（例如这篇论文中的方法 https://arxiv.org/abs/1409.7495）。 应用其他的距离指标学习技术。我试了这篇论文中的方法（https://arxiv.org/abs/1703.07464），但是比我所用的方法性能要差一些。 当然，要收集更多的数据。 DEMO，代码和训练好的模型 我做了一个 demo。你可以在这里看到 vps389544.ovh.net:5555。你可以上传你自己的图像或者随便使用验证集中的图像来搜索。 代码和训练好的模型在这里：https://github.com/movchan74/street_to_shop_experiments。 原文链接： "
295,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737203&idx=4&sn=031d07168f84d269a5783429532ee81f&chksm=871ace4db06d475bb5457511442c8565fde63baefd5d7da5b96f2a6fc7f19159592a5fc9b245&scene=27,专栏 | 自动选模型+调参：谷歌AutoML背后的技术解析,AutoML 是 Google 最新的产品，能够根据问题自动确定最优参数和网络结构。本文章就关注解析 AutoML 背后的技术，由于 AutoML 缺乏技术文档，我们的解析有不到之处，还请多多更正。 罗马不是一天建成的。AutoML 并非一蹴而就，而是 Google 的研究者在过去几年不断思考中产生的理论与实践结合的完美产物。下图是 Google 的 AutoML 探索之路。   Alexnet 在 IMAGENET 取得冠军之后，Google 意识到了深度学习是未来的趋势，于是投入巨资进行神经网络的研究。从 Deepmind 被收购，Hinton 加入 Google，Tensorflow 的开源中可见 Google 对于 Deep Learning 的重视与远见。Google 在不断的调参数中发现了著名的 Inception 网络，并且结合 ReNet，发现了 Inception-ResNet，V4 和 Xception。这些发现让 Google 注意到了神经网络结构会对结构产生巨大影响，但是找到最优的结构需要耗费大量的人力和物力，并且对于一个新的数据集还需要投入同样的资源搜索合适的结构，这样的人工搜索是不能够 scalable 的。Inception-ResNet 的网络结构请参考论文：https://arxiv.org/pdf/1602.07261.pdf。Inception 系列网络跟 ResNet 的结果比较。 神经网络搜索初探： Neural Architecture Search with Reinforcement Learning（ICLR 2017 Best Paper） 为了增加网络结构搜索的 scalability，Google Residency Program 的成员 Barrret Zoph 在 Quoc Le 的带领下开始了神经网络自动调参的尝试，Neural Architecture Search with Reinforcement Learning 就是对这一工作的总结。该论文获得了 ICLR 2017 的 Best Paper。Barret Zoph 的工作成功在 CIFAR 和 PTB 上面搜索到了 state-of-the-art 的 CNN 和 LSTM 结构，最优 CNN 与 LSTM 结构对比如下： Barret Zoph 使用强化学习进行网络结构搜索，网络框架图如下图：   Controller 是由 RNN 构成能够产生每层网络的 Kernel 大小和 skip connection 的连接，产生了网络结构之后，使用网络结构的准确率作为 Reward function。Controller 会根据 reward function 的反馈进行网络结构调节，最后会得到最优的网络结构。Controller 生成网络结构预测如下图：   本篇论文会对 controller 预测的网络进行 SGD 学习，网络收敛之后的准确率为 Reward，然后使用 reinforcement learning 的 policy gradient 进行 controller 权值更新，policy gradient 公式如下：   期望值用下面的公式进行近似： 为了保证训练稳定，引入了 baseline，公式如下：   为了保证收敛速度，作者引入了 asynchronous 权值更新，在每个 GPU 中分配多个网络结构进行训练，跟 asynchronous reinforcement learning 的 idea 类似。该论文的 distribution 结构如下图：   本篇论文能够避免手动调参数，但是得到网络搜索需要 800GPU 搜索几个月的时间，最近 Google 使用 P100 可以在一周左右训练出模型，本论文仅仅在 CIFAR 上面进行实验，在大规模数据集 IMAGENET 上面的使用受限。 Large Scale Evolution of Image Classifiers（ICML 2017） 本篇论文通过 large scale evolution 的办法来搜索最优的神经网络，由于本人能力有限，我们不对这篇论文进行技术解析。该论文的结构搜索过程如下图：   有趣的现象是，evolution 搜索偏向于没有 skip connection 的神经网络。通过 evolution 办法搜索到的神经网络比 ResNet 结果好，但是低于 DenseNet，如下图：   为了让结构搜索的工作能够实用，Google 的研究者从 progressive Search，Transferable architecture 和 Bayesian 的角度进行探索，并且取得了进展。 Progressive Neural Architecture Search（PNAS） 本篇论文提出了通过 progressive 的办法进行网络搜索，会比 RL 方法快 2 倍，比 evolution 方法快 5 倍。 与之前的方法不同，本篇论文是在一个网络的基础上面加上新的 cell，然后使用类似与 A*搜索（Branch and Bound）的办法，搜索到最优的网络结构。Progressive 方法示意图如下：   PNAS 所使用的 Cell 结构如下：   Learning Transferable Architecture for Scalable Image Recognition 本篇论文是集大成者，Barret Zoph 在之前全部技术的基础上面，成功地将自动结构搜索在 IMAEGNET，COCO 等实用性的物体分类和检测任务上面成功运用。结果太 AMAZING 啦。竟然超过了 ResNet-FPN，COCO 物体检测结果如下：   Barret Zoph 首先在 CIFAR 上面使用之前的方法搜索出最优 cell，然后将 cell 重复的运用在 IMAGENET 上面（真的就是这么简单有效 >_<）。最优单个 cell 的结构如下图：   可能 AutoML 用的就是这种技术吧。 总结 Google 在大规模的调参中发现了手动调参不能够适应大数据时代的需求。于是进行从 reinforcement learning 和 evolution 两个角度进行了自动调参的初探。为了改进网络结构搜索的时间，Google 提出了 Progressive Search 和 Transferable Architecture 的办法。从中我们可以感受到 Google 一步一个脚印的做事方法，希望 AI 公司和个人都能够从中获得一些收益 >_<。  加入机器之心（全职记 
296,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737254&idx=4&sn=24b18b8ad27eb32041f075c49972e456&chksm=871ace18b06d470e6038325dd1b1811ba45f6c9803f76c9f2d9a6706146b556a0a462e50fd37&scene=27,ICLR 2018 | CMU提出新型智能体定位方法：「主动神经定位器」,"日前，ICLR 2018 接收论文名单公布，CMU 教授 Ruslan Salakhutdinov 等人的论文《Active Neural Localization》名列其中。该论文提出了一种新型智能体定位方法：主动神经定位器（Active Neural Localization），结合传统基于过滤的定位方法和策略模型，实现准确定位并最小化定位所需的步骤数量。 定位（localization）是在给出环境地图和智能体观测的情况下估计智能体位置的一类问题。智能体需要具有在不确定的情况下进行定位的能力，以执行多种下游任务，如规划（planning）、探查（exploration）和目标导航（targetnavigation）。定位被认为是移动机器人最基础的问题之一（Cox & Wilfong, 1990; Borenstein et al., 1996）。定位在很多现实应用中非常有用，例如自动驾驶汽车、工业机器人和快递无人机。 本论文解决了智能体初始位置未知时的全局定位问题。尽管全局定位的研究历史很长，但它仍然是一个开放性问题，目前还没有多少方法可以通过端到端的方式从数据中学习，大多数方法通常需要领域专家进行重要的手动调整和特征选择。目前大多数定位方法的另一个局限是它们是被动的，即它们基于收到的观测流被动地估计智能体的位置，且没有能力决定智能体所采取的动作。决定智能体动作的能力可带来更快更准确地定位，因为 智能体可学习快速导航至环境中的明确位置。 论文提出「主动神经定位器（Active Neural Localizer）」，这是一个能够使用基于像素的原始观测和环境地图进行主动定位的神经网络模型。该模型基于贝叶斯过滤算法进行定位（Fox et al., 2003），包含一个用于估计智能体观测似然度的感知模型、一个表示信念（belief）、具备乘法相互作用（multiplicative interaction）的结构化组件（用于传播信念），以及一个基于当前信念的策略模型（用于准确定位并最小化定位所需的步骤数量）。整个模型完全可微，并且使用强化学习进行训练，从而以端到端的方式同时学习感知模型和策略模型。论文作者使用多种 2D、3D 仿真环境来测试该模型。结果表明，主动神经定位器不仅能够泛化至同一个域中的未知地图，还能够泛化至跨域未知地图。研究者还提供了全新的模拟场景，为未来的主动定位研究做准备。 模型构成部分 感知模型 感知模型根据智能体的观测结果和地图信息中给出的状态来计算特征表征。地图信息中每个状态的似然度通过计算智能体观测的特征表征和该状态的特征表征的余弦相似度（cosine similarity）得到。余弦相似度通常用于计算表征的相似性（Nair & Hinton, 2010; Huang et al., 2013），并已经被用于文本定位中（Chaplot et al., 2016）。Chunjie 等人（2017 年）强调了余弦相似度相对于点积的优势。 在 2D 环境中，观测被用来计算 one-hot 向量，one-hot 向量的维度与表示深度（可直接用作特征表征）的维度相同。得到的似然图（Likelihood map）对所有具有观测深度的位置都具有统一的非零概率，而其它位置都具有零概率。对于 3D 环境，每个观测的特征表征使用可训练的深度卷积网络获取（LeCun et al., 1995）。图 2 是在 2D 和 3D 环境下智能体的观测和计算出的对应似然图的示例。 策略模型 策略模型基于智能体当前信念给出下一个动作的概率。它使用强化学习进行训练，具体来说，它使用的是 Asynchronous Advantage Actor-Critic（A3C）算法（Mnih et al., 2016）。用于预测策略和价值函数（value function）的方法是叠加信念地图（belief map）与地图设计矩阵（map design matrix），先穿过两个卷积层，然后穿过一个全连接层。策略和价值损失使用智能体观测到的奖励（reward）进行计算，然后反向传播通过整个模型。 上图中，每行分别显示一个 episode 中连续的时间步。每列分别显示智能体观测、观测之前和观测之后基于位置的信念、地图设计和智能体观察世界的视角。智能体的真实位置也标记在地图设计中（但是智能体自己看不到）。信念地图显示在特定位置的概率，颜色越深代表概率越高。智能体对自己方位的信念和真实的方位也用颜色突出显示。例如，红色信念地图显示智能体在 x-y 坐标面向东的概率。注意地图设计不是信念地图的一部分，它被叠加在信念地图上以获得更好的可视化效果。在所有时间步中，所有与智能体视角相似的位置在信念地图中具有高概率。这个例子表明定位时动作决策的重要性。在 t=3 时，智能体对自己位置不是很确定，因为有 4 个具有相同视角的位置。智能体执行最优动作组合来降低不确定性，向前并左转，从而成功定位。 论文： Active Neural Localization 论文链接：https://arxiv.org/abs/1801.08214 摘要： 定位是在给出环境地图和智能体观测的情况下估计智能体位置的一类问题。就所需步骤数量来看，传统的定位方法（过滤基于观测的信念）并非最优，因为它们不进行智能体动作的决策。我们提出「主动神经定位器」（Active Neural Localizer），一个可以完全微分并能够准确高效地学习定位的神经网络模型。该模型包含了传统的基于过滤的定位方法思想（通过使用一个具有乘法相互作用（multiplicative interaction）的结构化状态信念来传播信念），同时将其与策略模型相结合，以准确定位并最小化定位所需的步骤数量。主动神经定位器通过端到端的强化学习进行训练。我们使用了多种仿真环境来测试该模型，包括二维迷宫、Doom 游戏引擎中的随机迷宫和虚幻游戏引擎（Unreal）中的拟真（photo-realistic）环境。在二维环境中的测试结果表明在理想设置下习得策略的有效性，而在三维环境中的测试结果表明，模型具备从基于 RGB 的原始像素观测中同时学习策略模型和感知模型的能力。实验还表明在 Doom 环境下基于随机纹理训练的模型能够很好地泛化至虚幻引擎中的拟真办公空间环境。  "
297,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737203&idx=2&sn=838f80d8be20c20efb321405d476c3bc&chksm=871ace4db06d475bd7cf451ecaa10ed455223c285b1cc6a778aa4337e225cdfafc6c982beda4&scene=27,入门 | 理解深度学习中的学习率及多种选择策略,"TowardsDataScience 学习率是最影响性能的超参数之一，如果我们只能调整一个超参数，那么最好的选择就是它。相比于其它超参数学习率以一种更加复杂的方式控制着模型的有效容量，当学习率最优时，模型的有效容量最大。从手动选择学习率到使用预热机制，本文介绍了很多学习率的选择策略。 这篇文章记录了我对以下问题的理解： 学习速率是什么？学习速率有什么意义？ 如何系统地获得良好的学习速率？ 我们为什么要在训练过程中改变学习速率? 当使用预训练模型时，我们该如何解决学习速率的问题？ 本文的大部分内容都是以 fast.ai 研究员写的内容 [1], [2], [5] 和 [3] 为基础的。本文是一个更为简洁的版本，通过本文可以快速获取这些文章的主要内容。如果您想了解更多详情，请参阅参考资料。 学习速率是指导我们该如何通过损失函数的梯度调整网络权重的超参数。学习率越低，损失函数的变化速度就越慢。虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在高原区域的情况下。 下述公式表示了上面所说的这种关系。 一般而言，用户可以利用过去的经验（或其他类型的学习资料）直观地设定学习率的最佳值。 因此，想得到最佳学习速率是很难做到的。下图演示了配置学习速率时可能遇到的不同情况。 此外，学习速率对模型收敛到局部极小值（也就是达到最好的精度）的速度也是有影响的。因此，从正确的方向做出正确的选择意味着我们可以用更短的时间来训练模型。 有更好的方法选择学习速率吗？ 在「训练神经网络的周期性学习速率」[4] 的 3.3 节中，Leslie N. Smith 认为，用户可以以非常低的学习率开始训练模型，在每一次迭代过程中逐渐提高学习率（线性提高或是指数提高都可以），用户可以用这种方法估计出最佳学习率。 如果我们对每次迭代的学习进行记录，并绘制学习率（对数尺度）与损失，我们会看到，随着学习率的提高，从某个点开始损失会停止下降并开始提高。在实践中，学习速率的理想情况应该是从图的左边到最低点（如下图所示）。在本例中，是从 0.001 到 0.01。 上述方法看似有用，但该如何应用呢？ 目前，上述方法在 fast.ai 包中作为一个函数进行使用。fast.ai 包是由 Jeremy Howard 开发的一种高级 pytorch 包（就像 Keras 之于 Tensorflow）。 在训练神经网络之前，只需输入以下命令即可开始找到最佳学习速率。 现在我们已经知道了什么是学习速率，那么当我们开始训练模型时，怎样才能系统地得到最理想的值呢。接下来，我们将介绍如何利用学习率来改善模型的性能。 传统的方法 一般而言，当已经设定好学习速率并训练模型时，只有等学习速率随着时间的推移而下降，模型才能最终收敛。 然而，随着梯度达到高原，训练损失会更难得到改善。在 [3] 中，Dauphin 等人认为，减少损失的难度来自鞍点，而不是局部最低点。 所以我们该如何解决这个问题？ 我们可以采取几种办法。[1] 中是这么说的： …无需使用固定的学习速率，并随着时间的推移而令它下降。如果训练不会改善损失，我们可根据一些周期函数 f 来改变每次迭代的学习速率。每个 Epoch 的迭代次数都是固定的。这种方法让学习速率在合理的边界值之间周期变化。这是有益的，因为如果我们卡在鞍点上，提高学习速率可以更快地穿越鞍点。 在 [2] 中，Leslie 提出了一种「三角」方法，这种方法可以在每次迭代之后重新开始调整学习速率。 另一种常用的方法是由 Loshchilov＆Hutter [6] 提出的预热重启（Warm Restarts）随机梯度下降。这种方法使用余弦函数作为周期函数，并在每个周期最大值时重新开始学习速率。「预热」是因为学习率重新开始时并不是从头开始的，而是由模型在最后一步收敛的参数决定的 [7]。 下图展示了伴随这种变化的过程，该过程将每个周期设置为相同的时间段。 因此，我们现在可以通过周期性跳过「山脉」的办法缩短训练时间（下图）。 研究表明，使用这些方法除了可以节省时间外，还可以在不调整的情况下提高分类准确性，而且可以减少迭代次数。 在 fast.ai 课程中，非常重视利用预训练模型解决 AI 问题。例如，在解决图像分类问题时，会教授学生如何使用 VGG 或 Resnet50 等预训练模型，并将其连接到想要预测的图像数据集。 我们采取下面的几个步骤，总结了 fast.ai 是如何完成模型构建（该程序不要与 fast.ai 包混淆）的： 1. 启用数据增强，precompute = True 2. 使用 lr_find() 找到损失仍在降低的最高学习速率 3. 从预计算激活值到最后一层训练 1~2 个 Epoch 4. 在 cycle_len = 1 的情况下使用数据增强（precompute=False）训练最后一层 2~3 次 5. 修改所有层为可训练状态 6. 将前面层的学习率设置得比下一个较高层低 3~10 倍 7. 再次使用 lr_find() 8. 在 cycle_mult=2 的情况下训练整个网络，直到过度拟合 从上面的步骤中，我们注意到步骤 2、5 和 7 提到了学习速率。这篇文章的前半部分已经基本涵盖了上述步骤中的第 2 项——如何在训练模型之前得出最佳学习率。 在下文中，我们会通过 SGDR 来了解如何通过重启学习速率来减少训练时间和提高准确性，以避免梯度接近零。 在最后一节中，我们将重点介绍差异学习（differential learning），以及如何在训练带有预训练模型中应用差异学习确定学习速率。 什么是差异学习 差异学习（different learning）在训练期间为网络中的不同层设置不同的学习速率。这种方法与人们常用的学习速率配置方法相反，常用的方法是训练时在整个网络中使用相同的学习速率。 在写这篇文章的时候，Jeremy 和 Sebastian Ruder 发表的一篇论文深入探讨了这个问题。所以我估计差异学习速率现在有一个新的名字——差别性的精调。:) 为了更清楚地说明这个概念，我们可以参考下面的图。在下图中将一个预训练模型分成 3 组，每个组的学习速率都是逐渐增加的。 这种方法的意义在于，前几个层通常会包含非常细微的数据细节，比如线和边，我们一般不希望改变这些细节并想保留它的信息。因此，无需大量改变权重。 相比之下，在后面的层，以绿色以上的层为例，我们可以从中获得眼球、嘴巴或鼻子等数据的细节特征，但我们可能不需要保留它们。 这种方法与其他微调方法相比如何？ 在 [9] 中提出，微调整个模型太过昂贵，因为有些模型可能超过了 100 层。因此人们通常一次一层地对模型进行微调。 然而，这样的调整对顺序有要求，不具并行性，且因为需要通过数据集进行微调，导致模型会在小数据集上过拟合。 下表证明 [9] 中引入的方法能够在各种 NLP 分类任务中提高准确度且降低错误率。  "
298,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737203&idx=5&sn=86a334807d312d43713842dd48869c3f&chksm=871ace4db06d475b67cc58362a4961c2b37c17711932bbe158da1642dae47deb4aacd6394c1e&scene=27,学界 | 综述论文：机器学习中的模型评价、模型选择与算法选择,"本论文回顾了用于解决模型评估、模型选择和算法选择三项任务的不同技术，并参考理论和实证研究讨论了每一项技术的主要优势和劣势。进而，给出建议以促进机器学习研究与应用方面的最佳实践。 机器学习已经成为我们生活的中心，无论是作为消费者、客户、研究者还是从业人员。无论将预测建模技术应用到研究还是商业问题，我认为其共同点是：做出足够好的预测。用模型拟合训练数据是一回事，但我们如何了解模型的泛化能力？我们如何确定模型是否只是简单地记忆训练数据，无法对未见过的样本做出好的预测？还有，我们如何选择好的模型呢？也许还有更好的算法可以处理眼前的问题呢？ 模型评估当然不是机器学习工作流程的终点。在处理数据之前，我们希望事先计划并使用合适的技术。本文将概述这类技术和选择方法，并介绍如何将其应用到更大的工程中，即典型的机器学习工作流。 1.1 性能评估：泛化性能 vs. 模型选择 让我们考虑这个问题：「如何评估机器学习模型的性能？」典型的回答可能是：「首先，将训练数据馈送给学习算法以学习一个模型。第二，预测测试集的标签。第三，计算模型对测试集的预测准确率。」然而，评估模型性能并非那么简单。也许我们应该从不同的角度解决之前的问题：「为什么我们要关心性能评估呢？」理论上，模型的性能评估能给出模型的泛化能力，在未见过的数据上执行预测是应用机器学习或开发新算法的主要问题。通常，机器学习包含大量实验，例如超参数调整。在训练数据集上用不同的超参数设置运行学习算法最终会得到不同的模型。由于我们感兴趣的是从该超参数设置中选择最优性能的模型，因此我们需要找到评估每个模型性能的方法，以将它们进行排序。 我们需要在微调算法之外更进一步，即不仅仅是在给定的环境下实验单个算法，而是对比不同的算法，通常从预测性能和计算性能方面进行比较。我们总结一下评估模型的预测性能的主要作用： 评估模型的泛化性能，即模型泛化到未见过数据的能力； 通过调整学习算法和在给定的假设空间中选择性能最优的模型，以提升预测性能； 确定最适用于待解决问题的机器学习算法。因此，我们可以比较不同的算法，选择其中性能最优的模型；或者选择算法的假设空间中的性能最优模型。 虽然上面列出的三个子任务都是为了评估模型的性能，但是它们需要使用的方法是不同的。本文将概述解决这些子任务需要的不同方法。 我们当然希望尽可能精确地预测模型的泛化性能。然而，本文的一个要点就是，如果偏差对所有模型的影响是等价的，那么偏差性能评估基本可以完美地进行模型选择和算法选择。如果要用排序选择最优的模型或算法，我们只需要知道它们的相对性能就可以了。例如，如果所有的性能评估都是有偏差的，并且低估了它们的性能（10%），这不会影响最终的排序。更具体地说，如果我们得到如下三个模型，这些模型的预测准确率如下： M2: 75% > M1: 70% > M3: 65%, 如果我们添加了 10% 的性能偏差（低估），则三种模型的排序没有发生改变： M2: 65% > M1: 60% > M3: 55%.  但是，注意如果最佳模型（M2）的泛化准确率是 65%，很明显这个精度是非常低的。评估模型的绝对性能可能是机器学习中最难的任务之一。 2 Bootstrapping 和不确定性 本章介绍一些用于模型评估的高级技术。我们首先讨论用来评估模型性能不确定性和模型方差、稳定性的技术。之后我们将介绍交叉验证方法用于模型选择。如第一章所述，关于我们为什么要关心模型评估，存在三个相关但不同的任务或原因。 我们想评估泛化准确度，即模型在未见数据上的预测性能。 我们想通过调整学习算法、从给定假设空间中选择性能最好的模型，来改善预测性能。 我们想确定手头最适合待解决问题的机器学习算法。因此，我们想对比不同的算法，选出性能最好的一个；或从算法的假设空间中选出性能最好的模型。 几乎所有机器学习算法都需要我们机器学习研究者和从业者指定大量设置。这些超参数帮助我们控制机器学习算法在优化性能、找出偏差方差最佳平衡时的行为。用于性能优化的超参数调整本身就是一门艺术，没有固定规则可以保证在给定数据集上的性能最优。前面的章节提到了用于评估模型泛化性能的留出技术和 bootstrap 技术。偏差-方差权衡和计算性能估计的不稳定性方法都得到了介绍。本章主要介绍用于模型评估和选择的不同交叉验证方法，包括对不同超参数配置的模型进行排序和评估其泛化至独立数据集的性能。 本章生成图像的代码详见：https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/resampling-and-kfold.ipynb。 我们可以把超参数调整（又称超参数优化）和模型选择的过程看作元优化任务。当学习算法在训练集上优化目标函数时（懒惰学习器是例外），超参数优化是基于它的另一项任务。这里，我们通常想优化性能指标，如分类准确度或接受者操作特征曲线（ROC 曲线）下面积。超参数调整阶段之后，基于测试集性能选择模型似乎是一种合理的方法。但是，多次重复使用测试集可能会带来偏差和最终性能估计，且可能导致对泛化性能的预期过分乐观，可以说是「测试集泄露信息」。为了避免这个问题，我们可以使用三次分割（three-way split），将数据集分割成训练集、验证集和测试集。对超参数调整和模型选择进行训练-验证可以保证测试集「独立」于模型选择。这里，我们再回顾一下性能估计的「3 个目标」： 我们想评估泛化准确度，即模型在未见数据上的预测性能。 我们想通过调整学习算法、从给定假设空间中选择性能最好的模型，来改善预测性能。 我们想确定最适合待解决问题的机器学习算法。因此，我们想对比不同的算法，选出性能最好的一个，从算法的假设空间中选出性能最好的模型。 论文：Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning 论文链接：https://sebastianraschka.com/pdf/manuscripts/model-eval.pdf 摘要： 模型评估、模型选择和算法选择技术的正确使用在学术性机器学习研究和诸多产业环境中异常关键。本文回顾了用于解决以上三项任务中任何一个的不同技术，并参考理论和实证研究讨论了每一项技术的主要优势和劣势。进而，给出建议以促进机器学习研究与应用方面的最佳实践。本文涵盖了用于模型评估和选择的常见方法，比如留出方法，但是不推荐用于小数据集。不同风格的 bootstrap 技术也被介绍，以评估性能的不确定性，以作为通过正态空间的置信区间的替代，如果 bootstrapping 在计算上是可行的。在讨论偏差-方差权衡时，把 leave-one-out 交叉验证和 k 折交叉验证进行对比，并基于实证证据给出 k 的最优选择的实际提示。论文展示了用于算法对比的不同统计测试，以及处理多种对比的策略（比如综合测试、多对比纠正）。最后，当数据集很小时，本文推荐替代方法（比如 5×2cv 交叉验证和嵌套交叉验证）以对比机器学习算法。  "
299,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737254&idx=1&sn=2b9601756abbece51e0cc9b8ef3f653d&chksm=871ace18b06d470e473119f3e56baa868f7437378b4ae46aed5de683ab557e56d15d764a6cd4&scene=27,搞事！ICLR 2018七篇对抗样本防御论文被新研究攻破，Goodfellow论战,"ICLR 2018 大会的接收论文中，8 篇有关防御对抗样本的研究中，7 篇已经被攻破了——在大会开幕三个月之前。来自 MIT 和 UC Berkeley 的研究者定义了一种被称为「混淆梯度」（obfuscated gradients）的现象。在面对强大的基于优化的攻击之下，它可以实现对对抗样本的鲁棒性防御。这项研究引起了深度学习社区的讨论，GAN 提出者 Ian Goodfellow 也参与其中，并作了反驳。 GitHub 链接：https://github.com/anishathalye/obfuscated-gradients 上图展示了一个「对抗样本」：仅仅加入了一些轻微的扰动，这张猫的图片就可以愚弄 InceptionV3 分类器，使其将图片分类为「鳄梨酱」。这类「欺骗性图像」可以轻松地利用梯度下降法生成（Szegedy et al. 2013）。 为了解决神经网络对抗样本的问题（Szegedy et al., 2013），近期人们对于构建防御对抗样本，增加神经网络鲁棒性的研究越来越多。尽管人们对于对抗样本的认识已经提升，相关的防御方法也有提出，但迄今为止并没有一种完整的解决方法出现。对于 MIT 和 UC Berkeley 的研究人员来说，目前正被审核的所有对抗样本防御论文中提到的方法（Papernot et al., 2016；Hendrik Metzen et al., 2017；Hendrycks & Gimpel, 2017；Meng & Chen, 2017；Zantedeschi et al., 2017）都可以被更加强大的优化攻击（Carlini & Wagner, 2017）击败。 基于迭代优化的对抗攻击测试基准如 BIM（Kurakin et al., 2016a）、PGD（Madry et al., 2018）和 Carlini 与 Wagner 的方法（Carlini & Wagner, 2017）近期已经成为评估防御能力的标准，最新的防御方式似乎能够抵御基于优化的最强攻击。 论文作者宣称他们找到了很多防御机制鲁棒地抵抗迭代攻击的一个普遍原因：混淆梯度。缺乏好的梯度信号，基于优化的方法就不能成功了。在论文中，作者确定了三种类型的混淆梯度。某些防御方式会导致破碎的梯度，有意地通过不可微分运算或无意地通过数字不稳定性，可以得到不存在或不正确的梯度信号。一些防御是随机性的，导致依赖于测试时间熵的随机梯度（攻击者无法接触）。另一些防御会导致梯度消失/爆炸（Bengio et al., 1994），导致无法使用的梯度信号。 研究人员提出了克服这三种现象造成的混淆梯度的新技术。在研究中，一种被称为后向传递可微近似（Backward Pass Differentiable Approximation）的方法解决了不可微分运算导致的梯度破碎。我们可以使用 Expectation Over Transformation 计算随机防御的梯度（Athalye et al., 2017），通过再参数化和空间优化来解决梯度消失/爆炸问题。 为了调查混淆梯度的普遍程度，并了解该攻击技术的适用性，研究人员使用 ICLR 2018 接收论文中的防御对抗样本论文作为研究对象，研究发现混淆梯度的使用是一种普遍现象，在 8 篇论文里，有 7 篇研究依赖于该现象。研究者应用新开发的攻击技术，解决了混淆梯度问题，成功攻破其中的 7 个。研究人员还对新方法对这些论文的评估过程进行了分析。 此外，研究者希望这篇论文可以为对抗样本方向提供新的知识基础、攻击技术解释，避免未来的研究落入陷阱，帮助避免未来的防御机制轻易被此类攻击攻破。 为了保证可复现，研究人员重新实现了 8 篇防御研究的方法，以及对应的攻击方法。下表展示了这 8 篇防御研究方法在攻击下的稳健程度： 七篇论文中，我们能够看到唯一一个显著提高对抗样本防御鲁棒性的研究是论文《Towards Deep Learning Models Resistant to Adversarial Attacks》（Madry et al. 2018），如果不使用威胁模型以外的方法就无法攻破它。即便如此，这种方法也已被证明难以扩展到 ImageNet 的规模（Kurakin et al. 2016）。其他论文全部或多或少地依赖于「混淆梯度」。标准攻击应用梯度下降使网络损失最大化，为给定图片在神经网络中生成对抗样本。这种优化方法需要有可用的梯度信号才能成功。基于混淆梯度的防御会破坏这种梯度信号，使得优化方法失效。 研究人员定义了三种基于混淆梯度的防御方式，并构建了绕过它们的攻击方法。新的攻击方式适用于任何有意或无意的，不可微分运算或其他阻止梯度信号流经神经网络的防御法。研究人员希望未来研究能够基于这种新提出的方法进行更加有效的安全评估。 讨论 在对 ICLR 2018 的几篇论文攻击成功之后，是时候来对评估防御对抗样本方法的新规则了。MIT 和 UC Berkeley 的研究人员给出的建议仍然大体遵循前人的研究（Carlini & Wagner, 2017a；Madry et al., 2018）。 6.1 定义（逼真的）威胁模型 构建防御时，定义限制对抗样本的威胁模型非常关键。之前的研究使用单词 white-box、grey-box、black-box 和 no-box 来描述威胁模型。 本论文作者没有再次尝试重新定义词汇，而是概括防御的多个方面，它们对于对抗样本可能是已知的，但对于防御样本是未知的： 模型架构和权重； 训练算法和数据； 带有随机性的防御对抗样本，不管对抗样本是否知道所选随机值的确切序列或者仅仅是分布； 假设对抗样本不知道模型架构和权重，查询访问被允许。那么模型输出为 logits、概率向量或预测标签（即 arg max）。 尽管对抗样本的很多方面可能是未知的，但威胁模型不应该包含非逼真的约束。研究者认为任何有效的威胁模型都是对模型架构、训练算法所知甚少的，并且允许查询访问。 研究者认为限制对抗样本的计算能力并无意义。如果两个防御对抗样本具备同样的鲁棒性，但其中一个生成对抗样本需要一秒，另一个需要十秒，则鲁棒性并未提高。只有当对抗样本的计算速度比预测运行时有指数级提升时，将运行时作为安全参数才是可行的。但是，把攻击时间增加几秒并无太大意义。 6.2 研究结果应具体且可测试 定义完清晰的威胁模型之后，防御应该具体而可测试。例如，这些防御方法可以声称在失真度=0.031 时，对抗样本的鲁棒性为 90% 至最大，或声称平均两种对抗样本的失真度增加了基线模型的安全程度（在这种情况下，基线模型需要有明确的定义）。 不幸的是，研究者评估的大多数防御方法仅声明鲁棒性而未给出特定界限。这个建议最大的缺陷就是防御不应声称对无界攻击具备彻底的鲁棒性：不限制失真度，则任何图像可以随意转换，且「成功率」为 100%。 为了使防御声明可测试，防御必须是完全具体的，并给出所有超参数。发行源代码、预训练模型以及论文也许是使声明具体的最有效方法。8 篇论文中有 4 篇具有完整的源代码（Madry et al., 2018; Ma et al., 2018; Guo et al., 2018; Xie et al., 2018）。 6.3 评估自适应攻击（adaptive attack） 加强对现有攻击的鲁棒性（同时又是具体而可测试的）用处不大。真正重要的是通过具有防御意识的攻击积极评估自身的防御以证明安全性。 具体而言，一旦彻底认定一个防御，并且对手受限于威胁模式之下，攻克这一防御的尝试就变的很重要。如果它能被攻克，那么就不要设法阻止特定的攻击（即通过调整超参数）。一次评估之后，可接受对防御的调整，但调整之后要接受新的攻击。这样，通过最终的自适应攻击得出评估结果就类似于在测试数据上评估模型。 论文：Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples 论文链接：https://arxiv.org/abs/1802.00420  摘要： 我们发现了一种「混淆梯度」（obfuscated gradient）现象，它给对抗样本的防御带来虚假的安全感。尽管基于混淆梯度的防御看起来击败了基于优化的攻击，但是我们发现依赖于此的防御并非万无一失。对于我们发现的三种混淆梯度，我们会描述展示这一效果的防御指标，并开发攻击技术来克服它。在案例研究中，我们试验了 ICLR 2018 接收的 8 篇论文，发现混淆梯度是一种常见现象，其中有 7 篇论文依赖于混淆梯度，并被我们的这一新型攻击技术成功攻克。 「混淆梯度」引发争议 这篇论文甫一出现，立刻引起了研究社区的关注，GAN 提出者 Ian Goodfellow 也参与了讨论。Goodfellow 指出「混淆梯度」的概念实际上与此前人们提出的「梯度遮蔽（gradient masking）」概念相同。同时 ICLR 2018 中的一篇论文《Ensemble Adversarial Training: Attacks and Defenses》（Goodfellow 也是作者之一）实际上解决了这一问题。不过，这一观点并未被 MIT 与 UC Berkeley 的论文作者完全接受。 」 新研究的第一作者，MIT 博士生 Anish Athalye 致力于研究防御对抗样本的方法。也是此前「3D 打印对抗样本」研究的主要作者（参见： 围观！ MIT 科学家调戏了谷歌图像识别网络，后者把乌龟认成来福枪 ）。多篇 ICLR 接收论文在大会开始三个月前就遭反驳，看来，人们在防御对抗样本的道路上还有很长一段路要走。 "
300,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737028&idx=3&sn=a6a849924056f4a1300b04bd55c7f87a&chksm=871acdfab06d44ec6ec46b5aeb82df96b6371a81f1ff5f2e64aba4ce1274ee6bfcc24ed2a01e&scene=27,入门 | 机器学习新手必看10大算法,TowardsDataScience 本文介绍了机器学习新手需要了解的 10 大算法，包括线性回归、Logistic 回归、朴素贝叶斯、K 近邻算法等。 在机器学习中，有一种叫做「没有免费的午餐」的定理。简而言之，它指出没有任何一种算法对所有问题都有效，在监督学习（即预测建模）中尤其如此。 例如，你不能说神经网络总是比决策树好，反之亦然。有很多因素在起作用，例如数据集的大小和结构。 因此，你应该针对具体问题尝试多种不同算法，并留出一个数据「测试集」来评估性能、选出优胜者。 当然，你尝试的算法必须适合你的问题，也就是选择正确的机器学习任务。打个比方，如果你需要打扫房子，你可能会用吸尘器、扫帚或拖把，但是你不会拿出铲子开始挖土。 大原则 不过也有一个普遍原则，即所有监督机器学习算法预测建模的基础。 机器学习算法被描述为学习一个目标函数 f，该函数将输入变量 X 最好地映射到输出变量 Y：Y = f(X) 这是一个普遍的学习任务，我们可以根据输入变量 X 的新样本对 Y 进行预测。我们不知道函数 f 的样子或形式。如果我们知道的话，我们将会直接使用它，不需要用机器学习算法从数据中学习。 最常见的机器学习算法是学习映射 Y = f(X) 来预测新 X 的 Y。这叫做预测建模或预测分析，我们的目标是尽可能作出最准确的预测。 对于想了解机器学习基础知识的新手，本文将概述数据科学家使用的 top 10 机器学习算法。 1. 线性回归 线性回归可能是统计学和机器学习中最知名和最易理解的算法之一。 预测建模主要关注最小化模型误差或者尽可能作出最准确的预测，以可解释性为代价。我们将借用、重用包括统计学在内的很多不同领域的算法，并将其用于这些目的。 线性回归的表示是一个方程，它通过找到输入变量的特定权重（称为系数 B），来描述一条最适合表示输入变量 x 与输出变量 y 关系的直线。 例如：y = B0 + B1 * x 我们将根据输入 x 预测 y，线性回归学习算法的目标是找到系数 B0 和 B1 的值。 可以使用不同的技术从数据中学习线性回归模型，例如用于普通最小二乘法和梯度下降优化的线性代数解。 线性回归已经存在了 200 多年，并得到了广泛研究。使用这种技术的一些经验是尽可能去除非常相似（相关）的变量，并去除噪音。这是一种快速、简单的技术，可以首先尝试一下。 2. Logistic 回归 Logistic 回归是机器学习从统计学中借鉴的另一种技术。它是解决二分类问题的首选方法。 Logistic 回归与线性回归相似，目标都是找到每个输入变量的权重，即系数值。与线性回归不同的是，Logistic 回归对输出的预测使用被称为 logistic 函数的非线性函数进行变换。 logistic 函数看起来像一个大的 S，并且可以将任何值转换到 0 到 1 的区间内。这非常实用，因为我们可以规定 logistic 函数的输出值是 0 和 1（例如，输入小于 0.5 则输出为 1）并预测类别值。 Logistic 回归 由于模型的学习方式，Logistic 回归的预测也可以作为给定数据实例（属于类别 0 或 1）的概率。这对于需要为预测提供更多依据的问题很有用。 像线性回归一样，Logistic 回归在删除与输出变量无关的属性以及非常相似（相关）的属性时效果更好。它是一个快速的学习模型，并且对于二分类问题非常有效。 3. 线性判别分析（LDA） Logistic 回归是一种分类算法，传统上，它仅限于只有两类的分类问题。如果你有两个以上的类别，那么线性判别分析是首选的线性分类技术。 LDA 的表示非常简单直接。它由数据的统计属性构成，对每个类别进行计算。单个输入变量的 LDA 包括： 每个类别的平均值； 所有类别的方差。 线性判别分析 进行预测的方法是计算每个类别的判别值并对具备最大值的类别进行预测。该技术假设数据呈高斯分布（钟形曲线），因此最好预先从数据中删除异常值。这是处理分类预测建模问题的一种简单而强大的方法。 4. 分类与回归树 决策树是预测建模机器学习的一种重要算法。 决策树模型的表示是一个二叉树。这是算法和数据结构中的二叉树，没什么特别的。每个节点代表一个单独的输入变量 x 和该变量上的一个分割点（假设变量是数字）。 决策树 决策树的叶节点包含一个用于预测的输出变量 y。通过遍历该树的分割点，直到到达一个叶节点并输出该节点的类别值就可以作出预测。 决策树学习速度和预测速度都很快。它们还可以解决大量问题，并且不需要对数据做特别准备。 5. 朴素贝叶斯 朴素贝叶斯是一个简单但是很强大的预测建模算法。 该模型由两种概率组成，这两种概率都可以直接从训练数据中计算出来：1）每个类别的概率；2）给定每个 x 的值，每个类别的条件概率。一旦计算出来，概率模型可用于使用贝叶斯定理对新数据进行预测。当你的数据是实值时，通常假设一个高斯分布（钟形曲线），这样你可以简单的估计这些概率。 朴素贝叶斯之所以是朴素的，是因为它假设每个输入变量是独立的。这是一个强大的假设，真实的数据并非如此，但是，该技术在大量复杂问题上非常有用。 6. K 近邻算法 KNN 算法非常简单且有效。KNN 的模型表示是整个训练数据集。是不是很简单？ KNN 算法在整个训练集中搜索 K 个最相似实例（近邻）并汇总这 K 个实例的输出变量，以预测新数据点。对于回归问题，这可能是平均输出变量，对于分类问题，这可能是众数（或最常见的）类别值。 诀窍在于如何确定数据实例间的相似性。如果属性的度量单位相同（例如都是用英寸表示），那么最简单的技术是使用欧几里得距离，你可以根据每个输入变量之间的差值直接计算出来其数值。  K 近邻算法 KNN 需要大量内存或空间来存储所有数据，但是只有在需要预测时才执行计算（或学习）。你还可以随时更新和管理训练实例，以保持预测的准确性。 距离或紧密性的概念可能在非常高的维度（很多输入变量）中会瓦解，这对算法在你的问题上的性能产生负面影响。这被称为维数灾难。因此你最好只使用那些与预测输出变量最相关的输入变量。 7. 学习向量量化 K 近邻算法的一个缺点是你需要遍历整个训练数据集。学习向量量化算法（简称 LVQ）是一种人工神经网络算法，它允许你选择训练实例的数量，并精确地学习这些实例应该是什么样的。 学习向量量化 LVQ 的表示是码本向量的集合。这些是在开始时随机选择的，并逐渐调整以在学习算法的多次迭代中最好地总结训练数据集。在学习之后，码本向量可用于预测（类似 K 近邻算法）。最相似的近邻（最佳匹配的码本向量）通过计算每个码本向量和新数据实例之间的距离找到。然后返回最佳匹配单元的类别值或（回归中的实际值）作为预测。如果你重新调整数据，使其具有相同的范围（比如 0 到 1 之间），就可以获得最佳结果。 如果你发现 KNN 在你的数据集上达到很好的结果，请尝试用 LVQ 减少存储整个训练数据集的内存要求。 8. 支持向量机（SVM） 支持向量机可能是最受欢迎和最广泛讨论的机器学习算法之一。 超平面是分割输入变量空间的一条线。在 SVM 中，选择一条可以最好地根据输入变量类别（类别 0 或类别 1）对输入变量空间进行分割的超平面。在二维中，你可以将其视为一条线，我们假设所有的输入点都可以被这条线完全的分开。SVM 学习算法找到了可以让超平面对类别进行最佳分割的系数。 支持向量机 超平面和最近的数据点之间的距离被称为间隔。分开两个类别的最好的或最理想的超平面具备最大间隔。只有这些点与定义超平面和构建分类器有关。这些点被称为支持向量，它们支持或定义了超平面。实际上，优化算法用于寻找最大化间隔的系数的值。 SVM 可能是最强大的立即可用的分类器之一，值得一试。 9. Bagging 和随机森林 随机森林是最流行和最强大的机器学习算法之一。它是 Bootstrap Aggregation（又称 bagging）集成机器学习算法的一种。 bootstrap 是从数据样本中估算数量的一种强大的统计方法。例如平均数。你从数据中抽取大量样本，计算平均值，然后平均所有的平均值以便更好的估计真实的平均值。 bagging 使用相同的方法，但是它估计整个统计模型，最常见的是决策树。在训练数据中抽取多个样本，然后对每个数据样本建模。当你需要对新数据进行预测时，每个模型都进行预测，并将所有的预测值平均以便更好的估计真实的输出值。 随机森林 随机森林是对这种方法的一种调整，在随机森林的方法中决策树被创建以便于通过引入随机性来进行次优分割，而不是选择最佳分割点。 因此，针对每个数据样本创建的模型将会与其他方式得到的有所不同，不过虽然方法独特且不同，它们仍然是准确的。结合它们的预测可以更好的估计真实的输出值。 如果你用方差较高的算法（如决策树）得到了很好的结果，那么通常可以通过 bagging 该算法来获得更好的结果。 10. Boosting 和 AdaBoost Boosting 是一种集成技术，它试图集成一些弱分类器来创建一个强分类器。这通过从训练数据中构建一个模型，然后创建第二个模型来尝试纠正第一个模型的错误来完成。一直添加模型直到能够完美预测训练集，或添加的模型数量已经达到最大数量。 AdaBoost 是第一个为二分类开发的真正成功的 boosting 算法。这是理解 boosting 的最佳起点。现代 boosting 方法建立在 AdaBoost 之上，最显著的是随机梯度提升。 AdaBoost AdaBoost 与短决策树一起使用。在第一个决策树创建之后，利用每个训练实例上树的性能来衡量下一个决策树应该对每个训练实例付出多少注意力。难以预测的训练数据被分配更多权重，而容易预测的数据分配的权重较少。依次创建模型，每个模型在训练实例上更新权重，影响序列中下一个决策树的学习。在所有决策树建立之后，对新数据进行预测，并且通过每个决策树在训练数据上的精确度评估其性能。 因为在纠正算法错误上投入了太多注意力，所以具备已删除异常值的干净数据非常重要。 总结 初学者在面对各种机器学习算法时经常问：「我应该用哪个算法？」这个问题的答案取决于很多因素，包括：（1）数据的大小、质量和特性；（2）可用的计算时间；（3）任务的紧迫性；（4）你想用这些数据做什么。 即使是经验丰富的数据科学家在尝试不同的算法之前，也无法分辨哪种算法会表现最好。虽然还有很多其他的机器学习算法，但本篇文章中讨论的是最受欢迎的算法。如果你是机器学习的新手，这将是一个很好的学习起点。 原文链接：https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11 
301,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737203&idx=1&sn=43c2b6f0e62f8c4aa3f913aa8b9c9620&chksm=871ace4db06d475be8366969d74c4b2250602f5e262a3f97a5faf2183e53474d3f9fd6763308&scene=27,Capsule官方代码开源之后，机器之心做了份核心代码解读,"前几天，Sara Sabour 开源了一份 Capsule 代码，该代码是论文 Dynamic Routing between Capsules 中所采用的实现。其实早在去年刚公布此论文，机器之心就曾详解解读过核心思想与基本代码，我们采用的代码也是各研究者尝试复现论文结果的模型。而最近 Sara 开放的代码是标准的官方实现，因此我们希望能解读部分核心代码，并探讨其与 naturomics 等人实现过程的差异。 Sara 实现地址：https://github.com/Sarasra/models/tree/master/research/capsules 我们主要根据 Sara 的代码解释了 CapsNet 的架构与实现方法，包括 Squash 非线性函数、动态路由更新方法、PrimaryCaps 层与 DigitCaps 的实现过程，还有最后的 Margin Loss 度量函数。我们希望入门读者先了解 Capsule 的概念与 CapsNet 的基本架构，以下我们提供了 Capsule 论文解读与基本概念。 先读懂 CapsNet 架构然后用 TensorFlow 实现，这应该是最详细的教程了 终于，Geoffrey Hinton 那篇备受关注的 Capsule 论文公开了 浅析 Geoffrey Hinton 最近提出的 Capsule 计划 本文章所解释的代码均来自于 capsule_model.py 和 layers.py 两个文件，它们也是整个实现的核心部分。下面，我们将从基本的 Capsule 概念与 Squash 非线性函数开始解析 Sara 所完成的实现。 在论文中，Geoffrey Hinton 介绍 Capsule 为：「Capsule 是一组神经元，其输入输出向量表示特定实体类型的实例化参数（即特定物体、概念实体等出现的概率与某些属性）。我们使用输入输出向量的长度表征实体存在的概率，向量的方向表示实例化参数（即实体的某些图形属性）。同一层级的 capsule 通过变换矩阵对更高级别的 capsule 的实例化参数进行预测。当多个预测一致时（本论文使用动态路由使预测一致），更高级别的 capsule 将变得活跃。」 Capsule 中神经元的激活情况表示了图像中存在的特定实体的各种性质。这些性质可以包含很多种不同的参数，例如姿势（位置、大小、方向）、变形、速度、反射率、色彩、纹理等等。而输入输出向量的长度表示了某个实体出现的概率，所以它的值必须在 0 到 1 之间。 为了实现这种压缩，并完成 Capsule 层级的激活功能，Hinton 等人使用了一个被称为「squashing」的非线性函数。该非线性函数确保短向量的长度能够缩短到几乎等于零，而长向量的长度压缩到接近但不超过 1 的情况。以下是该非线性函数的表达式： 其中 v_j 为 Capsule j 的输出向量，s_j 为上一层所有 Capsule 输出到当前层 Capsule j 的向量加权和，简单说 s_j 是 Capsule j 的输入向量。 在 Sara 提供的实现中，她们使用以下方法定义非线性激活函数。其中输入张量对于全连接 Capsule 层来说维度为 [batch, num_channels, num_atoms]，对于卷积 Capsule 层来说，输入的维度为 [batch, num_channels, num_atoms, height, width]。该函数将输出一组激活张量 v_j，其维度等于输入张量的维度。 因为按照 Hinton 的思想，找到最好的处理路径就等价于正确处理了图像，所以在 Capsule 中加入 Routing 机制可以找到一组系数 c_ij，它们能令预测向量 u_j|i hat 最符合输出向量 v_j，即最符合输出的输入向量，这样我们就找到了最好的路径。 图1：该图展示了 Capsule 的层级结构与动态 Routing 的过程 按照论文 Dynamic Routing between Capsules 所述，c_ij 为耦合系数，该系数由动态 Routing 过程迭代地更新与确定。Capsule i 和后一层级所有 Capsule 间的耦合系数和为 1。此外，该耦合系数由「routing softmax」决定，且 softmax 函数中的 logits b_ij 初始化为 0，耦合系数 c_ij 的 softmax 计算方式为： b_ij 依赖于两个 Capsule 的位置与类型，但不依赖于当前的输入图像。我们可以通过测量后面层级中每一个 Capsule j 的当前输出 v_j 和 前面层级 Capsule i 的预测向量间的一致性，然后借助该测量的一致性迭代地更新耦合系数。本论文简单地通过内积度量这种一致性，即  ，这一部分也就涉及到使用 Routing 更新耦合系数。 有意思的是，Sara 的实现会添加一个 leaky_routing 函数，按照该函数的定义，它会添加额外的维度以路由分对数（logits）。如果需要执行路由的张量维度与上层任意 Capsule 单元不匹配，那么该函数将允许激活的 Capsule 单元在额外的维度中进行路由。如下参数 logits 为需要路由的张量，其中它的维度在全连接层的情况下为 [input_capsule_num, output_capsule_num]，在卷积层的情况下回多增加两个维度。output_dim 为分对数的第二个维度，即输出的 Capsule 单元数。 如上所示，tf.zeros_like 将构建一个与 logits 维度相同的张量，其中每个元素都为 0。在求和处理后，leaky_logits 将在第三个维度拼接 leak 和 logits 张量。在对 leaky_logits 第 3 个维度进行 Softmax 后就相当于计算了以下伪代码中的耦合系数 c_ij，我们需要使用它执行进一步的路由。此外，_leaky_routing 应该是没有应用 Squash 非线性激活。因此该函数真实的意义与过程可能需要进一步探讨，这里的理解不是很完全。 以上_leaky_routing 函数会在完整执行路由和非线性压缩的_update_routing 函数中调用，所以在我们查看完整的路由函数前可以先复习以下原论文中所描述的动态路由伪代码。 Routing 过程就是图1右边表述的更新过程，我们会计算 v_j 与 u_j|i hat 的乘积并将它与原来的 b_ij 相加而更新 b_ij，然后利用 softmax(b_ij) 更新 c_ij 而进一步修正了后一层的 Capsule 输入 s_j。当输出新的 v_j 后又可以迭代地更新 c_ij，这样我们不需要反向传播而直接通过计算输入与输出的一致性更新参数。 对于所有在 l 层的 Capsule i 和在 l+1 层的 Capsule j，先初始化 b_ij 等于零。然后迭代 r 次，每次先根据 b_i 计算 c_i，然后在利用 c_ij 与 u_j|i hat 计算 s_j 与 v_j。利用计算出来的 v_j 更新 b_ij 以进入下一个迭代循环更新 c_ij。该 Routing 算法十分容易收敛，基本上通过 3 次迭代就能有不错的效果。 在以下定义的路由更新过程中，_update_routing 函数会对经精炼的输入张量求和并执行 Squash 非线性变换。它的输出激活值可作为 PrimaryCaps 层和 DigitCaps 层的最终输出，后面我们将详细讨论这两个层的实现。按照 _update_routing 函数的说明，它会基于当前层的激活值与前一层投票结果（即线性组合结果）之间的相似性，迭代地更新 logits 路由结果，即对输入张量进行精炼。 如上所示，votes 为前一层经转换的输出张量。num_dims 为输入 votes 的维度数量，对于全连接 Capsule 层来说，它的维度为 4，对于卷积层来说，它的维度为 6。input_dim 为 输入层的 Capsule 单元数，output_dim 为输出层的 Capsule 单元数。num_routing 为路由的迭代次数，而 leaky 则代表着是否使用前面定义的渗漏路由 _leaky_routing。 以上  _update_routing 函数最终会输出一个激活张量，即上面原论文伪代码中的 v_j。在初步讨论了路由算法后，我们可以查看它到底用在了哪些地方，即哪些运算需要执行路由算法。从 Sara 的代码上看，CapsNet 应该使用了两次 Routing，即在第二层的卷积层和第三层的 Capsule 全连接层后各调用了一次。这一点与论文的描述和其他研究者所实现的复现有所不同。 下面，我们将依据原论文与 Sara 开源的实现讨论 CapsNet 主体架构和 Margin loss 度量。这一部分是该论文与实现的核心，因此我们将重点关注这一部分而忽略后面构建的重构网络与重构损失。 以下是 CapsNet 的整体架构： 第一个卷积层使用了 256 个 9×9 卷积核，步幅为 1，且使用了 ReLU 激活函数。该卷积操作应该没有使用 Padding，输出的张量才能是 20×20×256。此外，CapsNet 的卷积核感受野使用的是 9×9，相比于其它 3×3 或 5×5 的要大一些，这个能是因为较大的感受野在 CNN 层次较少的情况下能感受的信息越多。这两层间的权值数量应该为 9×9×256+256=20992。 随后，第二个卷积层 PrimaryCaps 开始作为 Capsule 层的输入而构建相应的张量结构，我们可以从上图看出第二层卷积操作后生成的张量维度为 6×6×8×32。如果我们先考虑 32 个（32 channel）9×9 的卷积核在步幅为 2 的情况下做卷积，那么实际上得到的是传统的 6×6×32 的张量，即等价于 6×6×1×32。因为传统卷积操作每次计算的输出都是一个标量，而 PrimaryCaps 的输出需要是一个长度为 8 的向量，因此传统卷积下的三维输出张量 6×6×1×32 就需要变化为四维输出张量 6×6×8×32。 第三层 DigitCaps 在第二层输出的向量基础上进行传播与 Routing 更新。第二层共输出 6×6×32=1152 个向量，每一个向量的维度为 8，即第 i 层共有 1152 个 Capsule 单元。而第三层 j 有 10 个标准的 Capsule 单元，每个 Capsule 的输出向量有 16 个元素。前一层的 Capsule 单元数是 1152 个，那么 w_ij 将有 1152×10 个，且每一个 w_ij 的维度为 8×16。当 u_i 与对应的 w_ij 相乘得到预测向量后，我们会有 1152×10 个耦合系数 c_ij，对应加权求和后会得到 10 个 16×1 的输入向量。将该输入向量输入到「squashing」非线性函数中求得最终的输出向量 v_j，其中 v_j 的长度就表示识别为某个类别的概率。 如下所示定义了构建 Capsule 层级的主体函数： 该函数在输入 5 维张量 [batch, 1, 256, height, width] 和目标类别的数量 num_classes 后会输出一个 3 维张量，它将表示 10 个类别的 Capsule 嵌入向量。也就是说，该函数将构建完整的 CapsNet 架构，并输出 DigitCaps 层最后得到的 10 个 16 维向量。 该主体函数主要调用了一个 slim 卷积 Capsule 层和一个 Capsule 层。slim 卷积 Capsule 层主要将输入张量转换为 Capsule 格式，即上图的 PrimaryCaps 层。而为了连接卷积 Capsule 层和顶部的全连接 Capsule 层，卷积 Capsule 层的网格位置将与不同类型的 Capsule 维度相合并，并且 Capsule 将为嵌入向量学习不同的变换。下面我们会详细讨论构建以上网络主体的两个函数。 在主体模型的代码中（capsule_model.py 第 54 行），第二个卷积层需要通过调用 Sara 等人定义的 conv_slim_capsule 函数实现，以下的代码构建了原论文中的 PrimaryCaps 层，其中 input_tensor 为原图片经过一次卷积后的特征图，并增加一个维度以作为一个 Capsule 单元包含神经元的个数（构成向量）。 在上面这些参数中，input_tensor 为五维张量即标准卷积的四维张量再加上一维 Capsule 单元数（capsule_model.py 第 194 行）。input_dim 为上一个 Capsule 层的单元数或维度，output_dim 为多个并行卷积操作后所得到的 Capsule 单元数或维度。input_atoms 为前一层 Capsule 单元的元素数，即一个 Capsule 单元包含的神经元数量，这里 256 代表第一个卷积层所产生的 256 张特征图，而 output_atoms 表示当前层的 Capsule 单元元素数，这里的 8 可以是代表 8 张 6×6×1×32 的特征图。其实小编认为我们可以如 Sara 等人的实现将 PrimaryCaps 层看成 32 个 Capsule 单元，每个单元包含 8 个标量神经元，或者将其看成 8 个 Capsule 单元，每个单元包含 32 个标量神经元，这两种表示方法应该是等价的。剩下其它的参数就和标准卷积层所定义的参数意义一样，所以读者可以阅读源代码详细地了解。 在 layers.py 文件中（268 行），Sara 等人定义了 conv_slim_capsule 函数以完成 PrimaryCaps 层的构建。该函数使用 slim 对给定五维的输入张量执行二维卷积，输入张量的维度一般为 [batch, input_dim, input_atoms, input_height, input_width]。然后该函数将使用动态路由算法精炼前面卷积运算的结果，并对每一个 Capsule 单元应用非线性 Squash 函数。 conv_slim_capsule 函数所输出的激活值张量维度为 [batch, output_dim, output_atoms, out_height, out_width]。如果 Padding 选择的是『SAME』，那么输出特征图的高和宽就与输入张量的宽和高。我们注意到执行卷积操作具体的函数为前面定义的_depthwise_conv3d，该函数将返回经过 2 维卷积的 6 维张量。 _depthwise_conv3d 函数在给定一个 5 维输入张量的情况下会执行 2 维卷积运算，输入张量的维度与 conv_slim_capsule 函数的输入相同。_depthwise_conv3d 函数会将输入 5 维张量中 Batch 和 input_dim 的乘积作为 1 维而压缩为 4 维张量，即压缩输入张量的第一维与第二维为一个维度。之所以需要压缩为 4 维，是因为我们需要将其作为 tf.nn.conv2d 的输入。在执行卷积后，我们需要重新将这 4 维张量分解为 6 维张量。即将第一维分解为 Batch 和 input_dim，将第二维分解为 output_dim 和 output_atom。 如上所示为 _depthwise_conv3d 函数，其中参数 input_dim、output_dim 和 input_atoms 等参数的意义与 conv_slim_capsule 函数一致。该函数会返回 6 维张量 [batch, input_dim, output_dim, output_atoms, out_height, out_width]、卷积后的维度和输入张量的维度，并在 conv_slim_capsule 函数中做进一步处理，下面我们将回头继续讨论构建 PrimaryCaps 层的函数。 如下定义了 conv_slim_capsule 函数，其层级的每一个 Capsule 单元都为卷积单元，它们在位置网格和不同的下层 Capsule 单元间共享卷积核权重。因此，该函数可训练的变量为卷积核权重 [kernel_size, kernel_size, input_atoms, output_dim * output_atoms] 和偏置项 [output_dim, output_atoms]。二维卷积的输出为一个单 Capsule 单元，其通道为 Capsule 单元（atoms）的数量。因此，conv_slim_capsule 函数可以构建在二维卷积层的顶部，其中该二维卷积的 num_routing=1、input_dim=1 和 input_atoms=conv_channels。 通过观察定义卷积权重的 kernel 变量，我们可以了解该运算本质上就是执行上一层特征图数为 input_atoms、本层卷积核数为 output_dim * output_atoms 的卷积操作。在完成卷积运算后，Sara 的实现接着调用了一次前面定义的路由算法，这似乎与 naturomics 等人复现的代码有一些不同，他们在实现卷积后会将卷积结果直接投入 Squash 非线性函数。当然，原论文似乎也没有体现这一点，我们都以为只有在 DigitCaps 层才会进行动态路由过程。 在后面的 capsule 函数中（layers.py 第 138 行），Sara 确实又调用了一次动态路由算法，我们会在后面讨论该函数。因为路由算法计算了 Squash 值，因此返回的激活值可作为 PrimaryCaps 层的输出。 在返回 PrimaryCaps 层的 5 维张量后，主体函数会将其转换维 3 维张量并馈送到 capsule 函数，从而构建一个全连接 Capsule 层。在给定输入张量 [batch, input_dim, input_atoms] 后，Capsule 层将执行以下操作： 对于每一个输入 Capsule 单元，将它与权重变量相乘以得到线性组合的结果（函数中表示为 votes 变量），这一步将得到原论文所述的 u_j|i hat，即 u_j|i hat = W_ij * u_i。线性组合后的结果维度为 [batch, input_dim, output_dim, output_atoms]。 通过迭代地执行路由过程更新与精炼前面线性组合的结果，即原论文中的 s_j = ∑ c_ij * u_j|i hat，其中 c_ij = softmax(b_ij)。 最后使用 Squash 函数将每个 Capsule 单元的输出压缩到 L2 范数小于 1 的情况。 此外，当前层的每一个 Capsule 单元对前一层的 Capsule 单元都保留一个权重张量。因此在训练中，capsule 函数的权重 [input_dim * num_in_atoms, output_dim * num_out_atoms] 和偏置项 [output_dim * num_out_atoms] 都是需要更新的参数。 如下展示了 capsule 函数的定义，其中 input_dim 同样为前一层的 Capsule 单元数，input_atoms 同样为前一层每个 Capsule 单元的元素数，其它参数的意义与上面几个函数都差不多。该函数将输出张量 [batch, output_dim, output_atoms]。 如上所示，tf.expand_dims 会将输入张量在最后扩充一维，而 tf.tile 会将扩充后的 4 维张量在最后一维复制 output_dim * output_atoms 次。在执行逐元素的乘积后，沿着第三维 input_atoms 对乘积结果求和。求和后的张量可作为最后的线性组合结果而投入路由算法中进行迭代更新，返回的张量即 DigitCaps 层最终输出的 10 个 16 维的向量，每个向量编码并表征着 10 类手写数字。 在调用完 capsule 函数后，整个_build_capsule 函数所构建的 CapsNet 架构就完成了。原论文使用了 Margin loss 来衡量这 10 个输出向量预测类别的准确度，而后面也可以使用全连接网络将这 10 个向量重构为不同手写数字的图像，并使用欧几里得距离度量重构损失。 在论文解读中，我们已经了解 DigitCaps 层输出向量的长度即某个类别的概率，那么我们该如何构建损失函数，并根据该损失函数迭代地更新整个网络？前面我们耦合系数 c_ij 是通过一致性 Routing 进行更新的，他并不需要根据损失函数更新，但整个网络其它的卷积参数和 Capsule 内的 W_ij 都需要根据损失函数进行更新。一般我们就可以对损失函数直接使用标准的反向传播更新这些参数，而在原论文中，作者采用了 SVM 中常用的 Margin loss，该损失函数的表达式为： 其中 c 是分类类别，T_c 为分类的指示函数（c 存在为 1，c 不存在为 0），m+ 为上边界，m- 为下边界。此外，v_c 的模即向量的 L2 距离。 因为实例化向量的长度来表示 Capsule 要表征的实体是否存在，所以当且仅当图片里出现属于类别 k 的手写数字时，我们希望类别 k 的最顶层 Capsule 的输出向量长度很大（在本论文 CapsNet 中为 DigitCaps 层的输出）。为了允许一张图里有多个数字，我们对每一个表征数字 k 的 Capsule 分别给出单独的 Margin loss。 以下_margin_loss 定义了 CapsNet 的损失函数，它会惩罚每个输入分对数偏离边缘的程度。如函数说明所示，该函数将衡量每一个错误分对数对于边缘的距离。对于负的分对数来说，边缘为 0.1，对于正的分对数，边缘为 0.9。若我们同时对这两个边缘先减去 0.5，那么当前的边缘将会都变为 0.4。 如上所示，margin 为 raw_logits 减去 0.5 的边缘，而 downweight 负成本的因素。 以上是 CapsNet 的主体代码，也是整个 Capsule 的核心。Sara 开源的实现还有很多重要的代码与函数，我们也将继续探讨与思考其中具体的过程，尤其是理解_leaky_routing 的作用。我们也希望有读者和我们一起解析与分析 Sara 的实现，并探讨其最终实现的基线结果。 以下是 Sara 实现的说明，其使用的是 Python 2.7，不过 naturomics 等研究者已经在 GitHub 上修改为了 Python 3。读者可进一步测试与实现它们： 要求： TensorFlow NumPy GPU 运行以下命令以测试配置是否正确： 若我们下载了 Sara 等人提供的 MNIST 数据集和预训练模型，并把它们放入$DATA_DIR/和$CKPT_DIR/ 目录下。那么我们可以运行以下命令而快速获得 CapsNet 在 MNIST 上的测试结果。 数据集：https://storage.googleapis.com/capsule_toronto/mnist_data.tar.gz 模型：https://storage.googleapis.com/capsule_toronto/mnist_checkpoints.tar.gz 下载并抽取二进制版的 cifar10 数据集到 $DATA_DIR/，cifar 10 预训练模型到 $CKPT_DIR/。 数据集：https://www.cs.toronto.edu/~kriz/cifar.html 模型：https://storage.googleapis.com/capsule_toronto/cifar_checkpoints.tar.gz Sample CIFAR10 训练命令： Sample MNIST 全部训练命令： 训练-验证传播模式： --validate=true 在更多 GPU 上训练：—num_gpus=NUM_GPUS Sample MNIST 基线训练命令： 该实现的详细使用方法请查阅 README 文件，我们以上只简要介绍了一部分。我们也尝试着实现 Sara 开源的模型，最开始是 xrange 等 Python 2.7 的函数会报错，在改为 Python 3 后运行仍然会有问题。所以我们更希望有读者能完成该模型的测试，并向大家展示 Dynamic Routing Between Capsules 论文所实现的结果。  "
302,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737028&idx=2&sn=ae35f305e2a533f2f403a99f7fcd89d3&chksm=871acdfab06d44ec1fd8362d4d33dc2a8671005a909b8434fd4a47ebf0facc58828739343d87&scene=27,深度 | 可视化分析ICLR 2018：你想要的接收论文情况都在这,前日，ICLR 公布了接收论文 ，ICLR 2018 共计 2.3% 的 Oral 论文、31.4% 的 Poster 论文、9% 被接收为 Workshop track，51% 的论文被拒收、6.2% 的撤回率。在 ICLR 2018 公布了接收论文情况后，Arthur Pajot 利用这些数据进行了可视化分析以更深入地了解本次大会的接收情况。本文从论文评分、研究机构和国家等方向出发可视化探讨 ICLR 2018 的接受论文分布情况。 可视化地址：http://webia.lip6.fr/~pajot/dataviz.html Arthur Pajot 在他的 GitHub 上也开放了本次接收论文的各项数据，读者可使用这些数据在自己的平台上运行该可视化分析。这份可视化分析是基于 NumPy、Matplotlib 和 Pandas 等基本科学计算工具，所以读者需要先导入对应的库和工具： 接收论文数据地址：https://github.com/pajotarthur/ICLR_data 我们需要将这些数据下载到本地，然后就能愉快地导入它们并执行如下或其他一些分析了。当然读者也可以直接查看以下的可视化分析而对整个大会的接收论文情况有一个深入的了解。 各种接收类型（包括拒绝，不包含放弃）的论文平均分数 上图展示了 ICLR2018 最终的各接收类型（oral、poster、workshop track、reject）论文的平均双盲评分。其中被接收的论文中，oral 论文的平均分为 7.304348，poster 论文的平均分为 6.497877，workshop track 论文的平均分为 5.362963；而被拒绝的 reject 论文的平均分为 4.644313。 论文评分分布直方图 四种接收类型的论文评分基本遵从高斯分布。比较有意思的是，在 Oral 论文中，还有几篇的评分相对于其它 Oral 论文较低（如绿色所示）。 论文评分分布直方图（非重叠版本） 接收论文数最多的作者排名 排名前三的作者有 Sergey Levine、Yoshua Bengio 和 Richard Socher，但是他们并没有 oral 的接收论文，而紧随其后的两位作者 Pieter Abbeel 和 Dawn Song 分别有两篇和一篇 oral 论文。此外，还有一些我们比较熟悉的作者，比如 Ian Goodfellow、Xi Chen、Demis Hassabis 等。 提交论文数最多的作者排名 按平均评分的作者排名，我们（随机地）挑选了提交论文数大于三篇的作者 各机构的接收论文数排名 如果一篇论文的作者都来自机构，则该机构发表了一篇论文。如果一篇论文有三名作者，只有一名来自机构，则相当于该机构发表了 1/3 篇论文。机构名称是从作者的电子邮箱地址上截取的，其中 gmail.com 的地址已被删除。 代码地址：https://github.com/pajotarthur/ICLR_data/blob/master/add_paper_origin.ipynb。 各机构的提交论文数排名 谷歌仍然是提交论文与接收论文最多的机构，而且超过其它机构非常多。其它排名前列的机构都是我们非常熟悉的科技巨头与顶尖科研院校。 各机构的论文接收率排名。我们（随机地）挑选了提交论文数大于三篇的机构。 按机构区分的接收论文的总数和类型分布 按机构区分的接收论文数排名 各国家的接收论文数排名 各国家的提交论文数的排名 从上可知，国内（CN）共提交了 50 篇论文，而有 3 篇入选 Oral、7 篇入选 Poster 论文。 各国家的论文接收率排名 按国家区分的接收论文的总数和类型分布 图中 com 代表来自机构，edu 代表来自大学，和其它国家需要区分开。从 com 和 edu 这两项可以看出（至少在 ICLR2018），机构的论文产出已经赶上甚至略微赶超了大学，彰显了机构的学术研究水平，这或许跟人工智能的研究阶段有关（工程超越理论）。而在国家层面，领先的国家有中国（cn）、英国（uk）和加拿大（ca），中国的论文接收情况基本是，拒绝率高，oral 率也高。 按国家区分的接收论文数的条形图 
303,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737028&idx=4&sn=92227fa1858bc0d6da3a90802337807e&chksm=871acdfab06d44ec347c270c8e3e7d8d182debf89d908979267b93aa0e9fd575e210f51b1021&scene=27,学界 | IBM Watson提出人机推理网络HuMaINs，结合人机两者优势,"人机交互可以用机器的优势弥补人类决策的非理性缺陷。近日，IBM Watson 研究中心联合多家研究机构提出了人机推理网络 HuMaINs 架构，论文集中讨论了三个主要问题，即架构设计、包含安全性/隐私挑战的推理算法，以及应用领域/用例。 在传统的经济学、认知心理学和人工智能领域的文献中，问题求解或推理过程通常按搜索问题空间的方式描述，该空间由问题的多个不同的状态构成，从一个初始状态开始，最终达到所需的目标状态 [1]。以初始状态为起点的每一个路径代表一个可能的策略。这些路径可能走向目标状态，或其它的非目标状态。以初始状态为起点的走向目标状态的路径被称为解决路径（solution path）。在初始和目标状态之间可能存在多个这样的路径，这些都是问题的解决方案。换种说法，即有多种方法可以求解一个给定的问题。问题求解过程是指在多个解决路径中识别最优（在给定的约束条件下）的解决路径。 该搜索过程的第一步是确定可用策略的集合，即确定策略空间。第二步是评估这些策略以确定最优的策略作为解决方案。在传统的经济学理论中，会假定理性决策者具有可能策略的集合的知识，可以评估每一个策略的后果，并拥有一个效用函数，并最大化该函数以确定最优的策略 [2]。然而，人们普遍认为人类不是理性的，而是有限理性的智能体。在有限理性框架中 [2,3]，决策者的认知、时间、信息和资源都是有限的。策略集合并不是完全先验地知道的，决策者也并不能完全知道选择特定策略的后果。因此，决策者可能无法总是为问题求解确定最优的策略。 另一方面，机器是「理性」的，因为它们有更强/更大的内存来存储替代方案，并有计算能力可以更精确地评估特定替代方案的结果。因此，机器可以帮助人类快速而准确地解决问题，因此出现了人机协作以解决问题的框架。在本论文中，通过定义 HuMaINs，作者探讨了用于推理的这一协作框架，以及与开发这一框架相关的研究挑战。该领域的三个基本研究思路已得到界定。 论文标题：Human-Machine Inference Networks For Smart Decision Making: Opportunities and Challenges 论文链接：https://arxiv.org/abs/1801.09626 人机推理网络（HuMaINs）的新兴范式以一种智能的方式整合了人类与机器的现代认知优势，从而解决不同的推理任务，表现要优于人类或机器单独执行的效果。尽管只针对于人类或传感器网络的推理性能优化技术已经相当成熟，HuMaINs 依然需要全新的信号处理和机器学习方案。在本论文中，我们概述了 HuMaINs 架构，并聚焦于三个主要问题，即架构设计、包含安全性/隐私挑战的推理算法，以及应用领域/用例。 图 1：概念性 HuMaINs 架构 图 1 展示了典型的人机推理网络（HuMaINs）。典型的 HuMaINs 由一个社交网络和一个机器网络构成。在社交网络中，人类互相交换主观观点；在机器网络中，机器互相交换客观度量。此外，通过社交网络和机器网络的交互，由人类行为特征决定的算法传递给机器，然后这些算法反过来影响人类的行为。因此，通过结合人类和机器的优势，人类和机器之间的智能协作可以提供更优的结果。 人机交互的架构可以分成三类：（1）人类直接控制自动化系统；（2）自动化系统监控人类，在必要时采取行动；（3）两者的结合。为了达到 HuMaINs 的目标，构建一个可以融合人类和机器的决策过程的架构是很关键的。文献 [6] 中宣称，为现代自动化系统创建顶尖的操作环境，需要在三个主要的领域持续开发新的技术：决策支持工具、人体工程和可视化工具、易用的复杂系统。 图 2：设计和分析 HuMaINs 的一般方法 HuMaINs 的关键研究领域是开发新的处理人类行为数据的算法。这个领域演变成了一种新兴研究领域的范式，称为行为信号处理（behavioral signal processing，BSP）[8]。BSP 处理的是人类行为的信号。它被定义为对人类动作和行为数据的处理，进行有意义的分析，通过人类专家和自动化处理的协作，以确保及时的决策和干预。其目标是支持人类，而不是取代人类 [8]。核心要素包括人类行为的定量分析，以及交互动力学的数学建模。Narayan 和 Georgiou 将 BSP 的要素描述为，利用语音和口语交流对人类行为进行度量和建模 [8]。 在 HuMaINs 中开发 BSP 算法有两个具体的研究方向： 1. 利用统计建模技术开发人类决策的数学模型，和认知心理学密切协作。 2. 设计鲁棒的融合算法以处理智能体提供的不可靠数据，这些数据由上述开发的模型建模。 "
304,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650737028&idx=1&sn=5229847549c4beb32a3502fcced4322c&chksm=871acdfab06d44ec66aea7ead7fe44fad66cfc42b5460c7203554cd894aa8bdfcadf9e4459df&scene=27,终于！吴恩达deeplearning.ai第5课开课了：敲黑板序列模型,在机器之心周二发布的文章《 》中，读者纷纷留言 deeplearning.ai 的第五课什么时候开始。终于，大家翘首期盼的课程今天开课了，同时也意味着该系列课程要结课了。自去年 8 月发布以来，吴恩达创业的第一个项目「深度学习教育课程」终于完整地呈现在人们的眼前。 课程链接：https://www.coursera.org/learn/nlp-sequence-models 和此前四门课程一样，新的课程仍将由吴恩达本人主讲。此外，斯坦福大学讲师 Kian Katanforoosh 与 Younes Bensouda Mourri 也将参与授课。 吴恩达 deeplearning.ai 的五门课程 第一门：神经网络和深度学习 课程学习时间：四周，每周 3-6 小时 第二门：提升深度神经网络—调整超参数、正则化与优化 课程学习时间：三周，每周 3-6 小时 第三门：构建机器学习项目 课程学习时间：两周，每周 3-4 小时 第四门：卷积神经网络 课程学习时间：四周，每周 4-5 小时 第五门：序列模型 课程简介 本课程将讲授如何构建自然语言、音频和其他序列数据的模型。在深度学习的帮助下，序列算法比两年前效果更好，用于大量有趣的应用，如语音识别、音乐合成、聊天机器人、机器翻译、自然语言理解等。学完本课，你将： 了解如何构建和训练循环神经网络（RNN）及其常用变体，如 GRU 和 LSTM。 使用序列模型处理自然语言问题，如文本合成。 将序列模型应用到音频应用中，如语音识别和音乐合成。 这是 Deep Learning Specialization 课程的第五课，也是最后一课。 适用人群 学完第一、二、四课的学习者。同样推荐大家学习第三课。 已经对神经网络（包括 CNN）具备深厚理解，并想学习如何开发循环神经网络的人。 该课程共分为三部分，学员可分三周完成。 第 1 周：循环神经网络（RNN） 学习循环神经网络。RNN 模型被证明在时序数据上性能非常好。它有多种变体，如 LSTM、GRU 和双向 RNN，本节将对此进行介绍。 第 2 周：自然语言处理 & 词嵌入 自然语言处理和深度学习的结合非常重要。使用词向量表示和嵌入层可以训练出在多个行业中性能优秀的循环神经网络。应用实例如情感分析、命名实体识别和机器翻译。 第 3 周：序列模型 & 注意力机制 序列模型可通过注意力机制获得增强。这一算法有助于模型在给定输入序列的情况下了解其注意力的聚焦点。本周，你还将学到有关语音识别的知识，以及如何处理音频数据。 参考资料 在学习该课程之前，机器之心为大家准备了一些学习资料做准备，内容包括循环神经网络的综述论文、LSTM、GRU等循环神经网络变体的介绍、LSTM的原理解读、教程等。 从90年代的SRNN开始，纵览循环神经网络27年的研究进展 LSTM、GRU与神经图灵机：详解深度学习最热门的循环神经网络 LSTM入门必读：从基础知识到工作方式详解 干货 | 图解LSTM神经网络架构及其11种变体（附论文） 深度 | LSTM 和递归网络基础教程 深度 | 从任务到可视化，如何理解LSTM网络中的神经元 在调用API之前，你需要理解的LSTM工作原理 教程 | 如何解决LSTM循环神经网络中的超长序列问题 教程 | 将注意力机制引入RNN，解决5大应用领域的序列预测问题 学界 | RNN 怎么用？给初学者的小教程 深度 | Facebook科学家Tomas Mikolov详解RNN与机器智能的实现（附视频+PPT） 前四课学习笔记 Deeplearning.ai 课程开课以来，一直受到大家的关注，也有众多读者积极的参与到学习中。机器之心在这段时间内也介绍了多篇该系列课程的学习笔记，还未学习前四课的同学可以参考一下文章： 吴恩达Deeplearning.ai课程学习全体验：深度学习必备课程（已获证书） 入门 | 吴恩达Deeplearning.ai 全部课程学习心得分享 最后，如果大家完成该课程后有课程心得分享，机器之心非常乐意推荐给所有读者，欢迎大家积极投稿。 
305,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736925&idx=5&sn=60650609e08cc670e215d1ddae6e45be&chksm=871acd63b06d447585477d5257c90820f94fe93e0808e743cb0f15228b7083a8216c068b0f31&scene=27,AAAI 2018 | 时空图卷积网络：港中文提出基于动态骨骼的行为识别新方案,"近日，香港中文大学提出一种时空图卷积网络，并利用它们进行人类行为识别。这种算法基于人类关节位置的时间序列表示而对动态骨骼建模，并将图卷积扩展为时空图卷积网络而捕捉这种时空的变化关系。 近年来，人类行为识别已经成为一个活跃的研究领域，它在视频理解中起着重要的作用。一般而言，人类行为识别有着多种模态（Simonyan and Zisserman 2014; Tran et al. 2015; Wang, Qiao, and Tang 2015; Wang et al. 2016; Zhao et al. 2017），例如外观、深度、光流和身体骨骼（Du, Wang, and Wang 2015; Liu et al. 2016）等。在这些模态当中，动态人类骨骼通常能与其他模态相辅相成，传达重要信息。然而，比起外观和光流建模，动态骨骼建模受到的关注较少。在这项工作中，我们系统地研究这种模态，旨在开发一种原则性且有效的方法模拟动态骨骼，并利用它们进行人类行为识别。 在 2D 或 3D 坐标形式下，动态骨骼模态可以自然地由人类关节位置的时间序列表示。然后，通过分析其动作模式可以做到人类行为识别。早期基于骨骼进行动作识别的方法只是在各个时间步骤使用关节坐标形成特征向量，并对其进行时序分析 (Wang et al. 2012; Fernando et al. 2015)。但这些方法能力有限，因为它们没有明确利用人类关节的空间关系，而这种空间关系对理解人类行为而言至关重要。最近，研究者开发了试图利用关节间自然连接的新方法 (Shahroudy et al. 2016; Du, Wang, and Wang 2015)。这些方法的改进令人鼓舞，表明了骨骼连通性的重要性。然而，现有的大多数方法依赖手动划分的部分或手动设定的规则来分析空间模式。因此，为特定应用设计的模型难以在其他任务中推广。 为了跨越上述限制，我们需要一种新方法自动捕捉关节的空间构型、时间动态中所嵌入的模式。这就是深度神经网络的力量。然而，如前所述，骨骼并未以 2D 或 3D 网格的方式展现，而是以图像的形式展现。这就使得使用诸如卷积网络等已证实的模型变得困难。最近，将卷积神经网络（CNN）泛化到任意结构图形的图卷积网络（GCN）得到了越来越多的关注，而且被成功应用于图像分类 (Bruna et al. 2014)、文献分类 (Defferrard, Bresson, and Vandergheynst 2016)、半监督学习 (Kipf and Welling 2017) 等领域。但是，顺着这条思路，大部分前人的工作都把输入假定为一个固定的图形。GCN 在大规模数据集上的动态图模型应用，例如人类骨骼序列，还有待探索。 在本文中，我们通过将图卷积网络扩展到时空图模型，设计用于行为识别的骨骼序列通用表示，称为时空图卷积网络（ST-GCN）。如图 1 所示，该模型是在骨骼图序列上制定的，其中每个节点对应于人体的一个关节。图中存在两种类型的边，即符合关节的自然连接的空间边（spatial edge）和在连续的时间步骤中连接相同关节的时间边（temporal edge）。在此基础上构建多层的时空图卷积，它允许信息沿着空间和时间两个维度进行整合。 ST-GCN 的层次性消除了手动划分部分或遍历规则的需要。这不仅能获得更强的表达能力和更高的性能（如我们的实验所示），而且还使其易于在不同的环境中推广。在通用 GCN 公式化的基础上，我们还基于图像模型的灵感研究设计了图卷积核的新策略。 这项工作的主要贡献在于三个方面：1）我们提出 ST-GCN，一个基于图的动态骨骼建模方法，这是首个用以完成本任务的基于图形的神经网络的应用。2）我们提出了在 ST-GCN 中设计卷积核的几个原则，旨在满足骨骼建模的具体要求。3）在基于骨骼动作识别的两个大规模数据集上，我们的模型与先前使用的手动分配部分或遍历规则的方法相比，需要相当少的手动设计，实现了更优越的性能。ST-GCN 的代码和模型已公开发布 1。 流程概览 基于骨骼的数据可以从运动捕捉设备或视频的姿态估计算法中获得。通常来说，数据是一系列的帧，每一帧都有一组联合坐标。给定 2D 或 3D 坐标系下的身体关节序列，我们就能构造一个时空图。其中，人体关节对应图的节点，人体身体结构的连通性和时间上的连通性对应图的两类边。因此，ST-GCN 的输入是图节点的联合坐标向量。这可以被认为是一个基于图像的 CNN 模拟，其中输入由 2D 图像网格上的像素强度矢量形成。对输入数据应用多层的时空图卷积操作，可以生成更高级别的特征图。然后，它将被标准的 SoftMax 分类器分类到相应的动作类别。整个模型用反向传播进行端对端方式的训练。现在，我们将介绍 ST-GCN 模型的各个部分。 论文：Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition 论文链接：https://arxiv.org/abs/1801.07455 人体骨骼动力学为人类行为识别提供了重要信息。传统的骨骼建模方法通常依赖手动划分部分或遍历规则，导致模型表达能力有限、泛化困难。在这项工作当中，我们提出了一个名为时空图卷积网络（ST-GCN）的新型动态骨骼模型，它通过自主学习数据中的时间、空间模式，超越了以往方法的局限性，具有更强的表现力和泛化能力。在 Kinetics 和 NTU-RGBD 两大数据集中，本模型与主流方法相比有了很大的提高。 "
306,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736925&idx=2&sn=3ec7f2370ce60ff1bc18f5122e0aa5f2&chksm=871acd63b06d4475c22d227bb03b696dbaea94b5c0ec5486c03db2a056b051ec2e436d3f5976&scene=27,教程 | 从零开始：TensorFlow机器学习模型快速部署指南,本文将介绍一种将训练后的机器学习模型快速部署到生产种的方式。如果你已使用 TensorFlow 或 Caffe 等深度学习框架训练好了 ML 模型，该模型可以作为 demo。如果你更喜欢轻量级的解决方案，请阅读本文。 GitHub 地址：https://github.com/hiveml/simple-ml-serving 其中包含的条目有： 检查 TensorFlow 安装：https://github.com/hiveml/simple-ml-serving/blob/master/test/test_tensorflow.sh 利用 stdin 运行在线分类：https://github.com/hiveml/simple-ml-serving/blob/master/test/test_label_image.sh 在本地主机上运行在线分类：https://github.com/hiveml/simple-ml-serving/blob/master/test/test_tf_classify_server.sh 将分类器放在硬编码代理器后面：https://github.com/hiveml/simple-ml-serving/blob/master/test/test_basic_proxy.sh 将分类器放在可实现服务发现的代理器后面：https://github.com/hiveml/simple-ml-serving/blob/master/test/test_seaport_proxy.sh 利用伪 DN 启用分类器：https://github.com/hiveml/simple-ml-serving/blob/master/test/test_p2p_proxy.sh 生产环境中的机器学习 第一次进入 Hive 的机器学习空间，我们就已经拥有数百万个真值标注图像，这可以让我们在一周时间内从头训练（即随机权重）适用于特定使用案例的顶尖深度卷积图像分类模型。更典型的 ML 用例通常基于数百个图像，这种情况我推荐大家对现有模型进行微调。例如，https://www.tensorflow.org/tutorials/image_retraining 页面上有如何微调 ImageNet 模型对花样本数据集（3647 张图像，5 个类别）进行分类的教程。 安装 Bazel 和 TensorFlow 后，你需要运行以下代码，构建大约需要 30 分钟，训练需要 5 分钟： 或者，如果你有 Docker，可以使用预制 Docker 图像， 进入容器中的交互式 shell，运行以上命令。你也可以阅读下文，在容器中按照下文说明进行操作。 现在，TensorFlow 将模型信息保存至/tmp/output_graph.pb 和 /tmp/output_labels.txt，二者作为命令行参数被输入至 label_image.py (https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/image_retraining/label_image.py) 脚本。谷歌的图像识别教程也与另一个脚本（https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py#L130）有关，但是在这个例子中，我们将继续使用 label_image.py。 将单点推断转换成在线推断（TensorFlow） 如果我们只想接受标准输入的文件名，一行一个，则我们可以轻松实现「在线」推断： 如果以性能为出发点来看，这太糟糕了：我们需要为每个输入样本重新加载神经网络、权重、整个 TensorFlow 架构和 Python！ 我们当然可以做得更好。让我们从编辑 label_image.py script 开始。它的地址为 bazel-bin/tensorflow/examples/image_retraining/label_image.runfiles/org_tensorflow/tensorflow/examples/image_retraining/label_image.py。 我们将以下行 改为： 这样速度快多了，但是仍然不是最好！ 原因在于第 100 行的 with tf.Session() as sess 构造。本质上，TensorFlow 在每次启用 run_graph 时，将所有计算加载至内存中。如果你试着在 GPU 上执行推断时就会明显发现这一现象，你会看到 GPU 内存随着 TensorFlow 在 GPU 上加载和卸载模型参数而升降。据我所知，该构造在其他 ML 框架如 Caffe 或 PyTorch 中不存在。 解决方案是去掉 with 语句，向 run_graph 添加 sess 变量： 代码地址：https://github.com/hiveml/simple-ml-serving/blob/master/label_image.py 运行后，你会发现每张图像花费时间约为 0.1 秒，这样的速度快到可以在线使用了。 将单点推断转换成在线推断（其他 ML 框架） Caffe 使用其 net.forward 代码，详见：http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb。 Mxnet 也很独特：它实际上已开源可用的推断服务器代码：https://github.com/awslabs/mxnet-model-server。 部署 计划是将代码封装进 Flask app。Flask 是一个轻量级 Python 网页框架，允许用极少的工作运行 http api 服务器。 作为快速推断，下列 Flask app 接受 multipart/form-data 的 POST 请求： 下面是对应的 Flask app，可连接上文提到的 run_graph： 看起来还不错，除了 Flask 和 TensorFlow 完全同步以外：执行图像分类时，Flask 按照接收请求的顺序一次处理一个请求，而 TensorFlow 完全占用线程。 如上所述，速度的瓶颈可能仍然在于实际计算量，因此升级 Flask 封装器代码没有太大意义。或许该代码足以处理加载。有两个明显的方式可以扩大请求吞吐量：通过增加工作线程的数量来水平扩大请求吞吐量（下一节将讲述），或利用 GPU 和批逻辑（batching logic）垂直扩大请求吞吐量。后者的实现要求网页服务器一次处理多个挂起请求，并决定是否等待较大批次还是将其发送至 TensorFlow 图线程进行分类，对此 Flask app 完全不适合。两种方式使用 Twisted + Klein 用 Python 写代码；如果你偏好第一类事件循环支持，并希望能够连接到非 Python ML 框架如 Torch，则需要使用 Node.js + ZeroMQ。 扩展：负载平衡和服务发现 现在我们已经有一个模型可用的服务器，但是它可能太慢，或我们的负载太高。我们想运行更多此类服务器，那么我们应该怎样在多个服务器上对其进行分布呢？普通方法是添加一个代理层，可以是 haproxy 或 nginx，可以平衡后端服务器之间的负载，同时向用户呈现一个统一的界面。下面是运行初级 Node.js 负载平衡器 http proxy 的示例代码： 为了自动检测后端服务器的数量和地址，人们通常使用一个「服务发现」工具，它可能和负载平衡器捆绑在一起，也可能分开。一些有名的工具，如 Consul 和 Zookeeper。设置并学习如何使用此类工具超出了本文范畴，因此，我使用 node.js 服务发现包 seaport 推断了一个非常初级的代理。代理代码： 工作线程代码： 但是，在应用到机器学习时，这个配置会遇到带宽问题。 系统如果每秒钟处理数十、数百张图片，它就会卡在系统带宽上。在目前的装配上，所有的数据需要通过我们的单个 seaport 主机，也是面向客户端的单个端点。 为了解决这个问题，我们需要客户不点击单个端点：http://127.0.0.1:12480，而是在后端服务器间自动旋转来点击。如果你懂网络架构，这听起来更像是 DNS 的活。 但是，配置定制的 DNS 服务器不在本文的讨论范围。把客户端代码改编遵循成 2 阶「手动 DNS」协议就行，我们能重复使用基本的 seaport proxy 来实现「端对端的」协议，其中客户能直接连接到服务器： 代理代码： （worker code 和上面一样） 客户端代码： 结论与拓展阅读 这个时候，你应该上手做点什么，但这肯定也不是不会过时的技术。在此文章中，还有很多重要的主题没被覆盖到： 在新硬件上的自动开发与装配 在自己的硬件上，值得关注的工具包括 Openstack/VMware，还有安装 Docker、管理网络路径的 Chef/Puppet，安装 TensorFlow、Python 等等的 Docker。 在云端，Kubernetes 或者 Marathon/Mesos 都非常棒 模型版本管理 一开始手动管理模型不是很难 TensorFlow Serving 是处理这个问题的不错工具，还有批处理和整体部署，非常彻底。缺点是有点难以配置，也难以编写客户端代码，此外还不支持 Caffe/PyTorch。 如何从 Matlab 迁移机器学习代码？ 在开发产品中不要用 Matlab（译者注：仅代表作者观点）。 GPU 驱动、Cuda、CUDNN 使用英伟达容器并尝试寻找一些在线 Dorckerfiles 后处理层。一旦你在开发产品过程中找到一些不同的机器学习模型，你可能想要混合这些模型，并为不同的使用案例匹配不同的模型——也就是模型 B 没结果跑模型 A，在 Caffe 上跑模型 C，并把结果传送到 TensorFlow 上跑的模型 D，等等。  https://thehive.ai/blog/simple-ml-serving 
307,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736925&idx=1&sn=105438ea62b0d732bbb588eafc175f16&chksm=871acd63b06d447583e591a292f4cd47a755d2c050d8d129bf581617f83448ba465f84aca6fd&scene=27,深度学习即将非法？欧盟《一般数据保护条例》五月生效,《欧盟一般数据保护条例》（General Data Protection Regulation，GDPR）是 20 年来数据隐私条例的最重要变化，它将取代《欧盟个人资料保护指令》95/46/EC，并将协调全欧洲的数据隐私法律，为所有欧盟民众保护和授权数据隐私，并将重塑整个地区的数据隐私保护形式。在 GDPR 中，有关「算法公平性」的条款要求所有公司必须对其算法的自动决策进行解释，这意味着目前大量 AI 应用依赖的深度学习算法不再符合法规。 GDPR 的重点内容，以及它对商业的影响，可在其官方网站上找到：https://www.eugdpr.org/ 几个月后，《一般数据保护条例》（GDPR）将生效，这被认为是对人工智能应用于商业的方式的一次彻底整改。2018 年 5 月 25 日，GDPR 将在欧盟全面实施。 113 天后就要到来的 Deadline 引起了 AI 社区和技术巨头的争论，他们现在正在努力满足欧盟的数据隐私和算法公平性原则。而对于欧盟公民来说，GDPR 统一数据保护法规，增加技术公司在收集用户数据时的责任，从而保护了公民权利。 GDPR 即将实施引起了很多人工智能学界人士的关注，《终极算法》作者，华盛顿大学教授 Pedro Domingos 在社交网络中说道：「自 5 月 25 日起，欧盟将会要求所有算法解释其输出原理，这意味着深度学习成为非法的方式。」 即将到来的规定将欧洲分成两大阵营：拥抱数据隐私和算法公平性原则的社会公民；对此不满的技术巨头，因为它们将面临新的挑战，如需要征求用户同意、处理 AI 的黑箱问题，这些问题可能最终导致 AI 相关应用不再合法，违者将接受全球营业额 4% 的罚款。 GDPR 要点概述 规定针对从欧盟公民处收集数据的企业：该规定不限于总部在欧盟地区的企业，而是覆盖到从欧盟公民处收集数据的所有组织。GDPR 要求此类企业反思其条款和条件（解释该公司如何使用个人数据来销售广告）中的内容，强制企业遵循 Privacy by Design 原则。 数据转移权：该规定声明，用户可要求自己的个人数据畅通无阻地直接迁移至新的提供商，数据以机器可读的格式迁移。这类似于在不丢失任何数据的情况下更改移动运营商或社交网络。对于谷歌、Facebook 等名副其实的数据挖掘公司和较小的数据科学创业公司而言，这就像是敲响了丧钟，当用户不再使用该公司产品时，它们将会丢失大量数据。 被遗忘权：GDPR 第 17 条强调，每个数据主体有权要求数据控制者删除个人数据，并且不能过分延长数据留存时间，控制者有义务遵循该规定。这对以 cookie 形式收集数据、从定向投放广告中获取收益的技术巨头而言是一项巨大损失。 算法公平性：自动决策的可解释权（The Right to Explanation of Automated Decision）指出数据主体有权要求算法自动决策给出解释，有权在对算法决策不满意时选择退出。例如，如果贷款申请人被自动决策拒绝时，有权寻求解释。对于技术公司而言，这是对人工智能的严重限制，将大幅减缓 AI 技术的发展。 AI 的高性能 vs 难解释性难题 在这里，我们不想深入研究欧盟指导方针的本质，但有必要指出目前被广泛应用的人工智能技术所面临的最大批评——深度学习及其不可解释性（即黑箱状态）。这可能会导致任何 AI 公司无法开展已有业务，甚至会被认为非法。目前，人工智能专家和科技公司从数据中获益，但又不断声称这些算法因为架构的复杂性而难以解释其输出的生成原理。 著名学者 Dr. PK Viswanathan 曾对人工神经网络的黑箱问题进行过研究。根据他的介绍，人们普遍认为神经网络是一个黑箱，但其机制并非完全不可解释，仍有一些分析其原理的方法。 例如，对于预测和分类问题，与 logistic 回归、随机森林等其他基于统计的有监督技术不同，神经网络是非参数、非线性的复杂关系模型，被认为具备泛逼近性。神经网络被用于所有分类和预测问题。神经网络的一些常见应用甚至包括：决定是否购买金融市场的产品，以及对风险进行判断。 人工神经网络的拓扑学 我们讨论两个隐藏层的多层感知机（一种前馈人工神经网络）。两个隐藏层的多层感知机因其预测准确率而知名。在两个隐藏层的多层感知机中，有两个输入神经元、两个隐藏层和四个节点，然后给出输出。网络初始化权重、偏置项、数值，激活函数可设定为 sigmoid 函数（或 logistic 函数），你可以用前馈方法改变权重的值。 在其架构中，隐藏层和黑箱点以及神经网络学习训练数据的方式密切相关，这正是对其主要批判的来源。 神经网络的推理方法是通过迭代地最小化损失函数不断改变权重的值。这被称为黑箱难题，它可以逼近任何函数，但对预测变量和输出之间的关系不能给出任何解释，PK Viswanathan 说。在监督学习问题中，人们可以解释 x 和 y 之间的准确关系，但无法捕捉神经网络生成这些关系的运作方式。 人们对于神经网络的最大批判是： 它缺乏可解释力，我们无法确定隐藏层内部发生了什么。但是神经网络在近似值和精确度上普遍得分较高。 在现实世界中很难解释摘要权重（synoptic weight），但是在传统技术中却不是，这进一步加深了黑箱难题。 解决深度学习的黑箱难题 现在，许多研究者已经致力于解释神经网络如何做出决策。其中一些方法如下： LIME：Local Interpretable Model-Agnostic Explanations 更广为所知的名称是 LIME，它是一个涉及以多种方式操作数据变量以查看什么把分值提高的最多的技术。在 LIME 中，局部指的是局部保真度，即解释应该反映分类器在被预测实例「周围」的行为。这一解释是无用的除非它是可阐释的——也就是说，除非人可以理解它。Lime 可以解释任何模型而无需深入它，因此它是模型不可知论的。 DARPA 的可解释性 AI（Explainable AI）：现在，DARPA 创建了一套机器学习技术来产生更可解释的模型，同时维持一个高水平的学习表现。可解释性 AI 可以使人类用户理解和管理即将到来的 AI 伙伴，其核心优势是新技术可以潜在地回避掉对额外层的需求。另一个解释组件可以从训练神经网络关联带有隐藏层节点的语义属性——这可以促进对可解释功能的学习。 原文地址：https://analyticsindiamag.com/deep-learning-going-illegal-europe/ 
308,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736910&idx=4&sn=1ac251342398cf256aa71159625899c7&chksm=871acd70b06d44664f61b6ca0b1987ab5a0df304d2012c45d0f7cd68a1565566fb329edb8f3f&scene=27,前沿 | 结合感知和概念思维，DeepMind提出可微归纳逻辑框架∂ILP,最近，DeepMind 在 JAIR 上发表论文《Learning Explanatory Rules from Noisy Data》，表明将直观感知思维和概念可解释性推理思维整合到单个系统中是可能的。他们介绍的系统∂ILP 对噪声数据具备鲁棒性，且可以高效地利用数据，并生成可解释的规则。 假设你在踢足球，足球到了你脚下，你决定把球传给无人盯防的前锋。看似一个简单的动作其实需要两种不同类型的思维。 首先，你识别到脚下有一颗足球，这需要直观感知思维，你无法清晰地表达你是如何知道脚下有一颗足球的，你只是看到它就在那里。其次，你决定将球传给某个前锋，这个决定需要概念性思维。你的决策是有依据的，你把球传给那个前锋是因为她没有被盯防。   这种区别很有意思，因为这两种类型的思维对应两种不同的机器学习方法：深度学习和符号程序合成（symbolic program synthesis）。深度学习聚焦于直观感知思维，而符号程序合成聚焦于概念性的、基于规则的思维。每个系统有不同的指标，深度学习系统对带噪数据具备鲁棒性，但其工作原理难以解释，且需要大量训练数据；而符号系统更容易解释，只需要少量的训练数据，但难以处理带噪数据。虽然 ，但如何将二者整合到 AI 系统中，尚无清晰的结论。 最近，DeepMind 在 JAIR 上发表论文《Learning Explanatory Rules from Noisy Data》，表明将直观感知思维和概念可解释性推理思维整合到单个系统中是可能的。他们所描述的系统∂ILP 对噪声是鲁棒的，可以高效地利用数据，并生成可解释的规则。 作者展示了∂ILP 如何处理归纳任务。给定一对表示数字的图像，其任务是输出标签（0 或 1）指出左图的数字是否比右图的数字小。解决这个问题需要上述的两种思维：直观感知思维将图像识别为特定数字的表征，概念性思维理解完全一般性的「小于」关系。 给标准的深度学习模型（如带 MLP 的卷积神经网络）提供足够的训练数据，它可以有效地解决这个任务。一旦网络经过了训练，你就可以向网络输入一对它未见过的图像，网络可以正确地对其分类。然而，只有用每对数字的多个样本进行训练，网络才能正确地泛化。这种模型的视觉泛化能力很好：泛化到新的图像上，假定它已经见过测试集中的所有数字对（参见下图中的绿框部分）。但它无法进行符号泛化：泛化到它从未见过的数字对（参见下图中的蓝框）。  和 Joel Grus 近日通过发人深省的文章指出了这一点。 ∂ILP 和标准的神经网络不同，它可以进行符号泛化；∂ILP 也和标准的符号程序不同，它可以进行视觉泛化。它从可读取、可解释和可验证的样本中学习显式的程序。向∂ILP 提供部分样本集（期望结果），它可以生成一个满足需求的程序。它利用梯度下降来搜索程序空间。如果程序的输出和参考数据的期望结果相冲突，系统将修正程序以更好地匹配数据。 ∂ILP 能够进行符号泛化。在它见过足够的 x < y、y < z、x < z 样本之后，它就会考虑 < 关系可传递的可能性。一旦它意识到这条泛化规则后，就可以将其应用到未见过的新的数字对。 实验结果总结如上图：标准深度神经网络（蓝色曲线）无法正确泛化至未见数字对。相反，∂ILP（绿线）在只见过 40% 数字对的情况下仍能达到较低的测试误差。这表明它能够进行符号泛化。 DeepMind 认为该系统可以初步回答深度神经网络能否进行符号泛化的问题。未来，DeepMind 计划将∂ILP 类系统整合进强化学习智能体和更大的深度学习模块。DeepMind 希望其系统可以同时具备推断和反应的能力。 论文：Learning Explanatory Rules from Noisy Data 论文链接：http://www.jair.org/media/5714/live-5714-10391-jair.pdf 摘要： 人工神经网络是强大的函数逼近器，能够对大量监督式、非监督问题提供解决方案。随着神经网络规模和表达性的增长，模型方差也在增长，出现了比较普遍的过拟合问题。尽管过拟合问题可以通过多种模型正则化方法得到改善，但最常见的解决方案是使用大量训练数据（可能很难获取足量训练数据），以充分逼近我们想要测试的范畴的数据分布。相反，逻辑编程方法如归纳逻辑编程（ILP）可以高效利用数据，从而模型可以被训练在符号范畴上进行推断。但是，这些方法无法应对多个范畴，而神经网络可以：逻辑编程方法对输入中的噪声或错误标注不具备足够的鲁棒性，更重要的是，它们无法应用到数据模糊的非符号范畴中，例如处理原始像素。本文提出一种可微归纳逻辑框架（Differentiable Inductive Logic framework），不仅能够解决传统 ILP 适合的任务，还展示了 ILP 不具备的对训练数据中噪声和误差的鲁棒性。此外，由于它能针对似然估计的目标函数执行反向传播来训练，它可与神经网络结合处理模糊数据，以应用于 ILP 无法处理的范畴，同时提供数据处理高效性和神经网络无法达到的泛化性能。 原文链接：https://deepmind.com/blog/learning-explanatory-rules-noisy-data/ 
309,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736925&idx=3&sn=f98e2854074accc45857de02a4c403d1&chksm=871acd63b06d4475d02848bee86f043618ab506c13a2edd0ebabb28174f85b642dbc31c2a0c9&scene=27,专栏 | 想免费用谷歌资源训练神经网络？Colab详细使用教程,"Colab 是谷歌内部类 Jupyter Notebook 的交互式 Python 环境，免安装快速切换 Python 2 和 Python 3 的环境，支持 Google 全家桶 (TensorFlow、BigQuery、GoogleDrive 等)，支持 pip 安装任意自定义库。 网址：https://colab.research.google.com Colab 自带了 Tensorflow、Matplotlib、Numpy、Pandas 等深度学习基础库。如果还需要其他依赖，如 Keras，可以新建代码块，输入 3 Google Drive 文件操作 授权登录 对于同一个 notebook，登录操作只需要进行一次，然后才可以进度读写操作。 遍历目录 可以看到控制台打印结果 title: Colab 测试, id: 1cB5CHKSdL26AMXQ5xrqk2kaBv5LSkIsJ8HuEDyZpeqQ, mimeType: application/vnd.google-apps.document title: Colab Notebooks, id: 1U9363A12345TP2nSeh2K8FzDKSsKj5Jj, mimeType: application/vnd.google-apps.folder 其中 id 是接下来的教程获取文件的唯一标识。根据 mimeType 可以知道 Colab 测试 文件为 doc 文档，而 Colab Notebooks 为文件夹（也就是 Colab 的 Notebook 储存的根目录），如果想查询 Colab Notebooks 文件夹下的文件，查询条件可以这么写： 读取文件内容 目前测试过可以直接读取内容的格式为 .txt（mimeType: text/plain），读取代码： 而 .csv 如果用 GetContentString() 只能打印第一行的数据，要用`` Colab 会直接以表格的形式输出结果（下图为截取 iris 数据集的前几行），iris 数据集地址为 http://aima.cs.berkeley.edu/data/iris.csv，学习的同学可以执行上传到自己的 Google Drive。 写文件操作 更多操作可查看 http://pythonhosted.org/PyDrive/filemanagement.html 授权登录 对于同一个 notebook，登录操作只需要进行一次，然后才可以进度读写操作。 把 iris.csv 的数据导入创建一个 Google Sheet 文件来做演示，可以放在 Google Drive 的任意目录 读取 打印结果分别为 [['5.1', '3.5', '1.4', '0.2', 'setosa'], ['4.9', '3', '1.4', '0.2', 'setosa'], ... 写入 这里以我在 Github 的开源 LSTM 文本分类项目为例子 https://github.com/Jinkeycode/keras_lstm_chinese_document_classification， 把 master/data 目录下的三个文件存放到 Google Drive 上。该示例演示的是对健康、科技、设计三个类别的标题进行分类。 新建 在 Colab 上新建 Python2 的笔记本 安装依赖 加载数据 授权登录 列出 GD 下的所有文件 缓存数据到工作环境 读取工作环境的数据 加载标签 文本预处理 构建神经网络 这里使用 Embedding 和 lstm 作为前两层，通过 softmax 激活输出结果 预测样本 sen 可以换成你自己的句子，预测结果为 [健康类文章概率, 科技类文章概率, 设计类文章概率], 概率最高的为那一类的文章，但最大概率低于 0.8 时判定为无法分类的文章。 原文链接： https://jinkey.ai/post/tech/xiang-mian-fei-yong-gu-ge-zi-yuan-xun-lian-shen-jing-wang-luo-colab-xiang-xi-shi-yong-jiao-cheng "
310,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736925&idx=4&sn=d4835cc05fd1833526df9a28e8596b16&chksm=871acd63b06d4475211354958e171c1ce650a9a79f9dc5347d28dc1837c91aa7a8ef25e47af3&scene=27,深度 | 脆弱的神经网络：UC Berkeley详解对抗样本生成机制,用于「欺骗」神经网络的对抗样本（adversarial example）是近期计算机视觉，以及机器学习领域的热门研究方向。只有了解对抗样本，我们才能找到构建稳固机器学习算法的思路。本文中，UC Berkeley 的研究者们展示了两种对抗样本的制作方法，并对其背后的原理进行了解读。 通过神经网络进行暗杀——听起来很疯狂吧？也许有一天，这真的可能上演，不过方式可能与你想象中不同。显然，加以训练的神经网络能够驾驶无人机或操作其他大规模杀伤性武器。但是，即便是无害的（现在可用的）网络——例如，用于驾驶汽车的网络——也可能变成车主的敌人。这是因为，神经网络非常容易被「对抗样本（adversarial example）」攻击。 在神经网络中，导致网络输出不正确的输入被称为对抗样本。我们最好通过一个例子来说明。让我们从左边这张图开始。在某些神经网络中，这张图像被认为是熊猫的置信度是 57.7%，且其被分类为熊猫类别的置信度是所有类别中最高的，因此网络得出一个结论：图像中有一只熊猫。但是，通过添加非常少量的精心构造的噪声，可以得到一个这样的图像（右图）：对于人类而言，它和左图几乎一模一样，但是网络却认为，其被分类为「长臂猿」的置信度高达 99.3%。这实在太疯狂了！ 上图源自： Explaining and Harnessing Adversarial Examples，Goodfellow et al 那么，对抗样本如何进行暗杀呢？想象一下，如果用一个对抗样本替换一个停车标志——也就是说，人类可以立即识别这是停车标志，但神经网络不能。现在，如果把这个标志放在一个繁忙的交叉路口。当自动驾驶汽车接近交叉路口时，车载神经网络将无法识别停车标志，直接继续行驶，从而可能导致乘客死亡（理论上）。 以上只是那些复杂、稍显耸人听闻的例子之一，其实还会有更多利用对抗样本造成伤害的例子。例如，iPhone X 的「Face ID」解锁功能依赖神经网络识别人脸，因此容易受到对抗性攻击。人们可以通过构建对抗图像，避开 Face ID 安全功能。其他生物识别安全系统也将面临风险：通过使用对抗样本，非法或不合宜的内容可能会绕开基于神经网络的内容过滤器。这些对抗样本的存在意味着，含有深度学习模型的系统实际上有极高的安全风险。 为了理解对抗样本，你可以把它们想象成神经网络的「幻觉」。既然幻觉可以骗过人的大脑，同样地，对抗样本也能骗过神经网络。 上面这个熊猫对抗样本是一个有针对性的 (targeted) 例子。少量精心构造的噪声被添加图像中，从而导致神经网络对图像进行了错误的分类。然而，这个图像在人类看来和之前一样。还有一些无针对性 (non-targeted) 的例子，它们只是简单尝试找到某个能蒙骗神经网络的输入。对于人类来说，这种输入看起来可能像是白噪声。但是，因为我们没有被限制为寻找对人而言类似某物的输入，所以这个问题要容易得多。 我们可以找到将近所有神经网络的对抗样本。即使是那些最先进的模型，有所谓「超人类」的能力，也轻微地受此问题困扰。事实上，创建对抗样本非常简单。在本文中，我们将告诉你如何做到。用于开始生成你自己的对抗样本的所有所需代码等资料都可以在这个 github 中找到：https://github.com/dangeng/Simple_Adversarial_Examples 上图展示了对抗样本的效果 MNIST 中的对抗样本 这一部分的代码可以在下面的链接中找到（不过阅读本文并不需要下载代码）：https://github.com/dangeng/Simple_Adversarial_Examples 我们将试着欺骗一个普通的前馈神经网络，它已经在 MNIST 数据集上经过训练。MNIST 是 28×28 像素手写数字图像的数据集，就像下面这样： 6 张 MNIST 图像并排摆放 首先，我们需要导入所需的库： 其中有 50000 张图像作为训练集，10000 张作为测试集。首先，加载预训练的神经网络（这是我从这个神经网络的介绍中找来的 http://neuralnetworksanddeeplearning.com/）： 有些人可能不太熟悉 pickle。这是 python 序列化数据（即，写入磁盘）从而保存类与对象的一种方法。使用 pickle.load() 可以打开保存的神经网络版本。 现在来说说这个预训练神经网络吧。它具有 784 个输入神经元（每个神经对应一个像素，共 28×28 = 784 个像素）、一个含有 30 个神经元的隐藏层，以及 10 个输出神经元（每个数字对应一个神经元）。所有的激活函数都是 sigmoid 类型。它的输出是一个指示网络预测的 one-hot 向量，并且通过最小化均方误差损失来进行训练。 为了证明这个神经网络实际上已经经过训练，我们可以写一个简单的函数： 该方法从测试集中选择第 n 个样本，将其显示出来，然后使用 net.feedforward(x) 在神经网络中执行。以下是一些图片的输出： 左侧显示的是 MNIST 图像。右侧绘制了神经网络的 10 个输出，称为激活。输出的激活越大，神经网络越有可能认为图像就是这个数字。 好的，所以现在我们有一个预训练的网络了。但是，我们怎么去欺骗它呢？让我们首先从一个简单的无针对性方法开始。然后，一旦我们把这个方法弄清楚，我们就可以用一个很棒的小技巧对此加以修改，从而获得一个针对性的方法。 无针对性攻击（ ） 这个想法就是要生成某种图像，使神经网络有确定的输出。例如，我们的目标标签/输出是： 也就是说，我们想要得到这样一个图像，使得神经网络的输出就是以上矢量。换句话说，我们想找到一个使神经网络认为是 5 的图像（记住，我们使用了零索引）。事实证明，我们可以将这作为一个优化问题，就像我们训练一个网络一样。我们将我们想要制作的图像为 ⃗x（一个 784 维矢量，因为将 28×28 像素的图像展平可使计算更容易）。我们将成本函数定义为： 其中，∥⋅∥22 代表 L2 范数的平方。y_goal 是上面的目标标签。在给定神经网络中，图像的输出是 。你可以看到，如我们生成的图像在给定的神经网络中输出非常接近目标标签 y_goal，那么相应的成本函数值就很小。如果网络的输出离目标很远，那么成本函数值就很大。因此，如果能找到最小化成本函数 C 的向量 ⃗x，也就找到了这样一个图像，它使得神经网络的预测恰为我们的目标标签。我们现在的问题，就是找到这个向量 ⃗x。 请注意，这个问题与我们如何训练一个神经网络非常相似。其中，我们也定义一个成本函数，然后选择使成本函数最小化的权重和偏差（也称为参数）。在对抗样本生成这一情况下，我们并不通过选择权重和偏差来最小化成本函数，而是将权重和偏差保持不变（实质上保持整个网络不变），并选择一个最小化成本函数的 ⃗x 输入。 为此，我们将采用与训练神经网络完全相同的方法。也就是说，我们将使用梯度下降！我们可以使用反向传播，找到成本函数于输入的偏导数，然后使用梯度下降进行更新，找到最小化成本的最佳值 ⃗x。 反向传播通常用于查找成本函数对权重和偏差的梯度，但从完全一般性的角度，反向传播只是一种算法，可以有效地计算计算图（也就是一个神经网络）中的梯度。因此，它也可以用来计算神经网络中成本函数对输入的梯度。 好的，下面让我们看看实际中生成对抗样本的代码： 首先我们创建 y_goal，在代码中被称为「goal」。接下来我们将向量 ⃗x 初始化为一个随机的 784 维向量。有了这个矢量，我们现在可以开始进行梯度下降，实际上这只有两行代码。  第一行 d = input_derivative（net，x，goal）使用反向传播计算∇xC（有些人可能对此代码的原理感到好奇，但是我们不在此描述它，这实际上涉及大量数学计算。如果你想知道反向传播做什么，也就是 input_derivative 做什么，请在以下网站中查看：http://neuralnetworksanddeeplearning.com/chap2.html） 梯度下降循环的第二行，也是最后一行，即 x - = eta * d，实现更新。我们沿着与步长大小 eta 相反的方向进行移动。 下面是每个类别的非针对性对抗样本示例，以及在神经网络中的预测： 左侧是非针对性对抗样本（一个 28×28 像素的图像）。右侧是给出此图像时绘制网络的激活。 难以相信，神经网络认为，一些图像实际上是一个数字，且置信度很高。这个「3」和「5」就是很好的例子。对于其他许多数字而言，神经网络对于每个数字的激活结果都很低，表明它非常混乱。看起来不错！ 在这里，可能你会有些困惑。如果我们想做一个对应于 5 的对抗样本，那么我们所希望的是找到一个 ⃗x，当给神经网络一个输入时，输出尽可能接近表示「5」的 one-hot 向量。但是，为什么梯度下降的结果，并非只找到一个「5」的图像？毕竟，神经网络几乎可以肯定地认为，「5」的图像实际上就是「5」（因为它实际上确定是「5」）。关于此事为何发生的一个可能理论如下： 所有可能的 28×28 的图像空间是非常巨大的。一共有 256^（28×28）≈10^1888 种不同可能性的 28×28 像素的黑白图像。为了便于比较，可观测宇宙中原子数量的普遍估计是 10^80 个。如果宇宙中的每个原子包含另一个宇宙，那么我们将有 10^160 个原子。如果宇宙的每个原子包含另一个宇宙，那个宇宙中的每个原子还包含另一个宇宙……这样嵌套约 23 次的话，基本上将可以达到 10^1888 个原子。基本上，可能的图像数量是如此令人难以置信的巨大。 但在这所有的照片中，本质上只有微不足道的一部分在人类看来像「数字」。鉴于有这么多图像，其中有很大一部分对神经网络来说看似是数字（这个问题的部分原因在于，神经网络没有在那些「不像」数字的图像上进行训练，所以如果给神经网络输入一个不像数字的图像，它的输出是非常随机的）。所以，当我们开始寻找对神经网络而言像数字的图像时，我们更有可能找到一个看起来像噪声的图像，而非纯属偶然地找到一个对人类而言和数字类似的图像。 针对性攻击（ ） 上述对抗样本的确很不错。但对人类来说，它们看起来像噪声。如果我们的对抗样本在人类看来像是什么别的东西，不是更棒吗？也许一个实际上是 2 的图像，神经网络会认为是 5 呢？事实证明，这是可能的！而且，只需要对我们原来的代码进行非常小的修改。我们可以做的是在需要最小化的成本函数中添加一个项。新的成本函数表示如下： 其中，x_target 是我们希望对抗样本看起来像的那个东西（因此 x_target 是一个 784 维矢量，与我们的输入相同的维度）。所以，我们现在所做的是同时最小化两个项。其中，左边的那项 我们之前已经见过。如果将其最小化，将会使得在 ⃗x 给定的情况下，让神经网络的输出接近 y_goal。如果最小化第二项 试图迫使我们的对抗图像 x 尽可能地接近 x_target（因为两个向量越接近，范数越小），这正是我们想要的！前面额外的 λ 是一个超参数，它能指明前后哪个项更重要。与大多数超参数一样，经过大量的试验和错误，我们发现将 λ 设置为 0.05 很不错。 如果你了解岭回归，你会发现上面的成本函数看起来非常熟悉，事实上，我们可以将上述成本函数解释为我们对抗样本中的模型先验。 实现对新的成本函数最小化的代码与原始代码几乎相同（我们将新的代码称为函数 sneaky_adversarial()，因为它使用针对性的样本，进行偷偷摸摸（sneaky）的攻击。命名总是编程中最难的部分……） 我们唯一改变的是梯度下降更新：x -= eta * (d + lam * (x - x_target))。附加项正是对成本函数中新的项的解释。让我们来看看这种新方法的结果： 左侧是针对性对抗样本的实例（一个 28x28 的像素图像）。右侧是在给出左图后，神经网络给出的激活。 请注意，与非针对性攻击一样，攻击后可能出现两种行为。一种可能是神经网络完全被欺骗，即我们想要的数字的激活非常高（例如「针对性 5」图像）；另一种可能是，对网络进行了混淆，导致所有的输出激活都很低（例如「针对性 7」图像）。有趣的是，现在有更多的图像在前一类，即完全欺骗神经网络，而不是混淆它。看起来，让对抗样本变得更「类似数字」，倾向于在梯度下降时进行更好地收敛。 保护神经网络免受对抗攻击 真棒！我们刚刚创建了能骗过神经网络的图像。我们的下一个问题是，是否可以防范这种攻击。如果仔细观察原始图像和对抗样本，你会发现对抗性示例子有一些淡灰色背景。 上图是一个背景中带噪声的对抗样本。单击图片可以切换原始图片和对抗样本。 （上图左侧是原始图片，右侧是对抗样本） 我们可以尝试一个非常简单的做法，那就是利用二进制阈值完全清除背景： 以下是结果： 以上展示了二进制阈值对 MNIST 对抗样本的影响。左侧是图像，右侧是神经网络的输出。单击图片可以在二进制图像和对抗样本之间切换。 原来，二进制阈值确实可以防范攻击！但这种防范对抗攻击的方式并不是很好。并非所有图像都有全白的背景。比如，我们可以看看文章一开始提到的熊猫图像。对图像进行二值化阈值处理可能会消除噪声，但也会极大程度地干扰大熊猫的形象。二值化后，网络（和人类）甚至可能都无法区分它是否是熊猫。 对熊猫进行二进制阈值处理会导致图像不稳定 我们还可以尝试一种更一般性的方法。那就是训练一个能在对抗样本和原始训练、测试集中表现都正确的神经网络。执行此操作的代码位于 ipython notebook 中（请注意，大概需要 15 分钟才能运行）。这样做的话，它在所有对抗图像的测试集中，准确率高达 94%，这相当不错。但是，这种方法的也有其局限性。主要原因是，在现实生活中，你无法知道攻击者如何生成对抗样本。 在这篇介绍性的文章中，我们无法一一阐释其他更多防范对抗攻击的方法，不过这个问题仍然是一个开放的研究课题。如果你对此感兴趣，还可以查阅更多关于这个主题的优秀论文。 黑盒攻击 对抗样本中有一个有趣而重要的观察，那就是对抗样本通常不特定于某个模型或架构。针对某个神经网络架构生成的对抗样本可以很好地转换到另一个架构中。换句话说，如果你想欺骗某个模型，你可以先创建自己的模型和基于此模型的对抗样本。那么，这些同样的对抗样本也很可能欺骗另一个模型。 这具有重大的意义。因为，这意味着有可能对一个完全的黑箱模型创建一个对抗样本。对于这个黑箱模型，我们无需了解其中的内部机制。实际上，伯克利的一个小组使用这种方法在商业性的人工智能分类系统中发起了一次成功的攻击：https://arxiv.org/pdf/1611.02770.pdf 小结 随着人类走向未来，日常生活中将会融入越来越多的神经网络和深度学习算法。我们必须小心，要记住，这些神经网络模型极其容易被蒙骗。尽管神经网络在某种程度上受到了生物学的启发，并且在各种各样的任务中具有接近（或超过）人类的能力，但对抗样本告诉我们，它们的操作方法与真实生物体的工作方式不同。正如我们所看到的，神经网络容易以一种对我们人类来说是完全陌生的方式灾难性地失败。 我们没有完全理解神经网络，因此用我们人类的直觉来描述神经网络并不明智。例如，你会经常听到人们说「神经网络将图像分类为猫，是因为橙色皮毛」。事实上，神经网络并不进行人类意义上的「思考」。基本上，它们只是一系列矩阵乘法，还有一些增加的非线性操作。正如对抗样本所表明的，这些模型的输出非常脆弱。我们必须小心，尽管神经网络的确具备人类的某些能力，但不要认为人的特质也属于神经网络。也就是说，我们不能将机器学习模型拟人化（https://blog.keras.io/the-limitations-of-deep-learning.html）。 总而言之，对抗样本应该让我们感到谦卑。它们表明，虽然我们的技术已经有了很大的飞跃，但是还有许多谜团等待我们去探索。 
311,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736910&idx=2&sn=8725ccdd03b7d1e3f5b463dfe18424fd&chksm=871acd70b06d44660649ff0bc5ab1887f91b086ff80028b92a89e4485fca927b76c50e1315e8&scene=27,前谷歌自动驾驶首席工程师朱佳俊创立的Nuro，今天正式发布Level 4无人配送车,刚刚，由前谷歌自动驾驶汽车团队核心成员朱佳俊和 Dave Ferguson 创立的硅谷机器人技术公司 Nuro 宣布发布 Level 4 全自动无人配送车。 Nuro 由谷歌自动驾驶团队的前首席工程师朱佳俊和 Dave Ferguson 于 2016 年创立。 Nuro 此次发布的第一个产品历时 17 个月的硬件、软件研发和努力，是全球领先的在城市地面道路行驶的 Level 4 无人配送车。该产品的电动车硬件系统和无人驾驶软件技术全部都由 Nuro 团队设计研发。Nuro 这款产品不是为低速园区内或者人行道而设计的，而是可以在绝大多数城市内的地面道路上行驶。该产品一开始就专注运送货物并为其优化设计，更轻、更便捷、更高效。车身结构和材料都是为了最大程度上保护周围的行人而全新设计。Nuro 将会大规模生产该车型，并和多家合作伙伴一起推出服务，包括餐厅、药房、生鲜超市、服装百货、干洗等。 Nuro 希望通过这第一款产品加速机器人技术改善人们在本地购买商品的体验。美国零售业在过去三十年发生了很大的变革，传统零售商都在加速线下到线上的转变。消费者对于消费体验的要求也越来越高，越来越多的消费者希望当天甚至 30 分钟内收到购买的商品。同城快递在中国已经非常发达，但是在美国由于人工贵和人口密度小，快递是非常昂贵的。由此导致了在美国，很多领域的网上购物渗透率仍然很低，比如网上生鲜购物只有 1～3% 的渗透率。机器人和自动驾驶技术可以极大的改善这些体验，降低最后几公里的物流成本，做到每一个人在任何时间和地点收发物品，也可以帮助商家合作伙伴改善用户体验。 创立 Nuro 之前，朱佳俊和 Dave Ferguson 在谷歌自动驾驶汽车团队一起并肩共事了 6 年。朱佳俊在谷歌工作了 8 年半，是谷歌自动驾驶汽车团队首席工程师之一，也是最早的项目创始团队成员之一，负责带领环境感知技术和仿真模拟的研发。Dave 是谷歌自动驾驶汽车团队机器学习和机器视觉技术的负责人，还是自动驾驶汽车业界最早的开拓者之一，他在卡内基梅隆大学读机器人博士期间就负责了 2007 美国国防部自动驾驶比赛冠军车辆的路径规划系统，也参与了 NASA 火星探测器的研发，同时还是英特尔个人机器人实验室创办人之一。在谷歌他们一同打造和带领了一支非常优秀的超百人工程师队伍。 Nuro 团队聚集了众多来自谷歌、Waymo、苹果、特斯拉、通用汽车、优步、Twitter 等科技公司的科研、工程和产品人才。自动驾驶团队包含了 2007 美国国防部自动驾驶汽车比赛冠军队成员，三位来自谷歌的首席工程师分别负责了三代谷歌自动驾驶汽车的软硬件核心技术研发，以及苹果自动驾驶汽车的资深部门负责人。机器人算法团队包含了很多来自全世界一流名校的毕业生，包括卡内基梅隆、斯坦福、加州伯克利、麻省理工、普林斯顿、哈佛、牛津、北大、复旦等。人工智能团队包含了 ImageNet 的往届冠军和 Deepmind 的前成员。 朱佳俊表示：「Nuro 是一家机器人公司。Nuro 代表 New Robotics。我们的使命是加速机器人技术给所有人生活带来的便利。我们希望因为 Nuro 的努力可以给人们带来更多的时间和生活上更多的便捷。自动驾驶技术是机器人应用必不可少的一部分，将来我们还会涉足更多的领域。谷歌从 10 年前开始在自动驾驶领域的开拓性的工作，加速了整个自动驾驶汽车领域的快速发展。由于谷歌的工作，自动驾驶汽车至少会早被实现 3 年，可以多拯救 400 万人的生命。我们创立 Nuro 的主要愿景是希望可以像谷歌一样去加速除了载人交通以外的其它行业的快速发展。」 Nuro 已经完成了 A 轮融资。A 轮分为两次，第一次是高榕资本领投，其他参与的国内投资人有网易创始人丁磊、真格基金等。第二次是美国的 Greylock Partners 领投，原有投资机构继续参与。两次一共融资 9200 万美元。 高榕资本创始合伙人岳斌表示：「从一开始我们就感受到，佳俊和 Dave 对于加速机器人技术给人们生活带来的便利，有着一种纯粹的执着和渴望。在这个喧嚣的行业里，他们选择了低调前行，用最领先的技术、工程化、产品能力和商业思考，推动行业进步。这次他们是直接带着商业落地的无人驾驶 Level 4 产品出现在大家的视野里，以后还会有更多的发布。高榕资本有机会参与 Nuro 的事业，是极大的荣幸。我们对 Nuro 团队充满敬意和感激。」 
312,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736910&idx=5&sn=0d6f110621f206f3eec7b9e5a0602e9c&chksm=871acd70b06d44666fd8b018e916b3e3f3386f4a6c0d36fa5a10689fffb036bcbe842713ce2a&scene=27,AAAI 2018 | 腾讯AI Lab参与提出EAD：基于弹性网络正则化的深度神经网络对抗样本攻击,"腾讯 AI Lab 在 2018 年 AAAI 中入选论文 11 篇，其中一篇与 IBM Research、The Cooper Union 和加州大学戴维斯分校合作的论文入选口头报告（Oral）。这篇论文提出一种基于弹性网络正则化的攻击算法，该算法将对抗样本攻击 DNN 的过程形式化为弹性网络正则化的优化问题。此外，对基于 L1 失真攻击的评估为对抗式机器学习和 DNN 安全应用提供了全新的洞见，并进一步完善对抗式机器学习框架。 深度神经网络（DNN）在机器学习和人工智能的诸多不同任务（诸如图像分类、语音识别、机器翻译、打游戏）上取得了当前最优结果。尽管很有效，DNN 在对抗样本（Szegedy et al. 2013; Goodfellow, Shlens, and Szegedy 2015）面前依然表现的很脆弱。比如，一个精心设计的图像扰动可以轻易使训练良好的 DNN 失去分类能力。更糟糕的是，人类的感知实际上也无法在视觉上辨认有效对抗样本的区别。比如，图 1 给出了由我们的算法生成的鸵鸟图像的三个对抗样本，由一种当前最优的图像分类模型 Inception-v3（Szegedy et al. 2016）分别识别为「安全」、「鞋店」和「吸尘器」。 DNN 在对抗样本面前所欠缺鲁棒性引起了安全类应用的强烈关注，包括交通信号识别、恶意软件检测等等。然而，在数字空间之外的现实世界，研究者已表明对抗样本在愚弄 DNNs 上依然有效（Kurakin, Goodfellow, and Bengio 2016a; Evtimov et al. 2017）。鉴于鲁棒性和安全性的存在，制作对抗样本的方法称作 DNN 攻击。特别是，有目标的攻击旨在制造被误分类为特定目标类别的对抗样本，而无目标的攻击旨在制作不是原始分类的对抗样本。迁移攻击旨在制作可从一个 DNN 模型迁移到另一个的对抗样本。除了评估 DNN 的鲁棒性，对抗样本还可用于训练能够适应对抗扰动的鲁棒模型，这称之为对抗训练（Madry et al. 2017），并已经用于解释 DNN（Koh and Liang 2017; Dong et al. 2017）。 在整篇论文中，我们使用对抗样本攻击基于深度卷积神经网络的图像分类器。制作有效对抗样本的原理在于操控预测结果，同时又保证与原始图像的相似性。具体来讲，在文献中原始与对抗样本之间的相似性已经进行了不同的失真度量（distortion metrics）。一个常用的失真度量是 L_q 范数，其中 上式表示任意 q ≥ 1 的 p 维向量 x = [x_1, . . . , x_p] 的 L_q 范数。尤其当制作对抗样本时， L∞失真度量用于评估最大的像素值变化（Goodfellow, Shlens, and Szegedy 2015），而 L2 失真度量则用于提升视觉质量（Carlini and Wagner 2017b）。 然而，尽管 L1 范数广泛用于图像降噪、还原（Fu et al. 2006）以及稀疏恢复（Candes and Wakin 2008），基于 L1 的对抗样本并未被严格探讨。在对抗样本中，L1 失真说明了扰动中的总变化，并作为 L0 度量的凸替代函数，它可以度量扰动所修改的像素（即稀疏性）数量。为了弥补差距，我们提出了一种基于弹性网络正则化（Elastic-net regularization）的攻击算法，称为弹性网络正则化攻击（EAD）。弹性网络正则化是 L1 和 L2 惩罚函数的线性混合，已成为解决高维特征选择问题（Zou and Hastie 2005）的标准工具。在 DNN 攻击中，由于其推广了基于 L2 失真而提出的当前最优攻击方法（Carlini and Wagner 2017b），EAD 开辟了新的研究方向，并能够制造面向 L1 的对抗样本，它更有效，并迥然不同于现有的攻击方法。 为了探讨 EAD 基于 L1 构建的对抗样本效用，我们在不同的攻击情景中对 MNIST、CIFAR10 和 ImageNet 进行了大量实验。相较于当前最优的 L2 和 L∞攻击（Kurakin, Goodfellow, and Bengio 2016b; Carlini and Wagner 2017b），当攻击无防备和防备式提取的 DNN（Papernot et al. 2016b）时，EAD 可以获取相似的攻击成功率。更重要的是，我们发现 L1 攻击在迁移攻击中取得了优于 L2 和 L∞攻击的表现。对于更困难的数据集（MNIST），EAD 最终提升了从无防备 DNN 到防备式提取 DNN 的攻击迁移性，并取得了 99% 的攻击成功率。此外，带有 L1 和 L2 实例的联合对抗训练可以进一步提高 DNNs 对于对抗样本的适应力。这些结果表明 EAD 产生了一个截然不同，但更有效的对抗样本集。此外，对基于 L1 失真攻击的评估为对抗式机器学习和 DNN 安全应用提供了全新的洞见，并表明 L1 也许可以补充基于 L2 和 L∞的样本，进一步完善对抗式机器学习框架。 对比方法 我们对比了 EAD 和以下目标攻击，它们都是在不同失真度量中制造对抗样本的高效方法。 C&W 攻击：当前最优的 L2 目标攻击方法，由 Carlini 和 Wagner（Carlini and Wagner 2017b）提出。当β = 0 时，它是 EAD 算法的一个特殊案例。 FGM：快速梯度攻击算法（Goodfellow, Shlens, and Szegedy 2015）。在本文中我们使用 FGM-L1、FGM-L2 和 FGM-L∞表示不同的失真度量方法。 I-FGM：迭代的快速梯度攻击算法（Kurakin, Goodfellow, and Bengio 2016b）。在本文中我们使用 I-FGM-L1、I-FGM-L2 和 I-FGM-L∞表示不同的失真度量方法。 论文：EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples  论文地址：https://arxiv.org/pdf/1709.04114.pdf 近来的一些研究已经突出说明了深度神经网络（DNN）面对对抗样本时的脆弱性——人们可以轻松创造出在视觉上难以区分，却能让经过良好训练的模型分类错误的对抗图像。现有的对抗样本生成方法大多基于 L2 和 L∞ 范数去度量，但很少有方法用到了 L1 范数，尽管它有一些优良的性质，例如鼓励生成更为稀疏的对抗噪音。   在本论文中，我们将使用对抗样本攻击 DNN 的过程转化为了使用弹性网络正则化（elastic-net regularized）的优化问题。在这种表示下，当前最佳的 L2 范数攻击算法成为了本文方法的一个特例 （在不考虑 L1 范数的情况下）。在 MNIST、CIFAR10 和 ImageNet 上的实验结果表明 EAD 算法可以生成具有很小 L1 失真的对抗样本，并且能在不同攻击场景中实现与当前最佳方法匹敌的攻击成功率。更重要的是，EAD 算法生成的对抗样本有着显著增强的攻击可迁移性，这为如何在对抗机器学习中使用 L1 范数失真以及增强 DNN 的安全性提供了全新的见解。 "
313,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736910&idx=3&sn=2745d02b93a3ea7edb811b4cbd537096&chksm=871acd70b06d4466c7ea64e753532d6dbc294c850c1c0ca371311a0e92e7652883c44d39f02f&scene=27,学界 | ICLR 2018接收论文公布：接收率高达42.7%,ICLR 作为深度学习顶级会议，今年共接收到了 981 篇有效论文。去年 11 月，ICLR 2018 论文评审结果出炉，今天主办方正式放出接收论文结果：2.3% 的 oral 论文、31.4% 的 poster 论文、9% 被接收为 workshop track，51% 的论文被拒收、6.2% 的撤回率。而备受关注的论文《Matrix capsules with EM routing》作者也得以揭晓：Geoffrey Hinton 为一作，其他两位作者为 Sara Sabour、Nicholas Frosst。 论文接收列表：https://openreview.net/group?id=ICLR.cc/2018/Conference 23 篇 Oral 论文都是非常优秀的论文，我们可以预计本次大会的获奖论文基本上就会在这 23 篇优秀论文中产生。上一次我们报道过的 ICLR 2018 论文双盲审评分列表前几名的论文也都在口头报告中，例如 On the Convergence of Adam and Beyond 和 Certifiable Distributional Robustness with Principled Adversarial Training 等。不过最近评分列表有所更新，排在前面的论文位置有所变动，例如 i-RevNet 的评分由 9；8；7 变为了 9；8；8，不过 i-RevNet 并不在 Oral 论文的列表内。 以下是评分排名较前的一些论文： 在 Oral 论文中，有很多非常有意思的研究主题，包括新型卷积架构、训练和推断方式、最优化方法和强化学习策略等。因此下面我们从少量 Oral 论文出发，并简要介绍这些比较有意思的话题。 论文 Certifiable Distributional Robustness with Principled Adversarial Training 到目前为止仍然获得了最高的评分（9；9；9），它也接收在 Oral 论文中。该论文的主题是关于对抗样本的，并希望利用分布式鲁棒优化的原则来保证对抗训练的性能。值得注意的是，最近 Goodfellow 在论文 Adversarial Spheres 也在研究具有良好数学定义的数据流形中的对抗样本，因此我们可以对模型学到的决策边界进行定性地描述。Goodfellow 表示，我们可以自然地改变数据流形的维度来研究输入维度的改变对神经网络泛化误差的影响。 论文链接：https://arxiv.org/pdf/1710.10571.pdf 摘要： 神经网络很容易受到对抗样本的干扰，因此研究人员提出了许多启发式的攻击与防御机制。我们采取了分布式鲁棒优化的原则，以保证模型在对抗性扰动输入的条件下保持性能。我们通过给予 Wasserstein ball 上的潜在数据分布一个扰动来构建 Lagrangian 罚项，并且提供一个训练过程以加强模型在最坏的训练数据扰动情况下能持续正确地更新参数。对于平滑的损失函数，我们的过程相对于经验风险最小化可以证明有适度的鲁棒性，且计算成本或统计成本也相对较小。此外，我们的统计保证允许我们高效地证明总体损失的鲁棒性。因此，该研究结果至少匹配或超越监督学习和强化学习任务中的启发式方法。 如下所示，鲁棒性的模型在原版的马尔科夫决策过程（MDP）中要比一般的模型学习更高效： 学习过程在 ICLR 2018 的接收论文中也非常重要，在 Oral 论文 ON THE CONVERGENCE OF ADAM AND BEYOND 中，研究者们重点探讨了 Adam 优化方法与 SGD 的收敛性能，并通过反例证明了 Adam 在某些情况下可能会不收敛。正如最近 Sebastian Ruder 总结的 2017 年最优化方法进展，从经验上来说，适应性学习率方法一般在目标识别、字符级语言建模和句法分析任务中要比带动量的 SGD 方法所搜索到的极小值差。 一般来说，权重衰减因素可以部分解释 Adam 方法在一些数据集上比带动量的 SGD 方法泛化性能更差的原因。而另一个导致 Adam 收敛性不那么好的原因就是这篇 Oral 论文所表示的指数滑动平均。该论文表示 Adam、RMSprop 和 Adadelta 等方法都是基于使用前面迭代所产生梯度平方的指数滑动平均值，在对该滑动平均值取平方根后用于缩放当前梯度以更新权重。指数均值的贡献是积极的：这种方法应该防止学习率在学习过程中变得逼近无穷小，这也是 Adagrad 算法关键的缺点。然而，这种梯度的短期记忆能力也成为了其它情况的障碍。 论文链接：https://openreview.net/pdf?id=ryQu7f-RZ 摘要： 近来提出的几种随机优化方法已经成功地应用于深度网络的训练，如 RMSPROP、ADAM、ADADELTA 和 NADAM 等方法，它们都是基于使用前面迭代所产生梯度平方的指数滑动平均值，在对该滑动平均值取平方根后用于缩放当前梯度以更新权重。根据经验观察，这些算法有时并不能收敛到最优解（或非凸条件下的临界点）。我们证明了导致这样问题的一个原因是这些算法中使用了指数滑动平均（exponential moving average）操作。本论文提供了一个简单的凸优化案例，其中 ADAM 方法并不能收敛到最优解。此外，我们还描述了过去文献中分析 ADAM 算法所存在的精确问题。我们的分析表明，收敛问题可以通过赋予这些算法对前面梯度的「长期记忆」能力而得到解决。因此本论文提出了一种 ADAM 算法的新变体，其不仅解决了收敛问题，同时还提升了经验性能。 在论文 Wasserstein Auto-Encoders 中，其提出了在变分自编码器中使用 Wasserstein 距离进行度量，从而让 VAE 能够产生与生成对抗网络相媲美的效果。 我们提出了 Wasserstein 自编码器（WAE）——一个用于构建数据分布的新型生成方法。WAE  最小化 模型分布与目标分布之间的 Wasserstein 距离的惩罚形式，这导致了与变分自编码器（VAE）[1] 所使用的不同的正则化器。这个正则化器鼓励已编码的训练分布匹配先验分布。我们对比了我们的算法与其他几种技术，表明它是对抗自编码器（AAE）的一个推广形式 [2]。我们的实验表明，WAE 具有 VAE（稳定训练、编码器 - 解码器架构、良好的潜在流形结构）的许多特性，同时产生质量更好的样本（测量标准是 FID 得分）。 如上图所示，VAE 与 WAE 都最小化了两项：即重构成本和正则化器对 P_z 和编码器 Q 产生的分布之间差异的惩罚。对于从 P_X 中抽出的所有不同输入样本 x，VAE 会迫使 Q(Z|X = x) 匹配 P_z。这展示在图（a）中，其中每一个红色的球形区域会被迫与分布 P_z（白色区域）相匹配。因为红色的区域相互交叉，这将导致重构会存在问题。相反，如图（b）中的绿色球形区域所示，WAE 会强迫连续混合混合 Q_Z := ∫Q(Z|X)dP_X 与 P_Z 相匹配。因此，不同的样本将有机会远离其它，重构也会有更好的性能。 除了 Oral 论文外，更多的接收论文属于 Poster（31.4%）。在这一部分的接收论文中，比较有意思的是 Hinton 的第二篇 Capsules 论文 Matrix capsules with EM routing 被接收了。而那篇以信息论为基础来分析深度学习的论文 On the Information Bottleneck Theory of Deep Learning 也得到了接收。 最后，该列表还展示了大会所拒收的论文和撤回的论文。在这些被拒的论文中，也有很多如固定 Adam 权重衰减和动量调参等学习算法。 
314,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736910&idx=1&sn=d780000b87000689f5632effaca300c9&chksm=871acd70b06d446612440a44d40525d6990647089ad2d7c5cf677b50148fd1d90bb0830a430a&scene=27,吴恩达宣布启动AI Fund：1.75亿美金进军AI创投,在发布深度学习课程项目  、面向制造业的 AI 公司   之后，人工智能著名学者吴恩达（Andrew Ng）于 1 月 30 日宣布了又一个重大动向：进军风险投资领域，成立人工智能创业投资机构 AI Fund。据悉，这家基金目前已募集了 1.75 亿美元的资金，专注于孵化各类人工智能创业公司。「AI Fund 的主要工作方式不是传统的风险投资模式：搜寻信息，从数千个应用中寻找值得投资的那一个，」吴恩达表示，「与之相反，AI Fund 从创业者开始，我们从零开始帮助创业者构建新的公司。」 至此，吴恩达此前提到的三大创业项目已经全部出炉。 寻找人才，并帮助他们快速实现自己的想法是吴恩达推动 AI Fund 的初衷。「我们决心启动 AI Fund 的时候，我们希望注重的是速度，」吴恩达表示。「AI Fund 旗下的公司创建时无需花费长达六个月的时间寻求融资，与之相反的是，它们会得到 AI Fund 直接提供的数百万美元初始资金。这意味着初创公司可以自由地招募人才、购买数据，迅速扩大公司规模。」AI Fund 的另一个优势在于，以这样高起点开始成长，各类 AI 创业公司可以有更长的孵化时间，它们不必再过早地把自己想要做的事情披露给媒体、或大量投资人了（而只希望其中的一个投资自己）。 据介绍，目前 AI Fund 的投资团队已经接手三个项目。其中包括此前吴恩达创立的制造业 AI 公司 Landing.ai，也包含目前无法透露的「新兴行业项目」。吴恩达认为目前的人工智能领域中蕴含着无数的机会，大量项目和产品值得投资。这家投资机构会专注于 AI 领域最为重要的资源——人才，并希望自己的行动不仅可以接近 AI 人才，还能创造更多的 AI 人才。 实际上，带领和指导新成立的 AI 团队对于这位深度学习著名研究者而言并不陌生。「当我前几年在百度领导 AI 业务时，我工作的一部分就是构建面向新机会的 AI 研究队伍，随后慎重地作出决定，选择是否会继续跟进某些方向。」吴恩达介绍道。「在百度时，我有时候同时在领导着六个新形成的项目。我认为 AI 产业在未来会变得越来越重要，同时新公司的构建思路也是可重复的。所以，在 AI Fund，我可以通过在百度 AI Group 期间学到的经验，以及领导 Google Brain 期间的经验来帮助人们继续构建新的 AI 项目。」 我们都或多或少地接触过「人工智能正循环」这个概念——哪家公司拥有越多的数据、更完善的产品，拥有越多的用户，谁的护城河就越高，技术进步就越快。AI Fund 的创建者们认为当前 AI 创业最重要挑战在在于要在六个月内快速发展出自己的产品——形成比别人更快的正循环，快过竞争对手一步，从而构建自己的护城河。AI Fund 提供的快速孵化将为创业者们带来优势，并逃离融资陷阱。 此外，吴恩达也一直在思考 AI 在自动化进程中将要扮演的角色。AI Fund 将持续关注并加速各类职业教育解决方案的项目。从教育到创业投资，我们可以看到，吴恩达对于人工智能产业的布局已经全面展开。 在 AI Fund 正式宣布成立前夕，机器之心对吴恩达进行了专访，我们和他聊了聊新成立的公司、近期的多个创业项目，以及人工智能业界趋势等话题。 机器之心：为什么要成立 AI Fund？   吴恩达： 这个主意是我们在去年某个时候产生的。我认为那些世界顶级的企业都将经历 AI 转型，而我们在考虑为这一时期的业界提供不同的选择。我们关注 AI 的原因是 AI 技术在近两年出现了非常大的进展，出现了大量机遇，涌现出大量新型技术，达到了过去两年根本不可能实现的成就。 在很多 AI 商业应用中，技术是非常重要的。例如人脸识别，准确率很重要；例如语音识别，Amazon Echo 或百度渡鸦，准确率也是很重要的。因此我们确实需要非常强大的 AI 团队去构建产品和技术，而我们在这方面很幸运，无论是创新还是产品都很棒。 机器之心：未来，AI Fund 是否会投资来自中国的创业公司？ 吴恩达： 我得说 we are open. 我经常去中国，也会从全球的角度看待投资。目前，我们专注于美国加州，但是不久之后我们一定会走出加州。 机器之心：您在学界取得了很大成功，又曾在谷歌、百度领导研究团队，现在又开始自己创业。请问您对自己的经历作何评价？ 吴恩达： 我很幸运能够在那么多优秀的地方工作过，优秀的大学、公司，受益匪浅。说实话，我认为李彦宏是最好的 CEO，我很荣幸曾与他共事。Larry Page 也是最伟大的 CEO 之一，很荣幸能在谷歌工作。 我认为在 AI 时代，一些企业将向 AI 企业转型。谷歌、百度在这方面做得很好。IT 时代有很多创业公司，如谷歌、Facebook、亚马逊、百度，也有一些成立较久的公司，如苹果、微软成功转型成为互联网公司。我认为在 AI 时代，一些大公司，如百度和谷歌会实现成功转型，同时，也会出现成功的创业公司。 机器之心：您目前还留有斯坦福大学的教授职位，怎样平衡公司业务与学术研究？ 吴恩达： 我认为这应该归功于我所遇到的那些优秀的团队。正是因为我遇到的这些优秀的团队和领导，我才有精力在斯坦福大学继续任教。现在，我需要把相当部分的精力投入于 Landing.ai 和 AI Fund 之中了。 机器之心：同时创办好几家公司并不是所有人都具备的能力，国内不少投资人一般不太会投那些无法专注于一家公司的创始人，怎样看待这个问题？ 吴恩达： 当我在百度的时候，我们会同时建立多种业务，我认为不同的人应该做不同的事。你知道，在人工智能变革互联网公司中，我为自己所扮演的角色已经相当自豪。但现在我想要变革更多的产业——所以我建立了 Landing.ai 和 AI Fund。是否「专注于一家公司」，这是需要根据自己的情况决定的。 机器之心：Landing.ai 涉足对流水线工人的职业 AI 技术培训，这是一个很有趣的方向，您认为这样的基础教育在未来将会如何开展？ 吴恩达： 你知道我大约在五六年前就开始做 MOOC 课程，希望能够为人们提供方便廉价的人工智能教育内容。我喜欢教育，这也是我在 2017 年宣布了 deeplearning.ai 的原因，我想要教更多人学习 AI。所以现在我依然花费大量时间在线上、在斯坦福大学、在公司里向人们教授人工智能。我乐于与大家分享有关人工智能的想法，因为我认为人工智能领域还有大量的工作需要去做。我自己、我的团队、甚至我建立的公司都不可能完成其中的所有任务，所以很高兴整个 AI 社区能不断学习、做极好的 AI 项目。 机器之心：在具体的应用上，您觉得 2018 年还有哪个行业受到深度学习的影响会更大？ 吴恩达： 我认为大部分产业都会深受影响，我自己会花大量时间在制造业上。例如，我们发现在很多工厂中，有成百上千的工人用眼睛检查物件的瑕疵。我们发现人工智能能够自动化该流程做质量检测。从谷歌和百度这样的大公司我们能够看到，AI 不只是用于一件事，而是能用于很多事，我希望在制造业中也是如此。所以，在工厂中，AI 不止能做检测，还有维护、自动化等。 我相信还有其他的产业，但现在我重点是专注于制造业。 机器之心：目前的机器学习能否摆脱对于大量数据的依赖？在这点上，哪一种新的方法最具前景？ 吴恩达： 我认为迁移学习被低估了，很多人没有意识到迁移学习的强大力量。在计算机视觉应用中，为了一个任务我们做的大量的工作是从百万张图像中学习。而在其他的任务中，我们可能没有这么多的图像。迁移学习的技术还在发展中，虽然有很多人听过迁移学习，但同样很多人意识已到它的重要性：从有大量数据的任务迁移到数据较少的任务。 在工作中，我们用到了很多迁移学习。我们发现在数据缺乏的任务中它是一个非常强大的工作，无论是对计算机视觉、自然语言处理，甚至在语音识别中都有使用案例。 以下为吴恩达博客中发布的 AI Fund 启动公告： AI Fund：构建具备变革能力的 AI 创业公司 亲爱的朋友们， 很高兴在这里宣布 AI Fund 的成立。我们已经募集了 1.75 亿美元的资金，未来将相继启动新的商业项目，利用 AI 改善人们生活。随着这些商业计划的进展，我们也希望能有更多人加入到 AI 领域中，为建立 AI 驱动的社会做出重要的贡献。 很感谢我们的投资者，感谢来自 NEA、Sequoia、Greylock Partners、软银集团和其它机构的支持。 在电气时代的早期阶段，很多创新都集中于照明设备的小步幅的改进。尽管这是很重要的基础建设，但真正带来革新的应用，即电力对多个行业带来的大幅改变，还需要更长的时间才会出现。AI 正是新的电力，目前我们正处于相似的转折点。 在带领百度的 AI 团队的时候，我的部分工作是相继地建立团队探索新方向，然后系统性地评估这些方向的潜在价值，并决定是否要进一步发展相应的 AI 商务。这个过程非常有趣，也让我更加明确地理解了自早期在谷歌大脑时就感受到的事情：我们可以开发系统性和可重复的流程，以开创和追寻新的 AI 机遇。 我们将会见证 AI 改变几乎所有的行业。AI Fund 团队目前正追赶着三个新的 AI 驱动方向，其中一些我们希望稍后向公众宣布。随着这些项目成熟并转向商业应用，AI Fund 将向这些团队提供额外的资金，从而加速其进展，避免因融资而分散注意力。六个月的时间足以决定一个新的 AI 方案是否能在竞争中占据优势，因此我们建立了 AI Fund，让我们团队的研究进展尽快进行。它还可以让我们的团队在做好充足准备的时候才公布他们的研究。 上个月我宣布成立 Landing.AI，一个致力于通过 AI 革新制造业的项目。而 AI Fund 将为 Landing.AI 提供资金支持，加速其发展。 正如任何社会转型一样，向人工智能的过渡也会让一些人感到震惊，其中一个迫切的问题是缓解 AI 和自动化对就业造成的影响。我的团队正持续大量地投资全球的培训项目，希望确保每个人可以学到 AI 时代所需的技能，每个人都有机会从事有意义的工作。 我将作为普通合伙人（General Partner）领导 AI Fund，Eva Wang 是合伙人和 COO，Steven Syverud 是另外一名合伙人。Eva 是 Fenwick & West 的前合伙人，擅长资金运作，熟悉相关法律知识；Steve 曾是 Sycamore CEO，领导过 Coursera Specializations 产品的开发，他的专长是产品和业务拓展。 如果你从事 AI，你要学会做重要的事。正如托马斯·爱迪生开启了电力赋能时代，现在 AI 赋能的时代即将到来。这正是 AI Fund 要做的事情，推动全球的重大转变。 Andrew Ng 
315,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736815&idx=3&sn=b7e2cc5e0e80d51ca2baad3a5d2ec262&chksm=871accd1b06d45c7fc780c8cfae735770918c6ead2d72c50fdce79ccf590171a8b40786db214&scene=27,学界 | 从监督式到DAgger，综述论文描绘模仿学习全貌,"模仿学习是学习器尝试模仿专家行为从而获取最佳性能的一系列任务。目前主流方法包括监督式模仿学习、随机混合迭代学习和数据聚合模拟学习等方法。本论文全面概述了当前的模拟学习算法及其收敛性保障，有助于读者深入了解这一领域。 模仿学习（Imitation Learning）背后的原理是是通过隐含地给学习器关于这个世界的先验信息，就能执行、学习人类行为。在模仿学习任务中，智能体（agent）为了学习到策略从而尽可能像人类专家那样执行一种行为，它会寻找一种最佳的方式来使用由该专家示范的训练集（输入-输出对）。当智能体学习人类行为时，虽然我们也需要使用模仿学习，但实时的行为模拟成本会非常高。与之相反，吴恩达提出的学徒学习（Apprenticeship learning）执行的是存粹的贪婪/利用（exploitative）策略，并使用强化学习方法遍历所有的（状态和行为）轨迹（trajectories）来学习近优化策略。它需要极难的计略（maneuvers），而且几乎不可能从未观察到的状态还原。模仿学习能够处理这些未探索到的状态，所以可为自动驾驶这样的许多任务提供更可靠的通用框架。在此论文中，作者首先介绍了相关的通用框架，然后展示了一些主要的模仿学习算法和保证收敛的方法。最后，作者们在现实应用上实验了 DAgger 方法，并给出了结果。 论文：Global overview of Imitation Learning 论文地址：https://arxiv.org/abs/1801.06503 摘要： 模仿学习是学习器尝试模仿专家级行为从而获取最佳表现的一系列任务。近期，该领域提出了数种算法。在此论文中，我们旨在给出这些算法的整体回顾，展示他们的主要特征并在性能和缺点上做出对比。 从示范中学习（LfD,Learning from Demonstration）是一个从专家给出的轨迹中学习复杂行为的实践框架，即使非常少或者不准确。我们在下面列出并对比了模型学习中最常用到的一些算法，且在自动驾驶汽车上进行实验对比这些模型算法间的不同。下面这些算法的理论证明和直观定理展示在附录部分。 3.1 监督学习 解决模仿学习的首个方法是监督学习。我们有一个由专家给出的训练轨迹集合，其中单个训练轨迹包含观察结果的序列专家行为的序列。监督式模仿学习的背后动机是训练一个分类器基于观察来模仿专家的行为。 这是一种被动的方法，目的是通过被动的观察全部轨迹学习到一种目标策略。监督式模仿学习的目标是在专家的驱动下于所有状态上训练一个策略，而专家只有在求解该目标时才提供信息。此外，我们需要假不同轨迹中的专家行为是独立同分布的。 这种方法的主要问题是它不能从失败中学习。假设该模型在某个时间步骤衍生出最优化的轨迹，那就无法返回到专家看到过的状态了，因此就会生成错误。总的来说，该朴素算法难以泛化到未知场景，下面的这种方法改正了这种问题。 3.2 前馈训练 前馈训练算法是 2012 年由 Ross 和 Bagnell 提出，它在每个时间步 t（t 属于全部时间步 T）上训练一种策略 π_t。即在每个 t 上，选择机器学习策略 π_t 以模仿专家策略 π*。这种迭代性训练在以下算法 1 中有详细描述： 在最糟糕的情况中，我们与经典监督式学习有相同的收敛，但总体讲，收敛是次线性的（sublinear），并且专家策略成功恢复了模型策略的错误。因此，前馈训练算法应该比之前算法表现更好。 但是，前馈训练法的一个主要缺点是它需要迭代所有的 T 周期，其中时间范围 T 可以相当大。因此，假设策略是不稳定的，算法将在实际应用中（T 过大或未定义）变得不切实际。后续的一些算法克服了这一问题。 3.3 基于搜索的结构预测（SEARN） SEARN 的想法来自 Daumé III et al. (2009) [3]，其并没有学习一些全局模型与搜索（一般模型的标准设置），而是简单地学习一个分类器以最优地搜索每一个决策。算法通过在每一步上遵循专家行动开始，迭代地收集示范并利用它们训练新策略。根据之前所有训练策略以及专家行动的混合，它通过采取行动编译了新的 episodes。最后，它慢慢地学习遵循混合策略并停止依赖专家以决定其要采取的行动。 简言之，该算法试图学习一个分类器，引导我们通过搜索空间。它的运作是通过保持当前策略以试图使用它生成新的训练数据，进而学习一个新策略（新分类器）。当学习了一个新分类器之后，我们用旧分类器进行插值。这一迭代如图 2 所示。 然而，这一基于搜索的结构预测可能过于乐观，并在实践中面临挑战，这主要因为其初始化方法不同于最优策略。下面我们将详细描述克服这一问题的其他方法。 3.4 随机混合迭代学习（SMILe） SMILe 同样由 Ross 和 Bagnell (2010)[2] 提出，以纠正前馈训练算法中的一些困难问题。它是一个基于 SEARN 的随机混合算法，利用其优点可以使实现更加简单，而且与专家的交互要求也较低。它在多次迭代中训练一个随机静态策略，接着利用训练策略的「几何」随机混合。 具体而言，我们从一个完全遵循专家行动的策略π_0 开始。在每次迭代中，我们训练一个策略π_i 以在以前的策略π_i-1 诱导的轨迹下模仿专家。接着，我们将新的训练策略添加到先前的几何折扣因子α(1 − α)^(i−1) 的策略混合中。因此，新的策略π_i 是 i 策略的混合，其中使用专家行动的概率为 (1 − α)^ i。算法 3 描述了 SMILe 算法。 这一方法的主要优势是我们可以随时中断过程，以便不考虑太大或未定义的时间范围。不幸的是，由于其随机政策，模型并不稳定。 3.5 基于归约（Reduction）的主动模拟学习（RAIL） Ross et al. (2011) [4] 提出的 RAIL 背后的原理是对独立同分布 (i.i.d) 主动学习器 L_a 执行一个包含 T 次调用的序列。我们注意到在 T 次调用全部发出之前就很可能能够很好地找到一个有用的静态策略，这有助于缓解前向循环的缺陷。实际上，这种主动学习器可以在一个时间点范围内提出查询请求，我们预计在之前的迭代中学习到的策略可能能够在整个范围内都取得不俗的表现。 具体而言，RAIL 会以显著不同的方式执行 T 次迭代：在每次迭代中都学习一个新的可以应用于所有时间步骤的静态策略。该模型的 t+1 次迭代会根据之前策略的状态分布学习一个新策略，这个新策略会在预测专家动作上实现较低的误差率。算法 4 给出了 RAIL 算法的描述。 RAIL 是一种理想化的算法，其目的是用于实现理论目标的分析。但是，主要由于在早期迭代中使用了未标注的状态分布（可以与 d_π∗ 有很大的差异），这种算法在实际应用上可能有很多低效的地方。 3.6.1 DAgger Ross 和 Bagnell 在 2010 年提出了 DAgger [5] 算法来解决从示范中学习的问题。DAgger 是一种迭代式的策略训练算法，使用了一种归约到在线（reduction to online）的学习方法。在每次迭代中，我们都在该学习器所遇到过的所有状态上重新训练主要分类器。DAgger 的主要优势是使用了专家（expert）来教学习器如何从过去的错误中恢复过来。这是一种基于 Follow-The-Leader 算法（每一次迭代都是一个在线学习示例）的主动式方法（我们需要读取专家本身）。 我们从完全由专家教授的第一个策略 π_0 开始，运行 π_0，看学习器访问了什么配置。我们生成一个新的数据集，其中包含有关如何从 π_0 的错误中恢复的信息。因为我们希望同时有来自 π_0 和 π_1 的信息，所以我们联合使用起始的仅有专家的轨迹和新生成的轨迹来训练 π_1。我们在每次迭代过程中都重复这一过程。我们选择在验证测试上表现最好的策略。 SEARN [3] 和 DAgger 之间的主要算法差异是每次迭代过程中分类器的学习以及将它们组合成一个策略方面。DAgger 可以组合在所有迭代中获得的训练信号，与之相反，SEARN 仅在第 i 次迭代上训练，即不聚合数据集。SEARN 是第一种实用的方法，之后是 DAgger。DAgger 对复杂问题和简单问题都适用；当收集的数据越多时，它的效果就越好，但仅需少量迭代就有效果。所以对手写识别和自动驾驶等很多应用而言，DAgger 都很有用。 3.6.2 DAgger by coaching 使用 DAgger 时，策略空间可能与学习策略空间相距甚远，这会限制学习能力，也可能无法根据状态推断得到信息。为了防止出现这种情况，HHH Daumé III 等人在 2012 年提出了 DAgger by coaching 算法 [6]。使用这一算法，我们可以执行易于学习的动作，即在学习器的学习能力范围内。当动作太难时，教练（coach）会降低目标然后渐进地教授。 算法 5 描述了 DAgger 算法和 DAgger by coaching 算法。 3.7 使用示范的近似策略迭代（APID） 对于之前的算法，我们都假设专家能表现出最优行为并且它们的示范是充分足够的。在真实世界中，这些假设并不总是有效的；为了解决这一艰难问题，我们将专家数据和交互数据结合到了一起（即混合 LfD 和 RL）。因此，APID（2013）[7] 尤其关注专家示范很少或不是最优的情况。这是一种使用近似策略迭代（API）方法正则化过的 LfD，其关键思想是使用专家的建议来定义线性约束，这些线性约束可以引导 API 所执行的优化。 具体来说，我们设置一个 API 的环境，然后使用专家提供的额外信息（尽管这些信息很少或不准确）。V^π 和 Q^π 表示 π 的价值函数和动作-价值函数，V∗ 和 Q∗ 表示最优策略 π∗ 的价值函数和动作-价值函数。我们有一个交互数据集  分别对应一个专家示例集合  ，也就是一个包含 n 个示例的（状态，动作）对样本分别对应于一个包含 m 个示例的（状态、示范动作）对。为了编码专家的次优性，我们为动作-价值最优策略增加了一个变量，以允许偶尔违反约束条件。最后，我们得到一个有约束的优化问题。在这种方法中，我们不必获取确切的 Bellman 算子 T^π，只需要样本即可，由此我们可以使用 Projected Bellman 误差。 3.8 聚合值以进行模拟（AggreVaTe） Ross 和 Bagnell (2014) [8] 提出的 AggreVaTe 是 DAgger 算法的一种扩展，可以学习选择动作以最小化专家的 cost-to-go（总成本），而不是最小化模拟其动作的 0-1 分类损失。第一次迭代中，我们通过观察执行该任务的专家来被动地收集数据。在每个轨迹中，在一个一致的随机时间 t，我们探索状态 s 中的动作 a，并观察执行该动作后专家的 cost-to-go Q。 我们使用 表示在状态 s 执行动作 a 的期望未来 cost-to-go，之后再执行 t-1 个步骤的策略 π。 和 DAgger 算法完全一样，AggreVaTe 通过与学习器交互来收集数据，方式如下： 在每次迭代中，我们都使用当前学习器策略 π_i 来执行任务，在一个一致的随机事件 t 中断，探索当前状态 s 中的动作 a，之后将控制回过来提供给专家，继续直到达到时间范围 T。 结果在当前策略 π_i 所访问过的状态分布下，得到专家 (s, t, a, Q) 的 cost-to-go 的新示例。 然后我们聚合数据集，并在所连接的数据集上训练策略 π_(i+1)。 下图是对此算法的完整描述： "
316,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736815&idx=2&sn=ec37871de475271c619b5fcfd4e0bdc5&chksm=871accd1b06d45c7eaf204b240e11be99e7997ba8f396055cbe173179c2dc7e49ebe5e3d79c8&scene=27,深度 | 解读R-Net：微软「超越人类」的阅读理解人工智能,人工智能的阅读能力在某些方面已经超越了人类，微软的 R-Net 就是达到了这一里程碑的人工智能之一。近日，谷歌工程师 Sachin Joglekar 在 Medium 上发文对 R-Net 进行了直观的介绍。 R-Net 论文：https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf 今年 1 月 13 日，很多媒体的新闻报道称微软和阿里巴巴开发的人工智能在 SQuAD 数据集测试上，阅读能力上超越了人类。尽管这并不完全准确，但这些公司所开发的模型确实能在某些阅读任务的某些指标上超越人类水平。这篇文章为微软实现这一成果背后的人工智能 R-Net 提供了一个直观的介绍。   首先，给出阅读问题……   给定一个段落 P：   「特斯拉于 1856 年 7 月 10 日（旧历法的 6 月 28 日）出生于奥地利帝国斯米连村（现属克罗地亚）的一个塞族家庭。他的父亲米卢廷·特斯拉是一位塞尔维亚东正教神父。特斯拉的母亲是久卡·特斯拉（娘家姓为 Mandić），她的父亲也是一位东正教神父；:10 她非常擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力。久卡从没接受过正规教育。尼古拉将自己的记忆和创造能力归功于他母亲的遗传和影响。特斯拉的祖先来自塞尔维亚西部靠近黑山的地方。:12」   然后询问一个问题 Q：   「特斯拉的母亲具有怎样的特殊能力？」   然后提供一部分连续文本作为答案 A：   「擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力」 斯坦福问答数据集（SQuAD）包含大约 500 篇文章，涉及的问答对数量接近 10 万（上面给出的例子就取自其中）。   在我们介绍微软的用于阅读理解的方法之前，我们先简要介绍一下他们论文中大量使用的两个概念：   1. 循环神经网络（RNN）   RNN 是一种特殊的神经网络，可用于分析时间（或序列）数据。标准的前馈神经网络没有记忆的概念，而 RNN 则通过使用「反馈的」语境向量（context vector）而将这一概念整合了进来：   从本质上讲，其在任何时间步骤 t 的输出都是过去语境和当前输入的一个函数。   双向 RNN（BiRNN）是一种特殊的 RNN。标准 RNN 是通过「记忆」过去的数据来记忆历史语境，而 BiRNN 还会从反方向进行遍历以理解未来的语境：   需要指出，尽管 RNN 理论上可以记住任何长度的历史，但通常来说整合短期语境比整合长期信息（相距 20-30 步以上）更好。   注：R-Net 使用 RNN（更具体来说是门控循环单元）的主要目的是模拟「阅读」文本段落的动作。   2. 注意力（attention）   神经网络中的注意力是根据人类重点关注视觉输入中的特定部分并大略查看其余部分的观看方式而建模的。   注意力可以用在这样的应用中：你的数据点集合中并非所有部分都与当前的任务有关。在这样的情况下，注意力是作为该集合中所有点的 softmax 加权平均而计算的。其权重本身则是作为 1）向量集和 2）某个语境的某个非线性函数而计算的。   注：R-Net 使用了注意力来在另一些文本的语境下突出文本的某些部分。   从直观上讲，R-Net 执行阅读理解的方式与你我进行阅读理解的方式相似：通过多次（准确地说是 3 次）「阅读」（应用 RNN）文本以及在每次迭代中越来越好地「微调」（使用注意力）各词的向量表征。   让我们分开解读其中的每一次过程……   第一次阅读：略览   我们从标准的 token（即词）向量开始，使用了来自 Glove 的词嵌入。但是，人类通常理解一个词在其周围词所构成的语境中的含义。   比如这两个例子：「May happen」和「the fourth of May」，其中「May」的含义取决于周围的词。另外也要注意背景信息可以来自前向，也可以来自反向。因此，我们在标准词嵌入之上使用了 BiRNN，以得到更好的向量。   问题和段落上都会应用这个过程。 第二次阅读：基于问题的分析   在第二次阅读中，该网络会使用文本本身的语境来调节来自段落的词表征。   让我们假设你已经在该段落的重点位置了：   「……她非常擅长 家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力。久卡从没接受过正规教育……」   在有了「制作」的前提下，如果你在问题 token 上应用注意力，你可能会重点关注：   「特斯拉的母亲具有怎样的特殊 ？」   类似地，网络会调整「制作」的向量，使之与「能力」在语义上靠得更近。   该段落中的所有 token 都会完成这样的操作——本质上讲，R-Net 会在问题的需求和文章的相关部分之间形成链接。原论文将这个部分称为门控式基于注意力的 RNN（Gated Attention-based RNN）。 第三次阅读：有自知的完整的段落理解 在第一次阅读过程中，我们在 token 临近周围词的语境中对这些 token 进行了理解。 在第二次阅读过程中，我们根据当前问题改善了我们的理解。   现在我们要鸟瞰整个段落，以定位那些对回答问题真正有帮助的部分。要做到这一点，光是有周围词的短期语境视角是不够的。考虑以下部分中突出强调的词：   特斯拉的母亲是久卡·特斯拉（娘家姓为 Mandić），她的父亲也是一位东正教神父；:10 她非常擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的 。久卡从没接受过正规教育。尼古拉将自己的记忆和创造 归功于他母亲的遗传和影响。   这都是指特斯拉的母亲所具有的能力。但是，尽管前者确实围绕描述该能力的文本（是我们想要的），但后面的能力则是将它们与特斯拉的才能关联了起来（不是我们想要的）。   为了定位答案的正确起始和结束位置（我们会在下一步解决这个问题），我们需要比较段落中具有相似含义的不同词，以便找到它们的差异之处。使用单纯的 RNN 是很难完成这个任务的，因为这些突出强调的词相距较远。   为了解决这个问题，R-Net 使用了所谓「自匹配注意力（Self-Matched Attention）」。 为什么需要自匹配？   在应用注意力时，我们通常会使用一些数据（比如一个段落词）来衡量一个向量（比如问题词）的集合。但在这个迭代过程中，我们会使用当前的段落词来衡量来自该段落本身的 token。这有助于我们将当前词与段落其余部分中具有相似含义的其它词区分开。为了增强这个过程，这个阅读阶段是使用一个 BiRNN 完成的。   在我看来，使用自匹配注意力这个步骤是 R-Net 最「神奇」的地方：使用注意力来比较同一段落中相距较远的词。   最后一步：标记答案   在最后一步，R-Net 使用了一种指针网络（Pointer Networks）的变体来确定答案所处的起始和结束位置。简单来说：   我们首先根据问题文本计算另一个注意力向量。这被用作这一次迭代的「起始语境（starting context）」。使用这个知识，再为该起始索引计算一组权重（为该段落中的每个词）。得到最高权重的词作为其答案的「起始位置」。   除了权重之外，这个两步 RNN 还会返回一个新的语境——其中编码了有关该答案的起始的信息。   再将上述步骤重复一次——这一次使用新的语境而不是基于问题的语境，用以计算该答案的结束位置。   搞定！我们得到解答了！（实际上，我们上述例子中给出的答案就是 R-Net 实际得出的答案。）   如果你对 R-Net 的详细细节感兴趣，请阅读他们的论文。如果代码能帮助你更好地理解（至少对我而言是如此），请参阅 YerevaNN 试图用 Keras 重建 R-Net 的精彩博文：http://yerevann.github.io/2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras/。 
317,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736815&idx=4&sn=21c703b45852a14f68c4f0754c3ad3fb&chksm=871accd1b06d45c79bbce2f17fbcc6c832080582559ac12c78361f65b2467054ba6c98e08a56&scene=27,公告 | 机器之心完成 A 轮融资，坚持全球化，探索新产品,近日，机器之心完成 A 轮融资，本轮融资由星路资本领投，人民网、TalkingData、百度风投和高捷资本跟投，上轮投资方科大讯飞继续跟投。 上轮融资后，机器之心在内容、数据产品、全球化布局、活动和产业服务等方面都取得了快速发展，在保证持续输出高质量内容的同时，也对新产品和新服务进行了积极拓展并取得了显著效果。 内容及产品 内容方面，公众号「机器之心」持续对最新研究成果、论文、学术会议、技术趋势和工程实现等进行报道，用户和阅读量一直保持着高速增长；发布了子品牌「机器之能」，关注人工智能的具体落地及产业化，采访报道了近百家人工智能创业公司和及相关行业解决方案。截止目前，两个微信公众账号的订阅人数合计 40 万，今日头条、知乎等其他内容平台订阅用户 50 万，累计阅读量 8000 万。 推出两份付费报告，为技术人员提供技术趋势参考的「人工智能技术趋势报告」及关注产业动态的「Industry Bi-weekly 全球人工智能行业评论」，得到广泛订阅和高度好评。 此外，公司还上线了首款数据库产品「AI 商用搜索」（人工智能应用实例库），以解决人工智能技术方与需求方之间的信息不对称，推动人工智能技术在各行业的落地，目前已经收录全球范围内人工智能应用实例近 2000 条，举办了多场闭门产业分享及对接活动，以「AI 商用搜索」为基础成功完成了十余次产业上下游对接服务。 全球化 机器之心于 2017 年初正式建立离岸团队，并积极进行本地化运营，在报道、数据、活动及产业服务等多个维度与众多海外业内顶级机构系统建立合作关系，进行双语原创内容生产，扩展海外市场，助力本土团队开展跨国业务及产业服务项目；建立并系统运营机器之心全球站及 Medium 英文内容原创账号 Synced，为全球读者提供技术博客，行业趋势新闻及报道，分析师报告等优质原创英文内容，制作发布机器之心中国机器智能产业原创报道的英文版本。 机器之心离岸记者及分析师团队现场报道了众多国内外高质量学术及产业会议，对美国、加拿大、欧洲等海外地区的优质机器智能公司进行了一手调研及采写；深入全球机器智能社区，对海外核心的机器智能中心进行实地探访，制作并发布多篇全球机器智能生态调查系列特稿，通过一手调研及系统的信息内容梳理真实地展现不同国家地区的机器智能中心格局异同。 在过去一年里，机器之心全球站发布超过 300 篇高质量英文原创内容，已成为机器智能垂直领域唯一具有全球品牌认知度的中国媒体；成为知名英文内容发行平台 Medium 授予 AI Top Writer 称号的 50 个内容账号之一，平台账号订阅数超过近八成获得此官方称号的 Medium 内容生产者；机器之心全球站内容多次获得业内顶尖专家，如 Yann LeCun 教授，Martin Mueller 教授等的公开认可及赞赏，获得来自联合国 ITU、Nature、Invest in Canada（加拿大政府）等官方社交媒体账号或官方网站的转发转载。 活动及评选 活动方面，机器之心举办了首届 GMIS 2017 全球机器智能大会，邀请了全世界 47 位顶级专家出席，共计 3000 人次专业观众出席，本次大会也得到人民日报、北京电视台及众多媒体的报道。同时，在中国、美国、加拿大举办了 6 场数据竞赛；在 NIPS、IJCAI、ICML 等学术会议期间举办了多场华人学者聚会。 2018 年年初，机器之心公布了首届年度奖项「Synced Machine Intelligence Awards」，包括「全球三十大最佳 AI 创业公司」、「中国十大最强 AI 研究实力企业」、「中国十大最具潜力早期 AI 公司」及「三十大最佳 AI 应用案例」，奖项征集期间共收到数百家企业的申请，包括行业巨头、垂直领域独角兽和早期创业公司等，覆盖医疗健康、金融、安防、交通、零售、教育等多个重要领域，最终评选结果的权威性、客观性和创新性得到专业人士的高度认同。 奖项及成绩 Medium AI Top Writer 今日头条「2017 年度榜单科技头条」 有道云笔记「2017 微信价值排行榜——科技智能第 2 名」 品途「2017 年度产业影响力媒体 Top20」 多次获得「知乎×新榜影响力机构号」 唯一一家通过 NIPS 2017 大会官方媒体审核的中国媒体 受邀参加联合国电信联盟 (ITU)「AI for Global Good Summit」的两家中国媒体之一（另外一家为 CCTV） 唯一受邀报道加拿大 C2 峰会的中国科技媒体 唯一受邀参与 Vector Institute 开幕式的亚洲媒体 作为官方受邀媒体参加 Google I/O、英伟达 GTC 等全球知名技术大会 接下来，机器之心将持续为用户提供高质量内容和产业服务，在全球化的社区建设、内容、数据、活动、服务等方面继续拓展，并将陆续推出全新的信息化产品。 对于本轮融资，星路资本董事、星路鼎泰大数据产业股权投资基金执行事务合伙人戚娟娟表示，在创始人赵云峰的带领下，机器之心拥有一支锐意进取、蓬勃创新的团队，在科技与媒体之间寻找到良好的结合点，使我们看到了中国媒体智能化变革的可能性。围绕大数据与人工智能的发展，媒体技术也必将从新媒体提升到更高阶段，信息与用户将以更有效率的方式得以链接。机器之心对此正在进行积极有益的探索，星路资本看好其发展思路与前景，并希望与这一优秀的团队共同进步。 人民创投总经理、人民网文化产业基金执行合伙人赵亚辉表示，机器之心是国内权威客观且极具影响力的人工智能媒体平台。人民创投此次投资机器之心，源于对其内容质量、行业影响力、媒体态度及未来转型方向的认可。人民网将与机器之心在人工智能内容的生产与发布等方面展开合作，一起推进人工智能创业项目投资和产业服务。 TalkingData 产业基金表示，机器之心是 AI 领域内容专业，调性独特的科技媒体，同时在数据科技领域的产品服务、数据竞赛等方面具有丰富经验，与 TalkingData 在 AI 与数据科学平台有很强的协同性，TalkingData 也会通过自身优势助力机器之心在 AI 与数据科学领域产品的探索。 百度风投（BV）CEO 刘维表示，很高兴又一次成为机器之心的投资人，很高兴看到机器之心的快速的发展和保持不变的专业性。机器之心汇集世界 AI 信息、服务世界 AI 社群的大格局，是 BV 作为专业 AI 基金为之共鸣和需要向其学习的。AI 社群规模将在未来十年的 AI 主战场时代有多个几何级数的增长，机器之心提供的内容、工具、服务将帮助更多人高效的融入这个时代，创造很大的社会价值。 科大讯飞表示，非常高兴能够再一次参与机器之心的投资，机器之心是目前国内集影响力、质量、品牌效应为一体的人工智能媒体。在过去的几年当中，机器之心链接国内外科技前沿，深入科技与产业的核心，服务于全球人工智能行业从业者用户。科大讯飞立足人工智能核心技术研发，创新创业十八载，始终坚持「用人工智能改变世界」，未来期待携手机器之心，在将来中国人工智能引领世界的路程中一同努力，共创辉煌！ 高捷资本创始人及管理团队合伙人黎蔓表示，机器之心是国内人工智能领域垂直媒体中的佼佼者，在行业内极具影响力，高捷资本此次投资机器之心，源于对其内容、行业影响力和团队的认可。高捷资本是一家聚焦人工智能、智能制造、新能源汽车和智能科技金融等相关领域的私募股权投资管理机构，为中国优秀的早期及早期高增长型科技公司提供资本和战略支持，管理团队在行业内拥有丰富的经验。此次，希望借助其过往丰富的经验，与机器之心一起携手，助力中国人工智能领域的发展。 机器之心正在全球招聘，请点击「 」查看职位信息。 
318,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736777&idx=3&sn=f81e65b75d69f9eb29a24e961c83ddb0&chksm=871accf7b06d45e1a967cd890649059d3e3c21a3d7fce72018b558e9d4463218fd429f5af0b2&scene=27,资源 | NiftyNet：开源的卷积神经网络和医疗影像分析平台,NiftyNet 近日，外科研究中心 WEISS、UCL 医疗影像计算中心（CMIC）和 HIG 等研究机构共同开源了 NiftyNet，即基于卷积神经网络的医疗影像分析平台，它为研究社区提供一个开放的机制来使用、适应和构建各自的医疗影像研究成果。 NiftyNet 是一个基于 TensorFlow 的开源卷积神经网络平台，来研究医疗影像分析和影像导向的治疗。NiftyNet 有着模块化的架构设计，能够共享网络架构和预训练模型。使用该模块架构，你可以： 使用内建工具，从建立好的预训练网络开始； 根据自己的图像数据改造已有的网络； 根据自己的图像分析问题快速构建新的解决方案。 NiftyNet 官网地址：http://www.niftynet.io/ GitLab 代码地址：https://cmiclab.cs.ucl.ac.uk/CMIC/NiftyNet 特征 NiftyNet 现在支持医疗影像分割和生成式对抗网络。该开源平台并非面向临床使用，其他的特征包括： 易于定制的网络组件接口； 共享网络和预训练模块； 支持 2D、2.5D、3D、4D 输入； 支持多 GPU 的高效训练； 多种先进网络的实现（HighRes3DNet、3D U-net、V-net、DeepMedic）； 对医疗影像分割的综合评估指标。 网络 在 NiftyNet 框架中，实现了众多模型，列表如下。所有的网络都可被应用在 2D、2.5D、3D 配置中，并且使用自己的默认参数来实现。 损失函数  NiftyNet 框架当前版本包含多种损失函数，包括 Dice Loss 等非常适合医疗影像任务的损失函数： 论文：NiftyNet: a deep-learning platform for medical imaging 论文链接：https://arxiv.org/abs/1709.03485 基于深度学习的医疗影像分析和计算机辅助诊断正逐渐成为主要的解决方案。虽然目前的深度学习框架是非常灵活与便捷的，但并不为医疗影像分析提供具体的功能，因此开发者需要大量的实现与试验才能构建医疗影像方面的应用。因此，许多研究团队存在大量重复的努力和不完整的基础设施开发。本研究提供了一个开源的深度学习医疗影像平台 NiftyNet。NiftyNet 的目标是加速和简化这些解决方案的开发，并为研究社区提供一个开放的机制来使用、适应和构建各自的研究成果。 NiftyNet 为各种医疗影像应用提供模块化的深度学习流程，包括语义分割、回归、图像生成和表征学习等常见的医学影像任务。NiftyNet 的处理流程包括数据加载、数据增强、网络架构、损失函数和评估指标等组件，它们都是针对并利用医学影像分析和计算机辅助诊断的特性而构建的。NiftyNet 构建在 TensorFlow 上，默认使用 TensorBoard 支持二维、三维图像和计算图的可视化。 我们使用 NiftyNet 构建了三个说明性的医疗影像分析应用：（1）对多个腹部器官计算机断层扫描（computed tomography）图像进行语义分割。（2）图像回归（image regression）以根据脑磁共振图像预测 CT 衰减图。（3）为指定的解剖姿势生成模拟的超声波图像。 NiftyNet 能使研究者快速开发和部署深度学习解决方案，并在构建语义分割、图像回归、图像生成和表征学习等应用外拓展新型解决方案。 
319,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736815&idx=1&sn=be5b6938ac346f2e43a803753002bc8f&chksm=871accd1b06d45c78b6fa0a48a169482db1f0113a5c9680f36fb6dd0bb53c01dabcf5377e431&scene=27,在Python 2.7即将停止支持时，我们为你准备了一份3.x迁移指南,"目前，Python 科学栈中的所有主要项目都同时支持 Python 3.x 和 Python 2.7，不过，这种情况很快即将结束。去年 11 月，Numpy 团队的一份声明引发了数据科学社区的关注： ，全面转向 Python 3。Numpy 并不是唯一宣称即将放弃 Python 旧版本支持的工具，pandas 与 Jupyter notebook 等很多产品也在即将放弃支持的名单之中。对于数据科学开发者而言，如何将已有项目从 Python 2 转向 Python 3 成为了正在面临的重大问题。来自莫斯科大学的 Alex Rogozhnikov 博士为我们整理了一份代码迁移指南。 Python 3 功能简介 Python 是机器学习和其他科学领域中的主流语言，我们通常需要使用它处理大量的数据。Python 兼容多种深度学习框架，且具备很多优秀的工具来执行数据预处理和可视化。 但是，Python 2 和 Python 3 长期共存于 Python 生态系统中，很多数据科学家仍然使用 Python 2。2019 年底，Numpy 等很多科学计算工具都将停止支持 Python 2，而 2018 年后 Numpy 的所有新功能版本将只支持 Python 3。 为了使 Python 2 向 Python 3 的转换更加轻松，我收集了一些 Python 3 的功能，希望对大家有用。 使用 pathlib 更好地处理路径 pathlib 是 Python 3 的默认模块，帮助避免使用大量的 os.path.joins： Python 2 总是试图使用字符串级联（准确，但不好），现在有了 pathlib，代码安全、准确、可读性强。 此外，pathlib.Path 具备大量方法，这样 Python 新用户就不用每个方法都去搜索了： pathlib 会节约大量时间，详见： 文档：https://docs.python.org/3/library/pathlib.html； 参考信息：https://pymotw.com/3/pathlib/。 类型提示（Type hinting）成为语言的一部分 PyCharm 中的类型提示示例： Python 不只是适合脚本的语言，现在的数据流程还包括大量步骤，每一步都包括不同的框架（有时也包括不同的逻辑）。 类型提示被引入 Python，以帮助处理越来越复杂的项目，使机器可以更好地进行代码验证。而之前需要不同的模块使用自定义方式在文档字符串中指定类型（注意：PyCharm 可以将旧的文档字符串转换成新的类型提示）。 下列代码是一个简单示例，可以处理不同类型的数据（这就是我们喜欢 Python 数据栈之处）。 上述代码适用于 numpy.array（包括多维）、astropy.Table 和 astropy.Column、bcolz、cupy、mxnet.ndarray 等。 该代码同样可用于 pandas.Series，但是方式是错误的： 这是一个两行代码。想象一下复杂系统的行为多么难预测，有时一个函数就可能导致错误的行为。明确了解哪些类型方法适合大型系统很有帮助，它会在函数未得到此类参数时给出提醒。 如果你有一个很棒的代码库，类型提示工具如 MyPy 可能成为集成流程中的一部分。不幸的是，提示没有强大到足以为 ndarrays/tensors 提供细粒度类型，但是或许我们很快就可以拥有这样的提示工具了，这将是 DS 的伟大功能。 类型提示 → 运行时的类型检查 默认情况下，函数注释不会影响代码的运行，不过它也只能帮你指出代码的意图。 但是，你可以在运行时中使用 enforce 等工具强制进行类型检查，这可以帮助你调试代码（很多情况下类型提示不起作用）。 函数注释的其他用处 如前所述，注释不会影响代码执行，而且会提供一些元信息，你可以随意使用。 例如，计量单位是科学界的一个普遍难题，astropy 包提供一个简单的装饰器（Decorator）来控制输入量的计量单位，并将输出转换成所需单位。 如果你拥有 Python 表格式科学数据（不必要太多），你应该尝试一下 astropy。你还可以定义针对某个应用的装饰器，用同样的方式来控制／转换输入和输出。 通过 @ 实现矩阵乘法 下面，我们实现一个最简单的机器学习模型，即带 L2 正则化的线性回归： 下面 Python 3 带有 @ 作为矩阵乘法的符号更具有可读性，且更容易在深度学习框架中转译：因为一些如 X @ W + b[None, :] 的代码在 numpy、cupy、pytorch 和 tensorflow 等不同库下都表示单层感知机。 使用 ** 作为通配符 递归文件夹的通配符在 Python2 中并不是很方便，因此才存在定制的 glob2 模块来克服这个问题。递归 flag 在 Python 3.6 中得到了支持。 python3 中更好的选择是使用 pathlib： Print 在 Python3 中是函数 Python 3 中使用 Print 需要加上麻烦的圆括弧，但它还是有一些优点。 使用文件描述符的简单句法： 在不使用 str.join 下输出 tab-aligned 表格： 修改与重新定义 print 函数的输出： 在 Jupyter 中，非常好的一点是记录每一个输出到独立的文档，并在出现错误的时候追踪出现问题的文档，所以我们现在可以重写 print 函数了。 在下面的代码中，我们可以使用上下文管理器暂时重写 print 函数的行为： 上面并不是一个推荐的方法，因为它会引起系统的不稳定。 print 函数可以加入列表解析和其它语言构建结构。 f-strings 可作为简单和可靠的格式化 默认的格式化系统提供了一些灵活性，且在数据实验中不是必须的。但这样的代码对于任何修改要么太冗长，要么就会变得很零碎。而代表性的数据科学需要以固定的格式迭代地输出一些日志信息，通常需要使用的代码如下： 样本输出： f-strings 即格式化字符串在 Python 3.6 中被引入： 另外，写查询语句时非常方便： 「true division」和「integer division」之间的明显区别 对于数据科学来说这种改变带来了便利（但我相信对于系统编程来说不是）。 Python 2 中的结果依赖于『时间』和『距离』（例如，以米和秒为单位）是否被保存为整数。 在 Python 3 中，结果的表示都是精确的，因为除法的结果是浮点数。 另一个案例是整数除法，现在已经作为明确的运算： 注意，该运算可以应用到内建类型和由数据包（例如，numpy 或 pandas）提供的自定义类型。 严格排序 防止不同类型实例的偶然性的排序。 在处理原始数据时帮助发现存在的问题。 旁注：对 None 的合适检查是（两个版本的 Python 都适用）： 自然语言处理的 Unicode 输出： Python 2: 6\n�� Python 3: 2\n 您好. Python 2 在此失败了，而 Python 3 可以如期工作（因为我在字符串中使用了俄文字母）。 在 Python 3 中 strs 是 Unicode 字符串，对非英语文本的 NLP 处理更加方便。 还有其它有趣的方面，例如： Python 2:  Python 3:  这些在 Python 2 里也能正确地工作，但 Python 3 更为友好。 保留词典和**kwargs 的顺序 在 CPython 3.6+ 版本中，字典的默认行为类似于 OrderedDict（在 3.7+版本中已得到保证）。这在字典理解（和其他操作如 json 序列化/反序列化期间）保持顺序。 它同样适用于**kwargs（在 Python 3.6+版本中）：它们的顺序就像参数中显示的那样。当设计数据流程时，顺序至关重要，以前，我们必须以这样繁琐的方式来编写： 注意到了吗？名称的唯一性也会被自动检查。 迭代地拆封 默认的 pickle 引擎为数组提供更好的压缩 节省 3 倍空间，而且速度更快。实际上，类似的压缩（不过与速度无关）可以通过 protocol=2 参数来实现，但是用户通常会忽略这个选项（或者根本不知道）。 更安全的解析 关于 super() Python 2 的 super（...）是代码错误中的常见原因。 关于 super 和方法解析顺序的更多内容，参见 stackoverflow：https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods 更好的 IDE 会给出变量注释 在使用 Java、C# 等语言编程的过程中最令人享受的事情是 IDE 可以提供非常好的建议，因为在执行代码之前，所有标识符的类型都是已知的。 而在 Python 中这很难实现，但是注释可以帮助你： 以清晰的形式写下你的期望 从 IDE 获取良好的建议 这是一个带变量注释的 PyCharm 示例。即使你使用的函数不带注释（例如，由于向后兼容性），它也能工作。 多种拆封（unpacking） 在 Python3 中融合两个字典的代码示例： 可以在这个链接中查看 Python2 中的代码对比：https://stackoverflow.com/questions/38987/how-to-merge-two-dictionaries-in-a-single-expression aame 方法对于列表（list）、元组（tuple）和集合（set）都是有效的（a、b、c 是任意的可迭代对象）： 对于*args 和 **kwargs，函数也支持额外的 unpacking： 只带关键字参数的 API 我们考虑这个代码片段： 很明显，代码的作者还没熟悉 Python 的代码风格（很可能刚从 cpp 和 rust 跳到 Python）。不幸的是，这不仅仅是个人偏好的问题，因为在 SVC 中改变参数的顺序（adding/deleting）会使得代码无效。特别是，sklearn 经常会重排序或重命名大量的算法参数以提供一致的 API。每次重构都可能使代码失效。 在 Python3，库的编写者可能需要使用*以明确地命名参数： 现在，用户需要明确规定参数 sklearn.svm.SVC(C=2, kernel='poly', degree=2, gamma=4, coef0=0.5) 的命名。 这种机制使得 API 同时具备了可靠性和灵活性。 小调：math 模块中的常量 小调：单精度整数类型 Python 2 提供了两个基本的整数类型，即 int（64 位符号整数）和用于长时间计算的 long（在 C++变的相当莫名其妙）。 Python 3 有一个单精度类型的 int，它包含了长时间的运算。 下面是查看值是否是整数的方法： 其他 Enums 有理论价值，但是 字符串输入已广泛应用在 python 数据栈中。 Enums 似乎不与 numpy 交互，并且不一定来自 pandas。 协同程序也非常有希望用于数据流程，但还没有出现大规模应用。 Python 3 有稳定的 ABI Python 3 支持 unicode（因此ω = Δφ / Δt 也 okay），但你最好使用好的旧的 ASCII 名称 一些库比如 jupyterhub(jupyter in cloud)、django 和新版 ipython 只支持 Python 3，因此对你来讲没用的功能对于你可能只想使用一次的库很有用。 数据科学特有的代码迁移问题（以及如何解决它们） 停止对嵌套参数的支持： 然而，它依然完美适用于不同的理解： 通常，理解在 Python 2 和 3 之间可以更好地「翻译」。 map(), .keys(), .values(), .items(), 等等返回迭代器，而不是列表。迭代器的主要问题有： 没有琐碎的分割和 无法迭代两次。 将结果转化为列表几乎可以解决所有问题。 遇到问题请参见 Python 问答：我如何移植到 Python 3？（https://eev.ee/blog/2016/07/31/python-faq-how-do-i-port-to-python-3/） 用 python 教机器学习和数据科学的主要问题 课程作者应该首先花时间解释什么是迭代器，为什么它不能像字符串那样被分片/级联/相乘/迭代两次（以及如何处理它）。 我相信大多数课程作者很高兴避开这些细节，但是现在几乎不可能。 Python 2 与 Python 3 共存了近 10 年，时至今日，我们必须要说：是时候转向 Python 3 了。 研究和生产代码应该更短，更易读取，并且在迁移到 Python 3 代码库之后明显更加的安全。 现在大多数库同时支持 2.x 和 3.x 两个版本。但我们不应等到流行工具包开始停止支持 Python 2 才开始行动，提前享受新语言的功能吧。 迁移过后，我敢保证程序会更加顺畅：「我们不会再做向后不兼容的事情了（https://snarky.ca/why-python-3-exists/）」。 原文地址：https://github.com/arogozhnikov/python3_with_pleasure "
320,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736777&idx=4&sn=c904f81f40dda96fecc64f0c3be3f0c5&chksm=871accf7b06d45e1ef07e07b6998ffd54e4de85b71e3abcf2888c0c4b94d6a9d0ce49b18529d&scene=27,学界 | 山东大学提出 PointCNN：让 CNN 更好地处理不规则和无序的点云数据,"卷积神经网络的成功自不必多言，但 CNN 在点云上的应用还存在诸多短板。山东大学近日公布的一项研究提出的 PointCNN 可以让 CNN 在点云数据的处理上取得更好的表现。机器之心对该研究论文进行了简单的编译介绍。 空间上的局部相关性（spatially-local correlation）是各种类型的数据都具有的一种性质，并且与数据的表示方法无关。对于可以表示在图像等规则域中的数据，卷积算子已经证明在探索这种相关性上十分有效，并且也是 CNN 在多种任务上取得成功的关键 [LeCun et al. 2015]。 对于本身维度就比环境空间少的数据（比如 3D 空间中的面或 2D 空间中的线）而言，如果将这些数据表示为环境空间中的点云（point cloud）而非整个空间的一个密集网格，那么效果还会更好。不仅如此，3D 点云可能也是 3D 传感器最常见的原始输出，正变得越来越容易获取。但是，点云不规则且无序，会使得卷积算子难以利用数据中的空间局部相关性。   我们在图 1 中说明了在点云上应用卷积的问题和难点。假设在图 1 中所有情况下的 C 维输入特征的无序集合 是一样的，并且我们有一个形状为 4×C 的卷积核 。在 (i) 中，通过遵循规则的网格结构给出的规范顺序，在局部的 2×2 块中的特征可以被投射进形状为 4×C 的 中；然后和 K 进行卷积，得到 ，其中 Conv(·,·) 就是在元素层面进行求积后再求和。在 (ii)、(iii) 和 (iv) 中，这些点是从局部近邻中采样得到的，因此可以是任意顺序。通过遵循如图给出的顺序，输入特征集 F 可以被投射进 中（ii 和 iii 的情况）和 中（iv 的情况）。基于此，如果直接应用卷积算子，那么这三种情况的输出特征可以按如下方式计算：   注意， 在所有情况下都成立， 在大多数情况下都成立。现在，可以清楚看到：直接进行卷积会导致形状信息丢失（即 ），也会因为顺序而不同（即 ）。 在这篇论文中，我们提出使用多层感知器 [Rumelhart et al. 1986] 根据 K 个输入点 (p1,p2, ...,pK) 的坐标来学习 K×K 的 X 变换，即 X=MLP(p1,p2, ...,pK)，然后将其用于对输入特征同时进行加权和重新排列，最后再将典型的卷积应用在这个变换后的特征上。我们将这个过程称为 X-Conv，这是我们的 PointCNN 的基本构建模块。图 1 中 ii、iii 和 iv 的 X-Conv 可以描述为：   其中 X 是 4×4 的矩阵，因为在图 1 中 K=4。注意，因为 Xii 和 Xiii 是从不同形状的点学习到的，因此它们相应的给输入特征加权的方式可能不同，所以 。对于 Xiii 和 Xiv，如果学到的它们满足 （其中Π是将 (c,a,b,d) 重新排列成 (a,b,c,d) 后所得到的重排矩阵），那么可以实现 。 根据图 1 给出的案例分析，可以清楚看到理想的 X 变换 X-Conv 有能力考虑到点的形状，同时又不依赖于点的顺序。在实际情况下，我们发现学习到的 X 变换远不理想，尤其是在重新排列的等价方面。尽管如此，使用 X-Conv 构建的 PointCNN 仍然显著优于直接在点云上应用典型的卷积，并且也可媲美或胜过之前最佳的用于处理点云数据的非卷积神经网络，比如 PointNet++ [Qi et al. 2017b]。   PointCNN 卷积算子的分层应用对 CNN 学习分层表征而言至关重要。PointCNN 具有同样的设计，并且将其泛化用在了点云上。在这一节，我们首先介绍 PointCNN 中的分层卷积，并与图像 CNN 进行了类比；然后我们会详细解释核心的 X-Conv 算子；最后我们还会给出用于分类和分割任务的 PointCNN 架构。   实验           论文：PointCNN   论文地址：https://arxiv.org/abs/1801.07791 摘要： 我们为基于点云的特征学习提出了一种简单且通用的框架。CNN 成功的关键是要能利用数据中以网格形式密集表示的空间上的局部相关性（比如图像）。但是，点云是不规则和无序的，因此在这些点关联的特征上直接求核的卷积会导致形状信息的丢失，同时还会因顺序不同而不同。为了解决这些问题，我们提出根据输入点学习一种 X 变换，然后将其用于同时加权与点关联的输入特征和将它们重新排列成潜在隐含的规范顺序，之后再在元素上应用求积和求和运算。我们提出的方法是典型 CNN 向基于点云的特征学习的泛化，因此我们将其称为 PointCNN。实验表明，PointCNN 能在多种有挑战性的基准数据集和任务上实现与之前最佳方法媲美或更好的表现。 "
321,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736777&idx=2&sn=dffb344a7e59f3984b852cc347d6064a&chksm=871accf7b06d45e12972c9e8fead01b39566ee2b7d14022097798c102343f4b51d1ce95de248&scene=27,深度 | 拓扑数据分析TDA，有望打破人工智能黑箱的神奇算法,"本文介绍了拓扑数据分析（TDA）的基本原理，给出了案例展示，并指出该方法可以高效地进行可视化分析，有望为人工智能黑箱提供可解释性。近日，中科大潘建伟团队 ，量子版本的 TDA 能够实现对经典最优 TDA 算法的指数级加速。 机器学习和人工智能都是「黑箱」技术——这是使用机器学习、人工智能进行数据研究遭受的批评之一。虽然它们能自动提供有用的答案，但是却不能给人类提供可解读的输出。因此，我们往往不能了解它们在做什么，又是如何做到的。 Ayasdi 对这个问题提出了解决方法，其中利用了该公司的核心技术——拓扑数据分析（TDA）。该方法能够提供强有力的、具有详细解释的输出。然而，在这篇文章中，我们将把工作扩展到目前 TDA 的「比较」方法之外。当前的方法使用的拓扑网络由数据集的数据点（行）构建。在这项新的工作中，Ayasdi 将特征（列）也融合在网络当中，据此展示了一个改进的、易解释的结果。 首先介绍一下该解释方法的工作原理。 假设我们有一个数据集，并且在其中已经辨别出了一些子组。这些子组可能是数据的一个组成部分（例如，某种疾病有许多不同的形式，比如炎症性肠病，或该数据含有一个幸存者/非幸存者的信息），或者说，这些子组是由行集合的某拓扑模型通过分割或热点分析创建的。 如果选择其中的两个子组，Ayasdi 技术允许研究者根据他们的 Kolmogorov-Smirnov 分数（KS 分数）生成特征列表。每个特征有两个分布——每个子组各有一个分布。KS 分数衡量两个子组之间的差异。与本结构相关的也就是标准统计意义上的 P 值。 其解释是，排列在第一位的变量是最能区分两个子组的变量，而其余的特征是按其区分能力排列的。因此，解释机制的输出是一个有序的特征表。通常，通过查看列表能获得有用的解释，即，是何因素导致了不同子组之间的区别。 然而，该列表解释起来往往很复杂。就像 Google 搜索后会得到一长串回复一样，人们很可能会发现列表顶部分布不成比例，较低的响应又不为人们所关注。我们怎样才能进一步提高这些「比较列表」的透明度和可理解性呢？ 重要的是，要记住，Ayasdi 构造的拓扑模型假定给出了一个数据矩阵，以及数据集行的差异性或距离函数。通常，该距离函数是欧几里得距离，但是也可以选择其他距离函数，例如相关距离、各种角度距离等。获得数据矩阵 M 后，人们可以将它转置为一个新的矩阵 M^T。其中，初始矩阵的列是转置矩阵的行，反之亦然，如下图所示。 在完成这个操作之后，可以为 M^T 矩阵的行集合（即原始矩阵 M 的列）构建拓扑模型。在集合中，人们可以选择不同的距离函数。我们不会深入讨论这一点，但总而言之，对任何数据矩阵行的通用可选项对于这个新矩阵也适用。 现在，假设我们有一个数据矩阵 M，以及在上述数据集中的一个子组 G。该子组可能通过先验信息得来，也可能通过在 M 矩阵中行的拓扑模型分割得来。对于矩阵 M 中的每一列 c_i（即转置矩阵 M^T 的每一行），我们现在可以计算子组 G 中每一行的均值，即 c_i 的平均值。 我们将把它记为 fi,G。当这个数字包含 i 时，我们在 M^T 的行集合上获得一个函数。因此，再次重申，M 矩阵中的行的一个子组将在 M^T 的行集合上产生一个函数。Ayasdi 拓扑模型的功能之一是，通过对应于节点的行，能够利用数据矩阵的行函数的平均值对拓扑模型的节点进行着色。这对于了解数据属性而言是一个非常有用的方法。尤其地，我们现在可以利用 M^T 矩阵的行集合中子组 G 的着色情况，查看该组的特征。 请看下例。 荷兰癌症研究所（NKI）构建了一个数据集，其中包括来自 272 名乳腺癌患者采样的微阵列分析。本案例中的微阵列分析提供了为研究筛选的一组基因中每个基因的 mRNA 表达水平。从这些基因中，我们选择了 1500 个表达水平最高的基因。我们得到一个 272 x 1500 的矩阵，其中 1500 列对应于数据集中具有最大方差的 1500 个基因，272 行对应于样本总量。对于这个数据集，数据矩阵中行集合的拓扑分析已经在 [1] 和 [2] 中进行了。 我们的拓扑模型展示如下。 上图表明，拓扑模型包括一个很长的「树干」部分，然后分裂成两个「小枝」。在数据集中，存在一个名为 eventdeath 的二进制变量。如果患者在研究期间存活，则 eventdeath = 0；如果患者死亡则 eventdeath = 1。令人感兴趣的是，患者存活情况与图的结构相对应。一种方法是通过变量 eventdeath 的平均值进行着色。其结果如下所示。 我们可以看到，上面的「小枝」呈深蓝色。这表明 eventdeath 变量值低，实际上其值为零——这意味着每个患者都存活了下来。然而，下面「小枝」的存活率差得多，尖端节点几乎完全由无法存活的患者组成。我们希望理解这种现象，看看数据中的哪些特征与「小枝」的产生有关，从而了解变量 eventdeath 的迥异行为。为此，我们可以从拓扑模型中选择多种不同的子组。 在上图中，A 组为高生存率组，B 组为低生存率组，C 组可以表征为与其他两组差异最大的组（根据组间距离进行确定）。如上所述，基于这三个组，我们可以在 1500 个特征上创建 3 个函数。 如果我们建立一组特征的拓扑模型，我们可以用每个函数的平均值来给它着色。下面的三张图片展示了其结果。 在比较 A 组和 B 组着色情况时，我们发现其差异十分显著。A 组着色后，某个区域呈亮红色，而 B 组着色后相应区域呈亮蓝色。结果可见下图。左侧的模型是 A 组着色，右侧模型是 B 组着色。 组 I 和组 II 的颜色明显不同。组 I 在 A 组中主要为红色，而在 B 组中主要为蓝色（小固相区域除外）。组 II 恰好相反，在 A 组中为蓝色，在 B 组中为红色。这些组可能与高雌激素受体表达有关，其中在组 I 中呈正相关，在组 II 中呈负相关。众所周知，雌激素受体表达是乳腺癌存活与否的「强信号」。如果我们比较所有三组（如下图）： 我们也可以看到，C 组似乎是 B 组的一个「较弱」形式，其中右上角的蓝色区域面积较小，下面区域的红色较弱。在左侧的「岛」上，C 组也显示出比 A 、B 组更强的红色着色。理解哪些基因参与了 A、B、C 三组右上角的强红色块将是非常有意思的。此外，研究哪些基因参与了左侧「岛」的表达也很有趣。了解这些基因组需要使用各种基于网络的生物学通路分析的工具。 总而言之，我们已经展示了如何对数据集中的特征空间使用拓扑建模，而不是利用行集合直接从数据集寻找洞察。具有超过 4 个特征的数据集不能直接使用标准图形技术直观地理解，但是具有成百上千个特征的数据集通过这种方式理解起来却很容易。该方法能直接识别行为一致的特征组，这通常在基因组和更普遍的生物学数据的分析中存在。 "
322,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736740&idx=3&sn=cdce446703e69b47cf48f12b3d451afc&scene=0,专栏 | 后RCNN时代的物体检测及实例分割进展,"物体检测是计算机视觉的重要任务之一，从最开始的 Viola-Jones 2001 的人脸检测开始，到 Ross 的 Deformable Part Model（DPM）2007 通用物体检测，再到现在基于深度学习的 Region Convolution（RCNN）2013 模型，我们见证了物体检测的准确率快速的提升，并且计算机视觉所使用的数学方法也从 Boosting，变成了 SVM，到最近的 Deep Neural Network。基于物体检测，研究者们成功的设计出了实例分割 Mask RCNN。RCNN 是近年来物体检测的基石，有大量的论文和博客在分析和改进 RCNN，本篇文章希望通过介绍除了 RCNN，YOLO，SSD 和 Mask RCNN 之外的 novel idea 来帮助大家开拓眼界，避免思维陷入单一模式。 说起神经网络，大家就会想起用 ResNet，VGG 或者 Inception 来作为主干。来自韩国 Intel 公司的组发明的   和 MSRA 的代季峰组的 Deformable Convolution 成功的跳出了惯性思维，大大提升了检测网络的速度和准确性。  : Deep but Lightweight Neural Networks for Real-time Object Detection  贡献了一个可以适用于 Fully Convolution Network 的网络结构，该网络的浅层使用了 CRelu 可以将 Channel 减半，在深层使用 Inception 加强网络的学习效果。CRelu 和完整网络结构如下： 网络参数能够减半的原因是因为，CRelu 发现浅层的网络冗余度高，并且存在反相现象，通过 CRelu 可以重建出另一半的通道。PVANet 跟 Faster RCNN 的速度对比如下表： Deformable Convolution Networks (ICCV 2017 oral) 本篇文章在 Spatial Transform Network 的基础上提出了 Deformable Convolution Network。个人感觉这篇论文是很有可能的 ICCV Best Paper 的论文之一。这篇论文和 Dilation Network 也有一定的关系。并且这篇论文提供了完整，清晰的代码，为之后的研究提供了很好的平台。 如上图（a）是传统的 CNN，采样是在 3 by 3 的 regular grid 进行采样。（b）为作者提出的 Deformable Convolution，可以看出采样点在 regular grid 的基础上面加上了 offset，能够让神经网络自己决定采样点的位置。（c）和（d）为一种特殊的采样方法，为 Deformable CNN 在某种位移情况下的表现形式。 网络的训练是基于 spatial transformer Network，通过 Bilinear interpolation 的方式进行学习，Backpropagation 也是可以微分的。Deformable CNN 的结构如下图： 作者在 Semantic segmentation 和物体检测任务上面测试了方法，without bells and whistles，作者可以达到 state-of-the-art 的结果（仅仅物体检测）。如果我们将采样点显示出来就会发现奇妙的现象。采样点会有选择性的位移到 foreground。 Deformable Part-based Model Fully Convolutional Networks for Object Detection (BMVC 2017 oral) 这篇论文和 Deformable CNN 是同期的论文，两者的出发点基本一样。但是训练的过程中，作者使用的 DPM 论文中的公式对物体的位移进行了限制，论文的流程图如下： 由于 NMS（Non Maximum Suppression）的存在，使得 RCNN 框架的物体检测不是完全的 end-to-end 模型，代季峰组的 Relation Network module 能够代替 NMS，实现 100% 的 end-to-end 模型。 Soft-NMS, improving object detection with One Line of Code (ICCV 2017) NMS 能除去重叠的 bounding box，是物体检测系统中不可或缺的部分。但是作者发现 NMS 也会滤掉靠的很近的物体。比如下图两匹靠得近的马左边的马由于跟右边的马有很大的 overlapping，从而造成误检。为了避免这种现象的产生，soft-NMS 将 overlapping 的 bounding box 的 confidence 降低而非直接变成零。如下图左边的马的 confidence 从 0.80 降低到了 0.4，从而使得改 bounding box 能够被检测到。soft-NMS 能够在所有的公开数据上面得到 1 个点左右的提升，并且结合 deformable CNN 得到了 state-of-the-art 的结果。 Relation Network for Object Detection (CVPR 2018 submitted) 这篇论文提出了对 relation 进行建模的的 relation module，可以代替 NMS ，实现 100% 的 end-to-end 物体检测。对关系的描述可以提高计算机视觉模型的效果是大家的共识，但是本篇论文是少有的成功运用关系的论文。Relation module 的结果会高于 soft-NMS。在可以预见的将来，relation module 会大量地运用于 CV 模型中。但是美中不足的是，relation network 到底捕捉到了什么样的关系是论文无法说明的。Relation Network 的模型跟最近提出的 non local convolution，attention is all you need 有着惊人的相似。相信如果从这个角度出发，应该会有很大的收获。Relation Module 的结构如下： Relation Module 跟 NMS 和 Soft-NMS 之间的结果对比。 在实例分割方面，除了 Mask RCNN 之外存在着很多新颖的方法，但是由于结果没有 Mask RCNN 好，经常会被人忽律，但是其中的 idea 还是很值得学习借鉴的。 Mask RCNN 已经成为了 instance segmentation 的 baseline 模型，这里就不多做介绍，结构如下： Deep Watershed Transform for Instance Segmentation (CVPR 2017) 这篇 instance segmentation 的论文在 mask rcnn 之前，这篇论文提出了学习一种类似于 watershed transform 的能量。Watershed transform 可以直接计算，但是由于自然图像的复杂使得结果有很多 local minimal，通过 deep learning 学习到的势能会更加稳定，如下图： 网络通过 GT angle 和 watershed energy 两种 supervision 训练得到。两种能量可以通过 mask 标注产生，Ground-truth 如下图： MaskLab : Instance Segmentation by Refining Object Detection with Semantic and Direction Features (CVPR 2018 submitted) MaskLab 的作者是来自 Google 的团队，第一作者同时也是 deeplab 的作者。应该算是 segmentation 的元老级人物。这篇论文跟 mask rcnn 类似，但是使用了 semantic segmentation 和 Direction prediction 两个分支的信息来生成 mask，结果会比 feature pyramid network 的 mask rcnn 好很多。这篇论文的创新性来自 Direction Pooling，通过每个区域的 direction 的 voting 来得到 region mask，idea 跟 deformable CNN 中的 deformable region pooling 类似。 MaskLab 框架如下： Directional Pooling 结构如下： Panoptic Segmentation (CVPR 2018 submitted) Panoptic Segmentation 中文名为全景分割，是由 Facebook 的大神联手推出的结合 instance segmentation 和 semantic segmentation 的新任务。作者发现，instance segmentation 只能够分别物体，但是没有办法对 stuff 进行预测。而传统的 semantic segmentation 不能够区分物体。作者提出了同时对物体进行分割，并且对 stuff 进行分类的新任务。 （a）为原图，（b）为 semantic segmentation 图，（c）为 instance segmentation，（d）为新提出的全景分割。与此同时作者提出了全景分割的 evaluation metric 和基于 mask rcnn + PSPNet 的 baseline 模型。新提出的任务应该还是有很大前进空间，相信各路大神应该会很快就攻占这一任务了吧 (>_<) Instance Segmentation Benchmark 下面我们比较一下不同方法在 cityscape 上面的结果，可以看出 Mask RCNN 结果远超过其他方法，最新提出的 MaskLab 借助大数据的力量微弱超过 mask rcnn。 Cityscape 上面 Watershed，SGN 和 Mask RCNN 的比较 COCO 数据集 Mask RCNN 跟 MaskLab 比较 加入机器之心（全职记 "
323,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736777&idx=1&sn=d837a23210743031688380e4596558ee&chksm=871accf7b06d45e189adbc7eb4e9d9fc518446e9aeb9a07b79a4a6ca08bdab21b926b3484a3a&scene=27,如何解决90％的自然语言处理问题：分步指南奉上,"自然语言处理（NLP）与计算机视觉（CV）一样，是目前人工智能领域里最为重要的两个方向。如何让机器学习方法从文字中理解人类语言内含的思想？本文中，来自 Insight AI 的 Emmanuel Ameisen 将为我们简述绝大多数任务上我们需要遵循的思路。 文本数据的 5W 和 1H！ 无论是成立的公司，还是致力于推出新服务，你都可以利用文本数据来验证、改进和扩展产品的功能。从文本数据中提取信息并从中学习的科学是自然语言处理（NLP）的一个活跃的研究课题。 NLP 覆盖领域很广，每天都会有新的令人兴奋的结果。但经过与数百家公司合作，Insight 团队发现其中有几个重要应用出现得尤其频繁： 识别不同的用户/客户群（例如预测客户流失、顾客终身价值、产品偏好） 准确检测和提取不同类别的反馈（正面和负面的评论/意见，提到的特定属性，如衣服尺寸/合身度等） 根据意图对文本进行分类（例如寻求一般帮助，紧急问题） 尽管网上有很多 NLP 论文和教程，但我们很难找到从头开始高效学习这些问题的指南和技巧。 结合每年带领数百个项目组的经验，以及全美国最顶尖团队的建议，我们完成了这篇文章，它将解释如何利用机器学习方案来解决上述 NLP 问题。我们将从最简单的方法开始，然后介绍更细致的方案，如特征工程、单词向量和深度学习。 阅读完本文后，您将会知道如何： 收集、准备和检验数据 建立简单的模型，必要时转化为深度学习 解释和理解模型，确保捕获的是信息而非噪声 这篇文章我们将给你提供一步一步的指导；也可以作为一个提供有效标准方法的高水平概述。 这篇文章附有一个交互式 notebook，演示和应用了所有技术。你可以随意运行代码，同步学习：https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb 数据源示例 每个机器学习问题都从数据开始，例如电子邮件、帖子或推文（微博）。文本信息的常见来源包括： 产品评论（来自亚马逊，Yelp 和各种应用商店） 用户发布的内容（推文，Facebook 上的帖子，StackOverflow 上的问题） 故障排除（客户请求，支持票据，聊天记录） 「社交媒体中出现的灾难」数据集 本文我们将使用由 CrowdFlower 提供的一个名为「社交媒体中出现的灾难」的数据集，其中： 编者查看了超过 1 万条推文，其中包括「着火」、「隔离」和「混乱」等各种搜索，然后看推文是否是指灾难事件（排除掉用这些单词来讲笑话或评论电影等没有发生灾难的情况）。 我们的任务是检测哪些推文关于灾难性事件，排除像电影这种不相关的话题。为什么？一个可能的应用是仅在发生紧急事件时（而不是在讨论最近 Adam Sandler 的电影时）通知执法官员。 这篇文章的其它地方，我们将把关于灾难的推文称为「灾难」，把其它的推文称为「不相关事件」。 标签 我们已经标记了数据，因此我们知道推文所属类别。正如 Richard Socher 在下文中概述的那样，找到并标记足够多的数据来训练模型通常更快、更简单、更便宜，而非尝试优化复杂的无监督方法。 Richard Socher 的小建议 我们遵循的首要规则是：「你的模型受限于你的数据」。 数据科学家的重要技能之一就是知道下一步的工作对象是模型还是数据。一个好的方法是先查看数据再清理数据。一个干净的数据集可以使模型学习有意义的特征，而不是过度拟合无关的噪声。 下面是一个清理数据的清单：（更多细节见代码 code (https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb)）： 1. 删除所有不相关的字符，如任何非字母数字字符 2. 把文字分成单独的单词来标记解析 3. 删除不相关的词，例如推文中的「@」或网址  4. 将所有字符转换为小写字母，使「hello」，「Hello」和「HELLO」等单词统一 5. 考虑将拼写错误和重复拼写的单词归为一类（例如「cool」/「kewl」/「cooool」） 6. 考虑词性还原（将「am」「are」「is」等词语统一为常见形式「be」） 按这些步骤操作并检查错误后，就可以使用干净的标签化的数据来训练模型啦！ 机器学习模型的输入是数值。如图像处理的模型中，用矩阵来表示各个颜色通道中每个像素的强度。 一个笑脸可以表示为一个数字矩阵。 如果我们的数据集是一系列的句子，为了使算法可以从数据中提取特征，我们需要表示为可以被算法识别的形式，如表示为一系列数字。 One-hot encoding（词袋模型） 表示文本的一种常见方法是将每个字符单独编码为一个数字（例如 ASCII）。如果我们直接把这种简单的形式用于分类器，那只能基于我们的数据从头开始学习单词的结构，这对于大多数数据集是不可实现的。因此，我们需要一个更高级的方法。 例如，我们可以为数据集中的所有单词建立一个词汇表，每个单词对应一个不同的数字（索引）。那句子就可以表示成长度为词汇表中不同单词的一个列表。在列表的每个索引处，标记该单词在句子中出现的次数。这就是词袋模型（Bag of Words），这种表示完全忽略了句子中单词的顺序。如下所示。 将句子表示为词袋。左边为句子，右边为对应的表示，向量中的每个数字（索引）代表一个特定的单词。 可视化词嵌入 在「社交媒体中出现的灾难」一例中，大约有 2 万字的词汇，这代表每个句子都将被表示为长度为 2 万的向量。向量中有很多 0，因为每个句子只包含词汇表中非常小的一个子集。 为了了解词嵌入是否捕获到了与问题相关的信息（如推文是否说的是灾难），有一个很好的办法，就是将它们可视化并看这些类的分离程度。由于词汇表很大，在 20,000 个维度上可视化数据是不可能的，因此需要主成分分析（PCA）这样的方法将数据分到两个维度。如下图所示。 将嵌入的词袋可视化。 看起来很难分为两类，也不好去降低维度，这是嵌入的一个特点。为了了解词袋模型特征是否有用，我们可以基于它们训练一个分类器。 遇到一个问题时，通常从寻找解决问题的工具入手。当我们要对数据进行分类时，出于通用性和可解释性的考虑，通常使用 Logistic 回归（Logistic Regression）。训练非常简单，结果也可解释，因为易于从模型提取出最重要的参数。 我们将数据分成一个用于拟合模型的训练集和一个用于分析对不可见数据拟合程度的测试集。训练结束后，准确率为 75.4%。还看得过去！最频繁的一类（「不相关事件」）仅为 57%。但即使只有 75% 的准确率也足以满足我们的需要了，一定要在理解的基础上建模。 混淆矩阵（Confusion Matrix） 首先要知道我们模型的错误类型，以及最不期望的是哪种错误。在我们的例子中，误报指将不相关的推文分类为灾难，漏报指将关于灾难的推文归为不相关事件。如果要优先处理每个可能的事件，那我们想降低漏报的情况。如果我们优先考虑资源有限的问题，那我们会优先降低误报的情况，从而减少误报的提醒。我们可以用混淆矩阵来可视化这些信息，混淆矩阵将我们模型预测的结果与真实情况进行比较。理想情况下（我们的预测结果与真实情况完全相符），矩阵为从左上到右下的一个对角矩阵。 混淆矩阵（绿色比例大，蓝色比例小） 我们的分类器的漏报情况（相对）高于误报情况。也就是说，这个模型很可能错误地将灾难归为不相关事件。如果误报情况下执法的成本很高，那我们更倾向于使用这个分类器。 解释模型 为了验证模型并解释模型的预测，我们需要看哪些单词在预测中起主要作用。如果数据有偏差，分类器会对样本数据作出准确的预测，但在实际应用时模型预测的效果并不理想。下图中我们给出了关于灾难和不相关事件的重要词汇。我们可以提取并比较模型中的预测系数，所以用词袋模型和 Logistic 回归来寻找重要词汇非常简单。 词袋：重要词汇 我们的分类器正确地找到了一些模式（广岛，大屠杀），但显然这是无意义数据的过度拟合（heyoo, x1392）。现在我们的词袋模型正在处理一个庞大的词汇表，所有词汇对它来说都是一样的。但一些词汇出现地非常频繁，而且只会对我们的预测加入噪声。接下来，我们试着用一个方法来表示词汇出现的频率，看我们能否从数据中获得更多的信号。 TF-IDF 为了使模型更关注有意义的单词，我们可以使用 TF-IDF（词频-逆文档频率）对我们的词袋模型进行评估。TF-IDF 通过对数据集中词汇出现的频率来加权，并减小高频但只是增加噪音的单词的权重。这是我们新嵌入的 PCA 预测。 将 TF-IDF 嵌入可视化。 由上图我们看到，两种颜色的数据差别更加明显。这使分类器分组更加容易。让我们来看一下这样结果是否会更好。训练新嵌入的 Logistic 回归，我们得到了 76.2％的准确率。 只是稍稍地进行了改进。那现在我们的模型可以选择更重要的单词了吗？如果模型预测时有效地绕过了「陷阱」，得到了更好的结果，那就可以说，这个模型得到了优化。 TF-IDF：重要词汇 挑出来的单词似乎更加相关了！尽管我们测试集的指标稍有增加，但模型使用的词汇更加关键了，因此我们说「整个系统运行时与客户的交互更加舒适有效」。 Word2Vec 我们最新的模型可以挑出高信号的单词。但很可能我们运作模型时会遇到训练集中没有单词。因此，即使在训练中遇到非常相似的单词，之前的模型也不会准确地对这些推文进行分类。 为了解决这个问题，我们需要捕获单词的含义，也就是说，需要理解「good」和「positive」更接近而不是「apricot」或「continent」。用来捕获单词含义的工具叫 Word2Vec。 使用预训练的单词 Word2Vec 是寻找单词连续 embedding 的技术。通过阅读大量的文本学习，并记忆哪些单词倾向于相似的语境。训练足够多的数据后，词汇表中的每个单词会生成一个 300 维的向量，由意思相近的单词构成。 论文《Efficient Estimation of Word Representations in Vector Space》的作者开源了一个模型，对一个足够大的可用的语料库进行预训练，将其中的一些语义纳入我们的模型中。预训练的向量可以在这篇文章相关的资源库中找到：https://github.com/hundredblocks/concrete_NLP_tutorial。 句子的表示 快速得到分类器的 sentence embedding 的一个方法是平均对句子中的所有单词的 Word2Vec 评估。这和之前词袋模型是一个意思，但这次我们保留一些语言信息，仅忽略句子的语法。 以下是之前技术的新嵌入的可视化： 可视化 Word2Vec 嵌入 这两种颜色的数据更明显地分离了，我们新的嵌入可以使分类器找到两类之前的分离。经过第三次训练同一个模型后（Logistic 回归），我们得到了 77.7％的准确率，这是目前最好的结果！可以检验我们的模型了。 复杂性/可解释性的权衡 我们的 embedding 没有向之前的模型那样每个单词表示为一维的向量，所以很验证看出哪些单词和我们的向量最相关，。虽然我们仍可以使用 Logistic 回归的系数，但它们和我们 embedding 的 300 个维度有关，而不再是单词的索引。 它的准确率这么低，抛掉所有的可解释性似乎是一个粗糙的权衡。但对于更复杂的模型来说，我们可以利用 LIME 之类的黑盒解释器（black box explainers）来深入了解分类器的工作原理。 LIME 可以通过开源软件包在 Github 上找到 LIME：https://github.com/marcotcr/lime 黑盒解释器允许用户通过扰乱输入并观察预测的变化来解释一个特定例子的任何分类器的决定。 让我们看一下数据集中几个句子的解释。 挑选正确的灾难词汇并归类为「相关」。 这里，这个词对分类器的造成的影响似乎不太明显。 但是，我们没有时间去探索数据集中的数千个示例。我们要做的是在测试例子的代表样本上运行 LIME，看哪些词汇做的贡献大。使用这种方式，我们可以像之前的模型一样对重要单词进行评估，并验证模型的预测结果。 Word2Vec：重要单词 模型提取的高度相关的词意味它可以做出更加可解释的决定。这些看起来像是之前模型中最相关的词汇，因此我们更愿意将其加入到我们的模型中。 我们已经介绍了生成简洁句嵌入快速有效的方法。但是由于忽略了单词的顺序，我们跳过了句子所有的语法信息。如果这些方法提供的结果不充分，那我们可以使用更复杂的模型，输入整个句子并预测标签，而不需要中间表示。一个常见的方法是使用 Word2Vec 或更类似的方法（如 GloVe 或 CoVe）将句子看作一个单词向量的序列。这就是我们下文中要做的。 高效的端到端结构 用于句子分类的卷积神经网络训练非常迅速，作为入门级深度学习体系效果非常理想。虽然卷积神经网络（CNN）主要因为在图像处理的使用而广为人知，但它们在处理文本相关任务时得到的结果也非常好，而且通常比大多数复杂的 NLP 方法（如 LSTMs 和 Encoder/Decoder 结构）训练地更快。这个模型考虑了单词的顺序，并学习了哪些单词序列可以预测目标类等有价值的信息，可以区别「Alex eats plants」和「Plants eat Alex」。 训练这个模型不用比之前的模型做更多的工作，并且效果更好，准确率达到了 79.5％！详见代码：https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb 与上述模型一样，下一步我们要使用此方法来探索和解释预测，以验证它是否是给用户提供的最好的模型。到现在，你应该对这样的问题轻车熟路了。 下面对我们成功使用的方法进行简要回顾： 从一个简单快速的模型开始 解释其预测 了解其错误类型 根据以上知识来判断下一步的工作——处理数据还是寻找更复杂的模型 这些方法只用于特定的例子——使用适当的模型来理解和利用短文本（推文），但这种思想适用于各种问题。希望这篇文章对你有所帮助，也欢迎大家提出意见和问题！ "
324,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736740&idx=4&sn=d819c68e412d814c32f38491ed8d09d5&scene=0,观点 | LeCun卸任成剧变？看FAIR研究员田渊栋如何看待,近日，人工智能先驱、纽约大学教授  ，改任首席 AI 科学家，接任者是 Jerome Pesenti，LeCun 将向其汇报。这在业界引起了一场舆论风波。比如，华盛顿大学教授、《终极算法》作者 Pedro Domingos 在华盛顿邮报的一篇文章中声称 Facebook 与谷歌、微软在人工智能方面还存在不小差距；LeCun 的卸任折射出了 Facebook 人工智能的研发与工程化之间的深层矛盾。面对这些外界看法，FAIR 内部人员如何看待 LeCun 卸任一事呢？卡耐基梅隆大学机器人系博士、FAIR 人工智能研究员田渊栋发表了自己的观点，机器之心经授权获得转载。 FAIR 现在一切风平浪静，大家该干啥干啥，没见到和之前有什么区别，还是做 long-term 的工作，还是偏重科研，还是没有产品压力。我都不知道这些传言是怎么来的。 FAIR 基本上是自底向上的管理模式，给一个环境让研究者们去探索，只要有进展，老板们不会干涉。现在大家知道围棋是好方向了，但退回 2015 年做围棋，碰到个有点控制欲的老板还不把你一脚踹到爪哇国去，但是 FAIR 不会，大家以「与别人不同」为荣，并且愿意沿着自己看准的方向坚持下去。当时我做围棋大家会质疑，但我可以继续做。做原创研究要」于无声处听惊雷「，要是都听着上头的，上头指哪下头打哪，就没有原创性了，很多时候大佬说的都是错的。 之前 Yann 的 reports 太多，他不想管也很正常。国外管人很难的，手下人都是大爷，要好好伺候着，有啥不顺心的要负责解决，出活碰到障碍要提供便利，没思路了要经常一起讨论，要拉合作，作统筹，而且还到处是坑，说错话做错事责任全在管理者头上。 研究其实分两种，一种灌水式研究，以文章中稿为目的；另一种是解决式研究，以解决问题为目的。FAIR 从成立之初就是做后一种研究，文章不看多而看精，看效果和质量，不仅看文章，还看开源代码，看影响力。FastText，Convolutional Machine Translation，MaskRCNN，还有最近的 detectron，包括我这边的 ELF，都是被广泛关注并且可以在各种场景下用上的。在过去的四年里，FAIR 给公司带来了不可估量的价值，让大家觉得 FB 也是一个技术上很厉害的公司，牛人会纠结是去 Google Brain，DeepMind 还是来我们这里，这还不够么？ 站在解决式研究的角度上看，它和产品没有什么不可调和的矛盾，最大的区别是检验标准。产品组标准相对客观简单，可能就几个数字，多少人在用，时长是不是涨了，等等；研究这边就多元化了，新方法，新方向，新思路，就算一时的」大逆不道「都可能是好的。所以一个研究团队要有好的头儿带，要有好的鉴赏力，知道什么是好的研究，什么是一般的研究，什么是在浪费时间，什么需要给空间让大家探索，并且可以把这个标准坚持下去。这个很重要，别看人家到处做讲座好像没干啥，大佬就是起这个作用的。 另一个是长期和短期的区别。太过注重短期效益，逼得紧，每周都要有进展，那就只能在原方法上修修补补（我以前在 Google 的组就是这样）；给一个宽容的环境，鼓励大家坚持长期的方向，就会有创新出来。如 @ 吴育昕所说，对长期的项目现在投入是越来越大了。注意宽容并不意味着我们很闲，事实上像我基本上是 7 天 13-14 小时工作制，算法到系统到理论分析都搞，搭建系统的速度，产品组同事们看了 commit 的频率都无法插手。宽容只是容许失败，容许尝试很多不一样的方向，只要你努力了，就算失败也不会让人难堪，反而可以堂堂正正说试过了，换一个方向再试一次。 所以，你们觉得这样的组会被干掉么？ 另外附带说一句，一直以来 FAIR 宣传自己是研究组，宣传各种文章各种研究成果，可能有点用力过猛，导致大家对它有误解，以为它只做研究，和产品关系不大。其实 FAIR 虽然没有产品压力，但和以前 MSR 有点不一样的地方是它一直在 deliver，一直有各种东西放进产品里面去，我上面列举的那些好项目，难道都是看着当摆设的？现在因为 AI 平台兼容性强很多，技术转化相对方便，要送训练好的模型去服务产品，真的不是什么难事情。以前听说 MSR 要花上一年时间才能转化研究成果，现在 FAIR 这里搞个新闻，明天一封 email 就来了要谈内部合作。我来 FAIR 的第一个小项目就是完全出于个人兴趣和我的 bootcamp mentor 做了个内部产品，不仅发布了还被 VP 点名了。不过我为啥要宣传这事呢？还是讨论高大上的游戏和强化学习比较吸睛啊。 可能大家只有「科研」、「工程」这几个概念，所以不知不觉把事情往非黑即白的方向上去想了，其实世界都是灰的。 原文链接：https://www.zhihu.com/question/266074007 
325,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736740&idx=5&sn=e25399341fea9ebeb8ad90c93785919a&scene=0,学界 | 用生成对抗网络解决NLP问题：谷歌大脑提出MaskGAN,"生成对抗网络（GAN）自推出以来，在计算机视觉领域中引起了一股风潮，在自然语言处理中却鲜有研究。看来，这或许需要 GAN 的提出者 Ian Goodfellow 自己来推动。谷歌大脑 William Fedus、Ian Goodfellow 和 Andrew M. Dai 共同提交的论文中，研究人员使用 GAN 和强化学习方法在 NLP 中做了自己的探索。目前，该论文已提交至 ICLR 2018 大会。 前言 循环神经网络（RNN）（Graves 等人, 2012）是序列型数据以及序列标记任务中最常见的生成模型。循环神经网络在语言模型（Mikolov 等人,2010）、机器翻译（Wu 等人,2016）和文本分类（Miyato 等人,2017）中显示出引人注目的结果。在这些模型中，文本通常是模型生成的，通过从条件分布中抽样得到，而条件分布基于前一个词语以及一个包含目前为止产生的词语的隐藏状态。这些训练通常通过最大似然以一种 teacher forcing 的方法进行训练，其中真实的词语作为条件被馈送回模型中以便生成句子中新的词语。 但是这会在生成抽样样本时造成问题——模型通常被动以未作为训练条件的序列作为条件。这导致了 RNN 中无法预测的动态隐藏状态。一些方法如 Professor Forcing（Lamb 等人, 2016）、Scheduled Sampling（Bengio 等人, 2015）可以解决这个问题。这些方法间接通过使隐藏状态动态变得可预测（Professor Forcing）或者通过随机将训练时抽样的词语作为条件。但是，这些方法都不直接指定基于 RNN 输出的损失函数的形式，从而无法鼓励高质量的样本抽样。而谷歌大脑提出的新方法可以实现这个目的。 生成对抗网络（GAN）（Goodfellow 等人, 2014）是一个在对抗设置下训练生成模型的框架，对抗分为两部分：生成图像并试图愚弄判别器的生成器（generator）和通过训练鉴别真实图像和合成图像的判别器（discriminatory）。相比其它方法，GAN 在生成超仿真图像的领域取得了巨大成功，但是它在文本序列领域并没有广泛的应用。这归结于自然文本的离散性，使得标准 GAN 中的传播梯度（propagate gradient）从判别器回到生成器变得难以实现。我们通过使用强化学习（RL）解决了这一问题，利用最大似然和随机梯度下降方法，在训练判别器的同时进行生成器的训练。GAN 也通常受到诸如训练不稳定性和模式下降（mode dropping）等问题，这两个问题在文本情况下都更加恶化。模式下降会在某些形式的训练集中由生成器很罕见地引起，例如，使得所有生成的火山图像变成同一个火山的多个变体。这在文本生成中变成一个重要的问题，因为数据含有许多复杂模式，从 bigram 到短语到长习语。训练稳定性也是一个问题，因为不同于图像，文本是自回归生成的，从而只有在完整的句子生成之后才能从判别器观察到损失函数的具体值。当生成越来越长的句子时，这个问题会复杂化。 我们通过文本 fill-in-the-blank 或 in-filling 任务训练我们的模型，从而减少这些问题带来的影响。这与 Bowman 等人（2016）提出的方法类似，但是我们的设置更具有鲁棒性。在这个任务中，一部分文本被删除或编辑。这个模型的目的是随后填充文本中缺失的部分，使其无法与原始数据相区分。当填充文本的过程中，模型自回归地对已经填充的字符（token）进行操作，如在标准的语言模型中以真实的已知文本作为条件时那样。如果整个文本被编辑，那这就简化到语言模型问题。 在以前的自然语言 GAN 研究中，设计每个时间步的误差分布被视为是重要的（Yu 等人, 2017; Li 等人, 2017）。文本填充任务自然地考虑到了这个问题，因为判别器会评估每个字符并因此向生成器提供一个细粒度的监督信号。例如，设想如果生成器生成一个在前 t-1 时间步完美匹配数据分布的序列，但是随后生成了一个异常字符 yt,(x1:t−1yt)。尽管整个序列由于错误字符而明显是合成的，判别模型在异常字符处产生了很高的损失信号，但是对于其它不是，判别模型将很可能产生一个更有信息的错误信号给生成器。 这项研究也为自然语言环境下的条件 GAN 模型提供了进一步的研究。在以下部分中， 我们将介绍一个由填充（MaskGAN）训练的文本生成模型。 在极大动作空间（action space）中考虑 actor-critic 架构。 考虑新的评估指标和生成合成训练数据。 方法细节 训练之前，我们先进行预训练。首先我们用标准最大似然训练一个语言模型。然后将预训练语言模型的权重应用于 seq2seq 编码器和解码器模块。在这些语言模型的基础上，我们使用最大似然方法在 in-filling 任务中预训练 seq2seq 模型，特别的是，Luong 等人（2015）提到的注意（attention）参数。我们通过超过 500 次的超参数扫描来选择在掩码任务中产生最小验证复杂度（validation perplexity）的模型。初始的算法不包含 critic，但是我们发现包含 critic 将梯度估计的方差降低了一个数量级，这显著改善了训练。 论文：MaskGAN: Better Text Generation via Filling in the ______ 论文链接：https://arxiv.org/abs/1801.07736 神经网络文本生成模型通常为自回归（autoregressive）语言模型或者 seq2seq 模型。这些模型通过序列抽样词语生成文本（抽样分布将前一个词语作为条件），并且对于几种机器翻译和摘要总结的基准是最先进的模型。这些基准通常通过验证复杂度定义，尽管这不是直接对生成文本质量的衡量。除此以外，这些模型通常使用极大似然或者 teacher forcing 进行训练。这些方法非常适合优化复杂度（perplexity），但是可能导致样本质量差，因为生成文本需要将可能从来没有在训练过程中观察到的词语序列作为条件。我们提出使用生成对抗网络（GAN）来提高样本质量，它通过显式地训练生成器产生高质量样本，并且已经在图像生成领域取得很大成功。GAN 最初设计用于输出可微分的值，所以生成离散语言是具有挑战性的。我们认为验证复杂度本身不代表模型生成的文本质量。我们引入条件 actor-critic GAN，它可以基于上下文填充缺失的文本。我们从定性和定量的角度证明，相比最大似然训练的模型，这种方法生成了更真实的有条件和无条件文本样本。 "
326,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736740&idx=1&sn=c23dbdd017f0520e05a5e67f8c7d4fb8&scene=0,AI创业公司融资新记录，三大国有银行共同投资第四范式,2018年1月26日，第四范式宣布获得来自中国工商银行、中国银行、中国建设银行等三家国有银行及所属基金的联合战略投资。第四范式成为继“中国银联”后，三大国有银行联合投资的唯一公司，显示出领先金融机构对第四范式产品实力与发展潜力的高度认可。 此前第四范式已经完成了A轮红杉中国领投，A+轮创新工场领投，B轮元生资本、众为资本领投的融资。本轮融资后，第四范式累计获得逾十家投资方。 第四范式致力于赋予企业 AI 能力，其核心产品「第四范式先知」企业 AI 核心系统，能够帮助企业搭建部署在自己系统的、基于 AI 的生产体系，使企业具备自主研发 AI 应用的核心能力。「第四范式先知」已经在包括金融、互联网在内的多个领域率先落地，例如在金融机构的核心业务风险管理方面，将传统的风控方式提升了数倍；在精准营销方面，基于高维度的数据搭建实时营销平台，实现客户营销响应率提升十倍以上。 此次融资昭示着金融业与 AI 的深层次结合已经成为金融机构的战略方向。对此，本轮投资方之一、中银国际董事总经理王立新先生表示：「在互联网技术的帮助下，近年来银行显著提升了对客户的综合服务水平，而人工智能技术的应用可以进一步帮助银行更好地了解客户，成为了提升银行产品质量与用户体验的新一波重要推动力。第四范式作为该领域的先行者，在银行及其它不同领域都积累了较多案例，取得不错的应用效果，并且驶入快车道。」 第四范式创始人、首席执行官戴文渊先生表示：「第四范式在过去几年除了不断强化自己的技术实力在业内持续领先以外，也形成了深厚的行业理解并落地了多个标杆级行业案例。第四范式在金融领域已经与数十家大中型银行展开深度合作，诸多案例取得了 100% 以上的效果提升。随着第四范式与银行的合作不断深入，我们也受到了来自国有银行的关注，双方希望共同探寻更全面、更紧密的合作，因此最终实现了三家国有银行联合投资这样的里程碑事件。本轮融资后，我们将加快产品和解决方案在全国范围内落地，为全国各地的包括银行、保险、证券业在内的更多企业带来 AI 技术应用，最终让所有企业与用户都能享受到人工智能带来的红利。 在融资消息发布前夕，机器之心就一些人们关心的问题和第四范式展开了对话： 机器之心：几大银行投资第四范式主要是看好公司的哪些积累和优势？ 第四范式： 首先是基于三家国有银行对第四范式全球领先的科技实力和行业落地的充分认可。目前第四范式已经与数十家大中型银行展开深度合作，基于先知平台已经落地了包括精准获客、个性化推荐、申请反欺诈、交易反欺诈、逾期/流失预警、流动性管理、智能催收、不良资产处置等在内的多个标杆解决方案。以交易反欺诈为例，在与某领先的全国性股份制商业银行的合作中，第四范式提升信用卡交易反欺诈识别准确率超过 7 倍。随着第四范式的平台与多场景的成功落地，也受到了来自国有大行的全面关注，双方希望共同探寻更全面、更紧密的合作，最终实现了这次三家国有银行联合投资的里程碑事件。 其次，战略投资也意味着银行将第四范式视为长远发展的重要伙伴，也是对第四范式未来发展潜力与合作空间的认可。对于人工智能+银行的未来应用方式，第四范式的思路是未来企业的智能核心。内置领先的 automl 算法，大幅降低使用门槛并且可结合业务实践、在上层进一步自主扩展。银行之所以选择与第四范式开展更紧密的合作，是因为银行认可这样一种 AI 赋能、全面升级的方式，提升银行的核心竞争力。 机器之心：银行为什么选择在这样一个时间点投资第四范式？ 第四范式： 第四范式在上个月的乌镇世界互联网大会上发布了先知新版本，即企业 AI 核心系统，先知平台将会以企业级平台的方式，将 AI 能力赋能于更多企业。在 2018 年，第四范式将加快第四范式产品和解决方案在全国市场的落地，为包括全国各地的银行、保险、证券等在内的更多企业带来 AI 技术应用。对于银行业来说，也需要引入更多的外部力量，在多方的共同努力下，使金融步入「智能化升级」的快车道。 机器之心：这次投资第四范式是战略投资还是财务投资？ 第四范式： 战略投资，会有比较强的战略的协同。 机器之心：接下来，银行和第四范式有什么资源协同？未来第四范式和这几家银行上会有哪些形式的合作？是否存在研究、技术或产品上深度融合的可能性？ 第四范式： 此次战略投资将推动第四范式和银行之间未来更为紧密的合作。一方面，金融行业蕴含着许多天然的 AI 应用场景，可以通过「数据智能核心」的赋能大幅提升核心效率，这是迈向智能银行和普惠金融坚实的一大步。另一方面，金融和科技行业一样，是其他行业的服务者，我们也正在尝试通过联合服务的方式，让金融和科技一起去帮助其他行业的升级变革。 机器之心：第四范式在本轮融资应该是有各类投资机构可以选择，选择这几家大型银行的原因？ 第四范式： 考虑的还是说这三家银行有着中国最优质的金融场景和信用，对于第四范式来说，在这三家机构里面，历练出来的产品和解决方案是经得起考验的，在市场上是标杆性的。我们也希望借助三大国有银行的优质资源，更多、更快的为行业赋能，在多方的共同努力下，共同推动金融业的智能化转型。  
327,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736740&idx=2&sn=1860eb0fe5542f11e28927c1c47c9878&scene=0,业界 | 谷歌正式发布TensorFlow 1.5：终于支持CUDA 9和cuDNN 7,昨天，谷歌在 GitHub 上正式发布了 TensorFlow 的最新版本 1.5.0，并开源了其代码。支持 CUDA 9 和 cuDNN 7 被认为是本次更新的最重要部分。机器之心对这次更新的重大改变以及主要功能和提升进行了编译介绍，原文请见文中链接。 GitHub 地址：https://github.com/tensorflow/tensorflow/releases/tag/v1.5.0 源代码（zip）：https://github.com/tensorflow/tensorflow/archive/v1.5.0.zip 源代码（tar.gz）：https://github.com/tensorflow/tensorflow/archive/v1.5.0.tar.gz 预构建的二进制文件现在是针对 CUDA 9 和 cuDNN 7 构建的。 从版本 1.6 开始，我们的预构建二进制文件将使用 AVX 指令。这也许会破坏较旧 CPUs 上的 TF。 Eager execution： 预览版现在可用。 TensorFlow Lite： dev 预览版现在可用。 支持 CUDA 9 和 cuDNN 7 加速线性代数（XLA）： 添加 complex64 支持到 XLA 编译器。 bfloat 支持现已添加到 XLA 架构。 使 XLA 和 XLA 一起工作。 通过决定性执行程序来生成 XLA 图。 tf.contrib： tf.contrib.distributions： 添加 tf.contrib.distributions.Autoregressive。 使 tf.contrib.distributions QuadratureCompound 类支持批处理。 从参数中推断 tf.contrib.distributions.RelaxedOneHotCategorical dtype。 通过 quadrature_grid_and_prob vs quadrature_degree 使 tf.contrib.distributions 正交族参数化。 添加 auto_correlation 到 tf.contrib.distributions。 添加 tf.contrib.bayesflow.layers，一个概率（神经）层的集合。 添加 tf.contrib.bayesflow.halton_sequence。 添加 tf.contrib.data.make_saveable_from_iterator。 添加 tf.contrib.data.shuffle_and_repeat。 添加新的自定义转换： tf.contrib.data.scan()。 tf.contrib.distributions.bijectors： 添加 tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow。 添加 tf.contrib.distributions.bijectors.Permute。 添加 tf.contrib.distributions.bijectors.Gumbel。 添加 tf.contrib.distributions.bijectors.Reshape。 支持 Reshape bijector 中的 shape 推理（即，包含-1 的 shape）。 添加 streaming_precision_recall_at_equal_thresholds，一种计算流式精确度和时间、空间复杂度为 O(num_thresholds + size of predictions) 的调用的方法。 更改 RunConfig 默认行为，不设置随机种子，使随机行为在分布式工作器上独立地随机。我们期待这可以普遍提高训练表现。依赖决定论的模型应明确设置一个随机种子。 通过 absl.flags 替换 tf.flags 的实现。 在 fp16 GEMM 中为 CUBLAS_TENSOR_OP_MATH 添加支持。 在 NVIDIA Tegra 计算卡上为 CUDA 添加支持。 文档更新： 明确你只能在 64 位机上安装 TensorFlow。 添加一个短文件解释 Estimators 如何保存检查点。 为由 tf2xla 桥支持的操作添加文档。 修改 SpaceToDepth 和 DepthToSpace 文件中的小的书写错误。 更新 mfcc_mel_filterbank.h 和 mfcc.h 中的文档命令，说明输入域是幅度谱的平方，权重 是在线性幅度谱（输入的 sqrt）上完成的。 修复 tf.distributions.bijectors.Bijector 中的文档字符串书写错误。 tf.assert_equal 不再引发 ValueError。它现在引发 InvalidArgumentError，如文档所述。 更新「开始」文件和 API 介绍。 谷歌云存储 (GCS)： 为 GCS 客户端添加用户空间 DNS 缓存。 为 GCS 文件系统自定义请求超时。 优化 GCS 的文件系统缓存。 Bug 修复 修复之前出现的整数变量分区后变成错误的 shape 的 bug。 修复 Adadelta 的 CPU 和 GPU 实现的准确度 bug。 修复当导入到 scope 时，import_meta_graph 处理分区变量时出现的 bug。警告：在以非空 import_scope 变量应用 import_meta_graph 之后，这可能会破坏带已保存分区变量图的加载中的检查点。 修复离线 debugger 中阻止查看事件的 bug。 添加 WorkerService.DeleteWorkerSession 方法到 gRPC 接口，修复内存泄漏。确保你的主服务器和辅助服务器在相同版本的 TensorFlow 上运行，以避免兼容性问题。 修复 BlockLSTM cell 的 peephole 实现中的 bug。 通过重写 log_det_jacobian 的 dtype 以在 TransformedDistribution 中匹配 log_prob。 确保 tf.distributions.Multinomial 不会在 log_prob 中下溢。在此之前，一个整数变量的所有分区会以非分区变量的 shape 进行初始化；经过修复之后，可以正确地初始化。 其它 添加必要的 shape 直到支持 bfloat16。 添加一种运行 ops 的方式，在 MonitoredSession 中使用阶跃函数。 添加 DenseFlipout 概率层（probabilistic layer）。 添加一种新的 flag：ignore_live_threads，可用于训练过程。如果将其设为 True，在成功完成训练之后，它会在拆除基础建设时忽略仍在运行的线程，而不会返回 RuntimeError。 重新标准化 DenseVariational，以作为其它概率层的更简化模板。 tf.data 现在在数据集元素中支持 tf.SparseTensor。 可以进行遍历 Tensor 的计算。 允许 SparseSegmentReduction ops 拥有丢失的 segment IDs。 修改自定义导出策略以包含多维稀疏浮点数分裂（multidimensional sparse float splits）。 由于 GPU 和 cuDNNv6 的支持，Conv2D、Conv2DBackpropInput、Conv2DBackpropFilter 现在支持任意的扩张。 Estimator 现在支持 Dataset：input_fn 可以返回 Dataset 而不是 Tensors。 添加 Revblock，一个可逆残差层的节省内存的实现。 减少 BFCAllocator 的内部碎片。 添加 cross_entropy 和 kl_divergence 到 tf.distributions.Distribution 中。 添加 tf.nn.softmax_cross_entropy_with_logits_v2，以允许标签的反向传播。 GPU 后端现在使用 ptxas 以编译生成的 PTX。 BufferAssignment 的协议缓存转储（protocol buffer dump）现在已确定。 改变 embedding op 以利用 DynamicStitch 的并行版本。 添加对稀疏多维特征列（sparse multidimensional feature columns）的支持。 加速仅有一个值的稀疏浮点数列（sparse float columns）的案例。 允许稀疏浮点数分裂（sparse float splits）以支持多价特征列（multivalent feature columns）。 添加 quantile 到 tf.distributions.TransformedDistribution。 在 GPU 上添加对 tf.depth_to_space 的 NCHW_VECT_C 支持。 在 GPU 上添加对 tf.space_to_depth 的 NCHW_VECT_C 支持。 API 变化 将 SqueezeDims 属性在 C++ API 中重命名为 Axis，作为 Squeeze op。 Stream::BlockHostUntilDone 返回 Status，而不是布尔值。 Minor refactor：将 stats 文件从 stochastic 移动到 common 并删除 stochastic。 
328,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736673&idx=5&sn=ef5da127663583073cbed80c0a4f5f76&scene=0,AAAI 2018 | 阿里&北大提出基于注意力机制的用户行为建模框架,"阿里巴巴数据技术团队与北京大学共同提出的 ATRank 是基于注意力机制的用户异构行为建模框架，可应用于推荐系统中。值得注意的是，该方法并没有使用 RNN、CNN 等技术，在保证优质效果的同时拥有更快的训练速度。目前，该研究已被选为 AAAI 2018 大会 Oral 论文。 论文：ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation 论文链接：https://arxiv.org/abs/1711.06632 摘要： 本文提出一种基于注意力机制的用户异构行为序列的建模框架，并将其应用到推荐场景中。我们将不同种类的用户行为序列进行分组编码，并映射到不同子空间中。我们利用 self-attention 对行为间的互相影响进行建模。最终我们得到用户的行为表征，下游任务就可以使用基本的注意力模型进行有更具指向性的决策。我们尝试用同一种模型同时预测多种类型的用户行为，使其达到多个单独模型预测单类型行为的效果。另外，由于我们的方法中没有使用 RNN,CNN 等方法，因此在提高效果的同时，该方法能够有更快的训练速度。 一个人是由其所表现出的行为所定义。而对用户精准、深入的研究也往往是很多商业问题的核心。从长期来看，随着人们可被记录的行为种类越来越多，平台方需要有能力通过融合各类不同的用户行为，更好的去理解用户，从而提供更好的个性化服务。 对于阿里巴巴来说，以消费者运营为核心理念的全域营销正是一个结合用户全生态行为数据来帮助品牌实现新营销的数据&技术驱动的解决方案。因此，对用户行为的研究就成为了一个非常核心的问题。其中，很大的挑战来自于能否对用户的异构行为数据进行更精细的处理。 在这样的背景下，本文提出一个通用的用户表征框架，试图融合不同类型的用户行为序列，并以此框架在推荐任务中进行了效果验证。另外，我们还通过多任务学习的方式，期望能够利用该用户表征实现不同的下游任务。 异构行为建模： 通常通过手动特征工程来表示用户特征。这些手工特征以聚合类特征或无时序的 id 特征集合为主。 单行为序列建模： 用户序列的建模通常会用 RNN（LSTM/GRU）或者 CNN + Pooling 的方式。RNN 难以并行，训练和预测时间较长，且 LSTM 中的 Internal Memory 无法记住特定的行为记录。CNN 也无法保留特定行为特征，且需要较深的层次来建立任意行为间的影响。 异构数据表征学习：参考知识图谱和 Multi-modal 的表征研究工作，但通常都有非常明显的映射监督。而在我们的任务中，异构的行为之间并没有像 image caption 这种任务那样明显的映射关系。 本文的主要贡献如下： 尝试设计和实现了一种能够融合用户多种时序行为数据的方法，较为创新的想法在于提出了一种同时考虑异构行为和时序的解决方案，并给出较为简洁的实现方式。 使用类似 Google 的 self-attention 机制去除 CNN、LSTM 的限制，让网络训练和预测速度变快的同时，效果还可以略有提升。 此框架便于扩展。可以允许更多不同类型的行为数据接入，同时提供多任务学习的机会，来弥补行为稀疏性。   整个用户表征的框架包括原始特征层，语义映射层，Self-Attention 层和目标网络。语义映射层能让不同的行为可以在不同的语义空间下进行比较和相互作用。Self-Attention 层让单个的行为本身变成考虑到其他行为影响的记录。目标网络则通过 Vanilla Attention 可以准确的找到相关的用户行为进行预测任务。通过 Time Encoding + Self Attention 的思路，我们的实验表明其的确可以替代 CNN/RNN 来描述序列信息，能使模型的训练和预测速度更快。 1. 行为分组 某个用户的行为序列可以用一个三元组来描述（动作类型，目标，时间）。我们先将用户不同的行为按照目标实体进行分组，如图中最下方不同颜色 group。例如商品行为，优惠券行为，关键字行为等等。动作类型可以是点击/收藏/加购、领取/使用等等。 每个实体都有自己不同的属性，包括实值特征和离散 id 类特征。动作类型是 id 类，我们也将时间离散化。三部分相加得到下一层的向量组。 即，某行为的编码 = 自定义目标编码 + lookup(离散化时间) + lookup(动作类型)。 由于实体的信息量不同，因此每一组行为编码的向量长度不一，其实也代表行为所含的信息量有所不同。另外，不同行为之间可能会共享一些参数，例如店铺 id，类目 id 这类特征的 lookup table，这样做能减少一定的稀疏性，同时降低参数总量。  分组的主要目的除了说明起来比较方便，还与实现有关。因为变长、异构的处理很难高效的在不分组的情况下实现。并且在后面还可以看到我们的方法实际上并不强制依赖于行为按时间排序。 2. 语义空间映射 这一层通过将异构行为线性映射到多个语义空间，来实现异构行为之间的同语义交流。例如框架图中想表达的空间是红绿蓝（RGB）构成的原子语义空间，下面的复合色彩（不同类型的用户行为）会投影到各个原子语义空间。在相同语义空间下，这些异构行为的相同语义成分才有了可比性。 类似的思路其实也在 knowledge graph representation 里也有出现。而在 NLP 领域，今年也有一些研究表明多语义空间的 attention 机制可以提升效果。个人认为的一点解释是说，如果不分多语义空间，会发生所谓语义中和的问题。简单的理解是，两个不同种类的行为 a,b 可能只在某种领域上有相关性，然而当 attention score 是一个全局的标量时，a,b 在不那么相关的领域上会增大互相影响，而在高度相关的领域上这种影响则会减弱。 尽管从实现的角度上来说，这一层就是所有行为编码向一个统一的空间进行映射，映射方法线性非线性都可以，但实际上，对于后面的网络层来说，我们可以看作是将一个大的空间划分为多语义空间，并在每个子空间里进行 self-attention 操作。因此从解释上来说，我们简单的把这个映射直接描述成对多个子语义空间进行投影。 3. Self Attention 层 Self Attention 层的目的实际上是想将用户的每一个行为从一个客观的表征，做成一个用户记忆中的表征。客观的表征是指，比如 A,B 做了同样一件事，这个行为本身的表征可能是相同的。但这个行为在 A,B 的记忆中，可能强度、清晰度是完全不一样的，这是因为 A,B 的其他行为不同。实际上，观察 softmax 函数可知，某种相似行为做的越多，他们的表征就越会被平均。而带来不一样体验的行为则会更容易保留自己的信息。因此 self attention 实际上模拟了一个行为被其他行为影响后的表征。 另外，Self Attention 可以有多层。可以看到，一层 Self-Attention 对应着一阶的行为影响。多层则会考虑多阶的行为影响。这个网络结构借鉴的是 google 的 self-attention 框架。 具体计算方式如下： 记 S 是整个语义层拼接后的输出，Sk 是第 k 个语义空间上的投影，则经过 self-attention 后第 k 个语义空间的表征计算公式为：   这里的 attention function 可以看做是一种 bilinear 的 attention 函数。最后的输出则是这些空间向量拼接后再加入一个前馈网络。 4. 目标网络 目标网络会随着下游任务的不同而定制。本文所涉及的任务是用户行为预测及推荐场景的点击预测的任务，采用的是 point-wise 的方式进行训练和预测。 框架图中灰色的 bar 代表待预测的任意种类的行为。我们将该行为也通过 embedding、projection 等转换，然后和用户表征产出的行为向量做 vanilla attention。最后 Attention 向量和目标向量将被送入一个 Ranking Network。其他场景强相关的特征可以放在这里。这个网络可以是任意的，可以是 wide & deep，deep FM，pnn 都行。我们在论文的实验中就是简单的 dnn。 离线实验 为了比较框架在单行为预测时的效果，我们在 amazon 购买行为的公开数据集上的实验。 训练收敛结果如下图： 用户平均 AUC 如下图： 实验结论：在行为预测或推荐任务中，self-attention + time encoding 也能较好的替代 cnn+pooling 或 lstm 的编码方式。训练时间上能较 cnn/lstm 快 4 倍。效果上也能比其他方法略好一些。  为了深究 Self-Attention 在多空间内的意义，我们在 amazon dataset 上做了一个简单的 case study。如下图：   从图中我们可以看到，不同的空间所关注的重点很不一样。例如空间 I, II, III, VIII 中每一行的 attention 分的趋势类似。这可能是主要体现不同行为总体的影响。另一些空间，例如 VII，高分 attention 趋向于形成稠密的正方形，我们可以看到这其实是因为这些商品属于同样的类目。 下图则是 vanilla attention 在不同语义空间下的得分情况。   论文中，我们离线收集了阿里电商用户对商品的购买点击收藏加购、优惠券领取、关键字搜索三种行为进行训练，同样的也对这三种不同的行为同时进行预测。其中，用户商品行为记录是全网的，但最终要预测的商品点击行为是店铺内某推荐场景的真实曝光、点击记录。优惠券、关键字的训练和预测都是全网行为。 我们分别构造了 7 种训练模式进行对比。分别是单行为样本预测同类行为（3 种），全行为多模型预测单行为（3 种），全行为单模型预测全行为（1 种）。在最后一种实验设置下，我们将三种预测任务各自切成 mini-batch，然后统一进行 shuffle 并训练。 实验结果如下表： all2one 是三个模型分别预测三个任务，all2all 是单模型预测三个任务，即三个任务共享所有参数，而没有各自独占的部分。因此 all2all 与 all2one 相比稍低可以理解。我们训练多任务 all2all 时，将三种不同的预测任务各自 batch 后进行充分随机的 shuffle。文中的多任务训练方式还是有很多可以提升的地方，前沿也出现了一些很好的可借鉴的方法，是我们目前正在尝试的方向之一。 实验表明，我们的框架可以通过融入更多的行为数据来达到更好的推荐/行为预测的效果。 总结 本文提出一个通用的用户表征框架，来融合不同类型的用户行为序列，并在推荐任务中得到验证。 未来，我们希望能结合更多实际的商业场景和更丰富的数据沉淀出灵活、可扩展的用户表征体系，从而更好的理解用户，提供更优质的个性化服务，输出更全面的数据能力。  "
329,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736673&idx=1&sn=d3c880d5a11a0865f5a4b0990c306344&scene=0,就喜欢看综述论文：情感分析中的深度学习,"近年来，深度学习有了突破性发展，NLP 领域里的情感分析任务逐渐引入了这种方法，并形成了很多业内最佳结果。本文中，来自领英与伊利诺伊大学芝加哥分校的研究人员对基于深度学习的情感分析研究进行了详细论述。 情感分析或观点挖掘是对人们对产品、服务、组织、个人、问题、事件、话题及其属性的观点、情感、情绪、评价和态度的计算研究。该领域的开始和快速发展与社交媒体的发展相一致，如评论、论坛、博客、微博、推特和社交网络，因为这是人类历史上第一次拥有如此海量的以数字形式记录的观点数据。早在 2000 年，情感分析就成为 NLP 中最活跃的研究领域之一。它在数据挖掘、Web 挖掘、文本挖掘和信息检索方面得到了广泛的研究。实际上，因其对商业和社会的整体重要性，它已经从计算机科学扩展到管理学和社会学，如营销、金融、政治学、传播学、健康科学，甚至历史学。这种发展原因在于观点是几乎所有人类活动的核心，是人类行为的重要影响因素。我们的信念、对现实的感知，以及我们所做的决策在很大程度上依赖于别人看到和评价世界的方式。因此，我们在做决策的时候，通常会寻求别人的意见。不只是个人，组织也是如此。 现有研究已经产生了可用于情感分析多项任务的大量技术，包括监督和无监督方法。在监督方法中，早期论文使用所有监督机器学习方法（如支持向量机、最大熵、朴素贝叶斯等）和特征组合。无监督方法包括使用情感词典、语法分析和句法模式的不同方法。现有多本综述书籍和论文，广泛地涵盖了早期的方法和应用。 大约十年前，深度学习成为强大的机器学习技术，在很多应用领域产生了当前最优的结果，包括计算机视觉、语音识别、NLP 等。近期将深度学习应用到情感分析也逐渐变得流行。本文首先概述深度学习，然后对基于深度学习的情感分析进行综述。 论文：Deep Learning for Sentiment Analysis: A Survey 论文链接：https://arxiv.org/pdf/1801.07883.pdf 作为一项学习数据的多层特征或表征的强大机器学习技术，深度学习的出现实现了当前最优的预测结果。伴随着在诸多应用领域的成功，深度学习近年来也被广泛应用于情感分析。本论文首先概述深度学习，接着全面调研深度学习在情感分析领域的应用现状。 前馈神经网络 常规前馈神经网络（FNN）不考虑输入数据可能具备的任何特定结构。尽管如此，它仍是非常强大的机器学习工具，尤其是与先进的正则化技术一起使用时。这些正则化技术帮助解决人们处理「深度」网络时遇到的训练问题：神经网络有大量隐藏层，隐藏层非常难以训练（梯度消失和过拟合问题）。 图 4.1：有 N + 1 层（N − 1 个隐藏层）的神经网络。浅层网络架构仅使用一个隐藏层。深度学习需要使用多个隐藏层，通常包含同样数量的隐藏神经元。数量大约是输入和输出变量数量的平均值。 FNN 由一个输入层、一个（浅层网络）或多个（深层网络，因此叫作深度学习）隐藏层，和一个输出层构成。每个层（除输出层以外）与下一层连接。这种连接是 FNN 架构的关键，具有两个主要特征：加权平均值和激活函数。 加权平均过程，即将前一层给神经元的激励值和对应的权重矩阵相乘而得出后一个神经元的输入值，这一过程展示在下图 4.2 中，我们可以说前一层神经元的加权和就是后一层神经元的输入。 正式地，加权平均的过程可以使用如下方程式表达： 此外，每一层的隐藏神经元可以定义为： 其中其中 v∈[0,N−1]、f∈[0,(F_v+1)−1]、t∈[0,(T_mb)− 1]。在这里 g 为非线性激活函数，是 FNN 另外一个十分重要的元素。因为激活函数的非线性属性，所以它允许预测任意的输出数据。 词嵌入 一般来说，Word2Vec 方法由两部分组成。首先是将高维 one-hot 形式表示的单词映射成低维向量。例如将 10，000 列的矩阵转换为 300 列的矩阵，这一过程被称为词嵌入。第二个目标是在保留单词上下文的同时，从一定程度上保留其意义。Word2Vec 实现这两个目标的方法有 skip-gram 和 CBOW 等，skip-gram 会输入一个词，然后尝试估计其它词出现在该词附近的概率。还有一种与此相反的被称为连续词袋模型（Continuous Bag Of Words，CBOW），它将一些上下文词语作为输入，并通过评估概率找出最适合（概率最大）该上下文的词。 对于连续词袋模型而言，Mikolov 等人运用目标词前面和后面的 n 个词来同时预测这个词。他们称这个模型为连续的词袋（CBOW），因为它用连续空间来表示词，而且这些词的先后顺序并不重要。 CBOW 可以看作一个具有先知的语言模型，而 skip-gram 模型则完全改变将语言模型的目标：它不像 CBOW 一样从周围的词预测中间的词；恰恰相反，它用中心语去预测周围的词： 自编码器与降噪自编码器 自编码器神经网络是一个三层神经网络，其目标是使输出值近似等价于输入值。下图展示了自编码器的一般架构： 图 3：自编码器神经网络 因为神经元使用了非线性激活函数，自编码器可以学习非线性表征。这令自编码器比主成分分析（PCA）或潜在语义分析（LSA）等线性方法要强大很多。 若我们将自编码器以层级的形式堆叠，那么高层的自编码器就使用低层自编码器的输出作为输入。这种堆叠的自编码器与受限玻尔兹曼机（RBM）是构建深度神经网络的早期方法。一旦我们以无监督的形式训练自编码器，那么描述 x（中间表征）多级表征的参数就能用来初始化监督式深度神经网络，这种神经网络在实验上已经证明要比随机初始化优秀。 降噪自编码器（DAE）是自编码器的扩展，DAE 背后的思想是强制隐藏层发现更鲁棒的特征，并阻止自编码器简单地学习恒等变换。也就是说，模型应该在存在噪声时仍能重构输入。这种技术也体现在情感分析中，例如从文档中删除或添加一些文字不应该改变文档的语义。 卷积神经网络 CNN 非常擅长处理图像数据，如下图所示，它们一般 由若干个卷积和池化操作组成，通常跟随着一个或多个全连接层（与传统的 FNN 层相似）。 相比于全连接神经网络，卷积网络每一个单元都只会和上一层部分单元相连接。一般每个卷积层的单元都可以组织成一个三维张量，即矩阵沿第三个方向增加一维数据。例如 Cifar-10 数据集的输入层就可以组织成 32×32×3 的三维张量，其中 32×32 代表图片的尺寸或像素数量，而 3 代表 RGB 三色通道。 卷积神经网络中最重要的就是卷积层，卷积层试图将神经网络中的每一小块进行更加深入的分析，从而得出抽象程度更高的特征。一般来说通过卷积层处理的神经元结点矩阵会变得更深，即神经元的组织在第三个维度上会增加。 图4：卷积神经网络 为了理解卷积层，下图展示了卷积核或滤波器（filter）将当前层级上的一个子结点张量转化为下一层神经网络上的一个长和宽都为 1，深度不限的结点矩阵。下图输入是一个 32×32×3 的张量，中间的小长方体为卷积核，一般可以为 3×3 或 5×5 等，且因为要计算乘积，那么卷积核的第三个维度必须和其处理的图像深度（即输入张量第三个维度 3）相等。最右边的矩形体的深度为 5，即前面使用了五个卷积核执行卷积操作。这五个卷积核有不同的权重，但每一个卷积层使用一个卷积核的权重是一样的，所以下图五层特征中每一层特征都是通过一个卷积核得出来的，也就是该层共享了权重。 循环神经网络 在传统的前馈神经网络中，所有的示例都被认为是独立的，它们没有时间关联性。 这种时间关联性是由循环神经网络实现的。一个典型的 RNN 结构如下： 如果将其展开，它会变成这样： 在这些图表中， x_t 是时间序列上的输入，而 h_t 是循环过程中的隐藏状态。我们看到 f 重复作用于不同时间步上的隐藏状态，并将它传入下一个时间步中，这就是 RNN特有的方式。基本上，你能输入句子中的词或者甚至是像 x_t 这样的字符串中的字符，然后通过该循环神经网络它会得出一个 y_t。 目标是用 y_t 作为输出，并将它与你的测试数据（通常是原始数据的一个小子集）比较。然后你会得出你的误差率。比较完之后，有了误差率，你就能使用一种叫随时间反向传播（BPTT）的技术。BPTT 返回检查这个网络，并基于误差率调整权重。这样也调整了这个网络，并让它学习去做得更好。 LSTM 网络 下面我们简要地向读者介绍 LSTM 单元选择记忆或遗忘的具体处理流程。 以下是 LSTM 单元的详细结构，其中 Z 为输入部分，Z_i、Z_o 和 Z_f 分别为控制三个门的值，即它们会通过激活函数 f 对输入信息进行筛选。一般激活函数可以选择为 Sigmoid 函数，因为它的输出值为 0 到 1，即表示这三个门被打开的程度。 若我们输入 Z，那么该输入向量通过激活函数得到的 g(Z) 和输入门 f(Z_i ) 的乘积 g(Z) f(Z_i ) 就表示输入数据经筛选后所保留的信息。Z_f 控制的遗忘门将控制以前记忆的信息到底需要保留多少，保留的记忆可以用方程 c*f（z_f）表示。以前保留的信息加上当前输入有意义的信息将会保留至下一个 LSTM 单元，即我们可以用 c' = g(Z)f(Z_i) + cf(z_f) 表示更新的记忆，更新的记忆 c' 也表示前面与当前所保留的全部有用信息。我们再取这一更新记忆的激活值 h(c') 作为可能的输出，一般可以选择 tanh 激活函数。最后剩下的就是由 Z_o 所控制的输出门，它决定当前记忆所激活的输出到底哪些是有用的。因此最终 LSTM 的输出就可以表示为 a = h(c')f(Z_o)。 RNN 与注意力机制 一般来说，我们可能会认为双向 RNN 与 LSTM 就能处理数据中的长期依赖性。但是在实践中，时序数据的长期依赖性问题仍然很难处理。因此，Bahdanau 等人提出了注意力机制。 神经网络中的注意力机制受到人类视觉中注意力的启发，即人类视觉注意力能够聚焦到图像的特定区域，并在这个区域有非常高的分辨率，而在其它区域有较低的分辨率。在自然语言处理中，注意力机制允许模型根据输入文本以及它到目前为止已经生成的隐藏状态来学习要注意什么，而不像标准 RNN 与 LSTM 那样将全部原文本编码成固定长度的向量。 下图 8 展示了在双向 RNN 中使用注意力机制的方法。其中每个解码器输出的序列 y_t 取决于所有输入状态的加权组合，而不只是如标准情况那样选择最后一个隐藏状态。a_t,T 定义了每个输入的隐藏状态应该加权多少以结合为输出向量。例如，a_2,2 有较大的值，那么它就代表着在第二个时间步上，解码器更多注意原语句中的第二个隐藏状态。所有的权重 a_t,T 加和为 1，因此能保证输出值的归一化。 记忆网络 Weston 等人介绍了记忆网络（MemNN）这个概念，它能用于问答系统。记忆网络通过结合多个推断组件和长期记忆而执行任务，这些组件可以是多个神经网络，而长期记忆充当着动态知识库的角色。记忆网络基本的四个可学习或推断组件分别为：I 组件将输入数据转化为内部特征表示；G 组件在给定新的输入下更新旧的记忆；O 组件生成输出（同样是在特征表示空间中完成）；R 组件将输出特征转化为响应格式。例如，给定问答系统一系列语句和问题，MemNN 会从这些语句中抽取特征并生成答案。 在推断的过程中，I 组件一次只读取一条语句，并将它编码为向量表征。然后 G 组件基于当前的语句表征更新一小块记忆，在所有语句都处理完后，记忆网络就生成了一个记忆矩阵（每一行表示一个语句），该矩阵储存了从语句中抽取的语义。对于问题，记忆网络会将它编码为向量表征，然后 O 组件使用向量从记忆中选择一些相关的证据，并生成一个输出向量。最后，R 组件将输出向量作为输入，并输出最终响应。 递归神经网络 词嵌入是将单词表示成低维的稠密的实数向 量。自从词向量技术的提出，到目前为止已经有很多方法来得到句法和语义方面的向量表示， 这种技术在 NLP 领域发挥着重要的作用。 如何用稠密的向量表示短语，这是使用词向量的一个难题。在成分分析中，我们一般使用递归神经网络 (Recursive Neural Network) 来解决这个问题。递归神经网络是一种通用的模型，用来对句子进行建模。句子的语法树中的左右子节点通过一层线性神经网络结合起来，根节点的这层神经网络的参数就表示整句句子。递归神经网络能够给语法树中的所有叶子节点一个固定长度的向量表示，然后递归地给中间节点建立向量的表示。 我们现在开始概述情感分析中的深度学习应用。但在此之前，我们首先简单介绍主要的情感分析任务。若想了解更多细节，请参考 Liu 写的关于情感分析的书。 研究者主要在三个粒度级别上研究情感分析：文档级、语句级和 aspect level。文档级情感分类将观点鲜明的文档（例如，产品评论）分类为整体积极的或消极的观点。它将整个文档当做基本的信息单元，并假定文档是观点鲜明的，包含对单个实体（例如，某个型号的手机）的观点。语句级情感分类对文档内单独的语句进行分类。然而，单独的语句不能假定为观点鲜明的。 传统上，人们首先将一个语句分类为观点鲜明的（或相反），即主观性分类。 然后观点鲜明的语句进一步被分类为积极的或消极的。语句级情感分类可以被形式化为三类分类问题，即判断某语句是中性的、积极的或消极的。和文档级、语句级情感分类相比，aspect level 情感分析或基于 aspect 的情感分析更加细粒化。它的任务是提取和总结人们对某实体的观点以及实体（也被称为目标）的特征。例如一篇产品评论，aspect level 情感分析的目的是分别总结对产品不同方面的积极和消极观点，虽然对产品的总体情感可能是倾向积极的或消极的。 基于 aspect 的情感分析由多个子任务构成，例如 aspect 提取、实体提取和 aspect 情感分类。例如，句子「the voice quality of iPhone is great, but its battery sucks」的实体提取应该识别「iPhone」作为实体，而 aspect 提取需要识别「voice quality」和「battery」作为两个 aspect。aspect level 情感分类需要将对音质的评论分类为积极的，将对电池续航的评论分类为消极的。出于简洁性，大多数算法将 aspect 提取和实体提取结合起来，称为 aspect 提取或情感/观点目标提取。 除了这些核心任务以外，情感分析还研究了情绪分析、嘲讽检测、多语言情感分析等。在接下来的章节中，我们将概述所有这些情感分析任务中的深度学习应用。 文档级情感分类 文档级情感分类是指为观点型文档标记整体的情感倾向／极性，即确定文档整体上传达的是积极的还是消极的观点。因此，这是一个二元分类任务，也可以形式化为回归任务，例如为文档按 1 到 5 星评级。一些研究者也将其看成一个五类分类任务。 情感分类通常被当做文档分类的特殊案例。在这种分类任务中，文档表征是很重要的部分，需要反映出文档字里行间所传达的原始信息。传统上，词袋模型（BoW）通过将文档看成其中单词的袋装形式，被用于在 NLP 和文本挖掘中生成文本表征。通过 BoW，文档被转换成固定长度的数值特征向量，其中每个元素可能代表词的存在（没出现或出现）、词频或 TF-IDF 分数。向量的维度等于词汇量大小。用 BoW 表征的文档向量通常是很稀疏的，因为单个文档仅包含少量的词汇。早期的神经网络使用的基本是这种特征设定。 虽然 BoW 很常用，它也有一些缺点。首先，BoW 模型忽略词的顺序，这意味着包含相同单词的两个文档的表征是完全相同的。BoW 的扩展版本 Bag-of-N-Grams 在短文本（n-gram）中考虑词序，但仍然存在数据稀疏性和高维度的缺陷。其次，BoW 几乎不能编码词的语义。例如，在 BoW 中，单词「smart」、「clever」和「book」之间的距离是相同的，但在语义上，相比「book」，「smart」应该更接近于「clever」。 为了克服 BoW 的缺陷，人们提出了基于神经网络的词嵌入技术以生成密集向量（或低维向量）用于词表征，从而在某种程度上可以编码单词的某些语义和句法属性。以词嵌入作为词的输入，可以利用神经网络得到文档的密集向量（或称为密集文档向量）表征。 除了以上两种方法，实际上也可以直接用 BoW 学习密集文档向量。我们在表 2 中区分了相关研究使用的不同方法。 当文档被适当地表征时，我们可以通过传统的监督学习方式，用多种神经网络模型进行情感分类。在某些案例中，神经网络可能只被用于提取文本特征或文本表征，然后这些特征被馈送到其它非神经网络的分类器（如 SVM），以获得最终的全局最优分类器。神经网络和 SVM 的特性以某种方式彼此互补，从而能结合各自的优势。 除了复杂的文档/文本表征之外，研究者还利用数据特征（如产品评论）进行情感分类。一些研究者发现产品评论对情感和其它附加信息（例如，用户信息和产品信息）进行联合分类建模很有帮助。此外，由于文档通常包含长期依赖关系，注意力机制也经常用于文档级情感分类。我们在表 2 中总结了已有的技术。 语句级的情感分类 语句级情感分类用来标定单句中的表达情感。正如之前所讨论的，句子的情感可以用主观性分类和极性分类来推断，前者将句子分为主观或客观的，而后者则判定主观句子表示消极或积极的情感。在现有的深度学习模型中，句子情感分类通常会形成一个联合的三类别分类问题，即预测句子为积极、中立或消极。 与文档级的情感分类相同，神经网络生成的语句表征对于语句级的情感分类也非常重要。另外由于句子相对文档而言较短，因此可以使用一些语法和语义信息（如解析树、观念词典和词性标签）来帮助分类。其他一些信息如评测打分、社会关系和跨域信息也可以考虑在内。例如，社会关系已被用于探索社交媒体数据中（如推文）的情感。 在早期的研究中，解析树（提供了一些语义和语法信息）与原始词一同用作神经模型的输入，这意味着我们可以更好地推断情感构成。但在那之后，CNN 和 RNN 成为主流，它们不需要利用解析树从句子中提取特征。取而代之的是，CNN 与 RNN 使用词嵌入（已经编码了一些语义和语法信息）作为输入。此外，CNN 和 RNN 模型架构也可以帮助我们学习语句内词间的固有联系。 Aspect Level 情感分类 与文档级和语句级的情感分类不同，aspect level 情感分类同时考虑了情感信息和目标信息（情感一般都会有一个目标）。如前所述，目标通常是一个实体或实体特征。出于简洁性，实体和实体特征通常都称为特征（aspect）。给定一个句子和目标特征，aspect level 情感分类可以推断出句子在目标特征的情感极性／倾向。例如，句子「the screen is very clear but the battery life is too short.」中，如果目标特征是「screen」，则情感是积极的，如果目标特征是「battery life」，则情感是消极的。下一节将讨论自动特征提取或目标提取。 Aspect Level 情感分类很有难度，因为建模目标与上下文的语境词的语义相关性很难。不同的语境词对句子在目标特征的情感极性有不同的影响。因此，使用神经网络构建学习模型时，捕捉目标词和语境词之间的语义关系非常必要。 使用神经网络的 aspect level 情感分类有三个重要任务。第一个任务是表示目标的语境词。该问题可以使用前两节提到的文本表示方法来解决。第二个任务是生成目标表示，其可与语境词进行恰当地互动。通常的解决方案是学习目标嵌入（与词嵌入类似）。第三个任务是识别特定目标的重要情感语境词。例如，在句子「the screen of iPhone is clear but batter life is short」中，「clear」是「screen」的重要语境词，「short」是「battery life」的重要语境词。近期该任务通过注意力机制得到解决。尽管很多深度学习技术可用于处理 aspect level 情感分类，但文献中仍然没有主导性技术。 带有词嵌入的情感分析 很明显词嵌入在深度学习情感分析模型中扮演了重要角色。研究也表明，即使不使用深度学习模型，词嵌入也可以在不同任务中用作非神经网络学习模型的特征。因此，该部分特别强调了词嵌入对情感分析的贡献。 我们首先介绍了情感编码词嵌入的工作。对于情感分析，直接使用 CBOW 或 Skip-gram 等常规的单词方法学习语境中的词嵌入可能会遇到问题，因为具有相似语境但情感极性相反（例如，「好」或「坏」）的单词可能被映射到嵌入空间的相近向量。因此，人们提出了情感编码词嵌入方法。Mass el al.101 学习了可以捕捉语义和情感信息的词嵌入。Bespalov et al.102 表明，n-gram 模型结合潜在表征将为情感分类提供更合适的嵌入。通过把语句的情感监督作为正则化项，Labutov and Lipson103 将带有 logistic 回归的现有词嵌入进行重嵌入。 用于情感分析的多模态数据 多模态数据已被用于情感分析，因为其比文本提供了更多的信息。深度学习模型把输入映射到一些特征空间，来自多模态数据的不同形式的输入也可以被这些模型投射到一些联合潜在空间或表征。因此，使用深度学习模型处理多模态数据的趋势不断增长。 例如 Wang et al. 提出一个 CNN 结构的深度网络，命名为深度耦合形容词与名词神经网络（DCAN），可用于视觉情感分类。DCAN 的核心思想是利用形容词和名词性文本描述，把它们看作两个（弱）监督信号以学习两个中间情感表征，然后结合学习的表征并用于情感分类。  "
330,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736673&idx=2&sn=872758c4ffa215518209e61151ff8876&scene=0,业界 | 滴滴成立AI Labs：由副总裁叶杰平教授领导,1 月 26 日消息，滴滴出行宣布成立 AI Labs（人工智能实验室），以加大人工智能前瞻性基础研究，吸引顶尖科研人才，加快推进全球智能交通前沿技术发展。 这是继滴滴研究院、滴滴美国研究院之后，滴滴科研网络进一步扩展。AI Labs 的成立也彰显了滴滴加大 AI 交通技术投入的决心。滴滴 AI Labs 由滴滴副总裁叶杰平教授领导，目前团队已有两百余位从事 AI 前沿技术研发的科学家和工程师，今年规模将进一步提升。 滴滴 AI Labs 将主要探索 AI 领域技术难题，重点发力机器学习、自然语言处理、语音识别、计算机视觉、运筹学、统计学等领域的前瞻技术研究及应用，积极布局下一代技术，不断提升用户出行效率并且优化出行体验，用技术构建智能出行新生态。同时，滴滴 AI Labs 也将秉持开放合作的理念，持续链接全球顶级创新资源，激励更多科技创新，吸引、培养更多 AI 人才。 滴滴出行总裁柳青称，滴滴一直在用 AI 在解决人们出行问题，滴滴发力 AI，是希望每天人们的出行能更加便捷，每天有几千万的司机能在滴滴的平台上获得收入，每天能够有更多的人觉得出门是一件安全的事情。我们成立 AI Labs，是希望面向未来进行前沿探索，让人工智能能够为全人类服务。我们也欢迎全球最顶级的人才加入到滴滴，「在滴滴定义出行的这个过程里，AI Labs 也将帮助定义出行领域的技术边界。」 滴滴出行 CTO 张博表示，未来十年全球交通产业会发生剧烈的变革，滴滴也将坚定地在技术和人才方面持续加大投入。当前滴滴在交通领域积累了海量优质数据，也拥有强大的数据处理、云计算能力。基于领先的大数据和技术优势，AI Labs 将持续在智能交通前沿做出更多探索，吸纳全球顶尖人才和研究机构融入、合作，用技术让出行更美好。 叶杰平指出，大数据和人工智能是未来交通创新的革命性技术，滴滴也早已在这些方面进行积极布局。无论是乘客发单前的预测目的地、推荐上车点，还是发单后的智能派单、ETA、路径规划，甚至行程中的安全驾驶，行程结束后的司乘判责环节，都大量地使用了人工智能技术。不仅如此，基于人工智能和大数据技术，我们还会精准预测未来城市的供需情况并提前调度，目前我们对 15 分钟后的需求预测准确率已达 85%。 据叶杰平介绍，除专注于 AI 领域前沿问题、深层次拓展滴滴的 AI 核心技术能力之外，AI Labs 还将积极加速技术能力与数据资源、应用环境的有机结合，推进 AI 技术在智能出行场景中的更多应用和创新优化，如滴滴大脑、滴滴助手等，「AI Labs 致力于成为交通出行前沿技术的驱动者和引领者和人工智能人才的培养者，我们也将广泛吸引行业顶尖人才，共同推动全球 AI 技术的发展。」  
331,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736673&idx=4&sn=d5cb11250b28912accbc08ddb5d9c97b&scene=0,专栏 | fastText原理及实践,"fastText是Facebook于2016年开源的一个词向量计算和文本分类工具，在学术上并没有太大创新。 在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 能够训练10亿词级别语料库的词向量在10分钟之内，能够分类有着30万多类别的50多万句子在1分钟之内。 本文首先会介绍一些预备知识，比如softmax、ngram等，然后简单介绍word2vec原理，之后来讲解fastText的原理，并着手使用keras搭建一个简单的fastText分类器，最后，我们会介绍fastText在达观数据的应用。 Softmax回归 Softmax回归（Softmax Regression）又被称作多项逻辑回归（multinomial logistic regression），它是逻辑回归在处理多类别任务上的推广。 在逻辑回归中， 我们有m个被标注的样本： 其中 。因为类标是二元的，所以我们有 。我们的假设（hypothesis）有如下形式： 代价函数（cost function）如下： 在Softmax回归中，类标是大于2的，因此在我们的训练集 中， 。给定一个测试输入x，我们的假设应该输出一个K维的向量，向量内每个元素的值表示x属于当前类别的概率。具体地，假设 形式如下： 代价函数如下： 其中1{·}是指示函数，即1=1,1=0 既然我们说Softmax回归是逻辑回归的推广，那我们是否能够在代价函数上推导出它们的一致性呢？当然可以，于是： 可以看到，逻辑回归是softmax回归在K=2时的特例。 分层Softmax 你可能也发现了，标准的Softmax回归中，要计算y=j时的Softmax概率： ，我们需要对所有的K个概率做归一化，这在|y|很大时非常耗时。于是，分层Softmax诞生了，它的基本思想是使用树的层级结构替代扁平化的标准Softmax，使得在计算 时，只需计算一条路径上的所有节点的概率值，无需在意其它的节点。 下图是一个分层Softmax示例： 树的结构是根据类标的频数构造的霍夫曼树。K个不同的类标组成所有的叶子节点，K-1个内部节点作为内部参数，从根节点到某个叶子节点经过的节点和边形成一条路径，路径长度被表示为 。于是， 就可以被写成： 其中： 表示sigmoid函数； 表示n节点的左孩子； 是一个特殊的函数，被定义为： 是中间节点 的参数；X是Softmax层的输入。 上图中，高亮的节点和边是从根节点到   的路径，路径长度 可以被表示为： 于是，从根节点走到叶子节点 ，实际上是在做了3次二分类的逻辑回归。 通过分层的Softmax，计算复杂度一下从|K|降低到log|K|。 n-gram特征 在文本特征提取中，常常能看到n-gram的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列。看下面的例子： 我来到达观数据参观 相应的bigram特征为： 我来 来到 到达 达观 观数 数据 据参 参观 相应的trigram特征为： 我来到 来到达 到达观 达观数 观数据 数据参 据参观 注意一点： n-gram中的gram根据粒度不同，有不同的含义。它可以是字粒度，也可以是词粒度的。上面所举的例子属于 的n-gram， 的n-gram看下面例子： 我 来到 达观数据 参观 相应的bigram特征为： 我/来到 来到/达观数据 达观数据/参观 相应的trigram特征为： 我/来到/达观数据 来到/达观数据/参观 n-gram产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。 你可能要问，这篇文章不是介绍fastText的么，怎么开始介绍起了word2vec？ 于是，你看到facebook开源的fastText工具不仅实现了fastText文本分类工具，还实现了快速词向量训练工具。word2vec主要有两种模型：skip-gram 模型和CBOW模型，这里只介绍CBOW模型，有关skip-gram模型的内容请参考达观另一篇技术文章： 漫谈Word2vec之skip-gram模型 模型架构 CBOW模型的基本思路是：用上下文预测目标词汇。架构图如下所示： 输入层由目标词汇y的上下文单词   组成，   是被onehot编码过的V维向量，其中V是词汇量；隐含层是N维向量 h ；输出层是被onehot编码过的目标词y。输入向量通过  维的权重矩阵W连接到隐含层；隐含层通过   维的权重矩阵  到输出层。因为词库V往往非常大，使用标准的softmax计算相当耗时，于是CBOW的输出层采用的正是上文提到过的分层Softmax。 前向传播 输入是如何计算而获得输出呢？先假设我们已经获得了权重矩阵 和 （具体的推导见第3节），隐含层 h 的输出的计算公式： 即：隐含层的输出是C个上下文单词向量的加权平均，权重为 。 接着我们计算输出层的每个节点： 这里 是矩阵 的第j列，最后，将 作为softmax函数的输入，得到 ： 在学习权重矩阵和过程中，我们首先随机产生初始值，然后feed训练样本到我们的模型，并观测我们期望输出和真实输出的误差。接着，我们计算误差关于权重矩阵的梯度，并在梯度的方向纠正它们。 首先 ，objective是最大化给定输入上下文，target单词的条件概率。因此，损失函数为： 这里， 表示目标单词在词库V中的索引。 如何 ? 我们先对E关于 求导： 函数表示： 于是， 的更新公式： 如何 ？ 我们首先计算E关于隐含层节点的导数： 然后，E关于权重的导数为： 于是， 的更新公式： 终于到我们的fastText出场了。这里有一点需要特别注意，一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。除非你决定使用预训练的embedding来训练fastText分类模型，这另当别论。 字符级别的n-gram word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征，比如：“apple” 和“apples”，“达观数据”和“达观”，这两个例子中，两个单词都有较多公共字符，即它们的内部形态类似，但是在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了。 为了克服这个问题，fastText使用了字符级别的n-grams来表示一个单词。 对于单词“apple”，假设n的取值为3，则它的trigram有: “<ap”,  “app”,  “ppl”,  “ple”, “le>” 其中，<表示前缀，>表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。 这带来两点 ： 1. 对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。 2. 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。 之前提到过，fastText模型架构和word2vec的CBOW模型架构非常相似。下面是fastText模型架构图： 注意： 此架构图没有展示词向量的训练过程。可以看到，和CBOW一样，fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。 不同的是， CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。 值得注意的是，fastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间。 这两个知识点在前文中已经讲过，这里不再赘述。 fastText相关公式的推导和CBOW非常类似，这里也不展开了。 现在抛开那些不是很讨人喜欢的公式推导，来想一想fastText文本分类的核心思想是什么？ 仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。 于是fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。 这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。 还有个问题，就是为何fastText的分类效果常常不输于传统的非线性分类器？ 假设我们有两段文本： 我 来到 达观数据 俺 去了 达而观信息科技 这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如tfidf值， “我”和“俺” 算出的tfidf值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。 但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度 ，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。 ，这是fastText效果好的一个原因；另一个原因就是  。 keras是一个抽象层次很高的神经网络API，由python编写，底层可以基于Tensorflow、Theano或者CNTK。它的优点在于：用户友好、模块性好、易扩展等。所以下面我会用keras简单搭一个fastText的demo版，生产可用的fastText请移步 github.com/facebookrese 。 如果你弄懂了上面所讲的它的原理，下面的demo对你来讲应该是非常明了的。 为了简化我们的任务： 1. 训练词向量时，我们使用正常的word2vec方法，而真实的fastText还附加了字符级别的n-gram作为特征输入; 2. 我们的输出层使用简单的softmax分类，而真实的fastText使用的是Hierarchical Softmax。 首先定义几个常量： VOCAB_SIZE = 2000 EMBEDDING_DIM =100 MAX_WORDS = 500 CLASS_NUM = 5 表示词汇表大小，这里简单设置为2000； 表示经过embedding层输出，每个词被分布式表示的向量的维度，这里设置为100。比如对于“达观”这个词，会被一个长度为100的类似于[ 0.97860014, 5.93589592, 0.22342691, -3.83102846, -0.23053935, …]的实值向量来表示； 表示一篇文档最多使用的词个数，因为文档可能长短不一（即词数不同），为了能feed到一个固定维度的神经网络，我们需要设置一个最大词数，对于词数少于这个阈值的文档，我们需要用“未知词”去填充。比如可以设置词汇表中索引为0的词为“未知词”，用0去填充少于阈值的部分； 表示类别数，多分类问题，这里简单设置为5。 ： 1.  。Embedding层的输入是一批文档，每个文档由一个词汇索引序列构成。例如：[10, 30, 80, 1000] 可能表示“我 昨天 来到 达观数据”这个短文本，其中“我”、“昨天”、“来到”、“达观数据”在词汇表中的索引分别是10、30、80、1000；Embedding层将每个单词映射成EMBEDDING_DIM维的向量。于是：input_shape=(BATCH_SIZE, MAX_WORDS), output_shape=(BATCH_SIZE,MAX_WORDS, EMBEDDING_DIM)； 2.  。投影层对一个文档中所有单词的向量进行叠加平均。keras提供的GlobalAveragePooling1D类可以帮我们实现这个功能。这层的input_shape是Embedding层的output_shape，这层的output_shape=( BATCH_SIZE, EMBEDDING_DIM)； 3.  。真实的fastText这层是Hierarchical Softmax，因为keras原生并没有支持Hierarchical Softmax，所以这里用Softmax代替。这层指定了CLASS_NUM，对于一篇文档，输出层会产生CLASS_NUM个概率值，分别表示此文档属于当前类的可能性。这层的output_shape=(BATCH_SIZE, CLASS_NUM) 4.  。损失函数我们设置为categorical_crossentropy，它就是我们上面所说的softmax回归的损失函数；优化器我们设置为SGD，表示随机梯度下降优化器；评价指标选择accuracy，表示精度。 ，你需要： 1.  。词汇表中每个词用一个整数（索引）来代替，并预留“未知词”索引，假设为0； 2.  。假设我们文本数据总共有3个类别，对应的类标分别是1、2、3，那么这三个类标对应的onehot向量分别是[1, 0, 0]、[0, 1, 0]、[0, 0, 1]； 3.  。就像之前的例子，“我 昨天 来到 达观数据”可能被转化为[10, 30, 80, 1000]；它属于类别1，它的类标就是[1, 0, 0]。由于我们设置了MAX_WORDS=500，这个短文本向量后面就需要补496个0，即[10, 30, 80, 1000, 0, 0, 0, …, 0]。因此，batch_xs的 维度为( BATCH_SIZE,MAX_WORDS)，batch_ys的维度为（BATCH_SIZE, CLASS_NUM）。 下面是构建模型的代码，数据处理、feed数据到模型的代码比较繁琐，这里不展示。 fastText作为诞生不久的词向量训练、文本分类工具，在达观得到了比较深入的应用。主要被用在以下两个系统： 1. 同近义词挖掘。 Facebook开源的fastText工具也实现了词向量的训练，达观基于各种垂直领域的语料，使用其挖掘出一批同近义词； 2. 文本分类系统。 在类标数、数据量都比较大时，达观会选择fastText 来做文本分类，以实现快速训练预测、节省内存的目的。 王江，达观数据自然语言处理工程师，负责达观NLP底层开发、私有化应用系统开发等工作。主要参与大型系统的开发，对机器学习、NLP等领域有浓厚兴趣。 "
332,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736647&idx=3&sn=3ac17df845f2e99a98aa7bc0dea3d147&chksm=871acc79b06d456fa040b2569f13336fb50414423c80454f1a2a76ab54a373a29a9f299ad6d1&scene=27,专栏  | Detectron精读系列之一：学习率的调节和踩坑,"大家期盼已久的 Detectron 终于开源啦，state-of-the-art 的模型就在一个命令行之间。但是面对庞大的 caffe2 和 detectron 函数库，多少会感觉有些迷茫。Detectron 精读系列会从细小的调参开始，到一些重要的函数分析，最后掌握 Detectron 函数库的全貌。在这个过程中，我们也会帮大家提前踩坑，希望大家可以从 Detectron 函数库学到更多通用的计算机视觉技能。 Detectron 函数库有一点复杂，在这次的解读中我们主要介绍 multi-gpu 训练的时候，学习率如何调节的问题。看似简单的问题，最近 Facebook，Google 和 Berkeley 多个组都发表了论文对这一问题进行了理论和实验的讨论，首先我们对这些论文的结果做一个简要的总结。三篇论文研究类似，但是每一篇相对于前一篇都有改进。 Facebook : Training Imagenet in 1 hour 贡献： 提出了 Linear Scaling Rule，当 Batch size 变为 K 倍时，Learning rate 需要乘以 K 就能够达到同样的训练结果。看似简单的定律，Facebook 的论文给出了不完整的理论解释和坚实的实验证明。除此之外，论文还讨论了如何处理 Batch Normalization 如何实现分布式 SGD。通过 Linear Scaling Rule，Facebook 成功的在一小时内训练了 Batch size 为 8192 的 Resnet 50。 缺陷： 当 Batch Size 超过 8000 时观测到了模型训练结果会严重变差。如下图： Berkeley : Large Batch Training of Convolution Networks Berkeley 的组发现 Facebook 提出的 Linear Scaling Rule 当 Batch Size 过大时训练不稳定，容易发散。并且当模型 Batch Size 超过 8000 时，结果会严重退化。作者提出了 Layer Wise Adaptive Rate Scaling（LARS）定律，从而能够在 Batch Size 为 32000 的情况下高效的训练 ResNet 50 网络。SGD 的权值更新等于梯度乘以 Learning rate，论文中作者提出了 global learning rate 和 local learning rate 决定，global learning rate 所有层共享，local learning rate 由梯度的变化速率决定，这一想法是受 ADAM 等 adaptive learning rate SGD 的方法的影响。下表格为 LARS 的训练结果：  Google : Don’t decrease the learning rate, increase the batch size LSR 和 LARS 大大的提高了模型的并行性，但是不能够提高模型的结果。Google 组的研究发现，通过增大 Batch Size 保持 Learning rate 不变的办法可以得到与 decreasing learning rate 类似的学习曲线，并且将 batch size 进一步扩大到了 65536，整个模型的训练过程只需要 2500 次参数更新。 收获与总结： 三篇论文通过理论和实验证明了 large batch size 能够加快模型的训练速度。面对动则几百块的 GPU，很多人对这三篇论文望而却步。其实本文提出的 idea 可以在 Faster RCNN，Mask RCNN 中得到很好的应用，我们只需要使用 multi-GPU，并且根据 Linear Scaling Rule 来调节学习旅就可以大大的提高 Detection 网络的训练速度，并且还有可能得到更好的结果。 Detectron 库的 toy example 已经给大家展示如何使用 Linear Scaling Rule 对应的 config 文件如下： 1 GPUs:  BASE_LR: 0.0025  MAX_ITER: 60000  STEPS: [0, 30000, 40000] 2 GPUs:  BASE_LR: 0.005  MAX_ITER: 30000  STEPS: [0, 15000, 20000] 4 GPUs:  BASE_LR: 0.01  MAX_ITER: 15000  STEPS: [0, 7500, 10000] 8 GPUs:  BASE_LR: 0.02  MAX_ITER: 7500  STEPS: [0, 3750, 5000] Large Batch Size 的初始训练不稳定，需要使用 warm up schedule 进行学习旅调整，具体论文在 lib/utils/lr_policy.py 中实现。 Linear Scaling Law 简单的让人难以相信，为了真实的测定这一定律是适用于 faster rcnn，我们在 mxnet faster rcnn github 上面测试了该定律。我们分别测定了不使用 Linear Scaling Law 的 faster rcnn 跟使用 Linear Scaling Law 的结果。从结果分析使用 Linear Scaling Law 能够加速训练，并且还有可能得到更高的准确率。训练集和测试集分别为：VOC 07 和 VOC 07 test 实验结果如下： Learning rate / GPUs / MAP / training sample per second  0.001 / 2 / 70.23 / 4  0.002  /  4 / 71.43 / 6  0.004 /  8 /  70.98 / 9  0.001 / 4 / 66.50 / 6  0.001 / 8 / 65.00 / 9 Detectron 函数库训练踩坑录 (o^^o) Detectron 条理清楚，但是免不了有一些小的 bug，下面我们就给大家分享一下我们遇到的小坑。 踩坑 1 Config 中大多数参数是为 8 路 GPU 准备的，直接把 GPU 数目改为 1 会让结果变为 NAN。 Solution 根据 Linear Scaling Law 请自行降低学习率，减少 batch size 会增大梯度的不准确性，这时候大的学习率会让神经网络 diverge。 踩坑 2 Multi GPU 训练的时候会 out of memory。错误如下： Solution Facebook 已经发现了这一错误，对 lib/utils/subprocess.py 进行了及时的修改。 踩坑 3 我们使用 multi GPU 训练的时候会出现服务器重启，初步怀疑 memory leakage，我们已经将该问题反馈回了 Facebook，如果遇到相同问题的同学，请稍加等候。 祝大家学习顺利……(o^^o)   "
333,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736647&idx=5&sn=5604640e4b3ac8f3bbc9fe2b335dcd15&chksm=871acc79b06d456f50b34c206e320350fad6981de035787b90074d6c8511f0362c3bc134df81&scene=27,学界 | 中科大潘建伟团队在光量子处理器上成功实现拓扑数据分析,"作者：黄合良 等 机器之心编译 近日，来自中国科学技术大学、中国科学院-阿里巴巴量子计算实验室等机构，由潘建伟院士、陆朝阳教授带领的团队完成了在光量子处理器上执行拓扑数据分析（TDA）的原理性实验演示验证。TDA 可以抵抗一定噪声的干扰，从数据中提取有用信息，而量子版本的 TDA 能实现对经典最优 TDA 算法的指数级加速。量子 TDA 算法也是继 Shor 算法（用于大数因子分解进行密码破译）、Grover 算法（用于搜索问题）、HHL 算法（用于解线性方程组）之后，人类在量子计算机上可使用的一种新算法。该研究为在量子计算机上进行高维数据处理、甚至人工智能算法领域的探索打开了方向。 论文：Demonstration of Topological Data Analysis on a Quantum Processor 论文链接：https://arxiv.org/abs/1801.06316 通过识别带噪声、非结构化数据的底层结构，拓扑数据分析（TDA）可以鲁棒地从数据中提取有用信息。最近，人们提出了一种有效的量子算法 [Lloyd, Garnerone, Zanardi, Nat. Commun. 7, 10138 (2016)]，用于计算数据点的贝蒂数（一种拓扑特征，描述散点图中各个维度的拓扑洞的总数）。我们利用一个六光子量子处理器实现了这个量子算法的原理性实验演示验证，成功地分析了一个包含三个数据点的网络的贝蒂数拓扑特征，为量子计算领域的数据分析提供了新的探索思路和研究方法。 在探索性数据分析和数据挖掘中，我们的收集到的大数据通常编码了非常有价值的信息，然而，这些数据往往规模很大，并且是非结构化的、带噪声的、不完整的，从而使得从数据中提取有用信息变得很有挑战性。TDA[1] 将拓扑学与数据分析进行结合，可以从无结构的数据中分析数据隐含的拓扑特征，对噪声具有较强的鲁棒性，提供了研究此类数据的一种分析方法。特别地，持续同调（persistent homology）[2][3] 通过计算和分析数据的贝蒂数，用于提取数据的有用信息。其中，贝蒂数是一种拓扑特征，第 k 个贝蒂数β_k 表示为数据集中的 k 维洞的数量。例如，β_0、β_1、β_2 分别代表了连通分支、一维洞和二维洞的数量。通过贝蒂数可以对数据进行了抽象化表示，将其转化为拓扑性的描述，这对于理解数据集的底层结构很有价值。使用 TDA 分析数据的贝蒂数这一分析方法近年来得到了快速发展，有望应用于图像识别 [4]、信号处理 [5]、网络科学 [6,7]、传感器分析 [8-11]、大脑连接 [12,13] 和 fMRI 数据分析 [14,15] 等。 然而实际上，当处理复杂数据的时候，经典的拓扑数据分析方法将面临计算量巨大的问题：n 个数据点的数据集拥有 2^n 个潜在拓扑结构，即使使用最强大的经典计算机也难以求解规模不太大的数据集。目前，估计数据集合所有阶贝蒂数的最优经典算法需要的时间复杂度为 O(2^nlog(1/δ)) [16–21]，其中δ为精度。此外，对某些类型的拓扑结构 [22]，计算所有阶贝蒂数是 PSPACE-hard 的。 最近，Lloyd 等人 [23,24] 将量子机器学习方法扩展到 TDA 中，以高效地计算所有阶的贝蒂数。如果从一个数据集中生成的 k-单纯形的比例足够大，则以精度δ计算所有阶贝蒂数的量子算法耗费的时间复杂度为 O(n^5/δ)，相比已知最好的经典算法，具有指数加速。此外，该算法不需要大规模的量子随机存取存储器（qRAM）[25]，只需要 O(n^2 ) 的比特数即可——用于存储 n 个数据点之间所有两两间距的信息。该算法的潜在计算加速能力和可行性有望令量子 TDA 作为未来量子计算机的新算法（量子算法包括 Shor 算法、[26,29]、量子模拟 [30-33]、求解线性系统 [34,35] 和线性向量的分类 [36-38] 等）。 我们首次利用一个小规模的光子量子处理器，进行了量子 TDA 算法的原理性演示验证。在实验中，我们在两种不同的距离参数下，监控和识别三个数据点的贝蒂数的拓扑特征，成功地展示了量子 TDA 算法的可行性，表明数据分析可能是未来量子计算的重要应用。 为了计算贝蒂数，我们首先以数据点之间的关系拓扑地表征数据。我们使用截断距离将数据点分类为单纯形（参见图 1（a）），即数据点的全连接子集。单纯形的集合构成一个单纯复形，然后可以从该拓扑结构中提取贝蒂数等特征。这些拓扑结构如图 1（b-d）所示。 通过确定ϵ所有范围的贝蒂数的完整集合，我们可以构建条形码（参见图 1（e））[39]，即贝蒂数的距离依赖形式的参数化版本。H_k 区域内的每个条形代表一个 k 维洞，条形的长度表示其在参数中的存在持续性。利用条形码，我们可以定量地将短条形当做拓扑噪声过滤掉，并将长条形作为重要的特征（条形的长度表示其对抗间距变化的存在持续性）。在图 1（e）中，在 H_1 区域有一个条形延展了很长的范围，从而我们确定该非结构化数据（图 1（b））的底层拓扑特征是一个圆。 图 1：（a）k-单纯形（图中展示了 k=0、1、2、3 的例子）是 k+1 个数据点的全连接集合。（b）数据点的散点图。（c）使用某些任意的指标以量化数据点之间的距离，（以数据点为，圆心直径为ϵ）圆彼此接触的数据点之间以边连接。（d）单纯复形是单纯形的集合。着色区域表示复形中的不同单纯形。（e）条形码的结构。水平轴代表距离ϵ。在 H_k 的任意区域中，条形和垂直线的交点数等于（距离ϵ对应的）贝蒂数β_k。 一般地，量子 TDA 算法有两个主要步骤（参见图 2（a））。首先访问数据，构造的拓扑结构 k-单纯形的均匀混态。该步骤的时间复杂度依赖于 k-单纯形的比例，在最差的情况下是指数量级的。但是在实际应用中，复杂拓扑结构中 k-单纯形的比例一般不会太小。所以，在实际应用中，无论用经典算法还是 Grover 算法（进一步的二次型算法加速）都可以高效地实现这个步骤。在量子算法中，这个步骤可以分成两小步：（1a）单纯复形量子态的制备；（1b）均匀混态的构造。（2）揭示结构中的拓扑不变量，这个步骤使用相位估计算法 [40] 实现，在量子计算机上，该算法相对经典算法能实现指数级加速，实际证明 [23,24] 它的时间复杂度为 O（n^5/δ），其中δ为计算精度。 图 2：量子 TDA 的量子线路。（a）原始量子算法线路的简单示意图。（b）包含三个数据点的散点图。 （c）1-单纯形量子态（3<ϵ_1<4） 的图表征。第一个和第二个数据点由一条边连接。 （d）1-单纯形量子态（4<ϵ_2<5） 第一个数据点分别和第二个和第三个数据点连接。（e）5 量子比特的优化量子线路。不同颜色的模块代表 4 个基本阶段（单纯复形制备、构造混态、相位估计、测量）。 图 3：实验装置。中心波长为 394nm 的紫外激光脉冲（脉冲持续时间为 150 飞秒，脉冲重复频率为 80MHz）通过三个偏硼酸钡（BBO）晶体 [42] 以生成三对纠缠光子对 (|H>|V>+|V>|H>)/√2（空间模式分别为 1-2、3-4、5-6，细节参见附录 1）。光子 2（3）和 4（5）在 PBS 上进行干涉。所有的光子使用 3-nm 带宽的过滤器进行光谱过滤。C-BBO：三明治形态的 BBO+HWP+BBO 组合；QWP：四分之一玻片；POL：起偏器；SC-YVO4：用于空间补偿的 YVO4 晶体；TC-YVO4：用于时间补偿的 YVO4 晶体。 图 4：最终的实验结果。最终的输出通过在泡利-Z 基上测量本征值寄存器确定。两个不同 1-单纯形的态输入的测量期望值（蓝色条形）和理论预测值（灰色条形）如（a）和（b）所示。误差线表示 1 个标准偏差，它是根据原始数据，通过泊松分布推导得出的（c）0<ϵ<5 的条形码。由于不存在 k≥1 的 k 维洞，这里只给出了第 0 个贝蒂数。当 0<ϵ<3 时，所有点之间都没有连接，因此第 0 个贝蒂数等于数据点个数，即 0<ϵ<3 时有三个条形。当 3<ϵ1<4 和 4<ϵ2<5 时，第 0 个贝蒂数分别等于 2 和 1。 （a）的量子态： （b）的量子态： "
334,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736647&idx=2&sn=52dbf020cf6290bcd13370bdbf6ede7c&chksm=871acc79b06d456fe7b7b2f161695c48bdf6a15e73dd96902b4a1c635255798ecb1d64fc88ca&scene=27,业界 | 前微软亚洲研究院资深研究员梅涛博士加盟京东，担纲计算机视觉与多媒体研发,"计算机视觉和多媒体领域的杰出科学家梅涛博士日前正式加入京东，出任京东集团 AI 平台与研究部 AI 研究院副院长，并担任计算机视觉与多媒体实验室主任。他将负责创建计算机视觉与多媒体实验室，以及京东 AI 平台与研究部在该领域的研究、创新和应用，向该部门负责人、京东集团副总裁周伯文博士汇报。 梅涛博士领导的实验室将聚焦于计算机视觉和多媒体领域的基础研究、算法创新和技术应用，将其深厚的计算机视觉技术的积累应用在京东零售、物流、金融、云计算等领域，并对相关基础设施模块进行赋能。同时，梅涛博士的团队还将探索计算机视觉与多媒体技术在一些新兴领域（如时尚和设计）的应用和落地，以提高和丰富用户在无界零售中的购物体验，使京东在相关领域的技术成为业界领先。 在加入京东之前，梅涛博士就职于「中国 IT 界的黄埔军校」——微软亚洲研究院，担任资深研究员，他同时还是中国科学技术大学和中山大学的客座教授和兼职博导，复旦大学的客座教授。梅涛博士在计算机视觉和多媒体领域的国际一流学术期刊及会议上发表论文 100 余篇，先后 11 次荣获最佳论文奖——包括包揽多媒体领域顶级学术期刊（IEEE T-MM, IEEE T-CSVT, ACM TOMM）的年度最佳论文和两次荣获多媒体顶级学术会议 ACM Multimedia 的最佳论文，并拥有 50 余项美国和国际专利。他领导的研究团队一直致力于图像和视频的深度理解、分析和应用，多次在图像和视频识别、描述和搜索的国际比赛中排名第一（如 TRECVid、COCO、ActivityNet 等）。他的研究成果先后 20 余次被成功转化到微软公司的关键产品和服务中，本人也多次被授予微软公司金星奖。 梅涛博士目前同时担任多媒体领域顶级期刊 IEEE T-MM 和 ACM TOMM 的编委，并且是即将到来的 2018 年 ACM 多媒体年会（ACM Multimedia）的程序委员会主席和 2019 年 IEEEE 多媒体年会（IEEE ICME）的大会主席。他还是 IEEE 信号处理学会 2018-2019 年度杰出工业界演讲者。因在大规模多媒体内容分析、理解和应用领域做出的卓越贡献，梅涛博士于 2016 年当选为美国计算机协会杰出科学家（ACM Distinguished Scientist）和国际模式识别学会会士（IAPR Fellow）。他于 1996 年进入中国科学技术大学自动化系学习，分别于 2001 年和 2006 年获学士和博士学位。梅涛博士表示，「加入京东非常兴奋，希望可以将自身在科研、技术和管理方面多年的积累用来更好的改变零售世界，提升用户体验」。 京东集团董事局主席兼执行官刘强东先生多次表示，京东下一个十二年发展的核心是技术，京东要用技术打造「无界零售」模式，基于京东海量精准丰富的大数据基础和非常明确的应用场景，研发自有技术平台，以云计算能力为基础，致力于机器学习、自然语言处理、虚拟现实、计算机视觉和语音识别等人工智能技术方面的研究，在包括智能消费、智能供应、智能物流、金融科技、实体零售科技在内的多元领域持续投入，通过技术创新向全社会提供「零售即服务（RaaS）」的解决方案。经过几年的战略性转型、重点布局和技术沉淀，京东已经成为一家名副其实的，以 AI 为核心驱动力的技术公司。 基于海量的精准的商品数据和用户数据，京东将 AI 技术应用到搜索和推荐方面，打造了智能搜索和推荐，为用户提供了亿人亿感的极致购物体验；在智能客服方面，已经推出了最新一代的智能客服产品——无人客服。在 17 年的 618 中，JIMI 在当日服务了 131 万人，占到了在线接待量的一半，大大降低了成本并提升了效率；基于深度学习、机器学习、计算机视觉等技术的持续发力，京东打造了无人机、无人车、无人仓、无人配送站等一系列智慧物流项目，并且还在向更多全新的领域探索与创新；此外，在智能家居方面，京东如今拥有人工智能技术应用的两大市场级的标杆产品，智能音箱、智能冰箱。以智能冰箱为例，它可以准确识别用户意图，并根据用户画像个性化推荐菜谱，用户还可以根据语音交互从智能冰箱学习烹饪方法，让用户的体验更加生动。 在加强自身的人工智能应用之外，京东在人才的建设上也一直在持续发力。2017 年京东在其硅谷研发中心已储备了近百位核心工程师及科学家团队，从事着 AI 等相关核心技术研发工作，并推动着京东与「外部顶级大脑」如 Stanford/ UC Berkeley/ MIT/ UIUC 等院校的深度技术合作。2017 年 11 月，京东集团已与斯坦福人工智能实验室启动京东-斯坦福联合 AI 研究计划，双方将围绕机器学习、深度学习、机器人、自然语言处理和计算机视觉等前沿技术方向，结合京东实际应用场景和数据，开展以研究项目为基础的合作。 对于梅涛博士的加入，京东集团副总裁、AI 平台与研究部负责人周伯文博士表示，「人才将成为京东 AI 平台与研究部最重要的资产，此次梅涛博士的加盟，借助其雄厚的 AI 研究的经验，必将与团队一起推动京东 AI 的发展创新，携手合作伙伴为无界零售时代提供最前沿的产品和服务，为消费者持续创造价值。」 "
335,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736673&idx=3&sn=598c2c3fa870b0e90892d87fc07f784c&scene=0,业界 | 亚马逊无人商店开业了，我们到现场排了队还来了一次花式测评,看来，亚马逊的技术足够先进，想钻无人商店「无人」的空子好像也不是那么容易。 西雅图市中心，7th Avenue 和 Blanchard Street 的交界处，吸引全球零售业眼球的门店 Amazon Go 正在对外营业。 旁边是 Amazon Day 1 办公大楼，环绕周遭的还有不少亚马逊的办公室。延迟开业期间，这家门店向这些亚马逊内部员工开放试运营。 在这期间，作为「无人零售」开启者——亚马逊 Amazon Go 的任何消息都成为业内技术公司、投资人以及零售商追逐焦点。 我们带着些许疑问，走进了这家总面积为 167 平方米的店铺，试图为大家还原无人店购物的真实场景。 「说好的不用排队呢?」推特上出现了网友们的实力吐槽。 如我们所观察的那样，当天的高峰时间段里，店外的确排起了长长的队伍，有当地前来一探究竟的吃瓜群众，也有打探消息的记者和科技博主们，尽管无人店周围路况不好、不允许长久停车。 这都让 Amazon Go「无需排队、无需结账、无需注册」提升体验的「初心」多少有点变味。 在早上 7 点至晚上 9 点的营业时间里，按照门店面积大约能够容纳 97 位顾客。不过据现场情况来看，店内人数一般都控制在 70 人以下。一旦店内人数超出上限，员工就会采取限流手段。一方面，员工们担心出现安全问题，事实上，更重要的原因在于消费者过于密集可能会影响天花板及货架上所安置的各种传感器的识别及检测效果。 进店购物，Go 门店「即拿即走」的一道门槛是入口闸机。在这里，我们需要先刷手机，确保账户信息连接。一个细节让顾客倍感温馨：店内专门提供了充电器，方便你不会因为手机没电败兴而归。 当天，店内配备了 17 名工作人员，分别为 10 名店员，2 名保安，还有 5 位在厨房准备三明治等食物。 经统计，167 平方米的空间里一共摆放了 57 个货架，货架的种类及具体数目如下： 事实上，Amazon Go 内的货品种类与一般国外超市并无太大差异，货品也多以食物为主，日用品为辅。特别值得注意的是，在酒品管制区旁挂着「We Check ID」警示牌。这意味着，店内会有专门的工作人员确认买酒的顾客年满 21 岁，这一点与普通的有人商店并无二致。 为了统计了 Amazon Go 开业第一天的人流量，我们记录了每隔三小时门店顾客进出情况： 和大多数开在商务区的便利店类似，门店的购物高峰集中在中午和晚上的「饭点」时间。不难发现，「尝鲜」的绝大部分为青年人，中老年人只占少数，比较而言，青年人对「无人店」的接受程度更高。 绕店一周以后，我们终于可以开始「买买买」了。 不同于传统结账时刷卡、付现或是扫码这类能真实感受到自己的资金正在流失的体验，无人店的顾客在挑选好了想要的货品之后，可以拿在手里或是放在包里，然后直接从出口离开商店即可，店内的传感器和各类传感设备会自动追踪每位顾客的购买过程，并实时自动更新虚拟购物车，保证顾客在离开时能够检测出取走的商品同时划去相应的费用。 为了对用户的购物体验展开全方位的测试，我们分别模拟了以下几种情况。 结论：无论是拿起 2 秒后放回，还是拿起后迅速放回连续进行 10 次，抑或是拿起多件相同的货品，App 上面的记录结果都完全准确。此外，在拿取多件 A 货品以及 B 货品后，把几件 A 货品及 B 货品一起放回 A 货品的货架上，App 也可以给出正确的结果。 结论：记录结果正确 结论：进店后就关机，然后东拿西放，记录结果正确 结论：顾客可以用 Amazon Go 的 App 扫收据上误收费的商品条目去掉收取了费用但实际上没买的物品 结论：购物过程中，穿脱外套不影响记录结果 结论：购物过程中对购物行为进行刻意遮挡或是用外套及纸袋完全遮住手不会影响记录结果 结论：可以退货，但是必须将商品拿给店员，店员再进行退款；直接将货品自行放回架上无法退款 虽然我们看到有网友讲述自己成功「骗过」Amazon Go、少结算了某样货品的经历。 但在我们随机询问的现场顾客中，几乎没有像我们这样揣着「考验」亚马逊这套识别检测系统的小心思。他们向我们表露了对整个体验的好感，认为这套系统「非常健全而且聪明」，也不忘点赞了店内货品价位合理甚至少数商品还有价格优势的情况。 营业中的 Amazon Go 正加速无人零售技术在消费者层面的市场教育，也让此前对这套高难度技术解决方案可行性大打问号的疑虑暂停下来。或许，2018 年的「无人零售」热潮再次开启了。（机器之心驻美西分析师范宸維提供现场支持） 
336,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736647&idx=4&sn=056cf6abc0a60107c5f7c3f80b66a0c8&chksm=871acc79b06d456f9d54b04d3018d84e709554b11648d42658b4e66994190082f71daf037733&scene=27,业界 | Petuum新研究提出形义感知型Grad-GAN：可基于虚拟游戏生成更具真实感的城市场景,"通过人工方式来标注真实世界数据是一件费时又费力的事。在自动驾驶训练数据的获取上，颇具真实感的视频游戏获取能够提供帮助。但视频游戏的渲染效果往往和真实世界的情况有所差异，Petuum 和 CMU 近日发布的一项研究论文试图解决这一问题；他们提出的一种「形义」（相对于自然语言处理中的「语义」）感知型 Grad-GAN 可以在虚拟到真实的城市场景生成上达到相当逼真和精细的结果。相关研究的代码也已经在 GitHub 上公布：https://github.com/Peilun-Li/SG-GAN。 受大规模有标注数据集的推动，深度学习模型近来已经在多种任务（比如分类和检测）上实现了非常出色的视觉感知表现 [14,19,20,31]。但是，由于各种场景中的像素方面的标注不足，更细粒度的任务仍然还有很大的提升空间。高质量的标注往往具有难以实现的难度，需要海量的人力工作才能得到，比如 Cityscapes 数据集 [7] 报告称人工标注单张图像的耗时超过 90 分钟。此外，之前的 domain adaption 研究 [18] 表明，在有限的和有偏差的数据集上学习到的模型往往难以很好地泛化到其它不同领域的数据集上。 缓解这个数据问题的一种可选解决方案是寻找一种自动化的数据生成方法。无需依靠成本高昂的人力劳动来标注真实世界数据，近来计算机图形学领域的研究进展 [23,32,33] 让自动或半自动地从视频游戏中获取图像以及它们对应的形义标签成为了可能，比如《侠盗猎车手 5》（GTA V）——这是一个基于洛杉矶的现实性开放世界游戏。在虚拟世界中，我们可以不受限制地轻松收集各种有标签数据，规模能比人类标注的真实世界数据大几个数量级。 但是，由于常见的严重的域转移问题（domain shift problem [29]），利用真实世界的知识来帮助解决真实世界的感知任务并不是一种容易实现的技术。由于渲染和物体模拟技术的限制，从虚拟世界收集的图像往往会得到与从真实世界收集的图像不一致的分布，如图 1 所示。因此我们希望构建虚拟世界数据和真实世界数据之间的桥梁，以便将两者共有的形义知识用于感知。之前的域适应方法可以概括为两个方向：最小化源特征分布和目标特征分布之间的差异 [12,15,16,17,18,36]；或通过对抗学习 [24,26,27,34,42,44] 或特征组合 [10,11,22,25,37] 明确确保这两个数据分布彼此接近。一方面，对于每个特定任务，这些基于特征的适应方法需要监督源域和目标域，这不是广泛适用的。另一方面，尽管通过生成对抗网络（GAN）得到了出色的适应表现 [13]，但已有的模型只能将源图像的整体颜色和纹理迁移到目标图像，而不会考虑每个形义区域的关键特征（比如道路与汽车），从而得到非常模糊和扭曲的结果。生成对抗网络中有一个鉴别器和一个生成器，其中鉴别器的训练目标是将虚假图像与真实图像区分开，生成器的目标是生成看起来真实的图像以欺骗鉴别器。当生成的图像模糊或扭曲时，细粒度细节的损失会严重阻碍它们对下游的视觉感知任务的促进作用。 图 1：真实世界图像和虚拟世界图像的视觉比较。（a）采样自 Cityscapes 数据集 [7] 的真实世界图像；（b）采样自 GTA-V 数据集 [33] 的虚拟世界图像 我们在本论文中提出了一种全新的形义感知型 GradGAN（SG-GAN：Semantic-aware GradGAN），其目标是为虚拟世界图像中不同的形义区域迁移个性化的风格（比如颜色、纹理，以逼近真实世界分布。我们的 SG-GAN 是一种基于图像的适应方法，不仅能够保留源域中关键的形义和结构信息，而且还能使每个形义区域接近它们对应的真实世界分布。 除了之前的 GAN 中所用的传统对抗目标，我们提出了两个用于实现上述目标的主要贡献。第一，我们引入了一种新的对梯度敏感的目标来优化生成器，这强调了虚拟图像和适应后的图像的形义边界一致性（semantic boundary consistency）。它可以规范化生成器，从而为每个形义区域渲染不同的颜色/纹理，以保持形义边界，这可以缓解常见的模糊问题。 第二，之前的研究成果往往学习的是整张图像的鉴别器，以便验证所有区域的逼真度，这会使原图像中所有像素的颜色/纹理容易坍缩成一种单调模式。我们这里认为每个形义区域的外观分布应该被有目的地区分对待。比如说，在真实世界中的道路区域往往具有粗糙的沥青混凝土纹理，而车辆区域往往很光滑而且反光。不同于最终检查全局特征图的标准鉴别器，我们实现了一种新的形义感知型鉴别器，可以以一种形义方面的方式来评估图像适应的质量。这种可感知形义鉴别器能学习不同的鉴别参数，从而可以根据每个形义标签来检查区域。这就是 SG-GAN 与已有的 GAN 的不同之处，让其成为了可以为不同形义区域个性化纹理渲染并得到具有更精细细节的适应图像的可控架构。 我们在适应 GTA-V 虚拟图像上进行了大量定性和定量实验，结果表明我们的 SG-GAN 可以在不改变形义信息的情况下成功生成逼真的图像。为了进一步证明适应图像的质量，我们使用适应后的图像训练了形义分割模型并在公开的 Cityscapes 数据集上对它们进行了评估。相对于为形义分割使用原始的虚拟数据，适应后的图像带来了显著的表现提升，这很好地表明了我们的 SG-GAN 在可感知形义的虚拟到真实场景适应方面的优越性。 形义感知型 Grad-GAN 我们提出的 SG-GAN 的目标是在保留不同内容的关键形义特征的同时执行虚拟到真实的域适应。SG-GAN 使用了生成对抗网络（GAN），并且相比于传统的 GAN 模型有两大改进，即一种在生成器上的新的软梯度敏感型目标和一种全新的形义感知型鉴别器。 图 2：我们提出的形义感知型 Grad-GAN（SG-GAN）的示意图。V 和 R 框中的黄点分别表示未配对的虚拟世界图像和真实世界图像。两个对称的生成器 G_v→r, G_r→v 的学习目标是根据彼此执行场景适应。除了周期一致性损失（cycle consistency loss [44]），为了确保原始图像及其适应后图像的形义边界是一致的，我们在生成器上加上了一个新的软梯度敏感型目标 L_grad。两个形义感知型鉴别器 SD_v 和 SD_r 是联合进行优化的，以分别检验适应后真实世界图像和虚拟世界图像的逼真度。 图 3：形义感知型鉴别器示意图。其以真实图像或适应后的图像为输入，然后使用一个对抗目标进行优化。每个输入首先会通过几个卷积层，然后得到的特征图会以元素的方式与形义掩码相乘，然后再求和得到单个通道的输出。这种耦合后的输出会被用于优化对抗损失，如等式 1 所示。图中的 ⊙ 表示元素上的乘法运算，⊕ 表示信道维度上的求和运算。为了更好的可视化，被采样的特征图被重新调整为 [0,255]。 实验 图 4：当前最佳方法与我们方法的变体的视觉比较 图 5：用于展示 L_grad 目标的有效性的放大 4 倍的适应后图像 表 1：在亚马逊 Mechanical Turk（AMT）上的 A/B 测试结果。每个单元格都比较了测试者选出的一种方法适应后的图像比另一种方法适应后的图像更逼真的比例，格式为「方法 A 的比例 - 方法 B 的比例」 图 6：展示形义感知型鉴别器 SD 的效果的有效性比较。（a）是输入的虚拟世界图像；（b）是（e）和（f）之间的绝对差；（c）是（a）和（e）之间的绝对差；（d）是（a）和（f）之间的绝对差；（e）是 SG-GAN-25K 生成的适应图像；（f）是无 SD 变体生成的适应图像；（g）是（e）放大 4 倍的细节；（h）是（f）放大 4 倍的细节。注意，通过比较（b）、（c）、（d），可以看到 SD 有助于为不同的形义类别实现更多色调和纹理变化。（g）和（h）的比较表明了 SD 生成更精细细节的能力，比如远处的交通灯和光滑的天空。 表 2：在 Cityscapes 500 张图像的验证集上得到的形义分割分数（%）比较 论文：用于虚拟到真实城市场景适应的形义感知型 Grad-GAN（Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption） 论文链接：https://arxiv.org/abs/1801.01726 视觉任务（比如分割）上的最近进展很大程度上取决于通过繁杂的人力劳动获得的大规模真实世界图像标注的可用性。此外，由于在有限和有偏差标注上训练的模型的泛化能力很糟糕，模型的感知表现往往在新场景下会出现显著的下降。在这项工作中，我们采用了迁移知识的方法——自动渲染虚拟世界中的场景标注以助力真实世界的视觉任务。尽管虚拟世界的标注可能有理想的多样性而且是无限的，但虚拟世界和真实世界之间不同的数据分布使得知识迁移颇具难度。因此，我们提出了一种全新的形义感知型 Grad-GAN（SG-GAN）来执行虚拟到真实的域适应，同时它还有能力保留重要的形义信息。除了之前的工作实现的简单的整体颜色/纹理转换之外，SG-GAN 能成功地为每个形义区域个性化外观适应，从而可以保留它们的关键特征，以便进行更好的识别。相对于传统的 GAN，SG-GAN 有两大主要贡献：1）一种用于保留形义边界的软梯度敏感型目标；2）一种用于验证每个形义区域的个性化适应的逼真度的形义感知型鉴别器。定性和定量实验表明了我们的 SG-GAN 在场景适应上相对于之前最佳的 GAN 的优越性。在 Cityscapes 上的进一步形义分割评估表明，使用 SG-GAN 得到的适应后虚拟图像能在原始虚拟数据基础上实现极大的分割表现提升。我们发布了我们的代码：https://github.com/Peilun-Li/SG-GAN。 "
337,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736647&idx=1&sn=274e610bf0a7904e018ec9df9834d426&chksm=871acc79b06d456f6cc45abb6bb54b330556cf3c8d75423ab50fa9cc75b2ef9507266ada43f4&scene=27,一次搞定多种语言：Facebook展示全新多语言嵌入系统,code.facebook Ves Stoyanov、Necip Fazil Ayan 传统的自然语言处理系统只能对应于特定语言，如果想要让其应用支持多种语言，则需要从头开始构建相应数量的新系统。Facebook 最近提出的多语言嵌入方法可以在一些「已知」语言上训练 Classifier，应用于「未知」语言上，成功解决了社交平台中 AI 应用的多语言支持问题。本文将向你简要介绍这一技术背后的原理。 在 Facebook 上，超过一半的用户使用非英语语言。整个平台上，人们使用的语言超过 100 种。这种多元化的环境对于我们的服务是很大的挑战——如何为每个用户提供首选语言的无缝体验，尤其是在这些体验是由 Facebook 机器学习和自然语言处理（NLP）系统提供支持的情况下。为了向整个社区提供更好的服务——无论是推荐（Recommendations）和 M 建议（M Suggestions），还是检测和删除违反政策的内容——我们都需要建立一个能够适应多语言 NLP 任务的机器学习系统。 显然，现有的适用于特定语言的 NLP 技术无法应对这种挑战，因为支持每一种语言意味着从头开始构建全新应用。Facebook 找到了应对之策。近日，他们展示了最新提出的多语言嵌入技术，它可以帮助处理多语言的问题，帮助人工智能应用更快速地处理新语言的问题，为用户提供更好的产品体验。 NLP 的一个常见任务是文本分类，即将预定义类别分配给文本文件的过程。文本分类模型几乎用于 Facebook 的所有部分，如识别用户是否在贴文中请求系统推荐，或者自动移除负面内容，如垃圾信息。分类模型通常通过向神经网络提供大量标注数据作为样本来进行训练。模型通过该过程学习如何对新样本进行分类，然后执行预测以为用户提供产品体验。 训练过程通常针对某种特定语言，这意味着对于你想要分类的每种语言，你都需要收集大量训练数据。收集数据成本高昂且耗时，当我们想要支持 100 多种语言时，收集就变得更加困难了。 我们使用的另一种方法是收集大量英语数据来训练英语分类器，然后如果需要分类另一种语言的文本（如土耳其语），则将土耳其语文本翻译成英语，然后将译文发送给英语分类器。 但是，该方法也有一些缺陷。首先，翻译中的误差会传输给分类器，导致性能下降。其次，它要求对我们想进行分类的非英语内容另外启用翻译服务。这导致分类产生极大延迟，因为翻译的耗时通常比分类要长。 我们认为这两种方法都不够好。我们想要更通用的解决方案，可以对我们支持的所有语言输出一致、准确的结果。 使用多语言词嵌入执行文本分类 目前文本分类模型使用词嵌入或将词表征为多维向量，将其作为理解语言的基本表征。词嵌入具有非常好的属性，它们非常易于操作，并且相似意义的词汇在向量空间中彼此距离很近。一般而言，词嵌入是针对特定语言的，每种语言的词嵌入需要单独训练，且存在于完全不同的向量空间。 实现多语言文本分类的一种方法是开发多语言词嵌入向量。利用这种技术，每种语言的词嵌入都存在于同一个向量空间中，且不同语言间语义相似的词在向量空间中距离相近。例如，土耳其语中的「futbol」和英语中的「scoccer」在嵌入空间中距离非常近，因为它们在不同语言中代表着相同的意思。 为了实现跨语言文本分类任务，我们可以使用这些多语言词嵌入作为文本分类模型的基本表征。由于新语言中的单词在嵌入空间中与已训练语言的单词相近，所以分类器也能在新语言上执行良好。因此，我们可以使用一种或多种语言进行训练，学习在一种从未训练过的语言中执行分类任务。 为了训练多语言词嵌入，我们首先使用 fastText 和数据（由来自 Facebook、Wikipedia 的数据组合而成）为每种语言分别训练词嵌入。然后我们利用词典将所有嵌入空间投影到共同空间（英语）。词典从平行数据（即由两种不同语言的意义相同的句子对构成的数据集）中自动导出，平行数据也用于训练翻译系统。 我们利用矩阵将嵌入投影到共同空间。该矩阵被用于最小化词嵌入 x_i 和它的投影 y_i 之间的距离。即，如果词典由（x_i，y_i）对构成，我们需要选择投影器 M，使得： 其中，M 表示令 L2 范数求和最小化的 W。此外，我们将投影矩阵 W 限制为正交矩阵，从而保持词嵌入向量之间的初始距离。 我们将这些嵌入整合到 DeepText，即我们的文本分类框架中。DeepText 包含多种将词嵌入作为基本表征的分类算法。我们在 DeepText 中将多语言词嵌入作为基本表征来训练多语言模型，并将词嵌入「固定」，或在训练过程中保持其不变。此外，工作流可以使用不同语言的训练集和测试集，并计算语言内和跨语言的性能。该方法使开发跨语言模型的进程变得更加容易。 对于一些分类问题，用多语言词嵌入训练的模型展现的跨语言性能非常接近于特定语言分类器的性能。我们观察到，当用在训练中未见过的语言进行测试时，准确率达到了 95%，和用特定语言数据集训练的分类器性能相当。之前的翻译输入方法的跨语言准确率通常只能达到特定语言模型的 82%。新的多语言方法的整体延迟时间相比翻译和分类方法，缩短了 20 倍到 30 倍。 我们完成了一些基本工作，如对于每个应用，从语言特定的模型转向多语言嵌入，作为通用的基础层： AI 支持的功能，如推荐（Recommendations）和 M 建议（M Suggestions）更快地支持多种新语言。 我们能够上线支持更多语言的产品和功能。 该方法通常比之前的方法准确度更高，这意味着人们以自己偏好的语言使用 Facebook 时会有更好的体验。 我们在 Facebook 的生态系统中以不同方式应用多语言嵌入，从检测违反政策内容的 Integrity 系统到支持 Event Recommendation 等功能的分类器。 正在进行的工作 通过多语言嵌入进行扩展的方法前途无限，但是我们仍然有很多工作要做。 研究人员发现，目前的多语言嵌入对英语、德语、法语、西班牙语，及与其相近的语言性能略微好一些。该技术仍在继续扩展的过程中，未来会专注于对我们不具备大量数据的语言尝试新技术。Facebook 还将继续研究捕捉跨语言文化背景细微差别（如词组「it's raining cats and dogs.」）的方法。 该研究的团队将与 FAIR 合作，从词嵌入到利用高级结构（如语句或段落）的嵌入改善多语言 NLP、捕捉语义含义。Facebook 希望这种技术的性能优于语言特定的模型，在文化和语言特定的信息和解析方式方面提高准确度。 原文链接： 
338,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736577&idx=3&sn=158015af74af324524ac4462406aa7e6&chksm=871ac3bfb06d4aa9f117c55abba570c5e179f6d6f341e8f794b64830a1f762775e16e0fa6db6&scene=27,教程 | 如何在Python中快速进行语料库搜索：近似最近邻算法,"最近，我一直在研究在 GloVe 词嵌入中做加减法。例如，我们可以把「king」的词嵌入向量减去「man」的词嵌入向量，随后加入「woman」的词嵌入得到一个结果向量。随后，如果我们有这些词嵌入对应的语料库，那么我们可以通过搜索找到最相似的嵌入并检索相应的词。如果我们做了这样的查询，我们会得到： 我们有很多方法来搜索语料库中词嵌入对作为最近邻查询方式。绝对可以确保找到最优向量的方式是遍历你的语料库，比较每个对与查询需求的相似程度——这当然是耗费时间且不推荐的。一个更好的技术是使用向量化余弦距离方式，如下所示： 想要了解余弦距离，可以看看这篇文章：http://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html 矢量化的余弦距离比迭代法快得多，但速度可能太慢。是近似最近邻搜索算法该出现时候了：它可以快速返回近似结果。很多时候你并不需要准确的最佳结果，例如：「Queen」这个单词的同义词是什么？在这种情况下，你只需要快速得到足够好的结果，你需要使用近似最近邻搜索算法。 在本文中，我们将会介绍一个简单的 Python 脚本来快速找到近似最近邻。我们会使用的 Python 库是 Annoy 和 Imdb。对于我的语料库，我会使用词嵌入对，但该说明实际上适用于任何类型的嵌入：如音乐推荐引擎需要用到的歌曲嵌入，甚至以图搜图中的图片嵌入。 制作一个索引 让我们创建一个名为：「make_annoy_index」的 Python 脚本。首先我们需要加入用得到的依赖项： 最后一行里非常重要的是「vector_utils」。稍后我们会写「vector_utils」，所以不必担心。 接下来，让我们丰富这个脚本：加入「creat_index」函数。这里我们将生成 lmdb 图和 Annoy 索引。 1. 首先需要找到嵌入的长度，它会被用来做实例化 Annoy 的索引。 2. 接下来实例化一个 Imdb 图，使用：「env = lmdb.open(fn_lmdb, map_size=int(1e9))」。 3. 确保我们在当前路径中没有 Annoy 索引或 lmdb 图。 4. 将嵌入文件中的每一个 key 和向量添加至 lmdb 图和 Annoy 索引。 5. 构建和保存 Annoy 索引。 我已经推断出 argparse，因此，我们可以利用命令行启用我们的脚本： 添加主函数以启用脚本，得到 make_annoy_index.py： 现在我们可以仅利用命令行启用新脚本，以生成 Annoy 索引和对应的 lmdb 图！ 写向 量Utils 我们在 make_annoy_index.py 中推导出 Python 脚本 vector_utils。现在要写该脚本，Vector_utils 用于帮助读取.txt, .bin 和 .pkl 文件中的向量。 写该脚本与我们现在在做的不那么相关，因此我已经推导出整个脚本，如下： 测试 Annoy 索引和 lmdb 图 我们已经生成了 Annoy 索引和 lmdb 图，现在我们来写一个脚本使用它们进行推断。 将我们的文件命名为 annoy_inference.py，得到下列依赖项： 现在我们需要在 Annoy 索引和 lmdb 图中加载依赖项，我们将进行全局加载，以方便访问。注意，这里设置的 VEC_LENGTH 为 50。确保你的 VEC_LENGTH 与嵌入长度匹配，否则 Annoy 会不开心的哦～ 有趣的部分在于「calculate」函数。 1. 从 lmdb 图中获取查询索引； 2. 用 get_item_vector(id) 获取 Annoy 对应的向量； 3. 用 a.get_nns_by_vector(v, num_results) 获取 Annoy 的最近邻。 再次，这里使用 argparse 来使读取命令行参数更加简单。 主函数从命令行中启用 annoy_inference.py。 现在我们可以使用 Annoy 索引和 lmdb 图，获取查询的最近邻！ 代码 本教程所有代码的 GitHub 地址：https://github.com/kyang6/annoy_tutorial "
339,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736577&idx=2&sn=4b49c25001bd90b4185c46f676dd364f&chksm=871ac3bfb06d4aa9ae9e6fde86c4a417780c61dc3c2204078a47b2e9c8e69925bc8a121a6b4d&scene=27,专栏 | 从论文到测试：Facebook Detectron开源项目初探,从 RCNN 到 Faster RCNN，再到最近的 FPN 和获得 ICCV Best Paper 的 Mask RCNN，深度 学习在物体检测中以绝对优势从众多机器学习算法中脱引而出。大家对 Facebook 的计算机视觉研究项目的开源期盼已久，经过 1 年多的漫长等待，今天 Facebook 终于开源了 Detectron，Detectron 开源项目使用 caffe2 和 python 接口。实现了 10 多篇计算机视觉最新的成果。下面我们简单介绍一下 Detectron 所实现的论文。并且对 Detectron 进行初次测试，我们会在随后的博客中更新我们自己测试得到的 Detectron 训练模型和速度标准。 Fast RCNN、Faster RCNN、RFCN、FPN、RetinaNet Detectron 实现了物体检测的标准模型，并且添加了 Feature Pyramid Network 和 RetinaNet 等 state-of-the-art 的物体检测模型。FPN 是 two-stage 检测的 state-of-the-art，RetinaNet 是 one-stage 的 best-performing 模型，并且也是 ICCV 的 best student paper。 ResNet，ResNeXt Detectron 实现了 Residual Network 和 ResNeXt 等基础的神经网络结构。ResNext 使用 depthwise convolution 的技术大大降低了参数，并且保证了分类结果。   物体检测可以得到 bounding box 如图（a），Human-object interaction 通过预测不同的 bounding box 之间的概率密度可以学习不同 bounding box 之间的关系。如图（c），人和刀之间的关系是切（cut）。 Mask RCNN 通过改进 Faster RCNN 可以实现 7 FPS 的 instance segmentation 和关键点检测，并且超过当时的所有方法。Mask RCNN 在 COCO 和 CITYSCAPES 数据集上面取得了好的结果。Mask RCNN 的示意图如下。   本篇论文发现了 large batch 可以大大的提升分类网络的收敛速度，通过把 batch size 从 256 提升到 8192，将训练时间从几周降低到了 1 个小时，大大提升了神将网络的训练速度。 CVPR 2018 投稿论文： Learning to segment everything 收集 mask rcnn 的标注十分昂贵，在 cityscapes 上面一张图的标注需要 1 个小时。这篇论文提出了 weight transfer 的办法来分割所有的物体，免去了收集分割数据的巨大时间和金钱耗费。本篇论文使用 bounding box detection branch 的权重来预测 mask branch 的权重来实现此目的。 Non Local Neural Convolution Convolution Neural Network 只能够传递邻域的信息，本篇论文参照 non local means 和 self attention 的方法设计出了 non local convolution，从而能够捕捉到非邻域的信息。如下图，中心点可以捕捉到非邻域的重要信息。   Detectron 框架初探 要使用 Detectron 框架需要安装 caffe2，caffe2 的安装请参照 caffe2 官网。然后参照 INSTALL.md 安装 Detectron，Detectron 提供了方便的测试和添加 op 功能。添加 op 具体参照 test_zero_even_op.py。 Detectron 框架包含 config，demo，lib，tests 和 tools 等文件夹。Config 包含着各个模型的训练和测试参数，lib 是 detectron 的核心文件夹，例如 data loader，model builder，operator definition 和 utils（学习率等非核心函数）。 Detectron 安装 Caffe2 安装，参照网址 https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=compile 核心命令： Detectron 安装，参照 https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md Detectron 测试 使用 Mask RCNN FPN ResNet 50 进行测试，命令如下： 在 Titan X 上面测试速度： Detectron 框架训练  在 COCO 数据集上面使用 FPN ResNet50 进行 Faster RCNN 训练 使用命令 ： 输出如下： Memory 占用如下：   Detecrton 框架给予 caffe2 和 python 接口，caffe2 对 Multi-GPU 和分布式训练提供了很好的支持，GPU 现存的利用率也大大提升，并且对很多 state-of-the-art 的方法提供了很好的 baseline 实现。相信 Detectron 框架会在未来的 computer vision 领域大放异彩。 安装小提示 1： 方法： sudo pip install future 安装小提示 2： caffe2 安装之后需要将 caffe2 添加到 PYTHONPATH 和 LD_LIBRARY_PATH 路径 nano ~/.bashrc 输入： 
340,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736547&idx=2&sn=bbbfcd0ae5f3399dce56a4a115d87d5c&chksm=871ac3ddb06d4acb46b0ba8c4387d6322b51c5894ca82e431f2bd753bad235ee1c47407a7baf&scene=27,深度 | 最优解的平坦度与鲁棒性，我们该如何度量模型的泛化能力,深度网络最优解附近的平坦度一直是我们理解模型泛化性能的重点，通常较为平坦的最优解有更好的鲁棒性。而本文作者则进一步提出一个好的指标可能不仅涉及平均损失函数极小值附近的平坦度，还涉及两个平坦度指标之间的比率。 我看到大家在 Twitter 和 Reddit 中谈论这篇论文《 》，于是撰写此文。 这篇论文与《Sharp Minima Can Generalize For Deep Nets》这篇极具洞察力的论文有关。 不可避免地，我开始以一种普遍性的角度思考极小值平坦度和泛化能力之间的关系。因此，我没有详细描述以上两篇论文，而是阐述了自己的一些思考。欢迎大家批评指教！ 本文要点 极小值的平坦度（flatness of minima）被认为与深度网络的泛化能力有关。 正如 Ding et al (2017) 所表明的那样，平坦度对于参数重设（reparametrization）非常敏感，因此不能单独用平坦度预测泛化能力。 Li et al (2017) 使用了一种参数归一化的形式，这一方案对于参数重设更具有鲁棒性。此外，绘制了一些对比深度神经网络的奇特图像。 虽然上述分析对 Dinh 等人所考虑的特定类型的参数重设而言具有不变性，但是它对其他类型的不变性而言可能仍具敏感性，所以这些图表和结论仍具有不确定性。 然后，我回到起点，思考如何构建与结构不相关的泛化指标，例如考虑平坦度比率。 最后，我想到，可以从基本原理的角度开发一个泛化的局部测量指标。所得到的度量取决于从不同小批量中计算的梯度的数据和统计特性。 深度网络的损失函数表面往往存在许多局部极小值。其中，许多网络在训练误差方面表现得同样好，但是它们可能具有非常不同的泛化能力。即，损失函数值极小值处的网络在训练集上可能表现得很好，也可能很差。有趣的是，小批量随机梯度下降（SGD）得到的极小值点似乎比大批量 SGD 有更好的泛化能力。所以，有一个大问题：局部极小值的哪种可测属性能预测泛化能力？这与 SGD 又有什么关系？ 至少在 1997 年，Hochreiter 和 Schmidhuber 猜测极小值的平坦度是一个很好的衡量标准。然而，正如 Dinh et al（2017）指出的那样，平坦度对于神经网络的参数重设是敏感的：我们可以在不改变输出的情况下对神经网络进行参数重设，同时使尖锐的极小值点看起来任意平坦，反之亦然。因此，单纯利用平坦度这一指标无法解释或预测良好的泛化能力。 Li et al（2017）提出了一种归一化方案，该方案在极小值附近对空间进行缩放。对于 Dinh 等人所使用的参数重设类型，该方案能让一维、二维图像的表观平坦度具有不变性。他们说，这使得我们能在极小值周围的损失函数表面得到更可信的可视化结果。此外，他们还使用一维图和二维图解释不同架构之间的差异，如 VGG 和 ResNet。我个人并不赞同这一观点，但似乎 ICLR 的审稿人很大程度上同意这一观点（https://openreview.net/forum?id=HkmaTz-0W）。上述方法理论基础很薄弱，且只针对一种可能类型的参数重设。 跟随着 Dinh 等人的思路，如果在参数重设的情况下泛化能力具有不变性，用来预测泛化能力的度量值也不应随之改变。以我的直觉，有一种实现不变性的好思路，即考虑两个值的比率——也许是两个平坦度的值——这两个值以同样的方式受参数重设的影响。 我认为，比较单一小批量损失函数的平均平坦度和平均损失函数的平坦度很有意义。为什么呢？这是因为损失函数平均值可能以不同的方式在极小值附近平坦化：其平坦可能因为它是许多平坦函数的平均值——这些函数图像相似，并且其极小值位置相近；其平坦也可能因为它是许多尖锐函数的平均值——这些尖锐函数的极小值散乱分布于平均极小值的附近。 凭直觉讲，前一种方案在数据子采样（subsampling）中更为稳定，因此从泛化的角度来看更为有利。后一种解决方案对于我们正在研究的某个特定小批次非常敏感，所以它有可能会导致更糟糕的泛化能力。 我们来给这部分做一个小结。我认为，仅关注平均损失函数的平坦度并不合理；而通过观察数据子采样对平坦度的影响更可能是理解泛化能力的关键。 在 Jorge Nocedal 的 ICLR 演讲谈到大批量 SGD 后（https://iclr.cc/archive/www/lib/exe/fetch.php%3Fmedia=iclr2017:nocedal_iclr2017.pdf），Leon Buttou 发表了一条评论，我认为这评论一针见血。从训练集采样小批量的过程，在某种程度上模拟了从一些基础数据分布中采样训练集和测试集的效果。因此有可能，从一个小批量到另一个小批量的泛化能力，也就代表了一个方法从训练集到测试集的泛化能力。 我们如何利用这种想法，提出某种基于小批量，特别是依据函数锐度或局部偏导数而构建的泛化能力度量方法？ 首先，我们考虑随机过程 f(θ)，这可以通过评估一个随机小批量的损失函数得到。随机性来自对数据进行的子采样。这是以 θ 为变量的损失函数概率分布。我认为，应该在任何给定的 θ 值下寻找泛化能力指标，并将其作为这个随机过程的局部性质。 为了简化问题，我们首先假定从这个过程中得到的每个 f(θ) 都是凸函数，或者至少有一个唯一的全局极小值。在这个随机过程中，如何描述模型从一个小批量到另一个小批量的泛化能力？ 我们独立地绘制 f_1(θ) 和 f_2(θ) 两个函数图（即，独立地评估两个小批量的损失函数）。我认为，以下将是一个有意义的度量： 基本上，我们需要知道在 f_1 的极小值点处 f_2 的值，并与 f_2 的全局极小值进行比较，这是一种 Regret Expression，因此我用 R 表示。 然而，在深度学习中，损失函数 f_1 和 f_2 是非凸的，它们有许多局部极小值。所以一般来说，这个定义并不是特别有用。然而，在特定的参数值 θ 的小邻域内对 R 进行局部计算是有意义的。让我们考虑拟合一个受限的神经网络模型，其参数取值范围只在 θ 变量的 ϵ 邻域以内。如果 ϵ 足够小，我们可以假设损失函数在这个 ϵ 球面内具有唯一的全局极小值。此外，如果 ϵ 足够小，则可以使用对 f_1 和 f_2 的一阶泰勒近似，解析地在 ϵ 球面内找到近似极小值。为此，我们只需要在 θ 处计算梯度。如下图所示： 左图显示了仅限于 θ 附近的 ϵ 球内在某小批量 f_1 上的假想损失函数。我们可以假设 ϵ 足够小，因此 f_1 在这个局部区域内是线性的。除非梯度恰好为 0，否则极小值将落在 ϵ 球的表面上，正好在 θ-ϵ(g_1/‖g_1‖ ) 处，其中 g_1 是 θ 处的 f_1 的梯度。图中的黄色五角星标注了这一点。右图是 f_2 的情况。它也是局部线性的，但是它的梯度 g_2 可能不同。ϵ 球内的 f_2 的最小值在 θ-ϵ(g_2/‖g_2‖) 处，如红色五角星所示。我们可以考虑如上所述的 regret-type expression，即评估黄五星位置的 f_2 值，并减去它在红五星位置上的值。将其表示如下（其中我已对 R 除以 ϵ）： 实际上，人们会对两个小批量取期望值以获得取决于 θ 的表达式。所以，我们刚刚提出了一个局部泛化能力指标，它是用不同小批量上的梯度期望值来表示的。该度量是局部的，这是因为它对于每个 θ 而言都有一个特定值。它依赖于数据，因为它取决于我们从小批量中采样的分布 p_D。 本指标取决于两个量： 来自不同小批量的梯度的预期相似性 1-cos（g_1，g_2），它可以看出各种小批量的数据是否在相似的方向上推动 θ。大多数情况下，在梯度采样于类球形对称分布的区域，这一项接近于 1。 梯度 ‖g_2‖ 的大小。有趣的是，可以将其表达为 当我们计算上式的期望值时，假设大部分余弦相似度是 1，我们最终得到这个表达式： 其中，期望值是在小批量中计算得到的。注意经验 Fisher 信息矩阵（empirical Fisher information matrix）迹的范数： 可以用来衡量极小值周围平均损失函数的平坦度，所以它们之间可能会有一些有趣的联系。但是，由于 Jensen 不等式，它们实际上并不完全等同。 小结 本文始于对一篇论文的回顾。但是后来我认为那篇论文并不是很有意义，因此我转而分享了一些关于如何解决泛化难题的不同想法。很有可能前人做过类似的分析，也有可能这种分析完全无用。无论如何，欢迎反馈。 第一个观察结果是，一个好的指标可能不仅涉及平均损失函数极小值附近的平坦度，而可能还涉及两个平坦度指标之间的比率。这样的指标在结构的参数重设下可能保持不变。 对此进一步考虑，我试图开发一个超越平坦度之外的泛化能力局部指标，它包括了测量梯度对数据子采样的敏感度。 由于数据子采样是泛化（训练集 vs 测试集）和小批量随机梯度下降中都出现的情况，所以，这些度量可能有助于利用 SGD 实现更好的泛化。 原文链接：http://www.inference.vc/sharp-vs-flat-minima-are-still-a-mystery-to-me/ 
341,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736577&idx=1&sn=a9da5ebdbb7ac6f7053d2f4c4af131c6&chksm=871ac3bfb06d4aa9e3c16199e35882076f929ea2b2f20ca8926b7430839d6d77f2fd2b755720&scene=27,Yann LeCun卸任FAIR实验室主任，改任首席AI科学家,机器之心编译 据Yann LeCun数小时前的Facebook消息称，他将不再担任Facebook 人工智能实验室主任一职，改任Facebook首席人工智能科学家，从而能更加专注于带领科学研究与AI策略。 Facebook 人工智能实验室领导者 Yann LeCun 卸任，随着 Facebook 更多地将 AI 应用到 News Feed 和其他产品上，接任者 Jérôme Pesenti 将在 Facebook 发挥更大作用。 Yann LeCun，FAIR 的创建者，即将卸任该研究组织领导者职位，现在的职位是首席 AI 科学家。接任者 Jérôme Pesenti，曾任 AI 创业公司 BenevolentTech 的 CEO 和 IBM 大数据事业群的 CTO，接任 LeCun 职位的同时，他还接管了 Facebook 应用机器学习事业群（AML），该部门主要负责将 AI 应用到 Facebook 的产品中，包括 News Feed。 Pesenti 将直接向 Facebook CTO 汇报。他的新职位结合了研究、开发和生产，强调了 AI 对于 Facebook 未来发展的重要性。「我们需要有人全面监督 Facebook 所有与 AI 相关的事务，包括研究、开发和生产。」LeCun 在一次 Quartz 采访中说道，「AML 和 FAIR 向 CTO 汇报，随着 AI 重要性、围绕深度学习建立更多系统重要性的提升，CTO 再也不用担心了。」 AI 可能是造成 Facebook 大部分困境的原因，包括平台上的假新闻 AI 是 Facebook 及其未来的命脉。该技术推动了精确的广告定位，使排名广告行业上极具优势。此外，AI 还为客户提供自动照片标注、新闻源源排名和翻译等功能。然而虽然人工智能使自动化提高，但这些技术可能对构建者而言并不是透明和可解释的。这样的特性给 Facebook 造成了很多困境，包括平台上的假新闻以及针对特定人群的歧视等。 作为全公司的 AI 首席科学家，LeCun 将继续控制 FAIR 的研究方向，但实验室的日常管理工作将落于 Pesenti 手中，后者将直接向公司 CTO Mike Schroepfer 负责。LeCun 和 AML 负责人 Joaquin Quiñonero Candela 将向 Pesenti 汇报工作。 LeCun 认为，自己的专长在于研究，特别是设置目标，并与其他研究者共同探索新的 AI 技术。他工作的其他部分包括管理 Facebook 超过一百名研究人员，以及向公司的其他成员传播 AI 技术，他表示自己「非常乐意将目前的工作交给接任者」。 Pesenti 目前并未接受采访，不过参与选拔接任者的 Yann LeCun 表示，Pesenti 是因为其在研究、管理和产品开发上出色的能力而被选中的。 AI 产品的新方向 应用机器学习团队将在 Pesenti 的领导下改变方向，LeCun 说。因为之前 Facebook AI 使用量的急剧增长需要 AML 团队与 Facebook 的基础设施部门深入合作，从而保证能够更快的转译、打标代码和服务器，但现在 AML 团队的基础设施成员加入了公司更大的基础建设部门。那些仍然留在 AML 的人将在长期研究项目中与 FAIR 更深入地合作，以将研究团队的工作整合到 Facebook 和它的公司，例如 Instagram 和 Oculus。 LeCun 在 2013 年创建 FAIR，建立了全世界最好的 AI 研究实验室。经过五年，这个实验室仍然在不断扩展，拥有 6 个分部，以及将近 100 个职员。该公司昨天宣布它将把巴黎的研究实验室的团队翻倍为 60 个研究员。LeCun 还补充道，Facebook 希望将目前在实验四工作的 10 个 Ph.D 学生扩充为原来的四倍，进而将巴黎的实验室总人数扩展为近 100 人。 巴黎和蒙特利尔的国际实验室吸引了美国以外的 AI 人才，并且巴黎实验室还吸纳了来自非洲的人才。LeCun 强调说目前巴黎团队的 30 人分别来自 12 个国家。美国对部分研究者的吸引力相比以前有所降低，而在法国更容易获得某些签证，LeCun 说。 LeCun，出生和求学于法国，作为博士后师从神经网络鼻祖 Geoff Hinton 之后，他在贝尔实验室开始了其计算机视觉的研究生涯。LeCun 希望计算机可以像人一样看到和理解这个世界。当时，贝尔实验室是进行神经网络研究的首选，试图通过大量简化人脑结构以复制大脑功能。 当时在贝尔实验室，LeCun 已开发了一种称为卷积神经网络的架构，它被证明在图像处理上优于以往任何模型。该技术很快应用到 ATM 以识别读取支票上的数字，一直到今天，卷积神经网络依然是图像和视频识别的标准方法。 LeCun 将继续在纽约大学任教。 Facebook 创始人兼首席执行官扎克伯格长久以来一直宣告 AI 将是一项带领人类打开美丽新世界的技术。 「我们不应该惧怕 AI。相反，我们应该对其为世界带来的美好抱有希冀，」扎克伯格在 2016 年宣布打造其自己的由 AI 赋能的智能家居系统的目标时讲到，「AI 将通过诊断疾病挽救生命，通过自动驾驶提高出行效率，它也将在帮助人类发现新星球、理解地球气候方面实现新突破，并在人类始料未及的领域发挥作用。」 原文地址：https://qz.com/1186806/yann-lecun-is-stepping-down-as-facebooks-head-of-ai-research/ 
342,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736577&idx=4&sn=d4237a2453b4ce4ceff2aef7ef1a99fe&chksm=871ac3bfb06d4aa9b8c0171e65a23ff23c766aea155f6f301863c88706f255a72e20477bf122&scene=27,资源 | 机器学习新框架Propel：使用JavaScript做可微分编程,链接：http://propelml.org/ Propel 提供 JavaScript 中的 GPU 后端类似 numpy 的基础设施。JavaScript 作为快速、动态语言，我们认为可以作为所有科学类程序员的理想工作流。 Propel 在浏览器、Node 中都能运行。在两个环境中，Propel 都能够使用 GPU 硬件对计算进行加速。在浏览器中，它能通过 deeplearn.js 利用 WebG，在 Node 上，它能使用 TensorFlow 的 C API。 Propel 有个重要的 autograd 式的 API，这不同于 TensorFlow。在运行过程中，会随着追踪计算图，通用的一种梯度函数提供了做反向传播的简洁借口。 浏览器做 demo 很棒，但不是强大的数字平台。WebGL 又和 CUDA 相距甚远。通过在浏览器外运行 Propel，用户能够面向多种 GPU，并做 TCP 连接。服务器边开发的模型能够更容易部署为 HTML demo。 基础的 Propel npm 程序包只是 Javasript 的，没有 TensorFlow 捆绑物。为了提升速度，你可以安装： 在 Node 中使用 Propel： 在浏览器中使用 Propel： 
343,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736547&idx=3&sn=a326f17f832cec4827596291930e76dd&chksm=871ac3ddb06d4acb3ffc56ffd56b340d5f15b7180a8aff5a38ea41c3c4e41e8d96da4c5a0a08&scene=27,资源 | 整合全部顶尖目标检测算法：FAIR开源Detectron,"昨天，Facebook AI 研究院（FAIR）开源了 Detectron，业内最佳水平的目标检测平台。据介绍，该项目自 2016 年 7 月启动，构建于 Caffe2 之上，目前支持大量机器学习算法，其中包括 Mask R-CNN（何恺明的研究，ICCV 2017 最佳论文）和 Focal Loss for Dense Object Detection，（ICCV 2017 最佳学生论文）。Facebook 称，该工具包已被应用与公司内部很多团队应用于各类应用中，一旦训练完成，这些计算机视觉模型可被部署在云端或移动设备上。 项目地址：https://github.com/facebookresearch/Detectron Detectron  Detectron 是 Facebook AI Research 实现顶尖目标检测算法（包括 Mask R-CNN）的软件系统。该系统是基于 Python 和深度学习框架 Caffe 2 而构建的。 在 FAIR 实验室，Detectron 目前已经支持很多研究项目的实现，包括： Feature Pyramid Networks for Object Detection (https://arxiv.org/abs/1612.03144) Mask R-CNN (https://arxiv.org/abs/1703.06870) Detecting and Recognizing Human-Object Interactions (https://arxiv.org/abs/1704.07333) Focal Loss for Dense Object Detection (https://arxiv.org/abs/1708.02002) Non-local Neural Networks (https://arxiv.org/abs/1711.07971) Learning to Segment Every Thing (https://arxiv.org/abs/1711.10370) Data Distillation: Towards Omni-Supervised Learning (https://arxiv.org/abs/1712.04440) 简介 Detectron 的目标是为目标检测研究提供高质量、高性能的代码库，它灵活的特性可以支持快速实现和验证新研究。Detectron 目前包含以下目标检测算法的实现： Mask R-CNN (https://arxiv.org/abs/1703.06870)—Marr Prize at ICCV 2017 RetinaNet (https://arxiv.org/abs/1708.02002)—Best Student Paper Award at ICCV 2017 Faster R-CNN (https://arxiv.org/abs/1506.01497) RPN (https://arxiv.org/abs/1506.01497) Fast R-CNN (https://arxiv.org/abs/1504.08083) R-FCN (https://arxiv.org/abs/1605.06409) 这些目标检测算法主要使用以下卷积网络架构： ResNeXt{50,101,152} (https://arxiv.org/abs/1611.05431) ResNet{50,101,152} (https://arxiv.org/abs/1512.03385) Feature Pyramid Networks (https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) VGG16 (https://arxiv.org/abs/1409.1556) 在这些目标检测算法中，我们比较熟悉的就是 Mask R-CNN，它是在 Faster R-CNN 上的扩展，即在用于边界框识别的分支上添加了一个并行的分支以预测目标掩码。该方法能够有效地检测图像中的目标，同时还能为每个实例生成一个高质量的分割掩码。而在 RetinaNet 中，研究者提出了全新的 Focal Loss 方法，并集中于稀疏、困难样本中的训练，避免了训练过程中可能出现的大量负面因素。该论文的研究者表示，当使用 Focal Loss 进行训练时，RetinaNet 可以达到此前一步检测器的速度，同时准确性高于业内最佳的两步检测器。除了这两个在 ICCV 2017 大为出彩的目标检测算法外，其它如 Fast R-CNN 和 R-FCN 等都是十分优秀和经典的目标检测方案。 在卷积网络架构中，值得注意的是特征金字塔型网络（FPN），它主要融合了多层特征而改进了 CNN 的特征提取方式。它利用了 CNN 固有的多尺度、多层级的金字塔结构去构建特征金字塔网络，并使用一种自上而下的 Skip Connector 在所有尺度上构建高级语义特征图。 上述的检测算法和 CNN 架构在目标检测任务上都有非常好的效果，他们基本上展现了该领域最优的水平。而 Detectron 包含了这些算法和架构的一大组基线结果和已训练模型，我们可以直接下载它们。例如下图是 RetinaNet 的基线结果与对应的模型下载地址，它同时还提供了训练和推断过程中的有用数据。 我们粗略统计了一下，该项目有 70 多个不同设定的预训练模型。因此 Detectron 基本上已经是最目前包含最全与最多目标检测算法的代码库了。此外，该项目也提供了安装指南，包括 Caffe 2 和 COCO 数据集。值得注意的是，该代码库用的是 Python 2，它还需要 NVIDIA GPU、Linux 系统和其它一些标准的 Python 数值计算包。 模型库与基线结果：https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md 安装指导：https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md 最后，Detectron 还提供了一个文档以展示如何使用该研究工具。例如我们在图像文件目录执行推断，我们可以直接使用 infer.simple.py 工具。在下面的案例中，我们使用了一个端到端已训练的 Mask R-CNN 模型（以 ResNet-101-FPN 为基本卷积架构）执行推断： 更多详细的使用文档请查看：https://github.com/facebookresearch/Detectron/blob/master/GETTING_STARTED.md。 "
344,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736547&idx=4&sn=d1318f4b85349de1026c0b0fb24e08bf&chksm=871ac3ddb06d4acbe9a79c41aff838b54383fc37d9b43456aa78fe59dbff06da1888d1782de0&scene=27,学界 | 如何通过方差偏移理解批归一化与Dropout之间的冲突,自批量归一化提出以来，Dropout 似乎就失去了用武之处，流行的深度架构也心照不宣地在批归一化上不采用 Dropout。而近日南京理工大学和清华大学的研究表明 Dropout 在网络测试的时候神经元会产生方差偏移，因而进一步分析与理解如何能避免方差偏移风险，并克服二者组合的局限性。 在批归一化提出之前，Dropout 几乎是所有的最优网络的标配，尽管很简单，但它成功的帮助网络缓解了过拟合问题。Ioffe & Szegedy 于 2015 提出批归一化（BN）技术，通过运用该技术，不仅可以加速现代模型的速度，而且可以以正则项的形式来提高基线水平。因此，批归一化几乎应用在近期所有的网络架构中，这说明了它强大的实用性和高效性。 但是，当 Dropout 和 BN 结合在一起时并不能获得额外的增益。事实上，当一个网络同时使用这两者时，甚至会得到更差的结果。Ioffe & Szegedy 于 2015 就已经认识到，BN 在某些情况下会削弱 Dropout 的效果。他们揭露了两者之间的不相容性，从而推测 BN 提供了与 Dropout 相似的正则化功能。很多现代模型　如 Resnet，Densenet 等为这一观点添加佐证，它们在只使用 BN 的情况下得到更好的结果。 但比较有意思的是 Wide ResNet（WRN），若我们在每个 Bottleneck 的两个卷积层之间加上 Dropout，性能却可以得到稳定的提升。因此到目前为止，以前的信息和线索仍不能解释 Dropout 和 BN 之间的复杂关系。 本论文作者发现理解 Dropout 与 BN 之间冲突的关键是网络状态切换过程中存在神经方差的（neural variance）不一致行为。试想若有图一中的神经响应 X，当网络从训练转为测试时，Dropout 可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而 BN 仍然维持 X 的统计滑动方差。这种方差不匹配可能导致数值不稳定（见图 1 中的红色曲线）。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。简单起见，作者们将这一现象命名为「方差偏移」。事实上，如果没有 Dropout，那么实际前馈中的神经元方差将与 BN 所累计的滑动方差非常接近（见图 1 中的蓝色曲线），这也保证了其较高的测试准确率。 本文作者从理论上证明了「方差偏移」要满足两个一般条件，并对上文提到的 Dropout 和 BN 之间的关系给出了令人满意的解释。除此之外，他们在 CIFAR10/100 数据集上进行的其它模型试验（即 PreResNet、ResNeXt、DenseNet、Wide ResNet）证明了他们的预期想法。 由于结合二者造成性能损失的主要原因已经发现，作者们采用了两种策略来探索如何打破这种局限。一个是在所有 BN 层后使用 Dropout，另一个就是修改 Dropout 的公式让它对方差并不那么敏感。因为这两种方法都能降低方差偏移的风险，它们大多数情况下都能工作得很好，且能取得额外的提升。 论文：Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift 论文地址：https://arxiv.org/pdf/1801.05134.pdf 摘要： 本篇论文首先从理论上和实验数据两方面回答了「为何通常两个强大的技术 Dropout 和 BN 结合起来用反而会造成性能损失？」理论上，我们发现，当模型状态由训练到测试时，Dropout 会使具体神经元的方差产生偏移。但是，BN 在整个测试阶段都会保留它在训练过程学习并累计得到的方差。当在 BN 之前应用 Dropout 时，该方差的不一致（我们将该现象命名为「方差偏移」）造成推理中的数值不稳定性，最终将导致更多的错误预测。尽管在 DenseNet、ResNet、ResNeXt 和 Wide ResNet 上做的实验验证了我们的发现。我们接下来探讨修改 Dropout 的几种策略，并通过避免方差偏移风险来克服二者组合的局限性。 该论文的第一作者李翔在知乎专栏解释了这篇论文的主要思想与发现，希望详细了解该研究的读者可查看原论文和作者专栏（https://zhuanlan.zhihu.com/p/33101420）。 
345,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736547&idx=5&sn=6a3c2dca1717ca886726ba92fcadecbc&chksm=871ac3ddb06d4acbd24aac5f2dbda97dde74b75c35454dffbb74b651e30f79ed7414ca927aee&scene=27,机器之心学生读者福利：这些海外华人教授在招博士生（第二期）,"上个月底， ，取得了不错的反响。为了向大家推送更多有益信息，我们在这篇文章中推荐了三位海外华人教授、助理教授的招生需求，读者们如果感兴趣，可根据文中的联系方式沟通、交流。此外，有位教授提醒说，美国部分院校秋季入学申请截止时间是 2 月 1 号，所以读者们请注意日期。 话不多说，本期博士招生信息如下（按照大学英文首字母排序）： 刘学教授本科毕业于清华大学，博士毕业于美国 Univ. of Illinois at Urbana-Champaign (UIUC)，现任麦吉尔大学计算机系正教授，特聘讲席教授（Chair Professor）。同时他还担任全球最大的在线交友 App—Tinder 的首席科学家，领导机器学习、人工智能算法和系统的研发。他与多家世界著名高校、公司和研究院建立了紧密和良好的合作，包括微软（Microsoft）、惠普（HP）、IBM、通用汽车（GM）、庞巴迪（Bombardier）、博世（Bosch）及多家高科技创业公司。刘学教授曾获得多项国际科研奖项和荣誉，包括多次国际顶级学术会议的最佳论文奖、加拿大计算机学会 2014 年杰出青年计算机科学家奖、麦吉尔大学汤姆林森科学家奖、加拿大 MITACS 2017 年杰出创新领袖奖，以及由 IBM Lotus Notes 的缔造者、前任微软首席软件架构师（Chief Software Architect）雷·奥兹先生（Ray Ozzie）设立的 Ray Ozzie 奖学金。刘教授还担任多家国际著名学术期刊的副主编和编委，并参与组织举办过 40 多场国际知名会议和研讨会，在国际顶级学术会议和期刊上累计发表过 280 余篇论文并获得过多项美国专利。   实验室介绍：McGill University 是世界著名一流大学，有很多著名的校友，其中包括加拿大总理 Justin Trudeau、现任斯坦福大学校长 Marc Tessier-Lavign、英属哥伦比亚大学校长 Santa Ono 和剑桥大学副校长 Stephen Toope 等。其所在城市蒙特利尔是一座富有浓郁学术、文化、创业和时尚气息的大都市，并被评选为 2017 年世界最适合学生的城市（「World's Best City for Students」）, 也被誉为当今世界的人工智能中心。坐落于蒙特利尔市中心的麦吉尔大学计算机学院不但培育了深度学习三大巨头之一的 Yoshua Bengio（现任蒙特利尔大学教授），还先后吸引了 Google Deepmind、Facebook AI Research、RBC 等大公司投资建立联合人工智能实验室。蒙特利尔有世界多家高科技公司，如 Google、Microsoft、Facebook、Element AI、 Nuance、 EA、 Ubisoft、 Airbnb、 Shopify、 Morgan Stanley、 Bombardier、 Expedia、华为、Nokia、Ericsson等。刘教授领导的 Cyber-Physical Systems（CPS）实验室主要从事人工智能、大数据、计算机网络和系统在物联网、自动驾驶、社交网络，绿色能源等领域的相关研究工作，充分利用学院的人工智能研究实力，并与多个高科技公司、实验室和初创公司合作，致力于培养硕士生、博士生、博士后的理论研究与应用问题相结合的研究和领导能力，并为学生创造大量的实习与工作机会。 招聘数量：2018 年度提供 2-3 位全奖的博士生和 1 位硕士生。常年招聘优秀博士后。同时可为有其他经费来源（如加拿大政府、国家公派或公司资助）的面试合格者提供最大的支持和帮助。   联系方式和详细信息请参考网站：http://cs.mcgill.ca/~xueliu。   汪昭然（https://www.princeton.edu/~zhaoran/），现任西北大学（Northwestern University）IEMS 系和 EECS 系助理教授（Assistant Professor）。博士毕业于普林斯顿大学 ORFE 系（Microsoft Research PhD Fellow），本科毕业于清华大学电子工程系。曾获 AISTATS Notable Paper Award 以及 INFORMS 和 ASA 的 Best Student Paper Awards。 研究方向：机器学习、统计、优化理论 (NIPS+ICML+COLT community)，具体方向是非凸优化、表示学习、增强学习。 招聘博士生数量：计划招收 2-3 名博士生以及访问学者（寒暑假实习生）。 联系方式： Email：zhaoranwang@gmail.com (mailto:zhaoranwang@gmail.com) 微信：ZR_Wang123 胡建军，现任美国南卡罗来纳大学计算机科学与工程系终身副教授（Tenured Associate Professor at University of South Carolina）。2004 年美国密歇根州立大学（MSU）计算机系博士（机器学习与进化计算），2005-2007 年普渡大学及南加州大学博士后。美国国家自然科学基金杰出青年基金（NSFCAREER Award）获得者。主要研究方向：机器学习、深度学习、生物信息学、材料信息学与声音的感知处理技术。   实验室简单介绍：Machine Learning and Evolution Lab。实验室的主要研究特色是多学科交叉研究，在材料、生物、基因组学、医学等相关领域都有深入的合作研究经历。目前主要研究机器学习、深度学习及其在生物信息学、材料信息学与声音感知处理中的应用。该实验室现有五位博士生、两位硕士研究生和两个本科生。USC 所在城市 Columbia 位于美国东南部，有得天独厚的地理位置，气候宜人。   招聘博士生数量：Fall2018 将招聘 2-3 位博士生。具有 GRE/TOEFL 成绩者即可申请：http://bit.ly/2Dmmvrt。研究成果/成绩突出的同学可以破格录取，详情请联系后面的电子邮箱。秋季入学申请截止 2 月 1 号。春季申请截止 10 月 1 日。如果申请者有其他经费来源（如国家公派、政府学校或公司项目资助）欢迎垂询。同样欢迎志同道合的访问学者。 研究方向：招收博士的方向是： 深度学习、声音的智能处理、材料基因组学。希望有机器学习基础的同学加入我们。 联系方式：jianjunh@cse.sc.edu；http://mleg.cse.sc.edu/  "
346,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736480&idx=4&sn=1f63c87f3c55f2e4103d97b6d6c66f3e&chksm=871ac31eb06d4a082f1528e05be58ed974ab895783c6c102e9962ebb5331ad45ca134d135b5f&scene=27,AAAI 2018 | 阿里巴巴提出极限低比特神经网络，用于深度模型压缩和加速,"国际知名的人工智能学术会议 AAAI 2018 即将于 2 月份在美国新奥尔良举办，据机器之心了解，阿里巴巴共有 11 篇论文被接收。在介绍的这篇论文中，阿里巴巴提出利用ADMM算法学习极限低比特神经网络的架构。 论文：Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM 论文地址：https://arxiv.org/pdf/1707.09870.pdf 研究背景 近年来，深度学习在人工智能领域取得了重大的突破。在计算机视觉、语音识别等诸多领域，深度神经网络 (DNN, Deep Neural Network) 均被证明是一种极具成效的问题解决方式。如卷积神经网络 (CNN, Convolutional neural network) 在计算机视觉诸多传统问题（分类、检测、分割）都超越了传统方法，循环神经网络 (RNN, Recurrent Neural Networks) 则在时序信号处理，如机器翻译，语音识别等超过传统方法。 在利用深度网络解决问题的时候人们常常倾向于设计更为复杂的网络收集更多的数据以期获得更高的性能。但是，随之而来的是模型的复杂度急剧提升，直观的表现是模型的层数越来越深，参数越来越多。这会给深度学习带来两个严重的问题： (1) 随着模型参数的增多，模型的大小越来越大，给嵌入式端模型的存储带来了很大的挑战。 (2) 随着模型的增大，模型 inference 的时间越来越长，latency 越来越大。 以上两个问题给深度学习在终端智能设备上的推广带来了很大的挑战。比如，经典的深度卷积网络 VGG-16 的模型大小达到 528M，用户很难接受下载一个如此大的模型到手机或者其他终端设备上。同时，在一般的智能手机上，VGG-16 识别一张图像的时间高达 3000+ms，这个 latency 对于大多数用户来说也是难以接受的。此外，由于深度网络的计算量很大，运行深度网络的能耗很高，这对于手机等终端设备也是一个巨大的挑战。 所提出的算法 在这个工作中，我们提出一种基于低比特表示技术的神经网络压缩和加速算法。我们将神经网络的权重表示成离散值，并且离散值的形式为 2 的幂次方的形式，比如 {-4，-2，-1，0，1，2，4}。这样原始 32 比特的浮点型权重可以被压缩成 1-3 比特的整形权重，同时，原始的浮点数乘法操作可以被定点数的移位操作所替代。在现代处理器中，定点移位操作的速度和能耗是远远优于浮点数乘法操作的。 首先，我们将离散值权重的神经网络训练定义成一个离散约束优化问题。以三值网络为例，其目标函数可以表示为： 更进一步，我们在约束条件中引入一个 scale 参数。对于三值网络，我们将约束条件写成 {-a, 0, a}, a>0. 这样做并不会增加计算代价，因为在卷积或者全连接层的计算过程中可以先和三值权重 {-1, 0, 1} 进行矩阵操作，然后对结果进行一个标量 scale。从优化的角度看，增加这个 scale 参数可以大大增加约束空间的大小，这有利于算法的收敛。如下图所示： 对于三值网络而言，scale 参数可以将约束空间从离散的 9 个点扩增到 4 条直线。 为了求解上述约束优化问题，我们引入 ADMM 算法。在此之前，我们需要对目标函数的形式做一个等价变换。 其中 Ic 为指示函数，如果 G 符合约束条件，则 Ic(G)=0，否则 Ic(G) 为无穷大。该目标函数的增广拉格朗日形式为： ADMM 算法将上述问题分成三个子问题进行求解，即 与其它算法不同的是，我们在实数空间和离散空间分别求解，然后通过拉格朗日乘子的更新将两组解联系起来。 第一个子问题需要找到一个网络权重最小化 在实验中我们发现使用常规的梯度下降算法求解这个问题收敛速度很慢。在这里我们使用 Extra-gradient 算法来对这个问题进行求解。Extra-gradient 算法包含两个基本步骤，分别是： 第二个子问题在离散空间中进行优化。通过简单的数学变换第二个子问题可以写成： 该问题可以通过迭代优化的方法进行求解。当 a 或 Q 固定时，很容易就可以获得 Q 和 a 的解析解。 实验结果 ImageNet 图像识别：我们分别在 Alexnet、VGG16、Resnet18、Resnet50、GoogleNet 等五个主流的 CNN 框架上验证了所提出的算法。实验中我们分别尝试了 Binary 网络、Ternary 网络、{-2, -1, 0, 1, 2}、{-4, -2, -1, 0, 1, 2, 4} 四种形式。在 Imagenet 上 Top-1 和 Top-5 准确度结果如下： Alexnet 和 VGG16： Resnet： GoogleNet： 其中 BWN[1] 和 TWN[2] 为我们对比的两种 Binary 网络和 Ternary 网络量化方法。从这些结果可以看出，在各个网络框架下，我们的算法都显著超过对比算法。同时，当比特数达到 3 时，量化之后的网络精度相比于原始网络几乎可以达到无损。在 Alexnet 和 VGG16 这两个冗余度比较高的网络上，量化之后的网络甚至可以取得超过原始网络的精度，这是因为量化操作可以起到一个正则的作用，从而提高这类网络的泛化性能。 Pascal VOC 目标检测：我们在 SSD 检测框架下对算法进行验证，分别采用了 VGG16+SSD 和 Darknet+SSD 两种网络结构。对于检测任务，尝试了 Ternary 网络和 {-4, -2, -1, 0, 1, 2, 4} 两种量化形式。实验结果如下： 对于 Darknet 我们使用了两种设置，第一种设置中所有的权重进行相同的量化；第二种设置中，1x1 的卷积核使用 INT8 量化，即括号中的结果。和识别中的结果类似，在 VGG+SSD 结构中，我们的算法几乎可以做到无损压缩。 "
347,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736480&idx=3&sn=2cc0d7f9a5df1c66dbc89f2d1459a5e1&chksm=871ac31eb06d4a08f7ad9eae0aa3f580ae5bb4e21cea114d7ad7658570b39f5a741db7a8880c&scene=27,入门 | 从Q学习到DDPG，一文简述多种强化学习算法,"towardsdatascience 本文简要介绍了强化学习及其重要概念和术语，并着重介绍了 Q-Learning 算法、SARSA、DQN 和 DDPG 算法。 强化学习（RL）指的是一种机器学习方法，其中智能体在下一个时间步中收到延迟的奖励（对前一步动作的评估）。这种方法主要用于雅达利（Atari）、马里奥（Mario）等游戏中，表现与人类相当，甚至超过人类。最近，随着与神经网络的结合，这种算法不断发展，已经能够解决更复杂的任务，比如钟摆问题。 虽然已经有大量的强化学习算法，但似乎并没有什么文章对它们进行全面比较。每次需要决定将哪些算法应用于特定的任务时，都让我很纠结。本文旨在通过简要讨论强化学习的设置来解决这个问题，并简要介绍一些众所周知的算法。 1. 强化学习入门 通常，强化学习的设置由两部分组成，一个是智能体（agent），另一个是环境（environment）。 强化学习图示 环境指的是智能体执行动作时所处的场景（例如雅达利游戏中的游戏本身），而智能体则表示强化学习算法。环境首先向智能体发送一个状态，然后智能体基于其知识采取动作来响应该状态。之后，环境发送下一个状态，并把奖励返回给智能体。智能体用环境所返回的奖励来更新其知识，对上一个动作进行评估。这个循环一直持续，直到环境发送终止状态来结束这个事件。 大多数强化学习算法遵循这一模式。下面我将简要介绍强化学习中的一些术语，以方便下一节的讨论。 定义 1. 动作（A）：智能体可以采取的所有可能的行动。 2. 状态（S）：环境返回的当前情况。 3. 奖励（R）：环境的即时返回值，以评估智能体的上一个动作。 4. 策略（π）：智能体根据当前状态决定下一步动作的策略。 5. 价值（V）：折扣（discount）下的长期期望返回，与 R 代表的短期返回相区分。Vπ(s) 则被定义为策略 π 下当前状态**s**的期望长期返回值。 6. Q 值或行动值 (Q)：Q 值与价值相似，不同点在于它还多一个参数，也就是当前动作 a。Qπ(s, a) 指当前状态**s**在策略π下采取动作 a 的长期回报。 无模型（Model-free）vs. 基于模型（Model-based） 这里的模型指的是环境的动态模拟，即模型学习从当前状态 s0 和动作 a 到下一个状态 s1 的转移概率 T(s1|(s0, a))。如果成功地学习了转移概率，那么智能体将知道给定当前状态和动作时，进入特定状态的可能性。然而，当状态空间和动作空间增长（S×S×A，用于表格设置）时，基于模型的算法就变得不切实际了。 另一方面，无模型算法依赖试错来更新知识。因此，它不需要空间来存储所有状态和动作的组合。下一节讨论的所有算法都属于这一类。 在策略（on-policy）vs. 离策略（off-policy） 在策略智能体基于当前动作 a 学习价值，而离策略智能体基于局部最优的贪心行为（greedy action）a* 学习价值。（我们将在 Q-Learning 和 SARSA 算法部分进一步讨论这个问题） 2.1 Q-learning 算法 Q-Learning 是基于贝尔曼方程（Bellman Equation）的离策略、无模型强化学习算法： 贝尔曼方程 其中，E 代表期望，ƛ 是折扣因子（discount factor）。我们可以将它重写成 Q 值的形式： Q 值形式的贝尔曼方程 最优的 Q 值 Q*，可以表示为： 最优 Q 值 目标是最大化 Q 值。在深入探讨优化 Q 值的方法之前，我想讨论两个与 Q-learning 密切相关的值更新方法。 策略迭代法 策略迭代法交替使用策略评估和策略改进。 策略迭代法 策略评估会评估从上次策略改进中获得的贪心策略的价值函数 V。另一方面，策略改进通过使每个状态的 V 值最大化的动作来更新策略。更新方程以贝尔曼方程为基础。它不断迭代直到收敛。 策略迭代的伪代码 价值迭代 价值迭代只包含一个部分。它基于最优贝尔曼方程来更新值函数 V。 最优贝尔曼方程 价值迭代的伪代码 在迭代收敛之后，通过对所有状态应用最大值函数直接导出最优策略。 注意，这两种方法都需要知道转移概率 p，这表明它是一个基于模型的算法。但是，正如我前面提到的，基于模型的算法存在可扩展性问题。那么 Q-learning 如何解决这个问题呢？ Q-Learning 更新方程 α 指学习速率（即我们接近目标的速度）。Q-learning 背后的思想高度依赖于价值迭代。然而，更新方程被上述公式所取代。因此，我们不再需要担心转移概率。 Q-learning 的伪代码 注意，下一个动作 a』 的选择标准是要能够最大化下一个状态的 Q 值，而不是遵循当前的策略。因此，Q-Learning 属于离策略算法。 2.2 状态-动作-奖励-状态-动作（State-Action-Reward-State-Action，SARSA） SARSA 很像 Q-learning。SARSA 和 Q-learning 之间的关键区别是 SARSA 是一种在策略算法。这意味着 SARSA 根据当前策略执行的动作而不是贪心策略来学习 Q 值。 SARSA 的更新方程 动作 a_(t+1) 是在当前策略下的下一个状态 s_(t+1) 执行的动作。 从上面的伪代码中，你可能会注意到执行了两个动作选择，它们始终遵循当前策略。相比之下，Q-learning 对下一个动作没有约束，只要它能最大化下一个状态的 Q 值就行了。因此，SARSA 是一种在策略算法。 2.3 深度 Q 网络（Deep Q Network，DQN） Q-learning 是一种非常强大的算法，但它的主要缺点是缺乏通用性。如果你将 Q-learning 理解为在二维数组（动作空间×状态空间）中更新数字，那么它实际上类似于动态规划。这表明 Q-learning 智能体不知道要对未见过的状态采取什么动作。换句话说，Q-learning 智能体没有能力对未见过的状态进行估值。为了解决这个问题，DQN 引入神经网络来摆脱二维数组。 DQN 利用神经网络来估计 Q 值函数。网络的输入是当前的动作，而输出是每个动作对应的 Q 值。 用 DQN 玩雅达利游戏 2013 年，DeepMind 将 DQN 应用于雅达利游戏，如上图所示。输入是当前游戏场景的原始图像，经过包括卷积层和全连接层的多个层，输出智能体可执行的每个动作的 Q 值。 问题归结为：我们如何训练网络？ 答案是基于 Q-learning 更新方程来训练网络。回想一下 Q-learning 的目标 Q 值是: 目标 Q 值 ϕ 相当于状态 s，𝜽 代表神经网络里的参数。因此，网络的损失函数可定义为目标 Q 值与网络 Q 值输出之间的平方误差。 另外两种技术对于训练 DQN 也很重要： 1. 经验回放（Experience Replay）：由于典型强化学习设置中的训练样本高度相关，且数据效率较低，这将导致网络更难收敛。解决样本分布问题的一种方法是采用经验回放。从本质上讲，样本转换会被存储，然后从「转换池」中随机选择该转换来更新知识。 2. 分离目标网络（Separate Target Network）：目标 Q 网络与用来估值的网络结构相同。根据上面的伪代码，在每个 C 步骤，目标网络都被重置为另一个。因此，波动变得不那么严重，带来了更稳定的训练。 2.4 深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG） 虽然 DQN 在高维问题上取得了巨大的成功，例如雅达利游戏，但动作空间仍然是离散的。然而，许多有趣的任务，特别是物理控制任务，动作空间是连续的。而如果你把动作空间分离得太细来趋近连续空间，你的动作空间就太大了。例如，假设自由随机系统的自由度为 10。对于每一个自由度，你把空间分成 4 个部分，你最终就会有有 4¹⁰= 1,048,576 个动作。对于这么大的动作空间来说，收敛也是极其困难的。 DDPG 依赖于「行动者-评论家」（actor-critic）架构。行动者用来调整策略函数的参数𝜽，即决定特定状态下的最佳动作。 而评论家用于根据时间差分（temporal difference，TD）误差来评估行动者估计出来的策略函数。 在这里，小写的 v 表示行动者已经确定的策略。看起来很熟悉对吗？看着像 Q-learning 的更新方程！TD 学习是一种学习如何根据给定状态的未来值来预测价值的方法。Q-learning 是 TD 学习的一种特殊类型，用于学习 Q 值。 DDPG 还从 DQN 借鉴了经验回放和分离目标网络的思想。DDPG 的另一个问题是它很少对动作进行探索。一个解决方案是在参数空间或动作空间中添加噪声。 OpenAI 这篇博客认为在参数空间上添加噪声比在动作空间上添加要好得多。一个常用的噪声是 Ornstein-Uhlenbeck 随机过程。 原文链接：https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287 "
348,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736480&idx=2&sn=f5b817d58831d6984264ccbb4aefbaca&chksm=871ac31eb06d4a0856b9c6c065131917ffe9a17f35d06e421adbd6bcdbb4d00ab6b7d58f65cd&scene=27,深度 | 从概念到实践，我们该如何构建自动微分库,"选自M edium 像 PyTorch 或 TensorFlow 这样通用的自动微分框架是非常有用和高效的，而且在大多数情况下，几乎不需要再写一些更专门化的东西。然而本文作者构建了一个自动微分库，以高效地计算小批量数据上的训练。此外，作者还详细描述了在构建自动微分库中的过程与思考，是理解自动微分理念的优秀博文。 我最近开始写自己的 autodiff 程序包。这篇博客文章记录了我一路以来学到的东西，并把它当成 Julia Evans 的「穷人版」博客文章。 因为有许多博客文章都解释了自动微分的机制，比我解释的要好得多，所以这里我跳过了解释。此外，在构建神经网络结构方面还有其他一些有趣的文章，因此，虽然我的库遵循非常相似的模式（静态计算图和依赖类型），但我并没有过多地关注类型系统。 最后，如果你想直接跳转到代码部分，最终的结果在以下 GitHub 地址，同时还有一个基于神经网络的 FizzBuzz 解决方案。 自动微分代码：https://github.com/maciejkula/wyrm FizzBuzz：https://github.com/maciejkula/fizzbuzz 动机 关于为什么我想要有自己的 autodiff/backprop 框架，而不是直接用 PyTorch 或者 TensorFlow 的原因有以下几点。 1.PyTorch 和 TF 在拟合每个 x 小批量所需计算量很少的模型时非常慢。因为在计算机视觉问题中，每个小批量处理的计算量非常大，以至于框架开销几乎不成问题。但这在矩阵分解式的模型中却不可忽略，这种模型在推荐系统中是有用的。且即使在 GPU 上，拟合这些模型也很慢。 2. 我希望能够用我的 autodiff 库像 Python 包那样以最小的依赖关系来编写和构造模型。这个库能够生成一个相当小的、且独立的二进制文件，这是相对于繁琐的 TF 和 PyTorch 依赖的优势。 3. 这是一个有趣的学习经验，并且让我更详细地了解神经网络库的内部工作机制。 受到对推荐模型（也可能是 NLP) 有效的轻量级解决方案需求的启发，我编写了一系列的设计约束条件（design constraints）。 1. 我希望框架能够自然地支持稀疏梯度：即绝大多数梯度都为零的情况。这在 NLP 和使用大型嵌入层的推荐模型中非常常见。在任何给定的小批量中，只有很小一部分嵌入层被使用，其余记录的梯度均为零。在执行梯度更新时能够跳过零对于快速创建这些模型非常重要。 2. 我希望除实际计算之外，框架有最小的开销。因为我主要想要拟合小的、稀疏的模型，所以开销是关键。在 PyTorch 中，此类模型的运行时间以 Python 中的循环为主要开销。为了避免这种情况，我的库必须在它的拟合循环中放弃 Python，并且需要完全用编译语言编写以充分利用编译器优化的性质。 3. 模型图必须逐个定义，就像 Chainer 或者 PyTorch 一样。这种方法的可用性和可调试性对我来说是非常有价值的，以至于我甚至不想回到 TensorFlow 的处理方式。同时，我很高兴图形一旦被定义就是静态的。这有助于保持较小的开销：我可以分配一次中间计算缓冲区并继续使用它们，而不是写一个复杂的缓冲池系统（或者，更糟糕的是，在每次传递的时候不断地分配和释放内存）。 4. 我希望性能可以与可用 CPU 内核的数量大致呈线性关系。这意味着在整个图形的层次上进行并行化，而不是对单独的操作。每个计算线程将有它自己的计算图副本，但在更新时写入共享参数缓冲区。这实际上是 Hogwild! 方法，这个方法中多个计算线程同时更新共享参数缓冲区而没有任何锁定。只要梯度相对稀疏，就可以在模型质量下降很少的情况下进行近线性的缩放。 这里还列出了一些我现在不想添加或不太关心的事情： 1.GPU 支持。我主要想要拟合小型模型（或者至少有很多参数但每个小批量的计算很少的模型）。 2.CNNs，或者，实际上具有两个维度以上的张量。 考虑到需求（和非需求）列表，我们就能自然地得出一些设计决策。 1. 整个事情将用一种编译语言（compiled language）编写，这种编译语言能够生成没有运行时间的本地共享对象，模型也将用相同的语言来定义。 2. 这个语言将会是 Rust，这是一门令人惊叹的语言，而且非常适合这种任务。因此下面的许多东西都有 Rust 的味道。然而，我所描述的设计权衡在 C++、其他静态类型和 AOT 编译的编程语言中是相同的。 3. 我将会使用反向模式自动微分。这样，我可以很容易地通过多输入的任意（静态）计算图进行反向传播。 在编写库时，我经常想到 API，我希望能够将这个微分库公开并获得社区的帮助。在这种情况下，我想写如下内容： 并让它工作。 准备工作完成之后，我们可以进入有趣的部分：弄清楚如何实现计算图。 表示计算图 我们选择什么样的数据结构来表示计算图？我了解有以下两种方案： 1. 基于向量：所有计算节点都被连续地存储在一个向量中，并使用索引来寻址它们的父节点。例如，在创建输入节点时，对象 InputNode 被压入向量，且索引为 0。如果随后将该节点平方，SquareNode 将被压入索引为 1 的分量，并知道它的父节点是索引 0。在正向传播过程中，SquareNode 将使用该索引来获取其输入的值。 2. 基于图形。节点被放置在内存中的任意位置，并用指向其父节点的索引来维护计算图的结构。(向量表示可以看作是图模型的线性化。) 基于矢量的方法有很多优点。 1. 所有的节点都在同一个地方。他们连续地储存在内存中，可能会减少内存的寻址问题。 2. 他们的所有权很容易解释。这使得克隆计算图图非常简单：只需克隆节点向量即可。这一点很重要，因为我依靠于为我的并行处理方法提供多个图的副本。 3. 节点按拓扑顺序排列。我们可以通过简单地沿着向量向前迭代来正确地执行前向传播，且没有重复的工作。 但是它也有缺点。 我们在节点向量中存储了什么类型的对象是不清楚的。所有的节点类型都不一样（不同的大小），但向量都是同质的类型。Rust 为这种问题提供了两种解决方案，但是都不是特别令人满意。 第一种是枚举（sum 类型；ADTs； tagged unions）。我们定义一个 Node 类型作为所有可能的节点类型的集合，并将其储存在节点向量中。这样，所有的节点就具有相同的类型了。但我们仍然需要将 Node 的方法从封装的 Node 类型分配到所包含的内部节点。这可以通过模式匹配（联合类型标签上的 switch 语句）完成；有 Rust 对模式识别和宏的支持，编写必要的代码是轻而易举的。 但是，这种做法会增加运行时间成本。每次我们使用一个节点，我们需要经过一个 switch 语句来解决内部类型问题。原则上，优化编译器会将这种代码编译成跳转表（jump tables）。实际上，在我的实验中为分配代码生成的程序集仅仅是对所有可能性的线性扫描，强加了与框架支持的具体节点类型数量呈线性关系的分配成本。更糟的是，编译器不愿意内联 switch 本身和被调用的函数。前者是因为它增加了分支预测的失误，后者增加了函数调用的开销。（最近的分值预测攻击加剧了这个问题： compiler mitigations 可能会导致像这样的间接指令更加昂贵。） 对节点向量使用 sum 类型的最后一个缺点是它会导致一个封闭的系统（类似于 Scala『s 的 封闭特性）：库的下游用户不能添加新的节点类型。 另一种方法是用 Rust 的运行时多态机制（polymorphism mechanism）： trait objects。trait objects 是对目标具体类型进行抽象的一种方法：我们将他们隐藏在指向数据的指针和他们方法表的后面，而不是将结构存储在内联中。调用方法时，我们跳转到 vtable，找到函数并执行。通过使用 trait ojbects，我们将这些 fat pointers 放到节点向量中而不是节点自身里面。 然而，这种解决方案恰恰引入了我们开始时想要避免的那种间接性。此外，它完全否认了编译器在内联方面做的努力：被调用的函数直到运行时才知道。 那么基于图的设计呢？在这里，每个节点都在内存中被放置在自己的位置，并且可以通过索引指向其祖先。因为每个节点可以重复使用任意次数，我用 Rust 中的 Rc<T>相当于 C++中的 shared_ptr。 这种方法的一个直接缺点是模糊了图的所有权结构，使克隆和序列化/反序列化变得困难：因为节点可以被重复利用，单纯的克隆/反序列化将导致创建相同节点的多个副本。 第二个缺点是缺少一个容易获得的拓扑排序：前向和后向传递都递归地完成，而且必须小心地避免重复计算共享子图的值。 使用图形表达的优点是在编译时已知任何节点的父节点类型。每一个节点在其父节点类型上是（递归地）通用的：添加两个 InputNodes 将会产生一个 AddNode<InputNode, InputNode>。将其添加到另一个输入节点会产生 AddNode<AddNode<InputNode, InputNode>，InputNode>等等。除了一个在类型系统中表现更好的设计，这给了我分配和内联的静态方法。 结果 使用一些非正式的基准，基于图的方法比基于向量的方法快大约 30%。最后的结果可以在我很普通的双核笔记本上，20 毫秒内在 Movielens 100K 数据集上完整地运行一个 BPR 学习-排序分解模型。此外，它的性能会随着处理器内核的增加而线性增长。 除了底层的图形结构之后，这里还利用了很多优化。 1. 我用 Rust 的 SIMD 内在函数进行了很多操作，如向量点积和标量加法。 2. 对于大多数操作，我假定 C 为连续矩阵并直接在底层数据上迭代，而不是用 ndarrays 迭代方法。事实证明，这样做要快得多，大概是因为它允许 LLVM 自动对向量实现向量化。 3. 事实证明，LLVM 足够智能，能够自动向量化大部分不涉及缩减步骤（主要是赋值）的数值循环。与（2）结合起来看，这种方法使得很多的数值循环以最小的优化努力获得更高的效率。 仍然有很多方法可以使计算速度更快。 1. 此时，代码在正向传递中不会缓存任何子图的结果：如果一个节点在正向传递中被用了两次，所有依赖它的计算将会执行两次。这可以通过一个简单的拓扑排序算法很容易的解决，一旦评估了它们的值，就将该节点标记为已评估。 2. 类似地，在后向传递中梯度被直接传递给参数节点。如果一个节点被多次使用，这意味着在逐步向下传递梯度时做了不必要的工作。累积所有的梯度并且只递归一次将节省这项工作。 3. 对输入有一些不必要的复制，在可能的情况下更好的利用索引应该会产生一些小的性能收益。 下一步是什么 我写了（并继续维护）很多的开源 Python ML 包。这些模型是在 Cython 中手工编写的，尽管它们表现的很好，但是扩展它们是困难的。部分是因为 Cython 的局限性，另一部分的原因在于手动派生更新规则所需的努力。 我希望这个库（或它的一些变体）可以使这个任务变得简单一些，并且可以让我更轻松地实现复杂模型以将它们作为独立的 Python 包发布出去。 附录 结果表明，当图形表达应用到递归神经网络时有一些问题：在递归的每一步，结果类型的复杂度增加，导致了相当奇怪的类型： 不用说，在经过一些迭代步骤后，编译器放弃了。这可以通过实现一个融合的 LSTM 单元来解决，而不是将其从更简单的操作中组装起来，或者选择通过 trait objects 选择性擦除。目前为止，我已经使用了第二种方案：通过将每个 LSTM 单元的输出值装入 trait object 来将其具体类型删除。 原文链接：https://medium.com/@maciejkula/building-an-autodifferentiation-library-9ccf32c7a658 "
349,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736547&idx=1&sn=9c57bf436dee384038b1c9c8f884fbde&chksm=871ac3ddb06d4acb17009cc40f2e0bdb843505eab2ec09bf1d8c8fce8ff5ea27486683f8e754&scene=27,机器学习老中医：利用学习曲线诊断模型的偏差和方差,"选自 Nurhachu Null 学习曲线是监督学习算法中诊断模型 bias 和 variance 的很好工具。本文将介绍如何使用 scikit-learn 和 matplotlib 来生成学习曲线，以及如何使用学习曲线来诊断模型的 bias 和 variance，引导进一步的优化策略。 在构建机器学习模型的时候，我们希望尽可能地保持最低的误差。误差的两个主要来源是 bias（偏差）和 variance（方差）。如果成功地将这两者减小了，我们就能构建更加准确的模型。 但是如何诊断 bias 和 variance 呢？如果检测到了这两者中存在的异常，又该如何处理呢？ 在这篇文章中，我们将使用学习曲线来回答以上两个问题。我们会使用实际的数据集来预测电厂的电能输出。 在预测电厂的电能输出的时候，会生成学习曲线。（图源：Pexels:https://www.pexels.com/photo/black-metal-current-posts-157827/） 假设读者熟悉 scikit-learn 和相关的机器学习理论。如果对交叉验证和监督学习不陌生，那么阅读此文是比较合适的。如果你是机器学习的新手，而且从来没有尝试过 scikit，你可以在这里学习一下（https://www.dataquest.io/blog/machine-learning-tutorial/）。 首先我们简单了解一下 bias 和 variance。 bias-variance 权衡 在监督学习中，我们假设特征和目标之间是有实际联系的，并且要用一个模型来预测这种未知关系。当假设是正确的时候，确实会存在这样一个模型，它能够完美地描述特征和目标之间的关系 f。 事实上，f 总是完全未知的，我们使用一个模型 f^来估计它（请注意 f 和 f^表达上的略微不同之处）。我们使用一个确定的训练集来得到一个确定的模型 f^。如果我们使用了一个不同的训练集，我们很可能会得到一个不同的 f^。f^随着我们对训练集的改变而变化的程度就叫做 variance。 为了估计真正的 f, 我们会使用线性回归或者随机森林等不同的方法。以线性回归为例，其假设特征和目标之间是线性的。然而，在大多数现实场景中，特征和目标之间的关系是复杂的，远非简单的线性关系。简化的假设为模型引入了 bias（偏差）。与实际关系对应的假设越错误，bias 就会越高，反之亦然。 通常，模型 f^在特定测试集上测试的时候会有一些误差。bias 和 variance 给模型带来的额外误差可以用数学的形式表示出来。为了得到较低的误差，我们需要尽可能将两者保持在各自的最小值。然而，这几乎是不可能的。在 bias 和 variance 之间存在 trade-off（权衡）。 低 bias 的模型会很好地适应训练数据。如果我们改变训练集，会得到特别不一样的模型 f^。 可以从图中看到，低 bias 的方法能够捕捉到不同训练数据集中的大部分差异（甚至是在较小的数据集中）。如果我们改变数据集的时候 f^会改变很多，意味着该模型是高 variance。 模型的 bias 越低，它适应数据的能力就越强，同时 variance 也越高。所以，bias 越低，variance 越高。 反过来也说得通：bias 越高，variance 越低。一个高 variance 的模型构建的简单模型通常是不能很好适应数据集的。当我们改变数据集的时候，从高 bias 的算法得到的模型 f^ 通常不会有很大不同。 如果我们改变训练集的时候 f^ 不会改变太多，那么 variance 就比较低，这恰好证明了我们的观点：bias 越高，variance 越低。 从数学上分析，我们想要得到低 bias 和低 variance 的原因是很明显的。如上所述，bias 和 variance 只能增加模型的误差。尽管从一个更直觉的角度而言，我们希望低 bias 来避免构建太简单的模型。在大多数情况中，简单的模型在训练集上的表现是很糟糕的，并且它极有可能在测试数据上也是同样糟糕的表现。 类似地，我们希望较低的 variance 来避免构建一个过于复杂的模型。这样的模型几乎能够完美的适应训练集中的所有数据点。然而，训练数据通常都包含噪声，而且它仅仅是更大的数据中的一个小样本。过于复杂的模型能够捕获这种噪声。当在样本外的数据上测试的时候，性能通常会很差。这是因为模型在样本训练数据上学习得太极致了。它对一些东西特别了解，但是对于其它一无所知。 在实际中，我们需要接受一个 trade-off。我们不可能同时得到低 bias 和低 variance，所以我们期望得到某种中间结果。 图源：http://scott.fortmann-roe.com/docs/BiasVariance.html 下面我们要生成并解释学习曲线，同时会对 tradeoff 有一些直观了解。 学习曲线-基本思想 假设我们拥有一些数据，并且将它们分割成训练集和验证集。我们从训练集中拿一个例子（对，仅用一个样本）来训练模型，并用它来估计一个模型。然后我们在验证集上衡量这个基于一个训练样本的误差。在训练集上的误差是 0，因为它能够很容易地适应一个数据点。然而，在验证集上的误差会特别大。这是因为，这个模型是在一个样本上建立的，它几乎不能够准确地泛化到之前没有见过的数据上。 现在我们考虑一下不是 1 个训练样本的情况，我们取 10 个训练样本来重复上述实验。然后我们取 50 个、100 个、500 个直至使用整个训练集。随着训练集的改变，误差得分会或多或少的改变。 因此我们会监控两个误差得分：一个针对训练集，另一个针对验证集。如果我们把两个误差得分随着训练集的改变画出来，最终我们会得到两个曲线。它们被称为学习曲线。 简而言之，学习曲线会展示误差是如何随着训练集的大小的改变而发生变化的。下图应该能够帮助你可视化我们到目前为止描述过的所有过程。在训练集这一列你可以看到，我们持续增加训练集的大小，这让我们的模型 f^发生了轻微的改变。 在第一行中，当 n=1（n 是训练集中样本的数量）的时候，模型能够完美地适应单个训练数据点。然而，同样的模型在具有 20 个数据点的验证集中性能很差。所以，模型在训练集中的误差是 0，但是在验证集中的误差特别高。 随着我们增加训练集的大小，模型不再完美地适应训练集了。所以训练误差变得更大了。但是因为模型在更多的数据上进行了训练，所以它能够更好地适应验证集。因此，验证误差降低了。要提醒您的是：下面三个实验中验证集是一样的。 如果把每个不同大小的训练集上的误差分数画出来，我们就能够得到两组看起来比较相似的学习曲线： 学习曲线可以诊断监督学习中的 bias 和 variance。下面我们探究一下如何进行： 数据介绍 上述的学习曲线是用于教学目的的理想情况。在实际中，它们通常看起来很不一样。所以，让我们使用现实的数据来讨论。 我们将要构建预测电厂每小时电能输出的回归模型。我们所用的数据来自于土耳其的研究者 Pınar Tüfekci 和 Heysem Kaya，数据可以在这里下载到（https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant）。因为数据是以.xlsx 的形式存储的，所以我们使用 pandas 的 read_excel() 函数来读取它： 我们快速解释一下每一列的名字： PE 这一列是目标变量，它描述的是每小时的电能输出净值。其他所有变量都是潜在特征，每个变量实际上都是每小时的平均值（而不像 PE 一样是净值）。 确定训练集的大小 我们首先确定用来生成学习曲线的训练集的大小。 最小值是 1，最大值是训练集的样本总数。我们的训练集共有 9568 个样本，所以最大值是 9568。 然而，我们还没有设置好验证集。我们将会使用 80:20 的比例来设置训练集和验证集，最终我们的训练集有 7654 个样本（80%），验证集有 1914 个样本（20%），用来生成学习曲线的训练集的最大样本数就是 7654。 在这种情况下，我们用以下 6 种大小的训练集： 应该注意，每个特定大小的训练集都会训练一个新的模型。如果你使用了交叉验证，也就是我们在本文中使用的方法，那么每个训练集大小会训练出 k 个不同的模型（k 是交叉验证的次数）。为了节省代码的运行时间，将交叉验证设置到 5-10 是比较现实的。 scikit-learn 中的 learning_curve() 函数 我们将使用 scikit-learn 中的 learning_curve() 函数来生成一个回归模型的学习曲线。不需要我们自己设置验证集，learning_curve() 函数会自己完成这个任务。 在下面的代码中，我们执行了以下几点： 从 sklearn 中完成需要的 import； 声明特征和目标； 使用 learning_curve() 函数生成需要的数据来绘制学习曲线。函数会返回一个包含三个元素的元组：训练集大小、训练集和验证集上的误差得分。在这个函数内部，我们使用了以下参数：  estimator-代表我们估计实际模型时所用的学习算法；  X-包含特征的数据；  y-包含目标的数据；  train_sizes—所用的特定的训练集大小；  cv-确定交叉验证分割策略（我们马上会讨论这个内容）；  scoring-代表所用的误差指标；我们使用 nearest proxy 和负 MSE，我们随后必须颠倒一下符号。 我们已经知道了什么是 train_sizes。现在让我们来查看一下 learning_curve() 函数返回的另外两个变量： 因为我们指定了 6 个不同的训练集大小，你或许期望看到每个误差得分有 6 个结果。然而，我们得到结果是每个误差得分对应着 6 行数字，每行有 5 个误差得分。 出现这个结果的原因是 learning_curve() 函数运行了 k-fold 交叉验证, 其中 k 的值是通过我们所赋的 cv 参数指定的。 在我们的实验中，cv = 5, 所以会有 5 次分割。每次分割都会在特定大小的训练集上训练出一个模型。上面数组中的每一列都代表一次分割，每一行代表一个训练集大小。下表给出的训练误差有助于您更好地理解这个过程。 为了画出学习曲线，对于每个训练集大小，我们只需要一个误差得分。基于这个原因，在下面的代码中我们会对每一行中的值求平均值，并且颠倒误差得分的符号（正如前面讨论过的一样）： 现在我们已经有了所有的数据了，只需要画出学习曲线。 然而，在绘制学习曲线之前，我们需要停下来做一个重要的观察。也许你已经注意到了，在有些不同大小的训练集上，误差得分是相同的。对于训练集样本数为 1 的那一行，出现这种情况并不意外，（因为都是 0），但是对于其他行呢？除了最后一行，我们有很多相同的值。例如，第二行中有很多值是和第二列相同的，为什么会这样呢？ 这是由于没有对每一份训练集做随机化处理形成的。让我们在下表的帮助下看看下一个例子。当我们的训练集大小是 500 的时候，前 500 个样本被选择。对于第一次分割，这 500 个样本会从第二块中选择。从第二次分割开始，这 500 个样本都将从第一块中选择。因为我们没有随机化训练集，从第二次分割时，这 500 个样本都是一样的。这能够合理解释前面提到的在 500 个训练样本的训练集中，从第二次分割开始所有的误差得分都是一样的结果。 同样的推理能够适用于 100 个样本的情况，类似的推理也适用于其他情况。 为了消除这种现象，我们需要在 learning_curve() 函数中将 shuffle 参数设置为 true。这就能够将训练集中的每一次分割的数据索引随机化。我们之前没有做随机处理的原因是： 数据集文档中已有说明，数据进行了 5 次随机化处理，所以这里没有必要再做随机化处理。 我想让你了解这种看起来很奇怪的现象，以免在实践的过程中困在这个问题上。 最后，我们绘制学习曲线。 学习曲线-高 bias 和低 variance 我们使用常规的 matplotlib 流程来绘制学习曲线： 我们可以从这幅图中提炼出很多信息。下面我们详细探讨： 当训练集的大小是 1 的时候，我们可以看到训练集中的 MSE 是 0。这是很正常的情况，因为模型能够完美地适应一个数据点，在训练集中的预测结果是完美的。 但是在验证集上（验证集有 1914 个样本）测试模型的时候，MSE 会剧烈增长到 423.4。由于这个值特别大，所以我们将 Y 轴的区间限制在了 0 到 40。这让我们能够准确地读到大多数 MSE。因为一个仅在单个样本上训练得到的模型是极其不可能泛化到 1914 个从未见过的新样本上的，所以这个结果也在预料之中。 当训练集样本数为 100 的时候，训练过程的 MSE 会急剧增大，而验证过程的 MSE 会减小。这个线性回归模型不能完美地预测所有的 100 个数据点，所以 MSE 会大于 0。然而，此时训练集的性能仍然优于验证集，这是由于在验证集上估计了更多的数据。 从 500 个数据点开始，验证集的 MSE 能够保持大致不变。这给我们一个重要信息：增加更多的训练数据点也不会带来更好的模型。所以与其浪费时间（金钱）来收集数据，我们更需要的是做点其他事情，例如尝试一下能够构建更加复杂模型的算法。 为了避免误解概念，需要注意的很重要的一点是：增加更多的训练数据样本确实是无济于事的。然而，增加更多的特征就是另外一回事了，因为增加特征能够增加模型的复杂度。 现在我们来讨论一下 bias 和 variance 的诊断。bias 问题的主要标志是较高的验证误差。在我们的例子中，验证 MSE 保持在接近 20 的值。但是这个值有多好呢？ 从技术角度而言，大小为 20 的 MSE 的单位是兆瓦特^2（MW^2）（因为计算 MSE 的时候取了平方）。但是我们目标列中的数值是以 MW 为单位的。给 20MW^2 取平方根，得到的近似值是 4.5MW。每个目标值代表的是一小时的最终电能输出。所以每小时我们的模型都会接近于 4.5MW 的平均值。Quora 有这么一个答案，4.5MW 的能量相当于 4500 个手持吹风机产生的热能。如果我们要预测更长时间（比如一天或者更长时间）的电能输出的时候，这种误差会更大。 由此可以确定，20MW^2 的 MSE 是相当大的。所以我们的模型存在 bias 问题。但是它是一个低 bias 问题呢还是高 bias 问题呢？ 为了找到这个答案，我们需要注意一下训练误差。如果训练误差特别小，这就说明估计模型能够很好地拟合训练数据，这就是说模型在对应的数据集上有较小的 bias。 如果训练误差比较高，就说明估计模型不能很好地拟合训练数据，也就意味着在对应的数据集上有较高的 bias。 在我们的例子中，训练过程的 MSE 稳定在 20MW^2 左右。我们在前面分析过，这是一个相当高的误差得分。因为验证过程的 MSE 比较高，所以训练 MSE 也是比较高的，我们的模型就有一个较高的 bias 问题。 现在让我们诊断一下最终的 variance 问题。对 variance 的估计可以通过以下两种方式完成： 通过检查验证学习曲线和训练学习曲线之间的差距； 通过检查训练误差：检查误差的值随着训练样本数增加的变化。 较小的差距代表较小的 variance。通常，差距越小，variance 越小。反之亦然：差距越大，variance 越大。 正如我们之前观察到的一样，如果 variance 比较大，那么说明模型过于拟合训练数据了。当模型过拟合的时候，它在泛化到从未见过的数据上时会存在问题。当这样一个模型分别在训练集和验证集上测试的时候，训练误差会比较低，验证误差通常会比较高。当我们改变训练集大小的时候，这种模式会继续存在，训练集和验证集之间的差距会决定这两个学习曲线之间的距离。 训练误差和验证误差之间的关系，以及训练学习曲线和验证学习曲线之间的差距可以总结如下： gap=validation error−training error 两个误差之间的差距越大，曲线之间的距离越大，variance 越大。 在我们的情况中，曲线之间的差距是比较小的，所以我们可以稳妥地说，模型的 variance 是比较低的。 高的训练 MSE 得分也是一个快速检测低 variance 的方式。如果学习算法的 variance 比较低，那么当我们改变训练集的时候算法会生成比较简单并且比较相似的模型。因为模型过于简单，它们甚至不能很好的拟合训练数据集（欠拟合）。所以这种情况应该是较高的训练 MSE。所以，高训练 MSE 可以作为低 variance 的标志。 在我们的例子中，训练 MSE 大约稳定在 20，我们已经证明过，这是一个很高的值。所以，除了较小的学习曲线差距之外，我们可以使用较大的训练误差来确认模型具有较低 variance 问题。 目前，我们可以总结如下： 我们的学习算法会遇到这几个问题:高 bias，低 variance，以及对训练数据的欠拟合。 在目前的学习算法下，增加更多的训练样本极不可能得到更好的模型。 在这种情形下我们的解决方案是转向一个更加复杂的学习算法。这应该能够降低 bias，并增加 variance。尝试增加训练样本的数量是一个误区。 通常，以下两种修正方式在处理高 bias 和低 variance 的问题时会比较奏效： 用更多的特征训练当前的学习算法，即通过增加模型的复杂度来降低 bias。 减少对当前算法的正则化。简言之，正则化能够避免算法在训练数据上过拟合。如果我们减少了正则化，模型会更好地拟合训练数据，所以，就会增加 variance，降低 bias。 学习曲线-低 bias 和高 variance 让我们看一下未正则化的随机森林回归器是如何运行的。我们使用和前面相同的流程生成学习曲线。这一次我们会将所有的内容封装在一个函数中，以便以后使用。作为对照，我们也会展示出线性回归模型的曲线。 观察学习曲线，我们可以发现已经成功地降低了 bias。虽然还存在很明显的 bias，但是已经不像之前那么大了。观察训练曲线我们可以判断，这次的模型具有较低的 bias 问题。 两条曲线之间的差距表明模型的 variance 有着大幅度的增大。较小的训练 MSE 证实了对高 variance 的判断。 较大的曲线差距和较低的训练误差同样也标志着过拟合问题的存在。当模型在训练集上性能较好，而在测试集上性能很差的时候，就是过拟合问题。 我们在这里还能观察到的另一个重要现象就是：增加新的训练样本很可能能够得到更好的模型。验证学习曲线并没有稳定在使用最大训练样本量的地方。它还有继续降低，朝着训练学习曲线收敛的潜力，这和我们在线性回归模型的情况中看到的收敛是类似的。 目前，我们可以得到如下结论： 随机森林出现了较高的 variance 和相当低的 bias，以及在训练集上的过拟合问题。 在目前的学习算法下，增加更多的学习样本非常有可能得到更好性能的模型。 至此，我们可以做以下的事来改善我们的模型： 增加更多的训练样本 为目前所用的算法增加正则化。这会增加模型的 bias，降低模型的 variance。 减少我们目前在训练及数据中所用的特征数。算法仍旧会很好地适应训练集，但是由于特征数目减少了，算法会构建相对简单的模型。这应该能够增加模型的 bias，降低模型的 variance。 我们还是要对随机森林算法尝试一下正则化。方式之一就是调整每个决策树叶子节点的最大值。这可以通过使调整 RandomForestRegressor() 函数的 max_leaf_nodes 参数来实现。你没有必要理解这个正则化技术。我们的目标是使你能够注意正则化对学习曲线带来的影响。 还不错，训练学习曲线和测试学习曲线之间的差距缩小了。bias 好像增大了一些，这正是我们想要的结果。 但是我们的工作还未结束。验证过程的 MSE 还有继续降低的潜力。为了达到这个目标，还有一些可以做的工作： 增加更多的训练样本 增加特征 特征选择 超参数优化 理想化的学习曲线和不可约化的误差 这两种学习曲线构成了一个可以对机器学习过程中的模型进行快速检查的很好的工具，那么我们怎么知道何时停止呢？怎么识别完美的学习曲线呢？ 对于我们之前的回归例子，你也许会认为最好的情形应该是两条学习曲线都收敛至 MSE 为 0 的时候。那是完美的情况，可是事实上，很不幸这是不可能的。无论是从实践角度还是理论角度。这是由于不可约误差（irreducible error）的存在。 当我们构建一个能够映射特征 X 和目标 Y 的关系的模型时，我们首先会假设存在这么一个关系。在假设正确的条件下，会存在一个能够完美描述 X 和 Y 之间关系的模型，就像这样： Y=f(X)+irreducible error  （1） 可是这里为啥会有一个 error 项呢？我们不是说 f 能够完美地描述 X 和 Y 之间的关系吗？ 存在误差的原因是 Y 并不是我们所拥有的有限特征 X 的函数。还有更多的特征能够影响 Y。而我们没有这些特征。还有可能是这种情况：X 包含测量误差，所以 Y 也是一个不可约误差的函数。 现在我们解释一下这个误差不可约的原因。当我们用 f^(X) 估计 f(X) 时，我们引入另一个误差—可约误差（reducible error）。 f(X)=^f(X)+reducible error  （2） 将公式（1）中的 f(X) 替换掉，我们得到下面的式子： Y=^f(X)+reducible error+irreducible error  （3） 可约误差可以通过构建更好的模型来减小。从方程（2）中我们可以发现：如果可约误差变成 0，我们的估计模型 f^(X) 等于真实模型了。然而，从方程（3）中我们可以看到，即使可约误差变成了 0，不可约误差仍旧存在。这就是这个误差被称作不可约误差的原因。 这告诉我们，在实际中性能最好的模型会收敛于某个不可约误差，而不是理想的误差值（对于 MSE，理想的误差值是 0；我们将会看到，其他的误差值会有和 MSE 不同的理想值）。 在实际中，不可约误差的准确值几乎总是未知的。我们也假设不可约误差和 X 是独立的。这意味着我们不能使用 X 来寻找真实的不可约误差。用更加严密的数学语言描述，就是：不存在从 X 到不可约误差之间的映射函数 g。 irreducible error≠g(X) 所以，没有办法基于我们所拥有的数据来知道不可约误差的真实值。实际上，最佳应对方法就是尝试得到尽可能小的误差得分，同时要记得：误差得分的极限是某个给定的不可约误差。 对于分类问题，又是怎么样的呢？ 目前，我们已经了解了回归问题中的学习曲线。对于分类任务，这个过程几乎是一样的。主要的区别就是：我们必须选择另一个误差度量--一个能够用来衡量分类器性能的度量。让我们看一个例子： 与我们之前看到的不一样的是，你要注意到训练学习曲线位于验证学习曲线上方。这是因为我们使用的误差得分是准确率，用准确率来描述模型的性能。准确率越高，模型性能越好。而 MSE 是在描述模型有多差。MSE 越小，模型性能越好。 这幅图中也存在不可约误差的含义。对于描述模型有多差的度量指标而言，不可约误差是以下限的形式存在：实际模型不可能比它还低。对于描述模型有多好的度量指标而言，不可约误差是以上限的形式存在：实际模型不可能比它高。 要注意的一点是，在更多数技术性写作中，贝叶斯误差通常指的是分类器的可能最佳错误得分。这个概念和不可约误差是类似的。 后续内容 在任何监督学习算法中，学习曲线构成了诊断模型 bias 和 variance 的很好工具。我们已经学到了如何使用 scikit-learn 和 matplotlib 来生成学习曲线，以及如何使用学习曲线来诊断模型的 bias 和 variance。 为了强化你所学到的内容，可以考虑以下的内容： 使用另一个数据集为回归模型生成学习曲线； 为分类任务生成学习曲线； 不使用 learning_curve() 函数，手写代码，从 0 开始得到一个监督学习任务的学习曲线。可选择交叉验证； 对比一下使用交叉验证和未使用交叉验证的学习曲线。这两种曲线应该对应同一个学习算法（数据也应该是一样的）。 原文链接：https://www.dataquest.io/blog/learning-curves-machine-learning/ "
350,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736429&idx=3&sn=0d28beeb70590ec6ede6f496a0d3c4a1&chksm=871ac353b06d4a4507a25f57c4296e874b31bc8d9224413d967ba80928092a95de8dd26f8c15&scene=27,资源 | Darkon：可更好理解深度学习模型的开源工具包,"darkon 由于难以理解，深度学习经常被称为黑箱。有鉴于此，Neosapience 开发了开源工具包 Darkon，它可以更好地理解深度学习模型，进而调试故障，解释决策等等。目前 Darkon 适用于所有 Tensorflow 模型。 项目地址：http://darkon.io/ Darkon 是一个更好地理解深度学习模型的开源工具包。由于难以理解，深度学习经常被称为黑箱。但是，可解释性与可控性是深度学习模型商业化推广的关键。人们通常认为准备性数据集上实现的高精度足以将模型进行商业化推广，但实际情况却是经常在实际应用中遭受失败，并会导致极端案例的出现。进而，有必要在医疗诊断、金融决策等一些应用中解释结果以信任系统。我们希望 Darkon 可以帮助你理解已训练模型，进而调试故障，解释决策等等。 本文中，我们将提供轻易分析深度学习模型的功能，它适用于任何 Tensorflow 模型（稍后会支持其他模型）。影响值在通过训练样本理解模型方面非常有用。分值可用于过滤拉低测试表现的不良训练样本。优先考虑修复被错误标注的潜在实例，并调试训练和测试样本之间的不匹配分布很有帮助。在本版本中，我们添加了 Grad-CAM 和有指导的 Grad-CAM，这对于理解 CNN 模型的决策很有帮助。 我们会慢慢使轻松分析深度学习模型的技术应用到你现有的项目之中。更多功能也将很快公布。 Demo 该 demo 展示了影响值的实例使用。如果你选择预训练网络和一个特定的测试样本，你可以在预测中看到结果，以及有益或有害的训练样本。训练样本通过影响值被分类，其中最高值对应于有益的样本，最低值对应于有害的样本。 Demo 地址：https://darkon-demo.herokuapp.com/ 依赖项 Tensorflow>=1.3.0：https://github.com/tensorflow/tensorflow 安装 只安装 Darkon 带有 TensorFlow CPU 的安装 带有 TensorFlow GPU 的安装 参考 [1] Cook, R. D. and Weisberg, S.「Residuals and influence in regression (https://www.casact.org/pubs/proceed/proceed94/94123.pdf)」, New York: Chapman and Hall, 1982 [2] Koh, P. W. and Liang, P.「Understanding Black-box Predictions via Influence Functions (https://arxiv.org/abs/1703.04730)」ICML2017 [3] Pearlmutter, B. A.「Fast exact multiplication by the hessian (http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf)」Neural Computation, 1994 [4] Agarwal, N., Bullins, B., and Hazan, E.「Second order stochastic optimization in linear time (https://arxiv.org/abs/1602.03943)」arXiv preprint arXiv:1602.03943 [5] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra「Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization (https://arxiv.org/abs/1610.02391)」ICCV2017 "
351,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736480&idx=1&sn=51bbe4e4eaf149d9436432ee48b4f263&chksm=871ac31eb06d4a08f2c7efdd5f276621758119472a02bd7225ff5143d2472cac60fe3ebef1ad&scene=27,智能零售来了！Amazon Go无人商店周一正式对公众开放,据《西雅图时报》报道，在长达 14 个月的漫长等待之后，亚马逊公司无人商店 Amazon Go 终于要摆脱「Beta 测试」状态，和所有人见面了。当地时间周一早晨 7 点开始，位于西雅图亚马逊总部办公楼下的全球首家无人商店将正式面对公众开放。无需现金，无需排队结账，拿着想要的东西直接出门，智能零售时代现在距离我们还有……20 多个小时。 西雅图的商店，地址接近亚马逊球屋（Amazon sphere），Amazon Go 副总裁 Gianna Puerini 说其目标是为「匆忙、饥饿的人」服务。消费者会找到随拿即走的食物、小吃和饮料等等（图：Bettina Hansen/西雅图时报） 经过近 14 个月只对亚马逊公司员工开放的试运行，周一这家标着 Amazon Go 标志的店面将公开亮相，这是亚马逊近年来投入最多努力的项目之一，旨在重塑实体购物的体验。 Amazon Go 即将为大众开放。也就是说，电商巨头亚马逊的无人便利店将在周一向公众开放，这是在接近 14 个月的试运行后的首次亮相。 商店要求消费者在进入时扫描他们的手机，随后使用摄像头和其它传感器追踪他们在店内购物时的路径。在消费者从货架上取下商品时更新虚拟购物车。客户的亚马逊账户在客户选取并离开商店时会收取相应的费用。 从周一早晨 7 点开始，该商店将面向所有使用 Amazon Go 智能应用和登陆了 Amazon 账号的顾客开放。 Amazon Go 是亚马逊从网上零售商到重塑实体购物体验最大的努力之一。对于面积这家面积不大、仅有 1800 平方英尺的便利店来说，虽然很长一段时间以来并没有向公众开放，但它引领的无人购物概念已经自 2016 年 12 月以来对零售行业产生了巨大的影响。 这家 Amazon Go 商店位于 7th Avenue 和 Blanchard Street 的交界处，亚马逊球屋附近。（图：Bettina Hansen/西雅图时报） 这种被亚马逊称之为「Just Walk Out」的购物概念引发了很多猜测，即亚马逊可以利用它的高科技概念作为基础，将自己的业务扩展到便利店或其它类别的实体零售当中。美国的杂货店工会却对此持批判态度，他们担心自动化收银员的工作会减少美国第二大常见工作的需求。 亚马逊曾表示，他们的目标并不是让收银员变得多余，而是为雇员和消费者提供真正的便利，因为消费者不再需要拥挤在收银员面前等着结算。 在 7th Avenue 和 Blanchard Street 的交界处，记者们上周曾参观了这家著名的无人商店，亚马逊员工也向人们展示了这种商店的使用和便捷之处。 商店有少数员工在整理货架，其他人则在一旁等着帮顾客解决一些问题或疑惑，还有一名员工会在入口处检查顾客的 ID。在商店后面的厨房中，有 6 名员工正在为前面货架上的三明治、沙拉和其它外卖午餐准备备货。 负责 Go 项目的亚马逊副总裁 Gianna Puerini 在接受采访时表示，他们的目标是在保证便利的同时让商店的价格和其它的市场保持一致。 一名员工在 Amazon Go 商店前整理货架。（图：Bettina Hansen/西雅图时报） 乍一看，店内商品的价格似乎与常见的小卖铺差不多。一罐 12 盎司的 Springdrift 苏打水价格是 1.25 美元，一罐 20 盎司的 Coffee Bean and Tea 茶包价格为 7.75 美元，5.99 美元就可以买到越南豆腐三明治。 货品的范围包括从小型加油站便利店到大型商店的商品。Amazon 有四种椰子汁，但是没有热食，且大部分商品是快餐、饮料和杂货。 「零售商店需要为顾客提供选择、便宜的价格和便利，」Puerini 说，「我认为我们在提供这三种服务。」 Amazon Go 副总裁 Gianna Puerini 演示消费者如何使用智能手机扫描进店。所有商店提供这类技术还需要一段时间，因为亚马逊员工需要模拟现实世界条件。（图：Bettina Hansen/西雅图时报） 当然，Amazon Go 不是亚马逊在实体零售方面的唯一尝试。亚马逊去年斥资 137 亿美元收购全食超市（Whole Foods，共 460 多家门店）。 去年，亚马逊还在西雅图开设了两个商品自提点，是超市货物快递项目试验项目的一部分，尽管最近取消了，但是 Amazon Fresh（一种超市食品快递服务）仍然在提供服务。 没有人认为亚马逊的首个 Amazon Go 项目会仅限于此。「我们希望能再开更多门店。」Puerini 说，但是她并未宣布任何扩展计划。（亚马逊或许已经有了计划，去年它曾招聘有经验的物业管理者。） 亚马逊可能并不打算在每个街角都建立一个 Go 商店。根据熟悉亚马逊早期计划的知情者，其内部计划认为一个店要想回本需要店周围几个街区中有数千个办公室职员。 受众为办公室职员的话，则 Amazon Go 商店「需要考虑为他们提供什么，」Puerini 说，「他们追求的是时间和饱腹。」 使 Amazon Go 商店运转的技术大部分在于货架。 Amazon Go 商店天花板上安装了很多摄像头，拍摄消费者和商店。（图：Bettina Hansen/西雅图时报） 数十个平装书大小的长方形黑色设备挂在天花板下面，默默地看着商店。 这些设备使用「多个传感器输入。」Puerini 说道，就像帮助自动驾驶汽车识别视野中的人和物体的系统。（即结合了可分析图像的视频摄像头和激光阵列。） 使系统运转需要一些时间。 2016 年 12 月，亚马逊宣布 Amazon Go 商店向员工开放，期望 2017 年初向外部人员开放。不过亚马逊并未实现这个目标。《华尔街日报》报道称测试时如果商店过于拥挤，系统会崩溃。 一位熟悉 Amazon Go 的工程师在一次采访中称，亚马逊早期花费了大量精力，使其计算机视觉算法可以有效地追踪消费者。 当一个商品被拿起，随后又被放回货架上时，它也会自动从购物者的账单中被去除。（图：Bettina Hansen/西雅图时报） 在亚马逊的自动顾客跟踪技术背后，这位工程师说道，真人店员会复审录像以确保系统正确运行——至少一开始会是这样。 这家科技公司显然认为它已经解决了无人商店正式上线前会面临的所有挑战，事实上，去年 11 月彭博社新闻曾报道过三名亚马逊员工身穿皮卡丘服装试图欺骗商店的跟踪系统，亚马逊的算法为每个毛茸茸的顾客进行了正确的结算。 Puerini 并没有详细介绍延迟背后的技术原因，并表示真人店员存在于购买确认流程中的情况「只会占用一小段时间」。 她表示，Amazon Go 并未迅速开放的部分原因在于员工对它的使用率非常高，使得 Amazon Go 商店的情况高度模拟了现实世界的环境，所以公司没必要尽快将之推向公众。 那么 Amazon Go 现在多久会出一次错？Puerini 没有正面回答这个问题，只是说：「系统非常准确。」 「如果出现了错误，」Puerini 说道，「顾客可以用 Amazon Go 的 APP 扫一下收据上误收费的商品条目，去掉收取了费用但自己实际上没买的物品。」 原文链接： https://www.seattletimes.com/business/amazon/amazon-go-cashierless-convenience-store-opening-to-the-public/ 
352,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736429&idx=2&sn=6421643f152771aa8ebb6c78db598f47&chksm=871ac353b06d4a457bdd0a98fe86c8f259e7ca03e1b8a512fd394b461602ab6c28786a9adf60&scene=27,教程 | Prophet：教你如何用加法模型探索时间序列数据,towardsdatascience 作为探索时间序列的第一步，Python 中的加法模型是必经之路。本文使用 和金融数据集探索如何对时序数据进行建模与分析。加法模型可以快速构建与部署，并解释和预测不确定性，是我们进一步采用LSTM等深度模型进行建模的基础。 时间序列是我们日常生活中最常见的数据类型之一。金融产品价格、天气、家庭能源使用量、甚至体重都具有变化规律。几乎每个数据科学家都会在日常工作中遇到时间序列，学习如何对时间序列进行建模是一项重要的数据科学技能。用于分析和预测周期性数据时，一种强大而简单的方法是加法模型（additive model）。这个想法很简单：将时间序列表示为每日、每周、每季度和每年度等不同时间维度的组合模式，并加以整体趋势。你的能源使用量可能会在夏天上升，在冬天下降，但是随着你家庭能源使用效率的提高，能源使用量总体呈下降趋势。加法模型可以向我们展示数据的模式/趋势，并根据这些观察结果进行预测。 下图显示了一个时间序列的加法模型，它分解为整体趋势、年度趋势和每周趋势。 本文将介绍使用由 Facebook 开发的 Python Prophet 预测软件包 创建金融时间序列数据的加法模型。同时，我们也将介绍如何使用 Pandas 进行数据操作，以及使用 Quandl 库访问金融数据。本文中已经包含了主要代码，完整的代码请详见 GitHub 上 Jupyter Notebook 中的全面分析。本文将从头开始介绍时间序列建模的每个步骤。 完整代码：https://github.com/WillKoehrsen/Data-Analysis/tree/master/additive_models 免责声明：金融数据过去的表现并不能作为未来表现的指标，你不能使用这里的方法来致富！这里选择使用股票数据的原因是因为它表现出某种每日波动频率。如果你真的想变得富有，只要学习数据科学就够了！ 获得金融数据 通常，一个数据科学的项目有大约 80％的时间花在获取和清洗数据上。本项目中，Quandl 库可以将这个工作量减少到 5％左右。Quandl 可以在命令行中通过 pip 命令安装： Quandl 是免费的，你可以每天提出 50 个访问请求而无需注册。如果注册一个免费的帐户，你会得到一个 API 密钥，允许无限制次数的请求。 首先，引入所需的库并获取数据。Quandl 中的数据几乎是无限的，但我想集中比较同行业中的两家公司，即特斯拉和通用汽车。特斯拉是一个引人注目的公司，不仅因为它是 111 年以来美国第一个成功的汽车创业公司，它也是 2017 年美国最值钱的汽车公司。它的竞争者是通用汽车，通用汽车最近已经通过制造一些非常酷的全电动车来展现拥抱未来汽车的迹象。 我们可以通过一句简单的 quandl 命令来获得两家公司的每日股票市值： Quandly 自动将数据放入 Pandas 数据框（DataFrame）中，DataFrame 是数据科学家的首选数据类型。（对于其他公司，只需用「TSLA」或「GM」替换股票代码，你也可以指定日期范围） 在建模之前，最好先了解一下数据的结构和范围。这也将有助于找出需要纠正的异常值或缺失值。 Pandas dataframe 可以很容易地用内置方法绘图： 仅仅比较这两家公司的股票价格，并没有显示出哪个更有价值，因为公司的总市值也取决于股票数量（市值=股价*数量）。Quandl 没有免费的股票数量数据，但是我找到了两家公司的平均年度股票数。这是不精确的，但是对我们的分析来说足够准确。有时我们不得不使用不完善的数据！ 在这里，我们使用 Pandas 的一些技巧，如改变列的索引（reset_index），同时使用 ix 命令添加索引和更改 dataframe 中的值。 这为特斯拉创建了名为「cap」的列。我们对通用汽车数据进行同样的处理，然后将两者关联（merge）。关联实质上是数据科学工作流的一部分，因为它允许我们在共享列的基础上合并不同的数据集。在这种情况下，该列是日期。我们进行「inner」关联，只保存两个数据框中有相同日期的数据行。 市值的单位为十亿美元。我们可以看到，开始时通用汽车的市场份额超过特斯拉 30 倍。随着时间推移，事情会保持不变吗？ 我们观察到特斯拉的急剧上升以及通用汽车在期间的小幅上涨。特斯拉在 2017 年甚至超过了通用汽车！ Tesla  was valued higher than GM  from 2017 - 04 - 10  to  2017 - 09 - 21. 在此期间，特斯拉销售约 4.8 万辆汽车，而通用汽车售出 150 万辆。即使销售了 30 多倍汽车，通用汽车的价值仍低于特斯拉。这绝对显示了有号召力的执行官和高质量的产品（如果极低产量）的力量。尽管特斯拉的价值现在低于通用汽车，但是一个好问题可能是，我们可以预测特斯拉再次超越通用汽车吗？什么时候会发生？为此，我们转向预测加法模型，预测未来。 Facebook 于 2017 年发布基于 Python 和 R 的 Prophet 包，它极大地帮助了数据科学家的工作。Prophet 设计目的是用日常观测数据分析时间序列，这些数据在不同尺度衡量下具有模式规律。它同时对建模节日效应的时间序列和添加人工变化点（changepoint）有出色的能力，但在本文中我们将仅运用基本功能来建模和运行。 我们首先引入 prophet，并将我们数据中的列重新命名为正确的格式。日期列必须被称为「ds」，数值列被称为「y」。在这里，数值列是市值。然后，我们创建 prophet 模型并传入数据训练，就像 Scikit-Learn 机器学习模型一样： 创建 prophet 模型时，我将 changepoint 先验设置为 0.15，高于默认值 0.05。这个超参数用于控制趋势对变化的敏感程度，数值越高越敏感，数值越低越不敏感。这个数值用于权衡机器学习中最基本的一对统计量：偏差与方差。 如果我们的预测曲线过于贴近训练数据，这称为过拟合，此时方差很大，并且模型将不能很好地推广到新的数据。另一方面，如果我们的模型没有捕捉到我们的训练数据中的趋势，这称为欠拟合，此时偏差很大。当模型欠拟合时，增加先验变化点可以使模型具有更大的灵活性来拟合数据；如果模型过拟合，需要减少先验来限制灵活性。由于股票日常变化很大，我们希望模型能够捕捉到这一点，所以我增加了灵活性以更好地拟合数据。在创建一个 prophet 模型中，我们也可以指定变化点，如时间，当希望序列从上升到下降趋势时，反之亦然；如节日，当希望影响时间序列时。如果我们不指定变化点，prophet 会为我们计算它们。 为了进行预测，我们需要用 prophet 模型创建所谓的用于预测的未来数据框。我们指定预测的未来时期区间（两年）和预测的频率（每天）。 我们的未来数据框包含未来两年特斯拉和通用汽车的估计市值。我们可以用 prophet 的绘图函数来可视化预测。 黑点代表实际值（注意实际值测量截止到 2018 年初），蓝线表示预测值，淡蓝色阴影区域表示不确定性（这是预测的关键部分）。未来时间距离越远，不确定性区域越大，因为初始的不确定性随着时间的推移而增长。在天气预报中也观察到这种情况，时间越远天气预报越不准确。 我们也可以观察由模型确定的变化点。变化点代表时间序列从上升到下降（或相反）的趋势。为了便于比较，我们可以在此时间范围内查看特斯拉的谷歌搜索趋势，看看这些变化是否一致。我们在一张图上同时画出变化点（垂直线）和搜索趋势： 某些特斯拉市值的变化点与特斯拉搜索的频率变化一致，但不是全部一致。从这个角度来说，我认为谷歌搜索频率并不是反映股票变化的一个很好的指标。 我们仍然需要计算出何时特斯拉的市值将超过通用汽车的市值。由于我们有两家公司未来两年的预测，那么在合并数据框之后，我们可以在同一个图上画出这两家公司的市值变化。   首先，我们将只画出估计值。估计值（在 prophet 包中称为「yhat」）平滑了数据中的一些噪音，因此看起来与原图略有不同： 我们的模型认为，特斯拉在 2017 年短暂超越通用汽车的事件只是噪音，在预测中，特斯拉到 2018 年初之后才会超越通用汽车。确切的日期是 2018 年 1 月 27 日，所以如果这个事件发生了，我将准确地预测了未来！ 在做上面的图表时，我们忽略了预测中最重要的部分：不确定性！我们可以使用 matplotlib 来画出存疑的区域： 上图更好地显示了预测内容。图中显示两家公司的市值都将预计增加，但特斯拉将比通用汽车增长更快。同样，随着时间的推移，不确定性会随着时间的推移而增加，而特斯拉的预测下限低于通用汽车的预测上限，这意味着或许通用汽车在 2020 年仍将处于领先地位。 趋势和模式 市值分析的最后一步是看整体趋势和模式。prophet 让我们可以很容易地看到整体趋势和不同维度的模式： 这个趋势非常明显：通用汽车的股价正在上涨并将继续上涨。年度模式很有意思，因为这似乎揭示了通用汽车的股价在年底会有所增长，但随后会缓慢下滑直到夏季。因此，我们可以尝试计算年度市值与通用汽车在此期间平均每月的销售额之间是否存在相关关系。 看起来月销量与市值不相关。八月份的月销售额是第二高的，但此时是市值的最低点！ 而且，每周趋势没有如预期显示出意义。经济学中的随机游走理论指出，股票价格每天都没有可预测的模式。正如我们的分析所证明的那样，长期来看，股票往往会上涨，但在每日来看，几乎没有我们可以利用的模式。 道琼斯工业平均指数（反映证券交易所 30 家最大公司的市场指数）很简单地说明了这一点： 显然，要是回到 1900 年投资，你就发财啦！或者实际上，当市场下跌的时候，不要撤资，因为根据历史规律它会回升。从全局来看，日常波动太小，甚至不能被看到，如果我们像数据科学家那样思考，会意识到，与投资全体市场并持有长期相比，短线投资股票是没有意义的。 Prophet 也可以应用于更大规模的数据测量，如国内生产总值（衡量一个国家经济总体规模）。我根据美国和中国的历史 GDP 创建了 prophet 模型并做了以下预测。 中国的 GDP 将超过美国的具体时间是 2036 年！由于观测频率低（每年一次），这个模型是有限的，但它提供了一个不基于宏观经济知识要求的基本预测。 有很多方法来模拟时间序列，从简单线性回归到具有 LSTM 的循环神经网络（recurrent neural network）。加法模型是有用的，因为它们可以快速开发和运行，可以解释并预测不确定性。Prophet 的能力令人印象深刻，我们在这里只涉及到基本功能。我鼓励你使用本文和 notebook 来探索 Quandl 提供的一些数据或者利用你自己的时间序列数据。作为探索时间序列的第一步，Python 中的加法模型是必经之路！ 
353,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736429&idx=1&sn=33a8b63ddfd10acb167a6d94466985a1&chksm=871ac353b06d4a4548f5c5f457d6305c0451d51581f1d3891da4815c483274d50e9c62eb1b9d&scene=27,Kaggle车辆边界识别第一名解决方案：使用预训练权重轻松改进U-Net,本文作者之一 Vladimir Iglovikov 曾取得 Kaggle Carvana Image Masking Challenge 第一名，本文介绍了他使用的方法：使用预训练权重改进 U-Net，提升图像分割的效果。 代码地址（包含预训练权重）：https://github.com/ternaus/TernausNet 随着处理密集计算的计算机硬件的发展和平民化，研究者能够处理拥有数百万参数的模型。卷积神经网络在图像分类、目标识别、场景理解等领域都取得了极大的成功。对几乎所有的计算机视觉问题，基于 CNN 的方法都优于其他技术，在很多情况下甚至超越了人类专家。目前，几乎所有的计算机视觉应用尝试使用深度学习技术来改进传统方法。它们影响到我们的日常生活，且这些技术的潜在应用场景似乎也很惊人。 可靠的图像分割是计算机视觉领域的重要任务之一。该问题对医疗图像领域极为重要，可以提高诊断能力，在场景理解领域中有助于创造安全的自动驾驶汽车。密集图像分割本质上是把图像分为有意义的区域，可看作是像素级别的分类任务。处理此类问题最直接（也缓慢）的方法是手动分割图像。然而，这种方法极为耗时，而且人类数据管理员不可避免地会出错、存在不一致问题。自动化该过程能提供尽可能快的、系统性的图像分割。该过程需要达到一定的准确率，以在生产环境中使用。 在过去几年中，研究者提出了不同的方法来解决该问题：创造一种 CNN，为一整张输入图像在单个前向传递中生成分割图。其中，最成功的最优方法基于全卷积网络（FCN）[2]。其核心思想是将 CNN 中的全连接层替换成卷积层，成为强大的特征提取器，直接输出空间特征图，而不是全连接层输出的分类分数。然后，上采样这些图，生成密集的逐像素输出。该方法以端到端的方式训练 CNN，分割任意大小的输入图像。此外，在 PASCAL VOC 等标准数据集上，该方法极大地改进了分割的准确率。 后来，这一方法进一步被改进为 U-Net 神经网络 [4]，U-Net 架构使用跳过连接（skip connection）将低层特征图与高层特征图结合起来，带来精确的像素级定位。在上采样部分，大量的特征通道向更高的分辨率层传播上下文信息。在卫星图像分析、医疗图像分析等二值图像分割竞赛中，这种类型的网络架构已经证明了自己。 在此论文中，作者展示了如何使用预训练权重轻松改进 U-Net 的性能。作者还将它应用到航空图像标注数据集 [8] 中，该数据集包含多个城市的高清航空图像。这些图像的每个像素都被标注为「建筑」或「非建筑」类别。该架构另一个成功应用案例与初始化方案是 Kaggle Carvana 图像分割竞赛 [9]，本论文作者之一使用它作为解决方案的一部分，获得了第一名。 II. 网络架构 通常，U-Net 架构包含一个收缩路径来捕捉上下文信息，以及一个对称的扩张路径以进行精准的定位（见图 1）。收缩路径遵循典型的卷积网络架构，即交替卷积和池化运算，并逐步下采样特征图，同时逐层增加特征图的数量。扩张路径的每个阶段由一个特征图上采样和紧随的卷积构成。 因此，扩张分支可以提高输出的分辨率。为了进行定位，扩张路径通过跳过连接将上采样特征和来自收缩路径的高分辨率特征结合起来 [4]。模型的输出是一个逐像素掩码，展示了每个像素的类别。该架构被证明对有限数据的分割问题很有用，示例参见 [5]。 U-Net 可以从相对较小的训练集中学习。多数情况下，图像分割的数据集由至多几千张图像构成，因为手动标记掩码是非常繁重的工作。通常 U-Net 以随机初始化权重开始训练。众所周知，要使网络训练避免过拟合，数据集应该足够大，包含数百万张图像。在 ImageNet [10] 数据集上训练的网络被广泛地用于其它任务的网络初始化。用这种方法，可以让网络非预训练的几层（有时仅仅是最后一层）利用数据集的特征进行学习。 我们使用 VGG 族 [11] 中非常简单的 CNN 作为 U-Net 网络的编码器，它由 11 个顺序层构成，称为 VGG11，参见图 2。VGG11 包含 7 个卷积层，每个紧随着一个 ReLU 激活函数和 5 个最大池化运算，每个运算之后将特征图减半。所有卷积层都有 3x3 的卷积核，通道数如图 2 所示。第一个卷积层有 64 个通道，然后网络加深，每个卷积层和最大池化运算之后通道数会加倍，直到通道数变为 512。在之后的卷积层中，通道数不变。 图 2：VGG11 网络架构。在这张图中，每个卷积层之后是 ReLU 激活函数。每个框中的数字表示对应特征图的通道数量。 为了构建解码器，我们移除所有的全连接层，并用包含 512 个通道的单一卷积层来替换它们，该层是网络的瓶颈中间部分，分离编码器与解码器。为了构建解码器，我们使用两倍于特征图大小的转置卷积层，同时把通道数量减少一半。转置卷积的输出接着被连接到解码器对应部分的输出。由此得到的特征图通过卷积运算来处理，以保持通道数量与对称编码器项相同。上采样步骤被重复 5 次以配对 5 个最大池化层，如图 1 所示。严格来说全连接层可以采用任何大小的输入，但是由于我们有 5 个最大池化层，每个层下采样图像两次，只有可被 32（2^5）整除的图像可以用作当前网络实现的输入。 图 3：三种以不同权重初始化的 U-Net 模型的 Jaccard 指标随训练 epoch 的变化。蓝线表示随机初始化权重的模型，橙线表示编码器以在 ImageNet 上预训练的 VGG11 网络权重初始化的模型，绿线表示整个网络在 Carvana 数据集上预训练的模型。 图 4：绿色像素的二进制掩膜表示分类族群（建筑）。图 A 展示初始图像和叠加的真实掩膜。图 B 到图 D 表示使用三种权重初始化方案并训练了 100 个 epoch 后得到的预测结果。图 B 表示随机初始化权重的模型，图 C 中的模型使用随机初始化权重，编码器以在 ImageNet 上预训练的 VGG11 网络权重进行初始化，图 D 中的模型使用在 Carvana 数据集上预训练的权重。 在本论文中，通过使用微调（fine-tuning）技术初始化网络编码器的权重，我们展示了如何提升 U-Net 的性能。这种神经网络被广泛用于图像分割任务，并在许多二值图像分割、竞赛中取得了当前最优结果。微调已广泛用于图像分类任务，但是就我们所知还未用于 U-Net 类型的架构。对于图像分割问题，微调应该是更自然的选择，因为收集大量数据集（尤其是医疗图像）并进行很好地标注是很困难的。此外，预训练网络可以大幅减少训练时间，同时有助于防止过拟合。考虑到存在更多先进的预训练编码器比如 VGG16 [11] 或任何预训练的 ResNet 网络，我们的方法还可进一步提升。有了这些改进的编码器，解码器可以像我们使用的一样简单。 论文：TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation 论文链接：https://arxiv.org/abs/1801.05746 摘要：像素级的图像分割是计算机视觉中的艰巨任务。由编码器和解码器构成的经典 U-Net 架构经常用于分割医学影像、卫星影像等。通常，神经网络使用在大型数据集（例如 ImageNet）上预训练的网络权重进行初始化，相比用小型数据集从零开始训练的网络能获得更好的性能。在某些特定的应用中，特别是医学和交通安全，模型的准确率至关重要。在本文中，我们展示了 U-Net 类型的架构如何利用预训练的编码器提升性能。我们的代码和相关的预训练权重已开源。我们比较了三种权重初始化方案：LeCun uniform、取自 VGG11 权重的编码器和在 Carvana 数据集上训练的完整网络。该网络架构是 Kaggle 竞赛（Carvana Image Masking Challenge）中获胜解决方案（在 735 名参赛者中排名第一）的一部分。 
354,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736406&idx=2&sn=fefe5b628349ca7086a53b507a496f11&chksm=871ac368b06d4a7ee31f8b8960fea4fcc578ee712dd135002720958d454ccd97fb6a46dfbcb6&scene=27,深度 | 对比TensorFlow提升树与XGBoost：我们该使用怎样的梯度提升方法,Nicolo Blog 几个月前，TensorFlow 发布了梯度提升方法的调用接口，即 TensorFlow 提升树（TFBT）。不幸的是，描述该接口的论文并没有展示任何测试效果和基准的对比结果，所以 Nicolò Valigi 希望能对 TFBT 和 XGBoost 做一个简要的对比，并分析它们之间的性能差异。机器之心介绍了该测试与 TFBT 的原论文，且 TF 1.4 及以上的版本也可测试该提升树模型。 本文将先介绍 Nicolò Valigi 的对比试验结果，然后再简述谷歌新提出来的 TensorFlow 提升树。此外，该试验之所以选择 XGBoost，是因为自从它发布以来，它就是许多数据挖掘问题的首选解决方案。而且因为 XGBoost 对未归一化或缺失数据的高效处理方式，以及快速和准确的训练过程，它很适合与 TFBT 进行基准测试。 该测试的源代码和结果地址：https://github.com/nicolov/gradient_boosting_tensorflow_xgboost 作者使用适当大小的航线数据集以测试两个解决方案，该数据集包含了从 1987 到 2008 年的美国商业航班记录，共计 1.2 亿个数据点。它的特征包含始发站、目的地、登记时间与日期、航线和飞行距离等，而作者尝试使用这些特征做一个二元分类器，以判断航班是否会延误超过 15 分钟。 数据集地址：http://stat-computing.org/dataexpo/2009/ 作者从 2006 年抽取 10 万个航班以作为训练集，并从 2007 年抽取 10 万个航班作为测试集。大约有 20% 的航班延误是超过 15 分钟的，这和当前的航班延误情况有些不一样。下图展示了该数据集航班延迟情况和起飞时间的关系： 作者并没有执行任何特征工程，因此采用的特征都十分基础： 对于 XGBoost 来说，作者使用 scikit 风格的封装，这令训练和预测只需要使用几行代码和几个 NumPy 数组。对于 TensorFlow，他使用 tf.Experiment、tf.learn.runner 方法和 NumPy 输入函数以节省一些代码。 试验结果 作者从 XGBoost 开始测试，并采用适当的超参数。很快我们就能得到非常不错的 AUC 曲线。但是作者表明 TFBT 训练较慢，可能我们需要耐心等一段时间。当他为这两个模型设置超参数 num_trees=50 和 learning_rate=0.1 后，作者不得不使用一个留出的数据子集以调整 TensorFlow 提升树的 TF Boosted Trees 和 examples_per_layer 两个超参数。这很可能与 TFBT 论文中提到的新型逐层学习算法相关，但我们并不详细探讨这个问题。作为对比的出发点，作者选择了两个值（1K 和 5K），它们在 XGBoost 中有相似的训练时间和准确度。 准确度数值： 训练运行时： 两套配置都显示 TensorFlow 提升树的结果不能匹配 XGBoost 的性能，包括训练时间和训练准确度。除了 CPU 使用时间过长的缺点外，TFBT 似乎在多核并行训练的效率上也不高，因此导致了总运行时的巨大差别。XGBoost 可以轻松加载 32 个核心中的 16 个，这在使用更多树的时候会有更好的效果，而 TFBT 只能使用 4 个核。 Nicolò Valigi 最后表明，即使通过几个小时的调整，他也无法使用 TFBT 实现与 XGBoost 相匹配的结果，无论训练时间还是准确性。而并行训练的实现也有限制，这意味着它也不能扩展到大型数据集。 前面 Nicolò Valigi 的试验表明 TensorFlow 提升树接口仍然达不到 XGBoost 的性能，但在 TensorFlow 上构建提升树的调用接口很有意义。因为这也意味着即使是传统的数据分析和机器学习算法，我们也可以直接调用 TensorFlow 完成。以下是提出 TFBT 的论文，我们对此作了简要介绍。 论文：TF Boosted Trees: A scalable TensorFlow based framework for gradient boosting 论文地址：https://arxiv.org/abs/1710.11555 TF 提升树（TFBT）是一种用于分布式训练梯度提升树的新型开源框架。该框架基于 TensorFlow，并且它独特的特征还包括新颖的架构、损失函数自动微分、逐层级（layer-by-layer）的提升方法、条理化的多类别处理和一系列可以防止过拟合的正则化技术，其中逐层级的提升方法可以减少集成的数量以更快地执行预测。 1. 前言 梯度提升树是最受欢迎的机器学习模型之一，自从梯度提升树算法被提出以来，它就主宰了许多带有真实数据的竞赛，包括 Kaggle、KDDCup[2] 等顶尖竞赛。除了出色的准确度，提升方法同样很容易使用，因为它们擅长处理非归一化、共线性或异常感染的数据。该算法同样支持自定义损失函数，并且通常要比神经网络或大型线性模型更具可解释性。由于梯度提升树非常受欢迎，目前有非常多的实现库，包括 scikit-learn [7]、R gbm [8]、Spark MLLib [5]、LightGBM [6] 和 XGBoost [2] 等。 在本论文中，我们介绍了另外一个可优化和可扩展的梯度提升树软件库，即 TF 提升树（TFBT），该算法库构建在 TensorFlow 框架 [1] 的顶层。TFBT 合并了一组新颖的算法以提升梯度提升树的性能，包括使用新的逐层提升过程提高一些问题的性能。TFBT 是一个开源的库，它可以在 TensorFlow 主流发行版的 contrib/boosted_trees 下找到。 2.TFBT 特征 在表 1 中，我们提供了一个简要地对比，从上可以了解当前主流梯度提升树软件库的特性： 除了上述描述的分布式训练、损失函数形式和正则化技术等特征以外，TF 梯度提升树主要还有以下两个特征： 逐层的提升方法（Layer-by-layer boosting）：TFBT 支持两种树型构建的模式，即标准的方式和新颖的逐层提升方式。其中标准模式即使用随机梯度的方式构建提升树序列，而逐层提升的方式允许构建更强的树和更深的模型。 多类别支持：TFBT 支持一对多（one-vs-rest）的方式，此外它还通过在每一个叶结点上储存每一个类别的分数而减少树的数目要求，这和其它一些变体一样。 因为 TFBT 是使用 TensorFlow 实现的，所以所有 TensorFlow 具体的特征都是可获取的： 易于编写自定义的损失函数，因为 TensorFlow 提供了自动微分工具 [1]，而其它如 XGBoost 那样的库要求使用者提供一阶导数和二阶导数。 我们能无缝转换和对比 TFBT 与其它 TensorFlow 封装模型，还能通过其它 TensorFlow 模型生成的特征轻松组合梯度提升树模型。 很容易通过 TensorBoard 进行调试。 模型能多块 CPU/GPU 和多个平台上运行，包括移动端，它们都很容易通过 TF serving[9] 进行部署。 3.TFBT 系统设计 TFBT 架构如下，我们的计算模型基于以下需求： 能够在数据集中训练且不需要适配工作站的内存。 能够处理特征数目众多的深度树模型。 支持不同模式构建提升树：标准的 one-tree-perbatch 模式和逐层提升树模式。 极小化平行化损失。 Nicolò Valigi测试地址： https://nicolovaligi.com/gradient-boosting-tensorflow-xgboost.html 
355,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736406&idx=3&sn=dfe0102d57b721a1e0cdd6cad4ab840e&chksm=871ac368b06d4a7e866e6e12edf16942c22215f6baec123057fb28976d830f79fbfb3e4359e4&scene=27,专栏 | 语音及文本类AI芯片的需求分析,"目前视频和图像类深度学习加速芯片已经呈现红海状态，而语音类人工智能芯片还处于上升期，文本处理等领域目前还处于探索时期。本文的目的就是从应用和算法的角度分析一下，我们需要一款怎样的语音／文本处理的深度学习芯片？它需要具有怎样的功能覆盖和参数灵活度？这样的芯片能应用到哪些地方？ 目前，语音文本类深度学习应用主要分为几个大的方面： 1. 语音识别（语音转文字） ，包括语音命令，语音听写和语音转录。语音命令往往比较短，例如“发短信给xxx”，“打开电视机”等等。语音听写则是对近场清晰语音的人对机听写，需要有一定的实时性。可以识别完整的一句话或一段内容。语音转录做的是人人对话（例如会议）时的速录员的工作，可以是非实时的录音，可以采用更复杂的处理技术。目前语音识别大类，尤其是后两者主要还是依靠调用云端API实现。在终端化上还处于尝试期。另外，对学习者的口语进行评分，也属于语音识别的范畴。 2. 语音生成（ ） 文字转语音比较明确，主要用于读出给定的文本，也可以进行风格化，即模仿某个人的声音。目前采用终端和云端都可以实现这个功能，甚至手机本身就可以处理。其难点是需要妥善处理分词、多音字和语气。 3. 人机对话（Chatbot） 这是文本处理的最典型的应用，主要用于聊天／客服机器人。有时候也会结合前两者用于人机语音对话，例如智能音箱。目前人机对话整体还处于一问一答阶段，基于上下文的对话机制仍处研究阶段，但对于特定场景，例如客服机器人，可以进行一定程度的多轮对话。人机对话往往和搜索引擎结合起来使用，当无法作出准确回答时，往往会提供搜索结果。 4. 自然语言处理（NLP） 当前的自然语言处理技术可以对一段文字进行词法分析（中文分词、词性标注、命名实体识别等），从而提供出用词统计信息，提取关键词，分析词与词的相似度等。句法分析可以得到句子的结构和词的依赖关系。找到句子的成分，分析语干，将非结构化的语言转换成一个结构化的语义框架，从而可以通过分析、数据库查找等技术进一步实现语义理解和知识挖掘。采用上述技术可以构建出知识图谱，将大量背景知识与当前的输入连接起来，可用于搜索的联想和商品推荐等应用。另外，也可以实现评论观点抽取、情感分析、阅读理解等。 另一个大类是翻译，往往指任意语言对的篇章级别翻译。 5. 视频和语言结合的应用 主要的应用是视频理解，即将一段视频转换为描述文字或结构化信息。 上述应用都属于自然信息的处理，另外一类是创作，例如音乐生成，写诗、创作文章等最近几年也取得了突破性进展。   • • • 这些领域在深度学习到来之前都有各自的发展，但都在深度学习中获得了发展提高。例如语音识别从原先GMM-HMM的基础框架向DNN-HMM框架转型，再到DNN-CTC转型。又例如在NLP中，规则和统计模型构建的词性-句法-语义多步方法被引入了CNN，LSTM的直接的跨步骤特征提取方法所革新。下面介绍一下使得这些领域取得革命性突破的深度学习框架。  1. 时序分析系列神经网络 由于语音和文本具有很强的时序特性。而由于卷积网的平移不变性使得对时序序列分析能力有一定程度的欠缺，因此需要带有时间能力的神经网络补充它的不足。循环神经网络（RNN）及其衍化形式长短时记忆网络（LSTM）和门控循环单元（GRU）是典型代表，广泛应用于大量语音文本分析领域。由于这些网络只阐明了当前状态和历史的关系，而有时，当前状态会同时依赖于历史和未来，因此双向时序网络，例如BRNN，BLSTM得到了较多的应用，这种类型的网络可以认为是正向时间的时序网络和一个反向时间的时序网络合在了一起。值得注意的是，这些网络结构可以铺多层，即一层的输出是另一层的输入，组成更强大的时序网络。值得注意的是，由于RNN具有梯度弥散问题，因此实际应用时，几乎还是使用的其衍化形式，例如LSTM和GRU。GRU和LSTM相比性能上难分伯仲，可根据具体应用选择，而在资源开销上，GRU较LSTM更具优势，RNN占用资源最少，但实用性较低。   2. 深度学习技术 本用于视频领域的卷积神经网络（CNN）在语音／文本处理上获得了广泛应用。例如对音频识别起到了一定程度的改善作用。此时输入变为二维的语谱图（时间－频率图）。对于句子理解，有时会直接把一句话按照每单词作为一行（单词的高维向量表示），组成“图像”，通过CNN进行降维和特征提取。分析结果可以用于分类和对话。多层感知机网络（MLP）在这些网络中发挥着强大的胶水连接作用，以及Word embedding的作用等。对于阅读理解和多轮对话等应用，关注（Attention）机制可以从上下文或者历史知识中提取出对当前任务有用的因素，简单的关注可以认为是一种动态权重下的加权求和操作。   3. 非神经网络的分析技术 值得注意的是，文本/语音处理区别于视频处理的一大特色是，这些神经网络技术目前大部分情况还没法构成端到端的应用，因此这些先进的神经网络往往需要和传统的非神经网络技术混搭使用。例如，对于语音识别技术，往往可以分为语音模型、文本模型、搜索三部分。语音模型中的前期处理已经逐渐被CNN、LSTM等神经网络技术所替代，而后部为了解决语音的速率问题，还需要采用基于状态切换的HMM模型，或者基于搜索的音素合并算法CTC（BeamSearch）。这些算法可以有效弥补当前神经网络没有变长的时序(t)表达能力的问题。另外，对于文本模型而言，N-gram仍然具有很高的应用性，虽然也有基于RNN替代方法，但未必会产生完全替代。 • • • 综合以上情况，对于语音文本类应用而言，需求度分析可以整理如下表。 在神经网络实现细节变化方面，我们需要进一步考察。在文本和语音领域，激活函数往往不是ReLU，而是一些非线性函数。因此对数据精度的要求比较高，目前成熟的仍然是浮点。近几年有关于8b的LSTM结构研究，甚至部分量化为更低精度的研究，但并不能保证通用性。因此建议采用浮点数据类型。另外，残差连接对于语音识别仍具有意义。 在预处理方面，语音和文字都有各自较为成熟的预处理方案。对于语音，主要需要构建语谱图。有些时候也可以直接用音频作为输入。而对于文本，主要需要先将词分开并表示为向量。主要算法如表所示。 语音在经过神经网络进行处理后，后期处理的主要技术包括HMM或CTC，以及N-gram。 根据四元拆分方法，一个数字IP核分为控制，计算，存储，互联四个大的部分，下表对每一部分的需求进行简述。 在接口方面，主要考虑是嵌入式平台还是云平台，其中嵌入式平台往往直接通过麦克风获取语音，而对于远场而言，阵列麦克风模块是最好的选择。对于云而言，数据主要通过PCI-E或者网口传输。由于这些神经网络结构需要更大的内存开销，因此DDR/HBM需求较高。 对系统的整体理解有助于我们设计它的支撑平台，因此我们以上文的砖块为基础，重点介绍目前处于领先地位的若干神经网络体系结构。例中以知名公司推出的语音/文本处理神经网络算法为主要关注点。大多具有公开论文作为参考。可以得出的结论是，商业级别的LSTM(GRU)的单层尺寸一般为1000-2000左右，会放置3~7层。卷积网及其各类变种都有应用。CTC是目前较为流行的语音识别后端处理模式。Attention机制会有长远发展，值得关注。详细情况如表所示。 注：BLSTM, BGRU在有些文献中也写作biLSTM,biGRU。 • • • 今音频/文本处理正在深度学习的轨道上快速发展。例如谷歌的LAS算法在本文写作几天前公布，大大简化了语音识别框架，百度Deep Voice 3在大半个月前公布，训练速度提升了10倍。各大公司相继推出了基于深度学习算法的语音识别网络[8]。在NLP方面[7]，深度学习也正在进行着前所未有的变革。值得注意的是这些变革是刚性的，因为它确实刷新了各项性能指标，把相关领域研究推向了新的高度。另外，很多任务会加入很多个性化算法元素，而不是单纯的神经网络结构。飞速的算法进展以及个性化传统算法的引入给芯片设计的灵活度带来了考验。   综上，本文分析了当前语音文本深度学习算法的主要应用场景，阐述了支持这些应用场景的芯片应支持何种深度学习算法，最后分析了若干知名技术方案中这些网络的使用情况。结论为（1）大量深度学习算法和神经网络结构是这些领域的最优性能的必需算法，因此这些应用具有很强的神经网络加速需求。（2）仿存量远大于CNN，可以理解为存储访问带宽主导的设计。其特点可能会导致片上内存的增大和近内存计算架构得到长足发展。（3）语音/文本类网络结构复杂，传统处理算法和神经网络变种需要高效支持。我们期待未来会有更多的芯片公司关注语音/文本类深度学习应用的加速，开发出令人激动的新品，让这些应用能够走进每个人的日常生活。 由于此文涉及面较广，有可能不够准确，在此仅供抛砖引玉之用，各位如见到有错误和不足之处请务必留言指出。 Reference [1]. Google LAS (https://arxiv.org/pdf/1712.01769.pdf) [2]. Deep Speech 2 (https://arxiv.org/abs/1512.02595, http://blog.csdn.net/xmdxcsj/article/details/54848838) [3]. DFCNN (http://blog.csdn.net/real_myth/article/details/52274005) [4]. SLING (https://arxiv.org/abs/1710.07032) [5]. WaveNet (https://deepmind.com/blog/wavenet-generative-model-raw-audio/, https://arxiv.org/pdf/1609.03499.pdf) [6]. Deep Voice 3 (https://arxiv.org/abs/1710.07654, Deep Voice 2 https://arxiv.org/abs/1705.08947, http://news.zol.com.cn/631/6315926.html)  Deep Voice 1 https://arxiv.org/abs/1702.07825 [7]. NLP进展 (http://www.sohu.com/a/210427622_465975) [8]. 语音识别网络对比 (http://www.360doc.com/content/17/0729/00/41022878_675010230.shtml) - END - 吴臻志博士，清华大学类脑计算研究中心助理研究员。专长神经网络芯片设计，众核芯片设计，神经网络高效实现等。邮件 wuzhenzhi@gmail.com。同行及朋友可加微信zhenzhi-wu联系。 "
356,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736406&idx=4&sn=5bbfe8b4a76517a15b7eac02ac3481f4&chksm=871ac368b06d4a7e4d75f5cbe9c46026d63fd326ae4347f7e4d0f9a8c5abf4dc55b6a7d41a3f&scene=27,前沿 | DeepMind于Nature子刊发文提出非对称博弈的降维方法,"近日，DeepMind 在 Scientific Report 上发表论文《Symmetric Decomposition of Asymmetric Games》，表明一个非对称博弈可以分解为多个对称博弈，从而将博弈降维，并且非对称博弈和对称变体的纳什均衡也有非常简单的对应关系。 随着 AI 系统在现实世界中扮演的角色越来越重要，理解不同系统之间如何交互变得非常关键。 DeepMind 最新在 Scientific Report 上发表了一篇论文《Symmetric Decomposition of Asymmetric Games》，使用了博弈论的一个分支来解决这个问题。具体来说，DeepMind 研究者检验了两个智能系统在非对称博弈游戏（asymmetric game，包括 Leduc 扑克和多种棋牌游戏）的特定类型情景下的行为和反应。非对称博弈还可以自然地对现实世界场景建模，例如自动拍卖，其中买家和卖家以不同的动机行动。研究结果给出了对这些场景的新洞察，揭示了一种非常简单的分析方法。虽然 DeepMind 的兴趣主要在于如何将该理论应用到多个 AI 系统的交互中，但研究者相信这些结果还可以应用到经济学、进化生物学和经验博弈论（empirical game theory）等。 该方法被证明在数学上是很简单的，允许对非对称博弈进行快速、直接的分析。 博弈论是一种数学理论，用于分析竞争环境中决策者使用的策略，可以在多种情景中应用于人类、动物和计算机。博弈论在研究多智能体环境中很常用，多智能体环境中的系统数量超过一个，例如多个家庭机器人协作打扫房子。传统上通常使用简单的对称博弈游戏来分析多智能体系统的演化动态，例如「囚徒困境」，其中每个玩家都有相同的可选动作。虽然这些博弈游戏可以为多智能体系统的工作方式提供有用的洞察，并告诉我们如何让所有玩家取得想要的结果——即纳什均衡，但是它们无法对所有情景建模。 DeepMind 的新技术允许快速、简单地在更加复杂的非对称博弈中发现可用于实现纳什均衡的策略（非对称博弈游戏中每个玩家都有不同的策略、目标和奖励）。可以通过「性别大战」（一个博弈论研究中常用的协调博弈游戏）的例子展示这些博弈和用于分析它们的新技术。 在性别大战中，两个玩家需要在一个晚上协商去看歌剧还是看电影。两个玩家中，一个略微更喜欢歌剧，另一个更喜欢电影。这个博弈是非对称的，因为虽然两个玩家的可选策略是相同的，但是基于玩家偏好，选项所对应的奖励是不同的。为了维持他们的友谊，或者说均衡，两个玩家需要选择相同的选项，如果选择了不同的选项则收益为零。 该游戏有三个均衡：(i) 两个玩家都决定去歌剧院，(ii) 两人都决定去看电影，(iii) 最后的混合选择，其中每个玩家选择自己偏好的活动的比例为五分之三。最后一个「不稳定」选择可以用 DeepMind 的方法，通过将非对称博弈简化或分解成对称博弈而被迅速发现。这些分解出的对称博弈游戏本质上把每个玩家的奖励表作为一个独立的对称双人博弈，其均衡点与原来的非对称博弈一致。 下图为两个分解出的简单对称博弈游戏绘制了纳什均衡，我们可以快速发现非对称博弈 (a) 中的最优策略。也可以反过来操作，使用非对称博弈发现对称博弈中的均衡。 这一方法也适用于其他游戏，包括论文中详述的 Leduc 扑克。在所有这些情景中，该方法被证明在数学上是简单的，可以快速而直接地分析非对称博弈，我们希望这也有助于我们理解不同的动态系统，包括多智能体环境。 论文： Symmetric Decomposition of Asymmetric Games 论文链接：https://www.nature.com/articles/s41598-018-19194-4 我们提出了关于双人非对称博弈游戏的新理论洞察，允许优雅地将非对称博弈游戏分解为两个单人对称博弈游戏。具体来说，我们展示了如何通过预见和研究构成非对称博弈的收益表（A 和 B），将非对称双矩阵博弈 (A,B) 分解为它的对称变体，即两个独立的、单人的对称博弈。我们揭示了双人非对称博弈与其单人对称博弈之间形式上的多种令人惊讶的关系，促进了对原始非对称博弈进行分析的便利性（因为分解可以降维）。主要的研究成果揭示了，如果 (x,y) 是非对称博弈 (A,B) 的纳什均衡，则 y 是由收益表 A 决定的对称博弈游戏的纳什均衡，x 是由收益表 B 决定的对称博弈游戏的纳什均衡，反之亦然。并且两个单人对称博弈的纳什均衡的组合构成了非对称博弈的纳什均衡。通过在多个标准实例中检验更简单的对称博弈游戏的演化动态，我们展示了这些形式关系如何帮助发现和分析非对称博弈的纳什结构。  原文链接：https://deepmind.com/blog/game-theory-insights-asymmetric-multi-agent-games/ "
357,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736406&idx=1&sn=db8da5ddc9a9cf86e804d29eb817f078&chksm=871ac368b06d4a7ec4854a44cc758a2d9610a350569be43209d4f99868e7341e7a551155079e&scene=27,PyTorch一周年战绩总结：是否比TensorFlow来势凶猛？,今天 PyTorch 刚好一周年。自发布以来，由于调试、编译等多方面的优势，它成为 2017 年热度极高的框架之一。本文内容介绍了开源一周年以来，PyTorch 取得的成绩。在一些指标上，PyTorch 也与 TensorFlow 做了同期对比。PyTorch 是不是 2017 年的明星框架？ 截止到今天，PyTorch 已公开发行一周年。一年以来，我们致力于打造一个灵活的深度学习研究平台。一年以来，PyTorch 社区中的用户不断做出贡献和优化，在此深表感谢。 通过此文，我们打算对 PyTorch 一年的发展历程做一个总结：PyTorch 的进展、新闻以及社区亮点。 社区 我们很幸运，PyTorch 团队有一批强大、饱含热情的研究者和工程师，核心团队的工程师和研究者来自不同的国家、公司和大学，没有他们的付出就不会有今天的 PyTorch。 研究论文、工具包、GitHub PyTorch 才发行几天，社区用户已经开始借助 PyTorch 实现其最喜爱的研究论文，并把代码公布在 GitHub 上。开源代码对当今的研究者来说是一个主要而核心的工具。 人们一起创建了 torchtext、torchvision 和 torchaudio，以便利化平民化不同领域的研究。 首个 PyTorch 社区工具包（被命名为 Block）来自 Brandon Amo，有助于更轻松地处理块矩阵（block matrix）。来自 CMU 的 Locus 实验室后来继续公布 PyTorch 工具包及其大部分研究的实现。首个研究论文代码来自 Sergey Zagoruyko，论文名称为《Paying more attention to attention》。 来自 U.C.Berkeley 的 Jun-Yan Zhu、Taesung Park、Phillip Isola、Alyosha Efros 及团队发布了非常流行的 Cycle-GAN 和 pix2pix，用于图像转换。 参见论文： HarvardNLP 和 Systran 的研究者开始使用 PyTorch 开发和提升 OpenNMT，它最初开始于 Facebook Adam Lerer 的 [Lua]Torch 代码最初的再实现。 参见： 来自 Twitter 的 MagicPony 团队贡献了其超分辨率研究示例的 PyTorch 实现。 Salesforce 发布了若干个工具包，包括其亮点成果 PyTorch-QRNN，这是一种新型 RNN，相比于 CuDNN 优化的标准 LSTM 可提速 2 到 17 倍。James Bradbury 及其团队是 PyTorch 社区中最活跃和最有吸引力的团队之一。 来自 Uber、Northeaster、Stanford 的研究者围绕着其工具包 Pyro 和 ProbTorch，形成了一个活跃的概率编程社区。他们正在积极开发 torch.distributions 核心工具包。该社区非常活跃，快速发展，我们联合 Fritz Obermeyer、Noah Goodman、Jan-Willem van de Meent、Brooks Paige、Dustin Tran 及其他 22 名参会者在 NIPS 2017 上举办了首次 PyTorch 概率编程会议，共同探讨如何使世界贝叶斯化。 参见： 英伟达研究者发布了三个高质量 repo，实现了 pix2pix-HD、Sentiment Neuron 和 FlowNet2。对 PyTorch 中不同数据并行模型的扩展性分析对整个社区都很有益。 艾伦人工智能研究所发布 AllenNLP，包括多个 NLP 先进模型：标准 NLP 任务的参考实现和易用 web demo。 六月份，我们还首次取得了 Kaggle 竞赛冠军（团队 grt123）。他们获得了 2017 数据科学杯（关于肺癌检测）的冠军，后来公开了其 PyTorch 实现。 在可视化方面，Tzu-Wei Huang 实现了 TensorBoard-PyTorch 插件，Facebook AI Research 发布了与 PyTorch 兼容的 visdom 可视化包。 最后，Facebook AI Research 发布了多个项目，如 ParlAI、fairseq-py、VoiceLoop 和 FaderNetworks，在多个领域中实现了先进的模型和接口数据集。由于空间有限，这里就不将优秀项目一一列出，详细列表可参阅：https://github.com/soumith?tab=stars。 我们还要感谢那些在论坛中积极帮助别人的用户。你们提供了无比珍贵的服务，非常感谢！ 指标 从数字上来看： 在 Github 上有 87769 行代码引入 Torch。 在 Github 上有 3983 个 repository 在名字或者描述中提到了 PyTorch。 PyTorch binary 下载量超过 50 万，具体数字是 651916。 在论坛上，有 5400 名用户发表了 21500 条讨论，涉及 5200 个主题。 自发布以来，在 Reddit 上的/r/machinelearning 主题中有 131 条讨论提到了 PyTorch。同期，TensorFlow 被提及的次数为 255。 参见： PyTorch 和 TensorFlow 到底哪个更好？看看一线开发者怎么说 TensorFlow 开源一周年：这可能是一份最完整的盘点 研究指标 PyTorch 是一个专注于研究的框架。所以与衡量它的指标包括 PyTorch 在机器学习研究论文中的使用。 在 ICLR 2018 学术会议提交的论文中，有 87 篇提到了 PyTorch，相比之下 TensorFlow 228 篇，Keras 42 篇，Theano 和 Matlab 是 32 篇。 按照月度来看，arXiv 论文提到 PyTorch 框架的有 72 篇，TensorFlow 是 273 篇，Keras 100 篇，Caffe 94 篇，Theano 53 篇。 课程、教程与书籍 我们在发布 PyTorch 的时候，已经准备了很好的 API 文档，但教程有限，只有几个 ipython notebook，虽然有用但还不够。 Sasank Chilamkurthy 承担了改进教程的任务，教程详见：http://pytorch.org/tutorials/。 Sean Robertson 和 Justin Johnson 编写了 NLP 领域的全新教程，还有通过示例学习的教程。Yunjey Choi 写了用 30 行或者更少的代码部署大多数模型的教程。每个新教程都帮助用户用不同的学习方法更快地找到适合自己的学习路径。 Goku Mohandas 和 Delip Rao 把正在写的书中的代码做了改变，使用了 PyTorch。 我们看到，一些大学的机器学习课程是使用 PyTorch 作为主要工具讲授的，例如哈佛 CS 287。为了更进一步方便大众学习，我们还看到三个在线课程使用 PyTorch 讲授。 Fast.ai 的「Deep Learning for Coders」是个流行的在线课程。9 月份，Jeremy 和 Rachel 宣布下一个 fast.ai 的课程将几乎全部基于 PyTorch。 Ritchie Ng，在清华、新加坡国立大学都学习过的研究者，推出了名为「Practical Deep Learning with PyTorch」的 Udemy 课程。 来自香港科技大学的 Sung Kim 在 Yotube 上推出了面向普通观众的在线课程「PyTorch Zero to All」。 参见： 去年 PyTorch 实现了多个功能，包括 board 上的性能、修复大量 bug 等。去年完成的任务清单详见：https://github.com/pytorch/pytorch/releases。下面是其中的几个亮点： 高阶梯度 随着多篇关于实现梯度罚项的论文的发表，以及二阶梯度法的不断研究发展，高阶梯度成为必需的热门功能。去年 8 月，我们实现了一个通用接口，可使用 n 阶导数，加快支持高阶梯度函数的收敛，截至写作本文时，几乎所有 ops 都支持此界面。 分布式 PyTorch 去年 8 月，我们发布了一个小型分布式包，该包使用非常流行的 MPI 集合（MPI-collective）方法。它有多个后端，如 TCP、MPI、Gloo 和 NCCL2，以支持多种 CPU/GPU 集合操作和用例，这个包整合了 Infiniband 和 RoCE 等分布式技术。分布很难，我们在初始迭代时也有一些 bug。在后续版本中，我们作出了一些改进，使这个包更加稳定，性能也更强。 更接近 NumPy 用户最大的一个需求是他们熟悉的 NumPy 功能。Broadcasting 和 Advanced Indexing 等功能方便、简洁，节约用户的时间。我们实现了这些功能，开始使我们的 API 更接近 NumPy。随着时间的进展，我们希望在合适的地方越来越接近 NumPy 的 API。 性能 性能是一场仍在进行中的战斗，尤其对于想要最大化灵活性的动态框架 PyTorch 而言。去年，从核心 Tensor 库到神经网络算子，我们改善了 PyTorch 在 board 上的性能，能在 board 上更快的编写微优化。 我们添加了专门的 AVX 和 AVX2 内部函数，用于 Tensor 运算； 写更快的 GPU kernel，用于常用的工作负载，如级联和 Softmax； 为多个神经网络算子重写代码，如 nn.Embedding 和组卷积。 PyTorch 在 board 上的开销降低 10x 由于 PyTorch 是动态图框架，我们在训练循环的每次迭代时都要创建一个新图。因此，框架开销必须很低，或者工作负载必须足够大来隐藏框架开销。去年 8 月，DyNet 的作者（Graham Neubig 及其团队）展示了 DyNet 在一些小型 NLP 模型上的速度快于 PyTorch。这是很有意思的一个挑战，我们开始重写 PyTorch 内部构件，将框架开销从 10 微妙／算子降低到 1 微妙。 ATen 重新设计 PyTorch 内部构件的同时，我们也构建了 ATen C++11 库，该库现在主导 PyTorch 所有后端。ATen 具备一个类似 PyTorch Python API 的 API，使之成为便于 Tensor 计算的 C++库。ATen 可由 PyTorch 独立构建和使用。 输出模型用于生产：支持 ONNX 和 JIT 编译器 我们收到的一个普遍请求是将 PyTorch 模型输出到另一个框架。用户使用 PyTorch 进行快速研究，模型完成后，他们想将模型搭载到更大的项目中，而该项目只要求使用 C++。 因此我们构建了 tracer，可将 PyTorch 模型输出为中间表示。用户可使用后续的 tracer 更高效地运行当前的 PyTorch 模型，或将其转换成 ONNX 格式以输出至 Caffe2、MXNet、TensorFlow 等其他框架，或直接搭载至硬件加速库，如 CoreML 或 TensorRT。今年，我们将更多地利用 JIT 编译器提升性能。 原文链接： 
358,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736364&idx=2&sn=2d045c0af345b92ac890996f2557acf5&chksm=871ac292b06d4b84f10337e0598dafef952b7f628781f80815e304757ac65e36d7a344687e12&scene=27,业界 | 中国《人工智能标准化白皮书2018》发布完整版（附下载）,：专知内容组 【导读】1月18日，召开国家人工智能标准化总体组、专家咨询组成立大会，在会上，国家标准化管理委员会宣布成立国家人工智能标准化总体组、专家咨询组，负责全面统筹规划和协调管理我国人工智能标准化工作。会议贯彻了党的十九大会议关于推动人工智能和实体经济深度融合精神，会议还解读了《人工智能标准化助力产业发展》、《促进新一代人工智能产业发展三年行动计划（2018-2020）》，全面推进人工智能标准化工作，促进人工智能产业发展。 ▌ 编写单位（排名不分先后）    中国电子技术标准化研究院  中国科学院自动化研究所 北京理工大学  清华大学  北京大学  中国人民大学  北京航空航天大学  科大讯飞股份有限公司  华为技术有限公司  国际商业机器（中国）有限公司 阿里云计算有限公司  中国科学院计算技术研究所  中国电信集团公司  腾讯互联网加（深圳）有限公司  阿里巴巴网络技术有限公司  上海计算机软件技术开发中心  上海智臻智能网络科技股份有限公司  北京爱奇艺科技有限公司  北京有生志广科技有限公司  极限元（北京）智能科技股份有限公司  北京字节跳动科技有限公司（今日头条）  北京商汤科技开发有限公司  浙江蚂蚁小微金融服务集团有限公司  百度网络技术有限公司  英特尔（中国）有限公司  松下电器（中国）有限公司  重庆凯泽科技股份有限公司  海尔工业智能研究院有限公司  重庆中科云从科技有限公司  北京格灵深瞳信息技术有限公司 ▌ 目录   ▌ 1 前言   1.1 研究背景 人工智能概念诞生于 1956 年，在半个多世纪的发展历程中，由于受到智能 算法、计算速度、存储水平等多方面因素的影响，人工智能技术和应用发展经历 了多次高潮和低谷。2006 年以来，以深度学习为代表的机器学习算法在机器视 觉和语音识别等领域取得了极大的成功，识别准确性大幅提升，使人工智能再次 受到学术界和产业界的广泛关注。云计算、大数据等技术在提升运算速度，降低 计算成本的同时，也为人工智能发展提供了丰富的数据资源，协助训练出更加智 能化的算法模型。人工智能的发展模式也从过去追求“用计算机模拟人工智能”， 逐步转向以机器与人结合而成的增强型混合智能系统，用机器、人、网络结合成 新的群智系统，以及用机器、人、网络和物结合成的更加复杂的智能系统。  作为新一轮产业变革的核心驱动力，人工智能在催生新技术、新产品的同时， 对传统行业也具备较强的赋能作用，能够引发经济结构的重大变革，实现社会生 产力的整体跃升。人工智能将人从枯燥的劳动中解放出来，越来越多的简单性、 重复性、危险性任务由人工智能系统完成，在减少人力投入，提高工作效率的同 时，还能够比人类做得更快、更准确；人工智能还可以在教育、医疗、养老、环 境保护、城市运行、司法服务等领域得到广泛应用，能够极大提高公共服务精准 化水平，全面提升人民生活品质；同时，人工智能可帮助人类准确感知、预测、 预警基础设施和社会安全运行的重大态势，及时把握群体认知及心理变化，主动 作出决策反应，显著提高社会治理能力和水平，同时保障公共安全。  人工智能作为一项引领未来的战略技术，世界发达国家纷纷在新一轮国际竞 争中争取掌握主导权，围绕人工智能出台规划和政策，对人工智能核心技术、顶 尖人才、标准规范等进行部署，加快促进人工智能技术和产业发展。主要科技企 业不断加大资金和人力投入，抢占人工智能发展制高点。2017 年，我国出台了 《新一代人工智能发展规划》（国发〔2017〕35 号）、《促进新一代人工智能产业 发展三年行动计划（2018-2020 年）》（工信部科〔2017〕315 号）等政策文件， 推动人工智能技术研发和产业化发展。目前，国内人工智能发展已具备一定的技 术和产业基础，在芯片、数据、平台、应用等领域集聚了一批人工智能企业，在 部分方向取得阶段性成果并向市场化发展。例如，人工智能在金融、安防、客服 等行业领域已实现应用，在特定任务中语义识别、语音识别、人脸识别、图像识 别技术的精度和效率已远超人工。  标准化工作对人工智能及其产业发展具有基础性、支撑性、引领性的作用， 既是推动产业创新发展的关键抓手，也是产业竞争的制高点。当前，在我国人工 智能相关产品和服务不断丰富的同时，也出现了标准化程度不足的问题。人工智 能涉及众多领域，虽然某些领域已具备一定的标准化基础，但是这些分散的标准 化工作并不足以完全支撑整个人工智能领域。另一方面，人工智能属于新兴领域， 发展方兴未艾，从世界范围来看，标准化工作仍在起步过程中，尚未形成完善的 标准体系，我国基本与国外处于同一起跑线，存在快速突破的机会窗口。只要瞄 准机会，快速布局，完全有可能抢占标准创新的制高点，反之，则有可能丧失良 机。因此，迫切需要把握机遇，加快对人工智能技术及产业发展的研究，系统梳 理、加快研制人工智能各领域的标准体系，明确标准之间的依存性与制约关系， 建立统一完善的标准体系，以标准的手段促进我国人工智能技术、产业蓬勃发展。 1.2 研究目标及意义 本白皮书前期在国标委工业二部和工信部科技司的指导下，通过梳理人工智 能技术、应用和产业演进情况，分析人工智能的技术热点、行业动态和未来趋势， 从支撑人工智能产业整体发展的角度出发，研究制定了能够适应和引导人工智能 产业发展的标准体系，进而提出近期急需研制的基础和关键标准项目。  本白皮书并不预期成为人工智能领域的全面技术和产业综述，不求面面俱到， 仅针对目前人工智能领域涵盖的技术热点和产业情况进行分析，研究提出人工智 能标准体系。人工智能标准化工作尚处于起步阶段，本白皮书只作为人工智能领 域技术、产业和标准化之间初始的连接纽带，并将在今后不断根据技术、产业和 标准化的发展需求进行修订。本白皮书不过多地给出人工智能领域观点性的陈述， 力求以较为浅显易懂的语言和方式进行阐述。  本白皮书的意义在于与业界分享人工智能领域的研究成果和实践经验，呼吁 社会各界共同加强人工智能领域的技术研究、产业投入、标准建设与服务应用， 共同推动人工智能及其产业发展。  ▌ 2 人工智能概述   2.1 人工智能的历史及概念 人工智能始于 20 世纪 50 年代，至今大致分为三个发展阶段：第一阶段（20 世纪 50 年代——80 年代）。这一阶段人工智能刚诞生，基于抽象数学推理的可 编程数字计算机已经出现，符号主义（Symbolism）快速发展，但由于很多事物 不能形式化表达，建立的模型存在一定的局限性。此外，随着计算任务的复杂性 不断加大，人工智能发展一度遇到瓶颈；第二阶段（20 世纪 80 年代——90 年代 末）。在这一阶段，专家系统得到快速发展，数学模型有重大突破，但由于专家 系统在知识获取、推理能力等方面的不足，以及开发成本高等原因，人工智能的 发展又一次进入低谷期；第三阶段（21 世纪初——至今）。随着大数据的积聚、 理论算法的革新、计算能力的提升，人工智能在很多应用领域取得了突破性进展， 迎来了又一个繁荣时期。人工智能具体的发展历程如图 1 所示。 长期以来，制造具有智能的机器一直是人类的重大梦想。早在 1950 年，Alan Turing 在《计算机器与智能》中就阐述了对人工智能的思考。他提出的图灵测试 是机器智能的重要测量手段，后来还衍生出了视觉图灵测试等测量方法。1956年，“人工智能”这个词首次出现在达特茅斯会议上，标志着其作为一个研究领 域的正式诞生。六十年来，人工智能发展潮起潮落的同时，基本思想可大致划分 为四个流派：符号主义（Symbolism）、连接主义（Connectionism）、行为主义 （Behaviourism）和统计主义（Statisticsism）（注：由于篇幅原因，本白皮书不 对四个流派进行详细阐述）。这四个流派从不同侧面抓住了智能的部分特征，在 “制造”人工智能方面都取得了里程碑式的成就。 1959 年，Arthur Samuel 提出了机器学习，机器学习将传统的制造智能演化 为通过学习能力来获取智能，推动人工智能进入了第一次繁荣期。20 世纪 70 年 代末期专家系统的出现，实现了人工智能从理论研究走向实际应用，从一般思维 规律探索走向专门知识应用的重大突破，将人工智能的研究推向了新高潮。然而， 机器学习的模型仍然是“人工”的，也有很大的局限性。随着专家系统应用的不 断深入，专家系统自身存在的知识获取难、知识领域窄、推理能力弱、实用性差 等问题逐步暴露。从 1976 年开始，人工智能的研究进入长达 6 年的萧瑟期。  在 80 年代中期，随着美国、日本立项支持人工智能研究，以及以知识工程 为主导的机器学习方法的发展，出现了具有更强可视化效果的决策树模型和突破 早期感知机局限的多层人工神经网络，由此带来了人工智能的又一次繁荣期。然 而，当时的计算机难以模拟复杂度高及规模大的神经网络，仍有一定的局限性。 1987 年由于 LISP 机市场崩塌，美国取消了人工智能预算，日本第五代计算机项 目失败并退出市场，专家系统进展缓慢，人工智能又进入了萧瑟期。  1997 年，IBM 深蓝（Deep Blue）战胜国际象棋世界冠军 Garry Kasparov。 这是一次具有里程碑意义的成功，它代表了基于规则的人工智能的胜利。2006 年，在 Hinton 和他的学生的推动下，深度学习开始备受关注，为后来人工智能 的发展带来了重大影响。从 2010 年开始，人工智能进入爆发式的发展阶段，其 最主要的驱动力是大数据时代的到来，运算能力及机器学习算法得到提高。人工 智能快速发展，产业界也开始不断涌现出新的研发成果：2011 年，IBM Waston 在综艺节目《危险边缘》中战胜了最高奖金得主和连胜纪录保持者；2012 年， 谷歌大脑通过模仿人类大脑在没有人类指导的情况下，利用非监督深度学习方法 从大量视频中成功学习到识别出一只猫的能力；2014 年，微软公司推出了一款 实时口译系统，可以模仿说话者的声音并保留其口音；2014 年，微软公司发布全球第一款个人智能助理微软小娜；2014 年，亚马逊发布至今为止最成功的智 能音箱产品 Echo 和个人助手 Alexa；2016 年，谷歌 AlphaGo 机器人在围棋比赛 中击败了世界冠军李世石；2017 年，苹果公司在原来个人助理 Siri 的基础上推 出了智能私人助理 Siri 和智能音响 HomePod。  目前，世界各国都开始重视人工智能的发展。2017 年 6 月 29 日，首届世界 智能大会在天津召开。中国工程院院士潘云鹤在大会主论坛作了题为“中国新一 代人工智能”的主题演讲，报告中概括了世界各国在人工智能研究方面的战略： 2016 年 5 月，美国白宫发表了《为人工智能的未来做好准备》；英国 2016 年 12 月发布《人工智能：未来决策制定的机遇和影响》；法国在 2017 年 4 月制定了《国 家人工智能战略》；德国在2017年5月颁布全国第一部自动驾驶的法律；在中国， 据不完全统计，2017 年运营的人工智能公司接近 400 家，行业巨头百度、腾讯、 阿里巴巴等都不断在人工智能领域发力。从数量、投资等角度来看，自然语言处 理、机器人、计算机视觉成为了人工智能最为热门的三个产业方向。  人工智能作为一门前沿交叉学科，其定义一直存有不同的观点：《 中将已有的一些人工智能定义分为四类：像人一样思考的系 统、像人一样行动的系统、理性地思考的系统、理性地行动的系统。 上定义“人工智能就是机器展现出的智能”，即只要是某种机器，具有某种或某些 “智能”的特征或表现，都应该算作“人工智能”。 则限定人工智 能是数字计算机或者数字计算机控制的机器人在执行智能生物体才有的一些任 务上的能力。 定义人工智能是“研究、开发用于模拟、延伸和扩展人的 智能的理论、方法、技术及应用系统的一门新的技术科学”，将其视为计算机科 学的一个分支，指出其研究包括机器人、语言识别、图像识别、自然语言处理和 专家系统等。  本白皮书认为， 人工智能的定义对人工智能学科的基本思想和内容作出了解释，即围绕智能活动而构造的人工系统。人工智能是知识的工程，是机器模仿人类利用知识完成 一定行为的过程。根据人工智能是否能真正实现推理、思考和解决问题，可以将 人工智能分为弱人工智能和强人工智能。  弱人工智能 是指不能真正实现推理和解决问题的智能机器，这些机器表面看 像是智能的，但是并不真正拥有智能，也不会有自主意识。迄今为止的人工智能 系统都还是实现特定功能的专用智能，而不是像人类智能那样能够不断适应复杂 的新环境并不断涌现出新的功能，因此都还是弱人工智能。目前的主流研究仍然 集中于弱人工智能，并取得了显著进步，如语音识别、图像处理和物体分割、机 器翻译等方面取得了重大突破，甚至可以接近或超越人类水平。  强人工智能 是指真正能思维的智能机器，并且认为这样的机器是有知觉的和 自我意识的，这类机器可分为类人（机器的思考和推理类似人的思维）与非类人 （机器产生了和人完全不一样的知觉和意识，使用和人完全不一样的推理方式） 两大类。从一般意义来说，达到人类水平的、能够自适应地应对外界环境挑战的、 具有自我意识的人工智能称为“通用人工智能”、“强人工智能”或“类人智能”。 强人工智能不仅在哲学上存在巨大争论（涉及到思维与意识等根本问题的讨论）， 在技术上的研究也具有极大的挑战性。强人工智能当前鲜有进展，美国私营部门 的专家及国家科技委员会比较支持的观点是，至少在未来几十年内难以实现。  靠符号主义、连接主义、行为主义和统计主义这四个流派的经典路线就能设 计制造出强人工智能吗？其中一个主流看法是：即使有更高性能的计算平台和更 大规模的大数据助力，也还只是量变，不是质变，人类对自身智能的认识还处在 初级阶段，在人类真正理解智能机理之前，不可能制造出强人工智能。理解大脑 产生智能的机理是脑科学的终极性问题，绝大多数脑科学专家都认为这是一个数 百年乃至数千年甚至永远都解决不了的问题。  通向强人工智能还有一条“新”路线，这里称为“仿真主义”。这条新路线 通过制造先进的大脑探测工具从结构上解析大脑，再利用工程技术手段构造出模 仿大脑神经网络基元及结构的仿脑装置，最后通过环境刺激和交互训练仿真大脑 实现类人智能，简言之，“先结构，后功能”。虽然这项工程也十分困难，但都 是有可能在数十年内解决的工程技术问题，而不像“理解大脑”这个科学问题那 样遥不可及。  仿真主义可以说是符号主义、连接主义、行为主义和统计主义之后的第五个 流派，和前四个流派有着千丝万缕的联系，也是前四个流派通向强人工智能的关 键一环。经典计算机是数理逻辑的开关电路实现，采用冯•诺依曼体系结构，可以作为逻辑推理等专用智能的实现载体。但要靠经典计算机不可能实现强人工智能。要按仿真主义的路线“仿脑”，就必须设计制造全新的软硬件系统，这就是 “类脑计算机”，或者更准确地称为“仿脑机”。“仿脑机”是“仿真工程”的 标志性成果，也是“仿脑工程”通向强人工智能之路的重要里程碑。  2.2 人工智能的特征 （1） 。从根本上说， 人工智能系统必须以人为本，这些系统是人类设计出的机器，按照人类设定的程 序逻辑或软件算法通过人类发明的芯片等硬件载体来运行或工作，其本质体现为 计算，通过对数据的采集、加工、处理、分析和挖掘，形成有价值的信息流和知 识模型，来为人类提供延伸人类能力的服务，来实现对人类期望的一些“智能行 为”的模拟，在理想情况下必须体现服务人类的特点，而不应该伤害人类，特别 是不应该有目的性地做出伤害人类的行为。  （2） 。人工智能系统 应能借助传感器等器件产生对外界环境（包括人类）进行感知的能力，可以像人 一样通过听觉、视觉、嗅觉、触觉等接收来自环境的各种信息，对外界输入产生 文字、语音、表情、动作（控制执行机构）等必要的反应，甚至影响到环境或人 类。借助于按钮、键盘、鼠标、屏幕、手势、体态、表情、力反馈、虚拟现实/ 增强现实等方式，人与机器间可以产生交互与互动，使机器设备越来越“理解” 人类乃至与人类共同协作、优势互补。这样，人工智能系统能够帮助人类做人类 不擅长、不喜欢但机器能够完成的工作，而人类则适合于去做更需要创造性、洞 察力、想象力、灵活性、多变性乃至用心领悟或需要感情的一些工作。  （3）有适应特性，有学习能力，有演化迭代，有连接扩展 。人工智能系统 在理想情况下应具有一定的自适应特性和学习能力，即具有一定的随环境、数据 或任务变化而自适应调节参数或更新优化模型的能力；并且，能够在此基础上通 过与云、端、人、物越来越广泛深入数字化连接扩展，实现机器客体乃至人类主体的演化迭代，以使系统具有适应性、鲁棒性、灵活性、扩展性，来应对不断变化的现实环境，从而使人工智能系统在各行各业产生丰富的应用。  2.3 人工智能参考框架 目前，人工智能领域尚未形成完善的参考框架。因此，本章基于人工智能的 发展状况和应用特征，从人工智能信息流动的角度出发，提出一种人工智能参考 框架（如图 2 所示），力图搭建较为完整的人工智能主体框架，描述人工智能系 统总体工作流程，不受具体应用所限，适用于通用的人工智能领域需求。 人工智能参考框架提供了基于“角色—活动—功能”的层级分类体系，从 “智能信息链”（水平轴）和“IT 价值链”（垂直轴）两个维度阐述了人工智 能系统框架。“智能信息链”反映从智能信息感知、智能信息表示与形成、智能 推理、智能决策、智能执行与输出的一般过程。在这个过程中，智能信息是流动 的载体，经历了“数据—信息—知识—智慧”的凝练过程。“IT 价值链”从人 工智能的底层基础设施、信息（提供和处理技术实现）到系统的产业生态过程， 反映人工智能为信息技术产业带来的价值。此外，人工智能系统还有其它非常重 要的框架构件：安全、隐私、伦理和管理。人工智能系统主要由基础设施提供者、 信息提供者、信息处理者和系统协调者 4 个角色组成。 （1）基础设施提供者  基础设施提供者为人工智能系统提供计算能力支持，实现与外部世界的沟通， 并通过基础平台实现支撑。计算能力由智能芯片（CPU、GPU、ASIC、FPGA 等 硬件加速芯片以及其它智能芯片）等硬件系统开发商提供；与外部世界的沟通通 过新型传感器制造商提供；基础平台包括分布式计算框架提供商及网络提供商提 供平台保障和支持，即包括云存储和计算、互联互通网络等。  （2）信息提供者  信息提供者在人工智能领域是智能信息的来源。通过知识信息感知过程由数 据提供商提供智能感知信息，包括原始数据资源和数据集。原始数据资源的感知 涉及到图形、图像、语音、文本的识别，还涉及到传统设备的物联网数据，包括 已有系统的业务数据以及力、位移、液位、温度、湿度等感知数据。  （3）信息处理者  信息处理者是指人工智能领域中技术和服务提供商。信息处理者的主要活动 包括智能信息表示与形成、智能推理、智能决策及智能执行与输出。智能信息处 理者通常是算法工程师及技术服务提供商，通过计算框架、模型及通用技术，例 如一些深度学习框架和机器学习算法模型等功能进行支撑。  智能信息表示与形成是指为描述外围世界所作的一组约定，分阶段对智能信 息进行符号化和形式化的智能信息建模、抽取、预处理、训练数据等。  智能信息推理是指在计算机或智能系统中，模拟人类的智能推理方式，依据 推理控制策略，利用形式化的信息进行机器思维和求解问题的过程，典型的功能 是搜索与匹配。  智能信息决策是指智能信息经过推理后进行决策的过程，通常提供分类、排 序、预测等功能。  智能执行与输出作为智能信息输出的环节，是对输入作出的响应，输出整个 智能信息流动过程的结果，包括运动、显示、发声、交互、合成等功能。  （4）系统协调者  系统协调者提供人工智能系统必须满足的整体要求，包括政策、法律、资源和业务需求，以及为确保系统符合这些需求而进行的监控和审计活动。由于人工 智能是多学科交叉领域，需要系统协调者定义和整合所需的应用活动，使其在人 工智能领域的垂直系统中运行。系统协调者的功能之一是配置和管理人工智能参 考框架中的其他角色来执行一个或多个功能，并维持人工智能系统的运行。  （5）安全、隐私、伦理 安全、隐私、伦理覆盖了人工智能领域的其他 4 个主要角色，对每个角色都 有重要的影响作用。同时，安全、隐私、伦理处于管理角色的覆盖范围之内，与 全部角色和活动都建立了相关联系。在安全、隐私、伦理模块，需要通过不同的 技术手段和安全措施，构筑全方位、立体的安全防护体系，保护人工智能领域参 与者的安全和隐私。  （6）管理 管理角色承担系统管理活动，包括软件调配、资源管理等内容，管理的功能 是监视各种资源的运行状况，应对出现的性能或故障事件，使得各系统组件透明 且可观。  （7）智能产品及行业应用 智能产品及行业应用指人工智能系统的产品和应用，是对人工智能整体解决 方案的封装，将智能信息决策产品化、实现落地应用，其应用领域主要包括：智 能制造、智能交通、智能家居、智能医疗、智能安防等。 ▌ 3 人工智能发展现状及趋势    依据参考框架中所涉及到的人工智能相关技术，本节重点介绍近二十年来人 工智能领域关键技术的发展状况，包括机器学习、知识图谱、自然语言处理、计 算机视觉、人机交互、生物特征识别、虚拟现实/增强现实等关键技术。 3.1 人工智能关键技术    机器学习（Machine Learning）是一门涉及统计学、系统辨识、逼近理论、 神经网络、优化理论、计算机科学、脑科学等诸多领域的交叉学科，研究计算机 怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识 结构使之不断改善自身的性能，是人工智能技术的核心。基于数据的机器学习是 现代智能技术中的重要方法之一，研究从观测数据（样本）出发寻找规律，利用 这些规律对未来数据或无法观测的数据进行预测。根据学习模式、学习方法以及 算法的不同，机器学习存在不同的分类方法。  （1）根据学习模式将机器学习分类为监督学习、无监督学习和强化学习等。  监督学习  监督学习是利用已标记的有限训练数据集，通过某种学习策略/方法建立一 个模型，实现对新数据/实例的标记（分类）/映射，最典型的监督学习算法包括 回归和分类。监督学习要求训练样本的分类标签已知，分类标签精确度越高，样 本越具有代表性，学习模型的准确度越高。监督学习在自然语言处理、信息检索、 文本挖掘、手写体辨识、垃圾邮件侦测等领域获得了广泛应用。    无监督学习是利用无标记的有限数据描述隐藏在未标记数据中的结构/规律， 最典型的非监督学习算法包括单类密度估计、单类数据降维、聚类等。无监督学 习不需要训练样本和人工标注数据，便于压缩数据存储、减少计算量、提升算法 速度，还可以避免正、负样本偏移引起的分类错误问题。主要用于经济预测、异 常检测、数据挖掘、图像处理、模式识别等领域，例如组织大型计算机集群、社 交网络分析、市场分割、天文数据分析等。   强化学习  强化学习是智能系统从环境到行为映射的学习，以使强化信号函数值最大。 由于外部环境提供的信息很少，强化学习系统必须靠自身的经历进行学习。强化 学习的目标是学习从环境状态到行为的映射，使得智能体选择的行为能够获得环 境最大的奖赏，使得外部环境对学习系统在某种意义下的评价为最佳。其在机器 人控制、无人驾驶、下棋、工业控制等领域获得成功应用。  （2）根据学习方法可以将机器学习分为传统机器学习和深度学习。  传统机器学习  传统机器学习从一些观测（训练）样本出发，试图发现不能通过原理分析获 得的规律，实现对未来数据行为或趋势的准确预测。相关算法包括逻辑回归、隐 马尔科夫方法、支持向量机方法、K 近邻方法、三层人工神经网络方法、Adaboost 算法、贝叶斯方法以及决策树方法等。传统机器学习平衡了学习结果的有效性与 学习模型的可解释性，为解决有限样本的学习问题提供了一种框架，主要用于有 限样本情况下的模式分类、回归分析、概率密度估计等。传统机器学习方法共同 的重要理论基础之一是统计学，在自然语言处理、语音识别、图像识别、信息检 索和生物信息等许多计算机领域获得了广泛应用。  深度学习  深度学习是建立深层结构模型的学习方法，典型的深度学习算法包括深度置 信网络、卷积神经网络、受限玻尔兹曼机和循环神经网络等。深度学习又称为深 度神经网络（指层数超过 3 层的神经网络）。深度学习作为机器学习研究中的一 个新兴领域，由 Hinton 等人于 2006 年提出。深度学习源于多层神经网络，其实 质是给出了一种将特征表示和学习合二为一的方式。深度学习的特点是放弃了可 解释性，单纯追求学习的有效性。经过多年的摸索尝试和研究，已经产生了诸多 深度神经网络的模型，其中卷积神经网络、循环神经网络是两类典型的模型。卷 积神经网络常被应用于空间性分布数据；循环神经网络在神经网络中引入了记忆 和反馈，常被应用于时间性分布数据。深度学习框架是进行深度学习的基础底层 框架，一般包含主流的神经网络算法模型，提供稳定的深度学习 API，支持训练 模型在服务器和 GPU、TPU 间的分布式学习，部分框架还具备在包括移动设备、云平台在内的多种平台上运行的移植能力，从而为深度学习算法带来前所未有的 运行速度和实用性。目前主流的开源算法框架有 TensorFlow、Caffe/Caffe2、CNTK、 MXNet、Paddle-paddle、Torch/PyTorch、Theano 等。  （3）此外，机器学习的常见算法还包括迁移学习、主动学习和演化学习等。  迁移学习  迁移学习是指当在某些领域无法取得足够多的数据进行模型训练时，利用另 一领域数据获得的关系进行的学习。迁移学习可以把已训练好的模型参数迁移到 新的模型指导新模型训练，可以更有效的学习底层规则、减少数据量。目前的迁 移学习技术主要在变量有限的小规模应用中使用，如基于传感器网络的定位，文 字分类和图像分类等。未来迁移学习将被广泛应用于解决更有挑战性的问题，如 视频分类、社交网络分析、逻辑推理等。  主动学习  主动学习通过一定的算法查询最有用的未标记样本，并交由专家进行标记， 然后用查询到的样本训练分类模型来提高模型的精度。主动学习能够选择性地获 取知识，通过较少的训练样本获得高性能的模型，最常用的策略是通过不确定性 准则和差异性准则选取有效的样本。  演化学习  演化学习对优化问题性质要求极少，只需能够评估解的好坏即可，适用于求 解复杂的优化问题，也能直接用于多目标优化。演化算法包括粒子群优化算法、 多目标演化算法等。目前针对演化学习的研究主要集中在演化数据聚类、对演化 数据更有效的分类，以及提供某种自适应机制以确定演化机制的影响等。  3.1.2 知识图谱   知识图谱本质上是结构化的语义知识库，是一种由节点和边组成的图数据结 构，以符号形式描述物理世界中的概念及其相互关系，其基本组成单位是“实体 —关系—实体”三元组，以及实体及其相关“属性—值”对。不同实体之间通过 关系相互联结，构成网状的知识结构。在知识图谱中，每个节点表示现实世界的 “实体”，每条边为实体与实体之间的“关系”。通俗地讲，知识图谱就是把所有不同种类的信息连接在一起而得到的一个关系网络，提供了从“关系”的角度 去分析问题的能力。  知识图谱可用于反欺诈、不一致性验证、组团欺诈等公共安全保障领域，需 要用到异常分析、静态分析、动态分析等数据挖掘方法。特别地，知识图谱在搜 索引擎、可视化展示和精准营销方面有很大的优势，已成为业界的热门工具。但 是，知识图谱的发展还有很大的挑战，如数据的噪声问题，即数据本身有错误或 者数据存在冗余。随着知识图谱应用的不断深入，还有一系列关键技术需要突破。  自然语言处理是计算机科学领域与人工智能领域中的一个重要方向，研究能 实现人与计算机之间用自然语言进行有效通信的各种理论和方法，涉及的领域较 多，主要包括机器翻译、机器阅读理解和问答系统等。  （1）机器翻译  机器翻译技术是指利用计算机技术实现从一种自然语言到另外一种自然语 言的翻译过程。基于统计的机器翻译方法突破了之前基于规则和实例翻译方法的 局限性，翻译性能取得巨大提升。基于深度神经网络的机器翻译在日常口语等一 些场景的成功应用已经显现出了巨大的潜力。随着上下文的语境表征和知识逻辑 推理能力的发展，自然语言知识图谱不断扩充，机器翻译将会在多轮对话翻译及 篇章翻译等领域取得更大进展。  目前非限定领域机器翻译中性能较佳的一种是统计机器翻译，包括训练及解 码两个阶段。训练阶段的目标是获得模型参数，解码阶段的目标是利用所估计的 参数和给定的优化目标，获取待翻译语句的最佳翻译结果。统计机器翻译主要包 括语料预处理、词对齐、短语抽取、短语概率计算、最大熵调序等步骤。基于神 经网络的端到端翻译方法不需要针对双语句子专门设计特征模型，而是直接把源 语言句子的词串送入神经网络模型，经过神经网络的运算，得到目标语言句子的 翻译结果。在基于端到端的机器翻译系统中，通常采用递归神经网络或卷积神经 网络对句子进行表征建模，从海量训练数据中抽取语义信息，与基于短语的统计 翻译相比，其翻译结果更加流畅自然，在实际应用中取得了较好的效果。 （2）语义理解  语义理解技术是指利用计算机技术实现对文本篇章的理解，并且回答与篇章 相关问题的过程。语义理解更注重于对上下文的理解以及对答案精准程度的把控。 随着 MCTest 数据集的发布，语义理解受到更多关注，取得了快速发展，相关数 据集和对应的神经网络模型层出不穷。语义理解技术将在智能客服、产品自动问 答等相关领域发挥重要作用，进一步提高问答与对话系统的精度。  在数据采集方面，语义理解通过自动构造数据方法和自动构造填空型问题的 方法来有效扩充数据资源。为了解决填充型问题，一些基于深度学习的方法相继 提出，如基于注意力的神经网络方法。当前主流的模型是利用神经网络技术对篇 章、问题建模，对答案的开始和终止位置进行预测，抽取出篇章片段。对于进一 步泛化的答案，处理难度进一步提升，目前的语义理解技术仍有较大的提升空间。  （3）问答系统  问答系统分为开放领域的对话系统和特定领域的问答系统。问答系统技术是 指让计算机像人类一样用自然语言与人交流的技术。人们可以向问答系统提交用 自然语言表达的问题，系统会返回关联性较高的答案。尽管问答系统目前已经有 了不少应用产品出现，但大多是在实际信息服务系统和智能手机助手等领域中的 应用，在问答系统鲁棒性方面仍然存在着问题和挑战。  自然语言处理面临四大挑战：一是在词法、句法、语义、语用和语音等不同 层面存在不确定性；二是新的词汇、术语、语义和语法导致未知语言现象的不可 预测性；三是数据资源的不充分使其难以覆盖复杂的语言现象；四是语义知识的 模糊性和错综复杂的关联性难以用简单的数学模型描述，语义计算需要参数庞大 的非线性计算。  人机交互主要研究人和计算机之间的信息交换，主要包括人到计算机和计算 机到人的两部分信息交换，是人工智能领域的重要的外围技术。人机交互是与认知心理学、人机工程学、多媒体技术、虚拟现实技术等密切相关的综合学科。传统的人与计算机之间的信息交换主要依靠交互设备进行，主要包括键盘、鼠标、 操纵杆、数据服装、眼动跟踪器、位置跟踪器、数据手套、压力笔等输入设备，以及打印机、绘图仪、显示器、头盔式显示器、音箱等输出设备。人机交互技术 除了传统的基本交互和图形交互外，还包括语音交互、情感交互、体感交互及脑 机交互等技术，以下对后四种与人工智能关联密切的典型交互手段进行介绍。  （1）语音交互  语音交互是一种高效的交互方式，是人以自然语音或机器合成语音同计算机 进行交互的综合性技术，结合了语言学、心理学、工程和计算机技术等领域的知 识。语音交互不仅要对语音识别和语音合成进行研究，还要对人在语音通道下的 交互机理、行为方式等进行研究。语音交互过程包括四部分：语音采集、语音识 别、语义理解和语音合成。语音采集完成音频的录入、采样及编码；语音识别完 成语音信息到机器可识别的文本信息的转化；语义理解根据语音识别转换后的文 本字符或命令完成相应的操作；语音合成完成文本信息到声音信息的转换。作为 人类沟通和获取信息最自然便捷的手段，语音交互比其他交互方式具备更多优势， 能为人机交互带来根本性变革，是大数据和认知计算时代未来发展的制高点，具 有广阔的发展前景和应用前景。  （2）情感交互  情感是一种高层次的信息传递，而情感交互是一种交互状态，它在表达功能 和信息时传递情感，勾起人们的记忆或内心的情愫。传统的人机交互无法理解和 适应人的情绪或心境，缺乏情感理解和表达能力，计算机难以具有类似人一样的 智能，也难以通过人机交互做到真正的和谐与自然。情感交互就是要赋予计算机 类似于人一样的观察、理解和生成各种情感的能力，最终使计算机像人一样能进 行自然、亲切和生动的交互。情感交互已经成为人工智能领域中的热点方向，旨 在让人机交互变得更加自然。目前，在情感交互信息的处理方式、情感描述方式、 情感数据获取和处理过程、情感表达方式等方面还有诸多技术挑战。  （3）体感交互  体感交互是个体不需要借助任何复杂的控制系统，以体感技术为基础，直接 通过肢体动作与周边数字设备装置和环境进行自然的交互。依照体感方式与原理 的不同，体感技术主要分为三类：惯性感测、光学感测以及光学联合感测。体感 交互通常由运动追踪、手势识别、运动捕捉、面部表情识别等一系列技术支撑。与其他交互手段相比，体感交互技术无论是硬件还是软件方面都有了较大的提升， 交互设备向小型化、便携化、使用方便化等方面发展，大大降低了对用户的约束， 使得交互过程更加自然。目前，体感交互在游戏娱乐、医疗辅助与康复、全自动 三维建模、辅助购物、眼动仪等领域有了较为广泛的应用。  （4）脑机交互  脑机交互又称为脑机接口，指不依赖于外围神经和肌肉等神经通道，直接实 现大脑与外界信息传递的通路。脑机接口系统检测中枢神经系统活动，并将其转 化为人工输出指令，能够替代、修复、增强、补充或者改善中枢神经系统的正常 输出，从而改变中枢神经系统与内外环境之间的交互作用。脑机交互通过对神经 信号解码，实现脑信号到机器指令的转化，一般包括信号采集、特征提取和命令 输出三个模块。从脑电信号采集的角度，一般将脑机接口分为侵入式和非侵入式 两大类。除此之外，脑机接口还有其他常见的分类方式：按照信号传输方向可以 分为脑到机、机到脑和脑机双向接口；按照信号生成的类型，可分为自发式脑机 接口和诱发式脑机接口；按照信号源的不同还可分为基于脑电的脑机接口、基于 功能性核磁共振的脑机接口以及基于近红外光谱分析的脑机接口。  计算机视觉是使用计算机模仿人类视觉系统的科学，让计算机拥有类似人类 提取、处理、理解和分析图像以及图像序列的能力。自动驾驶、机器人、智能医 疗等领域均需要通过计算机视觉技术从视觉信号中提取并处理信息。近来随着深 度学习的发展，预处理、特征提取与算法处理渐渐融合，形成端到端的人工智能 算法技术。根据解决的问题，计算机视觉可分为计算成像学、图像理解、三维视 觉、动态视觉和视频编解码五大类。  （1）计算成像学  计算成像学是探索人眼结构、相机成像原理以及其延伸应用的科学。在相机 成像原理方面，计算成像学不断促进现有可见光相机的完善，使得现代相机更加 轻便，可以适用于不同场景。同时计算成像学也推动着新型相机的产生，使相机 超出可见光的限制。在相机应用科学方面，计算成像学可以提升相机的能力，从 而通过后续的算法处理使得在受限条件下拍摄的图像更加完善，例如图像去噪、去模糊、暗光增强、去雾霾等，以及实现新的功能，例如全景图、软件虚化、超 分辨率等。  （2）图像理解  图像理解是通过用计算机系统解释图像，实现类似人类视觉系统理解外部世 界的一门科学。通常根据理解信息的抽象程度可分为三个层次：浅层理解，包括 图像边缘、图像特征点、纹理元素等；中层理解，包括物体边界、区域与平面等； 高层理解，根据需要抽取的高层语义信息，可大致分为识别、检测、分割、姿态 估计、图像文字说明等。目前高层图像理解算法已逐渐广泛应用于人工智能系统， 如刷脸支付、智慧安防、图像搜索等。  （3）三维视觉  三维视觉即研究如何通过视觉获取三维信息（三维重建）以及如何理解所获 取的三维信息的科学。三维重建可以根据重建的信息来源，分为单目图像重建、 多目图像重建和深度图像重建等。三维信息理解，即使用三维信息辅助图像理解 或者直接理解三维信息。三维信息理解可分为，浅层：角点、边缘、法向量等； 中层：平面、立方体等；高层：物体检测、识别、分割等。三维视觉技术可以广 泛应用于机器人、无人驾驶、智慧工厂、虚拟/增强现实等方向。  （4）动态视觉  动态视觉即分析视频或图像序列，模拟人处理时序图像的科学。通常动态视 觉问题可以定义为寻找图像元素，如像素、区域、物体在时序上的对应，以及提 取其语义信息的问题。动态视觉研究被广泛应用在视频分析以及人机交互等方面。  （5）视频编解码  视频编解码是指通过特定的压缩技术，将视频流进行压缩。视频流传输中最 为重要的编解码标准有国际电联的 H.261、H.263、H.264、H.265、M-JPEG 和 MPEG 系列标准。视频压缩编码主要分为两大类：无损压缩和有损压缩。无损压 缩指使用压缩后的数据进行重构时，重构后的数据与原来的数据完全相同，例如 磁盘文件的压缩。有损压缩也称为不可逆编码，指使用压缩后的数据进行重构时， 重构后的数据与原来的数据有差异，但不会影响人们对原始资料所表达的信息产 生误解。有损压缩的应用范围广泛，例如视频会议、可视电话、视频广播、视频监控等。  目前，计算机视觉技术发展迅速，已具备初步的产业规模。未来计算机视觉 技术的发展主要面临以下挑战：一是如何在不同的应用领域和其他技术更好的结 合，计算机视觉在解决某些问题时可以广泛利用大数据，已经逐渐成熟并且可以 超过人类，而在某些问题上却无法达到很高的精度；二是如何降低计算机视觉算 法的开发时间和人力成本，目前计算机视觉算法需要大量的数据与人工标注，需 要较长的研发周期以达到应用领域所要求的精度与耗时；三是如何加快新型算法 的设计开发，随着新的成像硬件与人工智能芯片的出现，针对不同芯片与数据采 集设备的计算机视觉算法的设计与开发也是挑战之一。  生物特征识别技术是指通过个体生理特征或行为特征对个体身份进行识别 认证的技术。从应用流程看，生物特征识别通常分为注册和识别两个阶段。注册 阶段通过传感器对人体的生物表征信息进行采集，如利用图像传感器对指纹和人 脸等光学信息、麦克风对说话声等声学信息进行采集，利用数据预处理以及特征 提取技术对采集的数据进行处理，得到相应的特征进行存储。识别过程采用与注 册过程一致的信息采集方式对待识别人进行信息采集、数据预处理和特征提取， 然后将提取的特征与存储的特征进行比对分析，完成识别。从应用任务看，生物 特征识别一般分为辨认与确认两种任务，辨认是指从存储库中确定待识别人身份 的过程，是一对多的问题；确认是指将待识别人信息与存储库中特定单人信息进 行比对，确定身份的过程，是一对一的问题。  生物特征识别技术涉及的内容十分广泛，包括指纹、掌纹、人脸、虹膜、指 静脉、声纹、步态等多种生物特征，其识别过程涉及到图像处理、计算机视觉、 语音识别、机器学习等多项技术。目前生物特征识别作为重要的智能化身份认证 技术，在金融、公共安全、教育、交通等领域得到广泛的应用。下面将对指纹识 别、人脸识别、虹膜识别、指静脉识别、声纹识别以及步态识别等技术进行介绍。  （1）指纹识别 指纹识别过程通常包括数据采集、数据处理、分析判别三个过程。数据采集 通过光、电、力、热等物理传感器获取指纹图像；数据处理包括预处理、畸变校正、特征提取三个过程；分析判别是对提取的特征进行分析判别的过程。  （2）人脸识别 人脸识别是典型的计算机视觉应用，从应用过程来看，可将人脸识别技术划 分为检测定位、面部特征提取以及人脸确认三个过程。人脸识别技术的应用主要 受到光照、拍摄角度、图像遮挡、年龄等多个因素的影响，在约束条件下人脸识 别技术相对成熟，在自由条件下人脸识别技术还在不断改进。  （3）虹膜识别 虹膜识别的理论框架主要包括虹膜图像分割、虹膜区域归一化、特征提取和 识别四个部分，研究工作大多是基于此理论框架发展而来。虹膜识别技术应用的 主要难题包含传感器和光照影响两个方面：一方面，由于虹膜尺寸小且受黑色素 遮挡，需在近红外光源下采用高分辨图像传感器才可清晰成像，对传感器质量和 稳定性要求比较高；另一方面，光照的强弱变化会引起瞳孔缩放，导致虹膜纹理 产生复杂形变，增加了匹配的难度。  （4）指静脉识别 指静脉识别是利用了人体静脉血管中的脱氧血红蛋白对特定波长范围内的 近红外线有很好的吸收作用这一特性，采用近红外光对指静脉进行成像与识别的 技术。由于指静脉血管分布随机性很强，其网络特征具有很好的唯一性，且属于 人体内部特征，不受到外界影响，因此模态特性十分稳定。指静脉识别技术应用 面临的主要难题来自于成像单元。  （5）声纹识别 声纹识别是指根据待识别语音的声纹特征识别说话人的技术。声纹识别技术 通常可以分为前端处理和建模分析两个阶段。声纹识别的过程是将某段来自某个 人的语音经过特征提取后与多复合声纹模型库中的声纹模型进行匹配，常用的识 别方法可以分为模板匹配法、概率模型法等。  （6）步态识别 步态是远距离复杂场景下唯一可清晰成像的生物特征，步态识别是指通过身 体体型和行走姿态来识别人的身份。相比上述几种生物特征识别，步态识别的技术难度更大，体现在其需要从视频中提取运动特征，以及需要更高要求的预处理 算法，但步态识别具有远距离、跨角度、光照不敏感等优势。 虚拟现实（VR）/增强现实（AR）是以计算机为核心的新型视听技术。结合 相关科学技术，在一定范围内生成与真实环境在视觉、听觉、触感等方面高度近 似的数字化环境。用户借助必要的装备与数字化环境中的对象进行交互，相互影 响，获得近似真实环境的感受和体验，通过显示设备、跟踪定位设备、触力觉交 互设备、数据获取设备、专用芯片等实现。  虚拟现实/增强现实从技术特征角度，按照不同处理阶段，可以分为获取与 建模技术、分析与利用技术、交换与分发技术、展示与交互技术以及技术标准与 评价体系五个方面。获取与建模技术研究如何把物理世界或者人类的创意进行数 字化和模型化，难点是三维物理世界的数字化和模型化技术；分析与利用技术重 点研究对数字内容进行分析、理解、搜索和知识化方法，其难点是在于内容的语 义表示和分析；交换与分发技术主要强调各种网络环境下大规模的数字化内容流 通、转换、集成和面向不同终端用户的个性化服务等，其核心是开放的内容交换 和版权管理技术；展示与交换技术重点研究符合人类习惯数字内容的各种显示技 术及交互方法，以期提高人对复杂信息的认知能力，其难点在于建立自然和谐的 人机交互环境；标准与评价体系重点研究虚拟现实/增强现实基础资源、内容编 目、信源编码等的规范标准以及相应的评估技术。  目前虚拟现实/增强现实面临的挑战主要体现在智能获取、普适设备、自由 交互和感知融合四个方面。在硬件平台与装置、核心芯片与器件、软件平台与工 具、相关标准与规范等方面存在一系列科学技术问题。总体来说虚拟现实/增强 现实呈现虚拟现实系统智能化、虚实环境对象无缝融合、自然交互全方位与舒适 化的发展趋势。    综上所述，人工智能技术在以下方面的发展有显著的特点，是进一步研究人 工智能趋势的重点。 （1）技术平台开源化 开源的学习框架在人工智能领域的研发成绩斐然，对深度学习领域影响巨大。 开源的深度学习框架使得开发者可以直接使用已经研发成功的深度学习工具，减 少二次开发，提高效率，促进业界紧密合作和交流。国内外产业巨头也纷纷意识 到通过开源技术建立产业生态，是抢占产业制高点的重要手段。通过技术平台的 开源化，可以扩大技术规模，整合技术和应用，有效布局人工智能全产业链。谷 歌、百度等国内外龙头企业纷纷布局开源人工智能生态，未来将有更多的软硬件 企业参与开源生态。  （2）专用智能向通用智能发展 目前的人工智能发展主要集中在专用智能方面，具有领域局限性。随着科技 的发展，各领域之间相互融合、相互影响，需要一种范围广、集成度高、适应能 力强的通用智能，提供从辅助性决策工具到专业性解决方案的升级。通用人工智 能具备执行一般智慧行为的能力，可以将人工智能与感知、知识、意识和直觉等 人类的特征互相连接，减少对领域知识的依赖性、提高处理任务的普适性，这将 是人工智能未来的发展方向。未来的人工智能将广泛的涵盖各个领域，消除各领 域之间的应用壁垒。  （3）智能感知向智能认知方向迈进 人工智能的主要发展阶段包括：运算智能、感知智能、认知智能，这一观点 得到业界的广泛认可。早期阶段的人工智能是运算智能，机器具有快速计算和记 忆存储能力。当前大数据时代的人工智能是感知智能，机器具有视觉、听觉、触 觉等感知能力。随着类脑科技的发展，人工智能必然向认知智能时代迈进，即让 机器能理解会思考。   3.2 人工智能产业现状及趋势   人工智能作为新一轮产业变革的核心驱动力，将催生新的技术、产品、产业、 业态、模式，从而引发经济结构的重大变革，实现社会生产力的整体提升。麦肯 锡预计，到 2025 年全球人工智能应用市场规模总值将达到 1270 亿美元，人工智 能将是众多智能产业发展的突破点。  通过对人工智能产业分布进行梳理，提出了人工智能产业生态图，主要分为 核心业态、关联业态、衍生业态三个层次，如图 3 所示。 下面将重点对核心业态包含的智能基础设施建设、智能信息及数据、智能技 术服务、智能产品四个方面展开介绍，并总结人工智能行业应用及产业发展趋势。 智能基础设施为人工智能产业提供计算能力支撑，其范围包括智能传感器、 智能芯片、分布式计算框架等，是人工智能产业发展的重要保障。  （1）智能芯片  智能芯片从应用角度可以分为训练和推理两种类型。从部署场景来看，可以 分为云端和设备端两步大类。训练过程由于涉及海量的训练数据和复杂的深度神 经网络结构，需要庞大的计算规模，主要使用智能芯片集群来完成。与训练的计 算量相比，推理的计算量较少，但仍然涉及大量的矩阵运算。目前，训练和推理 通常都在云端实现，只有对实时性要求很高的设备会交由设备端进行处理。  按技术架构来看，智能芯片可以分为通用类芯片（CPU、GPU、FPGA）、基于 FPGA 的半定制化芯片、全定制化 ASIC 芯片、类脑计算芯片（IBM TrueNorth）。 另外，主要的人工智能处理器还有 DPU、BPU、NPU、EPU 等适用于不同场景 和功能的人工智能芯片。 随着互联网用户量和数据规模的急剧膨胀，人工智能发展对计算性能的要求 迫切增长，对 CPU 计算性能提升的需求超过了摩尔定律的增长速度。同时，受 限于技术原因，传统处理器性能也无法按照摩尔定律继续增长，发展下一代智能 芯片势在必行。未来的智能芯片主要是在两个方向发展：一是模仿人类大脑结构 的芯片，二是量子芯片。智能芯片是人工智能时代的战略制高点，预计到 2020 年人工智能芯片全球市场规模将突破百亿美元。  （2）智能传感器  智能传感器是具有信息处理功能的传感器。智能传感器带有微处理机，具备 采集、处理、交换信息等功能，是传感器集成化与微处理机相结合的产物。智能 传感器属于人工智能的神经末梢，用于全面感知外界环境。各类传感器的大规模 部署和应用为实现人工智能创造了不可或缺的条件。不同应用场景，如智能安防、 智能家居、智能医疗等对传感器应用提出了不同的要求。未来，随着人工智能应 用领域的不断拓展，市场对传感器的需求将不断增多，2020 年市场规模有望突 破 4600 亿美元。未来，高敏度、高精度、高可靠性、微型化、集成化将成为智 能传感器发展的重要趋势。  （3）分布式计算框架  面对海量的数据处理、复杂的知识推理，常规的单机计算模式已经不能支撑。 所以，计算模式必须将巨大的计算任务分成小的单机可以承受的计算任务，即云 计算、边缘计算、大数据技术提供了基础的计算框架。目前流行的分布式计算框 架如 OpenStack、Hadoop、Storm、Spark、Samza、Bigflow 等。各种开源深度学 习框架也层出不穷，其中包括 TensorFlow、Caffe、Keras、CNTK、Torch7、MXNet、 Leaf、Theano、DeepLearning4、Lasagne、Neon 等等。  信息数据是人工智能创造价值的关键要素之一。我国庞大的人口和产业基数 带来了数据方面的天生优势。随着算法、算力技术水平的提升，围绕数据的采集、 分析、处理产生了众多的企业。目前，在人工智能数据采集、分析、处理方面的 企业主要有两种：一种是数据集提供商，以提供数据为自身主要业务，为需求方 提供机器学习等技术所需要的不同领域的数据集；另一种是数据采集、分析、处理综合性厂商，自身拥有获取数据的途径，并对采集到的数据进行分析处理，最 终将处理后的结果提供给需求方进行使用。对于一些大型企业，企业本身也是数 据分析处理结果的需求方。  智能技术服务主要关注如何构建人工智能的技术平台，并对外提供人工智能 相关的服务。此类厂商在人工智能产业链中处于关键位置，依托基础设施和大量 的数据，为各类人工智能的应用提供关键性的技术平台、解决方案和服务。目前， 从提供服务的类型来看，提供技术服务厂商包括以下几类：  （1）提供人工智能的技术平台和算法模型。此类厂商主要针对用户或者行 业需求，提供人工智能技术平台以及算法模型。用户可以在人工智能平台之上， 通过一系列的算法模型来进行人工智能的应用开发。此类厂商主要关注人工智能 的通用计算框架、算法模型、通用技术等关键领域。  （2）提供人工智能的整体解决方案。此类厂商主要针对用户或者行业需求， 设计和提供包括软、硬件一体的行业人工智能解决方案，整体方案中集成多种人 工智能算法模型以及软、硬件环境，帮助用户或行业解决特定的问题。此类厂商 重点关注人工智能在特定领域或者特定行业的应用。  （3）提供人工智能在线服务。此类厂商一般为传统的云服务提供厂商，主 要依托其已有的云计算和大数据应用的用户资源，聚集用户的需求和行业属性， 为客户提供多类型的人工智能服务；从各类模型算法和计算框架的 API 等特定 应用平台到特定行业的整体解决方案等，进一步吸引大量的用户使用，从而进一 步完善其提供的人工智能服务。此类厂商主要提供相对通用的人工智能服务，同 时也会关注一些重点行业和领域。  需要指出的是，上述三类角色并不是严格区分开的，很多情况下会出现重叠， 随着技术的发展成熟，在人工智能产业链中已有大量的厂商同时具备上述两类或 者三类角色的特征。  智能产品是指将人工智能领域的技术成果集成化、产品化，具体的分类如表 1 所示。 表 1 人工智能的产品 随着制造强国、网络强国、数字中国建设进程的加快，在制造、家居、金融、 教育、交通、安防、医疗、物流等领域对人工智能技术和产品的需求将进一步释 放，相关智能产品的种类和形态也将越来越丰富。  人工智能与行业领域的深度融合将改变甚至重新塑造传统行业，本节重点介 绍人工智能在制造、家居、金融、交通、安防、医疗、物流行业的应用，由于篇 幅有限，其它很多重要的行业应用在这里不展开论述。  （1）智能制造  智能制造是基于新一代信息通信技术与先进制造技术深度融合，贯穿于设计、 生产、管理、服务等制造活动的各个环节，具有自感知、自学习、自决策、自执 行、自适应等功能的新型生产方式。智能制造对人工智能的需求主要表现在以下 三个方面：一是智能装备，包括自动识别设备、人机交互系统、工业机器人以及 数控机床等具体设备，涉及到跨媒体分析推理、自然语言处理、虚拟现实智能建 模及自主无人系统等关键技术。二是智能工厂，包括智能设计、智能生产、智能 管理以及集成优化等具体内容，涉及到跨媒体分析推理、大数据智能、机器学习 等关键技术。三是智能服务，包括大规模个性化定制、远程运维以及预测性维护 等具体服务模式，涉及到跨媒体分析推理、自然语言处理、大数据智能、高级机 器学习等关键技术。例如，现有涉及智能装备故障问题的纸质化文件，可通过自 然语言处理，形成数字化资料，再通过非结构化数据向结构化数据的转换，形成 深度学习所需的训练数据，从而构建设备故障分析的神经网络，为下一步故障诊 断、优化参数设置提供决策依据。  （2）智能家居  参照工业和信息化部印发的《智慧家庭综合标准化体系建设指南》，智能家 居是智慧家庭八大应用场景之一。受产业环境、价格、消费者认可度等因素影响， 我国智能家居行业经历了漫长的探索期。至 2010 年，随着物联网技术的发展以 及智慧城市概念的出现，智能家居概念逐步有了清晰的定义并随之涌现出各类产 品，软件系统也经历了若干轮升级。  智能家居以住宅为平台，基于物联网技术，由硬件（智能家电、智能硬件、 安防控制设备、家具等）、软件系统、云计算平台构成的家居生态圈，实现人远 程控制设备、设备间互联互通、设备自我学习等功能，并通过收集、分析用户行 为数据为用户提供个性化生活服务，使家居生活安全、节能、便捷等。例如，借助智能语音技术，用户应用自然语言实现对家居系统各设备的操控，如开关窗帘 （窗户）、操控家用电器和照明系统、打扫卫生等操作；借助机器学习技术，智 能电视可以从用户看电视的历史数据中分析其兴趣和爱好，并将相关的节目推荐 给用户。通过应用声纹识别、脸部识别、指纹识别等技术进行开锁等；通过大数 据技术可以使智能家电实现对自身状态及环境的自我感知，具有故障诊断能力。 通过收集产品运行数据，发现产品异常，主动提供服务，降低故障率。还可以通 过大数据分析、远程监控和诊断，快速发现问题、解决问题及提高效率。  （3）智能金融  人工智能的飞速发展将对身处服务价值链高端的金融业带来深刻影响，人工 智能逐步成为决定金融业沟通客户、发现客户金融需求的重要因素。人工智能技 术在金融业中可以用于服务客户，支持授信、各类金融交易和金融分析中的决策， 并用于风险防控和监督，将大幅改变金融现有格局，金融服务将会更加地个性化 与智能化。智能金融对于金融机构的业务部门来说，可以帮助获客，精准服务客 户，提高效率；对于金融机构的风控部门来说，可以提高风险控制，增加安全性； 对于用户来说，可以实现资产优化配置，体验到金融机构更加完美地服务。人工 智能在金融领域的应用主要包括：智能获客，依托大数据，对金融用户进行画像， 通过需求响应模型，极大地提升获客效率；身份识别，以人工智能为内核，通过 人脸识别、声纹识别、指静脉识别等生物识别手段，再加上各类票据、身份证、 银行卡等证件票据的 OCR 识别等技术手段，对用户身份进行验证，大幅降低核 验成本，有助于提高安全性；大数据风控，通过大数据、算力、算法的结合，搭 建反欺诈、信用风险等模型，多维度控制金融机构的信用风险和操作风险，同时 避免资产损失；智能投顾，基于大数据和算法能力，对用户与资产信息进行标签 化，精准匹配用户与资产；智能客服，基于自然语言处理能力和语音识别能力， 拓展客服领域的深度和广度，大幅降低服务成本，提升服务体验；金融云，依托 云计算能力的金融科技，为金融机构提供更安全高效的全套金融解决方案。  （4）智能交通  智能交通系统（Intelligent Traffic System，ITS）是通信、信息和控制技术在 交通系统中集成应用的产物。ITS 借助现代科技手段和设备，将各核心交通元素 联通，实现信息互通与共享以及各交通元素的彼此协调、优化配置和高效使用，形成人、车和交通的一个高效协同环境，建立安全、高效、便捷和低碳的交通。 例如通过交通信息采集系统采集道路中的车辆流量、行车速度等信息，信息分析 处理系统处理后形成实时路况，决策系统据此调整道路红绿灯时长，调整可变车 道或潮汐车道的通行方向等，通过信息发布系统将路况推送到导航软件和广播中， 让人们合理规划行驶路线。通过不停车收费系统（ETC），实现对通过 ETC 入口 站的车辆身份及信息自动采集、处理、收费和放行，有效提高通行能力、简化收 费管理、降低环境污染。  ITS 应用最广泛的地区是日本，其次是美国、欧洲等地区。中国的智能交通 系统近几年也发展迅速，在北京、上海、广州、杭州等大城市已经建设了先进的 智能交通系统；其中，北京建立了道路交通控制、公共交通指挥与调度、高速公 路管理和紧急事件管理等四大 ITS 系统；广州建立了交通信息共用主平台、物流 信息平台和静态交通管理系统等三大 ITS 系统。  （5）智能安防  智能安防技术是一种利用人工智能对视频、图像进行存储和分析，从中识别 安全隐患并对其进行处理的技术。智能安防与传统安防的最大区别在于智能化， 传统安防对人的依赖性比较强，非常耗费人力，而智能安防能够通过机器实现智 能判断，从而尽可能实现实时地安全防范和处理。  当前，高清视频、智能分析等技术的发展，使得安防从传统的被动防御向主 动判断和预警发展，行业也从单一的安全领域向多行业应用发展，进而提升生产 效率并提高生活智能化程度，为更多的行业和人群提供可视化及智能化方案。用 户面对海量的视频数据，已无法简单利用人海战术进行检索和分析，需要采用人 工智能技术作专家系统或辅助手段，实时分析视频内容，探测异常信息，进行风 险预测。从技术方面来讲，目前国内智能安防分析技术主要集中在两大类：一类 是采用画面分割前景提取等方法对视频画面中的目标进行提取检测，通过不同的 规则来区分不同的事件，从而实现不同的判断并产生相应的报警联动等，例如： 区域入侵分析、打架检测、人员聚集分析、交通事件检测等；另一类是利用模式 识别技术，对画面中特定的物体进行建模，并通过大量样本进行训练，从而达到 对视频画面中的特定物体进行识别，如车辆检测、人脸检测、人头检测（人流统 计）等应用。  智能安防目前涵盖众多的领域，如街道社区、道路、楼宇建筑、机动车辆的 监控，移动物体监测等。今后智能安防还要解决海量视频数据分析、存储控制及 传输问题，将智能视频分析技术、云计算及云存储技术结合起来，构建智慧城市 下的安防体系。  （6）智能医疗  人工智能的快速发展，为医疗健康领域向更高的智能化方向发展提供了非常 有利的技术条件。近几年，智能医疗在辅助诊疗、疾病预测、医疗影像辅助诊断、 药物开发等方面发挥重要作用。  在辅助诊疗方面，通过人工智能技术可以有效提高医护人员工作效率，提升 一线全科医生的诊断治疗水平。如利用智能语音技术可以实现电子病历的智能语 音录入；利用智能影像识别技术，可以实现医学图像自动读片；利用智能技术和 大数据平台，构建辅助诊疗系统。  在疾病预测方面，人工智能借助大数据技术可以进行疫情监测，及时有效地 预测并防止疫情的进一步扩散和发展。以流感为例，很多国家都有规定，当医生 发现新型流感病例时需告知疾病控制与预防中心。但由于人们可能患病不及时就 医，同时信息传达回疾控中心也需要时间，因此，通告新流感病例时往往会有一 定的延迟，人工智能通过疫情监测能够有效缩短响应时间。  在医疗影像辅助诊断方面，影像判读系统的发展是人工智能技术的产物。早 期的影像判读系统主要靠人手工编写判定规则，存在耗时长、临床应用难度大等 问题，从而未能得到广泛推广。影像组学是通过医学影像对特征进行提取和分析， 为患者预前和预后的诊断和治疗提供评估方法和精准诊疗决策。这在很大程度上 简化了人工智能技术的应用流程，节约了人力成本。  （7）智能物流  传统物流企业在利用条形码、射频识别技术、传感器、全球定位系统等方面 优化改善运输、仓储、配送装卸等物流业基本活动，同时也在尝试使用智能搜索、 推理规划、计算机视觉以及智能机器人等技术，实现货物运输过程的自动化运作 和高效率优化管理，提高物流效率。例如，在仓储环节，利用大数据智能通过分 析大量历史库存数据，建立相关预测模型，实现物流库存商品的动态调整。大数据智能也可以支撑商品配送规划，进而实现物流供给与需求匹配、物流资源优化 与配置等。在货物搬运环节，加载计算机视觉、动态路径规划等技术的智能搬运 机器人（如搬运机器人、货架穿梭车、分拣机器人等）得到广泛应用，大大减少 了订单出库时间，使物流仓库的存储密度、搬运的速度、拣选的精度均有大幅度 提升。  从人工智能产业进程来看，技术突破是推动产业升级的核心驱动力。数据资 源、运算能力、核心算法共同发展，掀起人工智能第三次新浪潮。人工智能产业 正处于从感知智能向认知智能的进阶阶段，前者涉及的智能语音、计算机视觉及 自然语言处理等技术，已具有大规模应用基础，但后者要求的“机器要像人一样 去思考及主动行动”仍尚待突破，诸如无人驾驶、全自动智能机器人等仍处于开 发中，与大规模应用仍有一定距离。  （1）智能服务呈现线下和线上的无缝结合 分布式计算平台的广泛部署和应用，增大了线上服务的应用范围。同时人工 智能技术的发展和产品不断涌现，如智能家居、智能机器人、自动驾驶汽车等， 为智能服务带来新的渠道或新的传播模式，使得线上服务与线下服务的融合进程 加快，促进多产业升级。  （2）智能化应用场景从单一向多元发展  目前人工智能的应用领域还多处于专用阶段，如人脸识别、视频监控、语音 识别等都主要用于完成具体任务，覆盖范围有限，产业化程度有待提高。随着智 能家居、智慧物流等产品的推出，人工智能的应用终将进入面向复杂场景，处理 复杂问题，提高社会生产效率和生活质量的新阶段。  （3）人工智能和实体经济深度融合进程将进一步加快  党的十九大报告提出“推动互联网、大数据、人工智能和实体经济深度融合”， 一方面，随着制造强国建设的加快将促进人工智能等新一代信息技术产品发展和 应用，助推传统产业转型升级，推动战略性新兴产业实现整体性突破。另一方面， 随着人工智能底层技术的开源化，传统行业将有望加快掌握人工智能基础技术并依托其积累的行业数据资源实现人工智能与实体经济的深度融合创新。  3.3 安全、伦理、隐私问题 历史经验表明新技术常常能够提高生产效率，促进社会进步。但与此同时， 由于人工智能尚处于初期发展阶段，该领域的安全、伦理、隐私的政策、法律和 标准问题值得关注。就人工智能技术而言，安全、伦理和隐私问题直接影响人们 与人工智能工具交互经验中对人工智能技术的信任。社会公众必须信任人工智能 技术能够给人类带来的安全利益远大于伤害，才有可能发展人工智能。要保障安 全，人工智能技术本身及在各个领域的应用应遵循人类社会所认同的伦理原则， 其中应特别关注的是隐私问题，因为人工智能的发展伴随着越来越多的个人数据 被记录和分析，而在这个过程中保障个人隐私则是社会信任能够增加的重要条件。 总之，建立一个令人工智能技术造福于社会、保护公众利益的政策、法律和标准 化环境，是人工智能技术持续、健康发展的重要前提。为此，本章集中讨论与人 工智能技术相关的安全、伦理、隐私的政策和法律问题。  人工智能最大的特征是能够实现无人类干预的，基于知识并能够自我修正地 自动化运行。在开启人工智能系统后，人工智能系统的决策不再需要操控者进一 步的指令，这种决策可能会产生人类预料不到的结果。设计者和生产者在开发人 工智能产品的过程中可能并不能准确预知某一产品会存在的可能风险。因此，对 于人工智能的安全问题不容忽视。  与传统的公共安全（例如核技术）需要强大的基础设施作为支撑不同，人工 智能以计算机和互联网为依托，无需昂贵的基础设施就能造成安全威胁。掌握相 关技术的人员可以在任何时间、地点且没有昂贵基础设施的情况下做出人工智能 产品。人工智能的程序运行并非公开可追踪，其扩散途径和速度也难以精确控制。 在无法利用已有传统管制技术的条件下，对人工智能技术的管制必须另辟蹊径。 换言之，管制者必须考虑更为深层的伦理问题，保证人工智能技术及其应用均应 符合伦理要求，才能真正实现保障公共安全的目的。 由于人工智能技术的目标实现受其初始设定的影响，必须能够保障人工智能设计的目标与大多数人类的利益和伦理道德一致，即使在决策过程中面对不同的 环境，人工智能也能做出相对安全的决定。从人工智能的技术应用方面看，要充 分考虑到人工智能开发和部署过程中的责任和过错问题，通过为人工智能技术开 发者、产品生产者或者服务提供者、最终使用者设定权利和义务的具体内容，来 达到落实安全保障要求的目的。  此外，考虑到目前世界各国关于人工智能管理的规定尚不统一，相关标准也 处于空白状态，同一人工智能技术的参与者可能来自不同国家，而这些国家尚未 签署针对人工智能的共有合约。为此，我国应加强国际合作，推动制定一套世界 通用的管制原则和标准来保障人工智能技术的安全性。    人工智能是人类智能的延伸，也是人类价值系统的延伸。在其发展的过程中， 应当包含对人类伦理价值的正确考量。设定人工智能技术的伦理要求，要依托于 社会和公众对人工智能伦理的深入思考和广泛共识，并遵循一些共识原则：  一是人类利益原则，即人工智能应以实现人类利益为终极目标。这一原则体 现对人权的尊重、对人类和自然环境利益最大化以及降低技术风险和对社会的负 面影响。在此原则下，政策和法律应致力于人工智能发展的外部社会环境的构建， 推动对社会个体的人工智能伦理和安全意识教育，让社会警惕人工智能技术被滥 用的风险。此外，还应该警惕人工智能系统作出与伦理道德偏差的决策。例如， 大学利用机器学习算法来评估入学申请，假如用于训练算法的历史入学数据（有 意或无意）反映出之前的录取程序的某些偏差（如性别歧视），那么机器学习可 能会在重复累计的运算过程中恶化这些偏差，造成恶性循环。如果没有纠正，偏 差会以这种方式在社会中永久存在。  二是责任原则，即在技术开发和应用两方面都建立明确的责任体系，以便在 技术层面可以对人工智能技术开发人员或部门问责，在应用层面可以建立合理的 责任和赔偿体系。在责任原则下，在技术开发方面应遵循透明度原则；在技术应 用方面则应当遵循权责一致原则。  其中，透明度原则要求了解系统的工作原理从而预测未来发展，即人类应当 知道人工智能如何以及为何做出特定决定，这对于责任分配至关重要。例如，在神经网络这个人工智能的重要议题中，人们需要知道为什么会产生特定的输出结 果。另外，数据来源透明度也同样非常重要。即便是在处理没有问题的数据集时， 也有可能面临数据中隐含的偏见问题。透明度原则还要求开发技术时注意多个人 工智能系统协作产生的危害。  权责一致原则，指的是未来政策和法律应该做出明确规定：一方面必要的商 业数据应被合理记录、相应算法应受到监督、商业应用应受到合理审查；另一方 面商业主体仍可利用合理的知识产权或者商业秘密来保护本企业的核心参数。在 人工智能的应用领域，权利和责任一致的原则尚未在商界、政府对伦理的实践中 完全实现。主要是由于在人工智能产品和服务的开发和生产过程中，工程师和设 计团队往往忽视伦理问题，此外人工智能的整个行业尚未习惯于综合考量各个利 益相关者需求的工作流程，人工智能相关企业对商业秘密的保护也未与透明度相 平衡。  人工智能的近期发展是建立在大量数据的信息技术应用之上，不可避免地涉 及到个人信息的合理使用问题，因此对于隐私应该有明确且可操作的定义。人工 智能技术的发展也让侵犯个人隐私（的行为）更为便利，因此相关法律和标准应 该为个人隐私提供更强有力的保护。已有的对隐私信息的管制包括对使用者未明 示同意的收集，以及使用者明示同意条件下的个人信息收集两种类型的处理。人 工智能技术的发展对原有的管制框架带来了新的挑战，原因是使用者所同意的个 人信息收集范围不再有确定的界限。利用人工智能技术很容易推导出公民不愿意 泄露的隐私，例如从公共数据中推导出私人信息，从个人信息中推导出和个人有 关的其他人员（如朋友、亲人、同事）信息（在线行为、人际关系等）。这类信 息超出了最初个人同意披露的个人信息范围。  此外，人工智能技术的发展使得政府对于公民个人数据信息的收集和使用更 加便利。大量个人数据信息能够帮助政府各个部门更好地了解所服务的人群状态， 确保个性化服务的机会和质量。但随之而来的是，政府部门和政府工作人员个人 不恰当使用个人数据信息的风险和潜在的危害应当得到足够的重视。  人工智能语境下的个人数据的获取和知情同意应该重新进行定义。首先，相关政策、法律和标准应直接对数据的收集和使用进行规制，而不能仅仅征得数据 所有者的同意；其次，应当建立实用、可执行的、适应于不同使用场景的标准流 程以供设计者和开发者保护数据来源的隐私；再次，对于利用人工智能可能推导 出超过公民最初同意披露的信息的行为应该进行规制。最后，政策、法律和标准 对于个人数据管理应该采取延伸式保护，鼓励发展相关技术，探索将算法工具作 为个体在数字和现实世界中的代理人。这种方式使得控制和使用两者得以共存， 因为算法代理人可以根据不同的情况，设定不同的使用权限，同时管理个人同意 与拒绝分享的信息。  本章节所涉及的安全、伦理和隐私问题是人工智能发展面临的挑战。安全问 题是让技术能够持续发展的前提。技术的发展给社会信任带来了风险，如何增加 社会信任，让技术发展遵循伦理要求，特别是保障隐私不会被侵犯是亟需解决的 问题。为此，需要（制订）合理的政策、法律、标准基础，并与国际社会协作。 在制订政策、法律和标准时，应当摆脱肤浅的新闻炒作和广告式的热点宣传，必 须促进对人工智能技术产品更深层地理解，聚焦这一新技术给社会产生重大利益 的同时也带来的巨大挑战。作为国际社会的重要成员，中国对保障人工智能技术 应用在正确的道路上、基于正确的理由得到健康发展担负重要的责任。  3.4 人工智能标准化的重要作用 当今，经济全球化和市场国际化深入发展，标准作为经济和社会活动的主要 技术依据，已成为衡量国家或地区技术发展水平的重要标志、产品进入市场的基 本准则、企业市场竞争力的具体体现。标准化工作对人工智能及其产业发展具有 基础性、支撑性、引领性的作用，既是推动产业创新发展的关键抓手，也是产业 竞争的制高点。人工智能标准的先进与完善与否，关系到产业的健康发展、以及 产品国际市场竞争力的强弱。  美国、欧盟、日本等发达国家高度重视人工智能标准化工作。美国发布的《国 家人工智能研究与发展策略规划》，欧盟发布的“人脑计划”，日本实施的“人工 智能/大数据/物联网/网络安全综合项目”，均提出围绕核心技术、顶尖人才、标 准规范等强化部署，力图抢占新一轮科技主导权。 我国高度重视人工智能标准化工作。在国务院《新一代人工智能发展规划》中将人工智能标准化作为重要支撑保障，提出要“加强人工智能标准框架体系研 究。坚持安全性、可用性、互操作性、可追溯性原则，逐步建立并完善人工智能 基础共性、互联互通、行业应用、网络安全、隐私保护等技术标准。加快推动无 人驾驶、服务机器人等细分应用领域的行业协会和联盟制定相关标准”。工信部 在《促进新一代人工智能产业发展三年行动计划（2018-2020 年）》中指出，要建 设人工智能产业标准规范体系，建立并完善基础共性、互联互通、安全隐私、行 业应用等技术标准；同时构建人工智能产品评估评测体系。  我国虽然在人工智能领域虽然具备了良好基础，语音识别、视觉识别、中文 信息处理等核心技术实现了突破，也具有巨大的应用市场环境，但整体发展水平 仍落后于发达国家，在核心算法、关键设备、高端芯片、重大产品与系统等方面 差距较大，适应人工智能发展的基础设施、政策法规、标准体系亟待完善。  综上分析，更应重视人工智能标准化工作对于促进技术创新、支撑产业发展 具有的重要引领作用：  （一）标准化工作有利于加快人工智能技术创新和成果转化。现阶段人工智 能技术发展迅速，市场上逐步出现了可规模化、可商业化的产品和应用，需要以 标准化的手段固化技术成果，实现快速创新推广；  （二）标准化工作有助于提升人工智能产品和服务质量。如市场上出现的人 脸识别系统、智能音箱、服务机器人等产品，质量残次不齐，需要标准的统一规 范，并配合以开展符合性测试评估的方式，提升产品和服务质量；  （三）标准化工作有助于切实保障用户安全。例如自动驾驶领域的“电车难 题”伦理难题、苹果手机指纹泄露用户隐私等问题，引起了人们的广泛关注。如 何保护用户权益是难点也是重点，这需要通过建立以人为本的原则，制定相关安 全标准规范，确保智能系统遵从并服务于人类伦理，并确保信息安全；  （四）标准化工作有助于营造公平开放的人工智能产业生态。当前，行业巨 头以开源算法、平台接口绑定等方式，打造自有深度学习框架等生态体系，造成 用户数据信息较难迁移。这需要统一的标准实现厂商之间的互操作与协同工作， 防止行业垄断、用户绑定，形成良性的产业生态。 
359,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736429&idx=5&sn=e4ce3214196cbcb200a418431814b174&chksm=871ac353b06d4a452795dfcc273cb8c7f3ff15fbcbbe3d2499d4a1ccdafd34d563f7e67f3310&scene=27,AAAI 2018 | 腾讯提出自适应图卷积神经网络，接受不同图结构和规模的数据,"近日，AAAI 2018 发布接收论文列表，腾讯 AI Lab 共入选 11 篇。在论文《Adaptive Graph Convolutional Neural Networks》中，腾讯联合德克萨斯大学阿灵顿分校提出自适应图卷积神经网络 AGCN，可接受任意图结构和规模的图作为输入。 论文：自适应图卷积神经网络（Adaptive Graph Convolutional Neural Networks） 论文链接：https://arxiv.org/pdf/1801.03226.pdf 摘要：图卷积神经网络（Graph CNN）是经典 CNN 的推广方法，可用于处理分子数据、点云和社交网络等图数据。Graph CNN 中的的滤波器大多是为固定和共享的图结构而构建的。但是，对于大多数真实数据而言，图结构的大小和连接性都是不同的。本论文提出了一种有泛化能力且灵活的 Graph CNN，其可以使用任意图结构的数据作为输入。通过这种方式，可以在训练时为每个图数据构建一个任务驱动的自适应图（adaptive graph）。为了有效地学习这种图，我们提出了一种距离度量学习方法。我们在九个图结构数据集上进行了大量实验，结果表明我们的方法在收敛速度和预测准确度方面都有更优的表现。 尽管卷积神经网络（CNN）被证明可成功解决大量机器学习问题（Hinton et al. 2012; Dundar et al. 2015），但是它们通常要求输入为张量。比如，图像和视频被分别建模为 2-D 和 3-D 张量。但是，在很多真实问题中，数据所在的栅格（grid）不规则或者非欧几里德域（non-Euclidean domain），如化学分子、点云和社交网络。相比于规则的张量，这些数据更适合被建构为图，从而能够处理不同的近邻顶点连接性（neighborhood vertex connectivity）和非欧几里德度量。在这种情况下，支持在栅格上进行卷积的平稳性和复合性无法再得到满足。因此，在图结构数据上重构卷积算子是必要的。 但是，把 CNN 从规则栅格扩展到不规则栅格并不容易。为了简洁地构建卷积核，早期 Graph CNN 假定数据仍然是低维的（Bruna et al. 2013; Henaff, Bruna, and LeCun 2015）。由于卷积器根据节点度（node degree）区别处理各个节点，卷积核过分关注于局部而不灵活，无法学习复杂图（带有无法预测且灵活的节点连接性，如分子和社交网络）的层次表征。 当前 graph CNN 遇到了以下瓶颈问题： 图度（graph degree）受限； 输入数据必须共享同样的图结构； 图固定，且非通过训练学得； 无法从拓扑结构中学习。 本文提出了一种新型频谱图卷积网络（Spectral Graph CNN），输入可以是多种图结构的原始数据，如包含不同数量苯环的有机分子。因此，该网络不使用共享频谱核，而是给批量中的每个样本一个特定的图拉普拉斯 矩阵（Graph Laplacian），客观描述其独特拓扑结构。根据其独特的图拓扑，定制化图拉普拉斯算子将带来定制化的频谱滤波器。 研究哪种图最适合某种监督学习任务是很有意思的。例如，化学键自然地为化合物构建出一个内在图。但是，它无法保证内在图上的卷积器能够提取所有有意义的特征。因此，我们训练了一个「残差图」（residual graph）以发现内在图不包含的残差子结构。此外，为了确保残差图是特定任务的最佳补充，我们设计了一个机制在训练其余 Graph CNN 时学习残差图。 我们在多个图结构数据集上 验证本文提出的频谱卷积网络，数据集包括化学分子、LIDAR 生成的点云。Graph CNN 的创新之处如下： 1. 构建独特的图拉普拉斯算子。为一个批量中的每个样本构建和学习独特的残差拉普拉斯矩阵，将学得的残差图拉普拉斯算子添加到初始图上。 2. 学习用于生成残差图的距离度量。通过学习共享的最优距离度量参数，图的拓扑结构随着预测网络的训练而更新。学习的复杂度独立于输入规模，成本仅相当于 3. 卷积中的特征嵌入。在卷积前，先进行顶点特征变换，使得顶点内不同特征之间和不同顶点特征均联系起来。 4. 接受灵活的图输入。由于 1 和 2，本文提出的网络可以输入不同的图结构和图大小，对图度没有限制。 SGC-LL 层 SGC-LL 层的运算如算法 1 所述： AGCN 网络 我们提出了一种新型的基于自适应图的频谱图卷积器（SGC-LL）。SGC-LL 学习最优距离度量和特征变换，进而学习残差图拉普拉斯矩阵（Residual Graph Laplacian）。目前，AGCN 是首个允许任意图结构和规模的频谱 Graph CNN。残差图拉普拉斯的监督式训练使得模型更适合预测任务。在不同图结构数据上的大量多任务学习实验表明，不同预测任务上 AGCN 优于之前顶尖的 Graph CNN。 "
360,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736364&idx=4&sn=919d7b0f7fdda88443c3a4b246c83c1e&chksm=871ac292b06d4b8457b9e8d890bf61e4db2d04dc991b1f0a6a1ba3255d9c0fb58e2cfc26fa81&scene=27,AAAI 2018 | 蚂蚁金服公开最新基于笔画的中文词向量算法,"词向量算法是自然语言处理领域的基础算法，在序列标注、问答系统和机器翻译等诸多任务中都发挥了重要作用。词向量算法最早由谷歌在2013年提出的word2vec，在接下来的几年里，该算法也经历不断的改进，但大多是仅适用于拉丁字符构成的单词（比如英文），结合中文语言特性的词向量研究相对较少。本文介绍了蚂蚁金服人工智能部与新加坡科技大学一项最新的合作成果：cw2vec——基于汉字笔画信息的中文词向量算法研究，用科学的方法揭示隐藏在一笔一划之间的秘密。 AAAI大会（Association for the Advancement of Artificial Intelligence），是一年一度在人工智能方向的顶级会议之一，旨在汇集世界各地的人工智能理论和领域应用的最新成果。该会议固定在每年的2月份举行，由AAAI协会主办。 第32届AAAI大会-AAAI 2018将于2月2号-7号在美国新奥尔良召开，其中蚂蚁金服人工智能部和新加坡科技大学合作的一篇基于汉字笔画信息的中文词向量算法研究的论文“cw2vec: Learning Chinese Word Embeddings with Stroke n-grams”被高分录用（其中一位审稿人给出了满分，剩下两位也给出了接近满分的评价）。我们将在2月7日在大会上做口头报告（Oral），欢迎大家一起讨论交流。 单个英文字符（character）是不具备语义的，而中文汉字往往具有很强的语义信息。不同于前人的工作，我们提出了“n元笔画”的概念。所谓“n元笔画”，即就是中文词语（或汉字）连续的n个笔画构成的语义结构。 如上图，n元笔画的生成共有四个步骤。比如说，“大人”这个词语，可以拆开为两个汉字“大”和“人”，然后将这两个汉字拆分成笔画，再将笔画映射到数字编号，进而利用窗口滑动产生n元笔画。其中，n是一个范围，在上述例子中，我们将n取值为3, 4和5。 在论文中我们提出了一种基于n元笔画的新型的损失函数，如下： 其中，W和C分别为当前词语和上下文词语，σ是sigmoid函数，T(w)是当前词语划窗内的所有词语集合，D是训练语料的全部文本。为了避免传统softmax带来的巨大计算量，这篇论文也采用了负采样的方式。C'为随机选取的词语，称为“负样例”，λ是负样例的个数，而 则表示负样例C'按照词频分布进行的采样，其中语料中出现次数越多的词语越容易被采样到。相似性sim(·,·)函数被按照如下构造： 其中， 为当前词语对应的一个n元笔画向量，而 是其对应的上下文词语的词向量。这项技术将当前词语拆解为其对应的n元笔画，但保留每一个上下文词语不进行拆解。S(w)为词语w所对应的n元笔画的集合。在算法执行前，这项研究先扫描每一个词语，生成n元笔画集合，针对每一个n元笔画，都有对应的一个n元笔画向量，在算法开始之前做随机初始化，其向量维度和词向量的维度相同。 如上图所示，对于“治理 雾霾 刻不容缓”这句话，假设此刻当前词语恰好是“雾霾”，上下文词语是“治理”和“刻不容缓”。首先将当前词语“雾霾”拆解成n元笔画并映射成数字编码，然后划窗得到所有的n元笔画，根据设计的损失函数，计算每一个n元笔画和上下文词语的相似度，进而根据损失函数求梯度并对上下文词向量和n元笔画向量进行更新。 为了验证这项研究提出的cw2vec算法的效果，在公开数据集上，与业界最优的几个词向量算法做了对比: 上图中包括2013年谷歌提出的word2vec的两个模型skipgram和cbow，2014年斯坦福提出的GloVe算法，2015年清华大学提出的基于汉字的CWE模型，以及2017年最新发表的基于像素和偏旁的中文词向量算法，可以看出cw2vec在word similarity，word analogy，以及文本分类和命名实体识别的任务中均取得了一致性的提升。同时，这篇文章也展示了不同词向量维度下的实验效果： 上图为不同维度下在word analogy测试集上的实验结果，左侧为3cosadd，右侧为3cosmul的测试方法。可以看出这项算法在不同维度的设置下均取得了不错的效果。此外，也在小规模语料上进行了测试： 上图是仅选取20%中文维基百科训练语料，在word similarity下测试的结果，skipgram, cbow和GloVe算法由于没有利用中文的特性信息进行加强，所以在小语料上表现较差，而其余四个算法取得了不错的效果，其中cw2vec的算法在两个数据集上均取得的了最优效果。 为了更好的探究不同算法的实际效果，这项研究专门选取了两个词语做案例分析。第一个是环境相关的“水污染”，然后根据词向量利用向量夹角余弦找到与其语义最接近的词语。GWE找到了一些和“污”字相关的词语，比如“污泥”，“污渍”和“污垢”，而JWE则更加强调后两个字“污染”GloVe找到了一些奇怪的相近词语，比如“循环系统”，“神经系统”。CWE找到的相近词语均包含“水”和“污”这两个字，猜测是由于其利用汉字信息直接进行词向量加强的原因。此外，只有cw2vec找到了“水质”这个相关词语，分析认为是由于n元笔画和上下文信息对词向量共同作用的结果。第二个例子，特别选择了“孙悟空”这个词语，该角色出现在中国的名著《西游记》和知名日本动漫《七龙珠》中，cw2vec找到的均为相关的角色或著作名称。 作为一项基础研究成果，cw2vec在蚂蚁和阿里的诸多场景上也有落地。在智能客服、文本风控和推荐等实际场景中均发挥了作用。此外，不单单是中文词向量，对于日文、韩文等其他语言也进行类似的尝试，相关的发明技术专利已经申请近二十项。 我们希望能够在基础研究上追赶学术界、有所建树，更重要的是，在具体的实际场景之中，能够把人工智能技术真正的赋能到产品里，为用户提供更好的服务。 论文下载链接： https://github.com/ShelsonCao/cw2vec/blob/master/cw2vec.pdf "
361,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736364&idx=1&sn=6a64302d8028ec3aaf7775294b15c724&chksm=871ac292b06d4b84ec464e0e3394748ac8c6bbb6e8278534960a50ef595a018fa3c61b0a6986&scene=27,李彦宏登上时代周刊封面，百度研究院全面升级,今天，《时代周刊》亚洲版封面刊登了一则有关百度 CEO 李彦宏的文章，题为《百度李彦宏帮助中国走向 21 世纪》，从搜索引擎开始介绍了百度在人工智能领域的发展历程。与此同时，美国时间 1 月 18 日，这家中国科技巨头又宣布了另一个重大消息，百度研究院在硅谷的全员大会也宣布设立商业智能实验室（Business Intelligence Lab，BIL）和机器人与自动驾驶实验室，同时三位世界级人工智能领域科学家 Kenneth Ward Church、浣军、熊辉加盟百度研究院。 All In AI 的中国科技巨头 在文章中，《时代周刊》写到，「2000 年，李彦宏创立了百度，这个以搜索引擎起家的公司如今和谷歌一样非常流行，并且占据了中国市场 80% 的份额，因此它成为了世界上第四大网站。百度的公司名来自于八百年前南宋词人辛弃疾的一句词：众里寻他千百度。目前，这家公司已经成为市值已经超过 880 亿美元的巨头，它与关注于社交的腾讯、关注于在线购物的阿里巴巴成为中国最具影响力的科技企业。」 百度也是中国首个宣布「All in AI」的企业，全面发展人工智能。以搜索立身，百度自创立之初就与自然语言处理等人工智能技术息息相关。早在 2013 年初，百度就组建了深度学习研究院，即百度研究院的前身。2014 年，百度研究院正式成立，包括 IDL、BDL 和 SVAIL。2017 年 3 月，百度明确把人工智能作为公司发展战略，整合 AI 核心技术，成立 AI 技术平台体系（AIG），任命副总裁王海峰为总负责人，推动研发领先的 AI 核心技术。 经过多年的发展，百度人工智能的布局逐渐明晰：自动驾驶、语音交互等成为百度 AI 生态中的重要一环。在今年年初的 CES 展会上，我们就看到百度上演了一场「中国速度」，发布自驾驶平台 Apollo2.0。 就像《时代周刊》文中介绍的，「百度的自动驾驶汽车平台 Apollo 已经集结了 130 个独立制造商，证明了百度在这个领域的成就。这个公司开发了语音识别软件 DuerOS，据称其识别中文的可靠性超过了人类。它的 AI 驱动的人脸识别软件是如此先进，以至于能通过匹配照片找到失散的儿童，让他们和父母团聚......」 百度已从去年底开始在道路上测试 Apllo 平台自动驾驶汽车，图片来自 Tony Law 全面升级百度研究院 尽管面临着腾讯、阿里以及国外科技公司的竞争，百度在人工智能领域加大投入的决心从未改变。1 月 18 日，百度研究院在硅谷召开全员大会，宣布设立商业智能实验室（Business Intelligence Lab，BIL）和机器人与自动驾驶实验室（Robotics and Autonomous Driving Lab，RAL），同时宣布三位世界级人工智能领域科学家 Kenneth Ward Church、浣军、熊辉加盟百度研究院。至此，百度研究院全新升级，建立起包括七位世界级科学家、五大实验室的「全明星」阵容。 在这次扩充后，百度研究院下属共五大实验室： 深度学习实验室（IDL） 大数据实验室（BDL） 硅谷人工智能实验室（SVAIL） 商业智能实验室（BIL） 机器人与自动驾驶实验室（RAL） 会上，百度副总裁、AI 技术平台体系（AIG）总负责人、百度研究院院长王海峰表示，这是百度研究院新征程的开始。百度研究院将聚焦前瞻基础研究，布局百度人工智能未来发展方向，服务百度作为人工智能公司的长期发展战略。 据介绍，新成立的商业智能实验室将聚焦用于新兴数据密集型应用的高效数据分析技术；而机器人与自动驾驶实验室则重点关注机器人技术，尤其是在自动驾驶领域夯实百度无人驾驶基础技术。 而在研究实力上，除了院长王海峰，还有徐伟、李平、杨睿刚三位已经任职于百度研究院的资深科学家，本次加盟的 Kenneth Ward Church、浣军、熊辉均是 AI 领域的世界级学者。 Kenneth Ward Church 是自然语言处理领域的大师级人物，是经验主义方法的奠基人之一。他在麻省理工学院获得学士、硕士及博士学位，曾先后在贝尔实验室、微软研究院、约翰霍普金斯大学、IBM Watson Research Center 工作。Church 创立了自然语言处理领域最重要的学术会议之一 EMNLP（Empirical Methods on Natural Language Processing）并多年担任主席，他曾于 2012 年担任自然语言处理领域最顶级的国际学术组织 ACL（Association for Computational Linguistics）主席，现为 ACL Fellow，是资历相当老的顶级大牛。 浣军曾任美国国家基金委项目主任，主管大数据，此前任堪萨斯大学终身教授。熊辉是美国罗格斯-新泽西州立大学终身正教授。 在全员大会上，Kenneth Ward Church 说，「人工智能的价值有目共睹。百度不仅致力于基础研究，还创造性地将实验室技术转化为真实的应用，让我们的世界变得更加美好。我很高兴能够加入这个团队，与才华横溢的研究者和工程师们一起探索人工智能的前沿技术。」 作为中国人工智能领域的领军企业，百度在 AI 研发领域的持续投入，彰显了决胜 AI 时代的雄心，在不远的未来，这家公司或许会成为 AI「中国速度」的加速引擎，推动中国 AI 加速创新。 
362,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736316&idx=5&sn=6c401761a831e0e35c79bc32410a1079&chksm=871ac2c2b06d4bd4efc9449ed6a4e04c29d6874d90e28ec9f8a59e86354aadc16f46fa71f8d6&scene=27,投票 | 2017年度最值得读的AI论文评选,2017 年，同样有无数优秀的论文涌现。从 AlphaGo 的从“零”开始到 Geoffrey Hinton 提出的 Capsule 计划，各大高校和科研机构为我们带来了很多令人兴奋的研究成果。 当然， ，将于评选结束后从参与投票的用户中进行抽选。 1. 长按识别下方二维码参与投票 2. 文末留言你喜欢某篇论文的原因 3. 分享本文到朋友圈并截图发至后台 2018年1月24日0点0分 PaperWeekly定制笔记本 x 5份 PaperWeekly定制行李牌 x 10份 长按扫描二维码， 参与投票 1. 为了方便大家在投票过程中查看论文详情， 。点击页面右上角的“…”按钮，在手机浏览器中打开表单。 2. 本次评选包含 和 两大方向，请在你所选择的参与方向下 。 3. 获奖名单将于1月25日公布，其中5位由小编根据文末留言选取，其他13位采用随机抽取，礼物随机发放。 ▼ 关于PaperWeekly PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击 ，小助手将把你带入 PaperWeekly 的交流群里。 ▽ 点击 |    
363,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736316&idx=4&sn=5aed5223f389a50bec0971bb2f189669&chksm=871ac2c2b06d4bd42c53ad3aa09d6409df21399fb2e0ea480f37c4be392918fd340634fd9942&scene=27,业界 | Uber提出SBNet：利用激活的稀疏性加速卷积网络,"自动驾驶系统有非常高的实时性需求。 近日，Uber 的研究人员提出了一种可以在改善检测准确度的同时极大提升速度的算法 SBNet 并在其工程开发博客上对该研究进行了介绍。机器之心对该介绍文章进行了编译，更多详情请参阅原论文。另外，本项目的代码也已在 GitHub 上发布。 论文地址：https://arxiv.org/abs/1801.02108 代码地址：https://github.com/uber/sbnet 为了实现更安全和更可靠的交通运输解决方案，Uber ATG Toronto 的研究者正致力于开发应用了卷积神经网络（CNN）和其它深度学习技术的技术。   CNN 在分析来自激光雷达（LiDAR）传感器的视觉图像和数据上有着广泛的应用。在自动驾驶领域，CNN 能让自动驾驶车辆看见其它汽车和行人、确定它们的准确位置以及解决许多之前无法使用传统算法解决的其它难题。为了确保我们的自动系统是可靠的，这样的 CNN 必须以非常快的速度在 GPU 上运行。在降低使用 CNN 的设备成本和功耗的同时开发改善响应时间和准确度的有效方式一直以来都是一个研究重点。   作为这种努力的一部分，我们开发了一个用于 TensorFlow 的开源算法——稀疏块网络（SBNet：Sparse Blocks Network），该算法可通过利用 CNN 激活中的稀疏性来加速推理。我们的研究表明，当 SBNet 与残差网络（ResNet）架构结合使用时，有可能带来一个数量级的提速。SBNet 允许使用更深和更宽的网络配置进行实时的推理，从而能在降低计算负载的情况下实现准确度的提升。   在这篇文章中，我们将讨论我们构建 SBNet 的方法，并将展示该算法在我们的自动驾驶 3D LiDAR 物体检测器中的实际应用，其实现了显著的时钟加速和检测准确度提升。 背景   传统的深度 CNN 是在数百层上一致为所有的空间位置应用卷积算子，这需要每秒执行数万亿次运算。在我们最新的研究中，我们基于这样一个认识进行了开发——很多运算都浪费在了对无关信息的过度分析上。在一个典型的场景中，仅有少部分被观测数据是重要的；我们将这个现象称为稀疏性（sparsity）。在自然界中，视觉皮层等生物神经网络根据周边视觉（peripheral vision）检测到的运动和减少视网膜周边部分中的密度和颜色信息，从而可以通过聚焦视网膜中央凹视觉来利用稀疏性。   在人工神经网络中，激活稀疏的 CNN 之前已经在手写识别等小规模任务上有研究发现了，但与高度优化的密集卷积实现相比还没有实现真正的加速。   但是，我们的研究表明通过在 CNN 激活中利用我们所说的块稀疏性（block sparsity），可以实现高达一个数量级的实际加速。如图 1 所示： SBNet 介绍   根据这些见解，我们开发了 SBNet，这是一个用于 TensorFlow 的开源算法，能够利用 CNN 的激活中的稀疏性，因此能显著提升推理速度。   为了实现我们算法的目标，我们根据表示激活非零的位置的掩码定义了块稀疏性（block sparsity）。这种掩码可以来自对该问题的之前已有的知识，或者可以直接根据对平均激活进行阈值化来得到。为了利用经过高度优化的密集的卷积算子，我们定义了两个运算操作来将稀疏的激活变换成仅包含非零元素的更小的特征图。   首先通过使用来自输入张量的重叠块来在注意掩码上执行池化运算，然后将其转换成一个传递给块收集操作的索引列表，SBNet 实现了这一目标，如图 2 所示：   然后聚（gather）操作会将 tile 沿批（batch）维度堆叠在一起，形成一个新的张量。然后使用已有的优化过的密集卷积实现，然后一个自定义的散（scatter） 操作会执行一个反向的运算并将结果写在原始密集输入张量之上。下面的图 3 给出了我们提出的使用了稀疏 gather/scatter 操作的稀疏卷积机制：   当我们为 SBNet 设计稀疏操作 API 时，我们希望能将其轻松地整合到流行的 CNN 架构（比如 ResNet 和 Inception）和其它定制的 CNN 构造模块中。为了实现这一目标，我们为我们引入的三种基本操作（reduce_mask、sparse_gather 和 sparse_scatter）发布了 CUDA 实现（和 TensorFlow wrapper。使用这些低层面的运算，我们能为不同的 CNN 架构和配置增加块稀疏性。   下面我们提供了一个 TensorFlow 示例，演示了 如何使用SBNet API进行单层稀疏卷积运算： 更多示例请参阅我们的 GitHub 库：https://github.com/uber/sbnet，其中还包含一个完整的 ResNet 模块实现。   接下来，我们将讨论如何将 SBNet 用于训练  Uber ATG 的 3D 车辆检测系统。   应用：根据 LiDAR 点检测 3D 车辆   在 Uber ATG Toronto，我们在根据 LiDAR 点检测 3D 车辆的任务上验证了 SBNet；由于这个任务需要稀疏的输入并且对推理的时间限制要求挺高，所以是一个有效的用例。在我们的模型中，LiDAR 能以每秒 10 次径向扫描的速度产生周围环境的 3D 点云。对于每一次扫描，我们都会采用人工的方式为周围的所有车辆标注 3D 边界框。除了点云和 3D 标签，我们也有从地图提取的道路布局信息。   图 4 给出了数据、车辆标签和道路地图的一张鸟瞰图：   首先，我们应用了一个基于 CNN 的方法来解决这一任务，并且从一个俯视视角以每像素 0.1m 的分辨率对该 LiDAR 点云进行了离散化；结果数据表征展现出了超过 95% 的稀疏度。然后，这些数据被输入了一个基于 ResNet 的单发检测器（ResNet-based single-shot detector）。（有关我们的基准检测器的更多详情，请参阅我们的研究论文）。   我们根据一个传统的密集卷积的基准检测器，对 SBNet 的两种变体进行了基准评估——我们将其中的所有层都替换成了对应的块稀疏的版本。这些变体基于两个不同的稀疏性信息来源：一个使用了预计算的道路地图（事先已知），另一个使用了预测得到的前景掩码。道路地图可以从离线的地图中提取，这不会给检测器增加计算时间。预测得到的前景掩码是使用另一个低分辨率 CNN 生成的，并且比道路地图都更高的稀疏度。   在使用 SBNet 时，我们测量了两种变体相对于基准检测器的显著提速。在下面的图 5 中，我们给出了输入数据在不同稀疏程度下测得的速度提升。   使用道路地图的变体平均有 80% 的稀疏度，对应实现了 2 倍的加速；使用预测掩码的变体平均有大约 90% 的稀疏度，对应实现了 3 倍的加速.   在检测准确度方面，使用 SBNet 架构重新训练检测器会在平均精度上得到 2 个百分点的增益。这说明使用数据稀疏性能够通过减少噪声和方差来使模型训练稳定化，从而在更快的推理时间之外还能得到更准确的 3D 车辆检测。 接下来   我们相信 SBNet 能够广泛应用于各种深度学习架构、模型、应用和稀疏源，我们期待看到深度学习研究社区通过不同的方式使用这些架构构建模块。有关 SBNet 和我们的研究的更详细解释，我们希望你能阅读我们的论文。   对最近这项成果的总结请参看下面的视频： 扩展阅读 M. Ren, A. Pokrovsky, B. Yang, R. Urtasun,「SBNet: Sparse Blocks Network for Fast Inference,」arXiv preprint arXiv:1801.02108, 2018. (GitHub) Y. LeCun, L. Bottou, Y. Bengio, P. Haffner,「Gradient-based learning applied to document recognition,」in Proceedings of the IEEE, 86 (11): 2278–2324, doi:10.1109/5.726791, 1998. Abadi et al,「TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,」arXiv preprint arXiv:1603.04467, 2016. B. Graham and L. van der Maaten,「Submanifold sparse convolutional networks,」arXiv preprint, arXiv:1706.01307, 2017. K. He, X. Zhang, S. Ren, J. Sun,「Deep residual learning for image recognition,」in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2017. A. Lavin and S. Gray,「Fast algorithms for convolutional neural networks,」in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2016. "
364,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736429&idx=4&sn=964770a2a0f2789b0574279617dfde38&chksm=871ac353b06d4a45fa94a2c3922666101ff285f577ee936220c687ccbf63e2ec7460746a1100&scene=27,深度 | 利用进化方法自动生成神经网络：深度进化网络DENSER,"CISUC 在为特定任务构建神经网络算法时，想要达到最佳性能需要大量的设计和手动调整。葡萄牙科英布拉大学计算设计和可视化实验室的研究者们利用进化算法的策略提出了深度进化网络结构表征（DENSER），可以自动进行多层深度神经网络的结构设计和参数调优，在没有先验知识的情况下，该方法生成的神经网络达到了业内最佳性能。 论文：DENSER: Deep Evolutionary Network Structured Representation 论文链接：https://arxiv.org/abs/1801.01563 深度进化网络结构表征（DENSER）是一种利用进化计算自动设计人工神经网络（ANN）的新方法。该算法不仅搜索最优的网络拓扑结构，而且对超参数 (如学习或数据增强参数) 进行调优。自动设计是通过两个不同层次的编码方式来实现的，其中外层编码网络的一般结构，内层编码与每层相关联的参数。允许的层数和超参数值的范围是通过一种人类可读的上下文无关语法来定义的。 这项工作的主要贡献是： DENSER，一种基于进化原理的通用框架，可自动搜索具有不同层次类型和/或目标的大型深度网络的适当结构和参数设置; 一个自动生成的 CNN，在没有任何先验知识的情况下，对 CIFAR-10 数据集的分类是有效的，其平均准确度为 94.27%; 证明了由 DENSER 进化的 ANN 概括性很好。具体来说，CIFAR-100 数据集的 78.75% 的平均精度是由一个拓扑结构专门为 CIFAR-100 数据集而进化的网络实现的。据我们所知，这是通过自动设计 CNN 的方法对 CIFAR-100 数据集报告的最好结果。 最好的训练模型可以在 https://github.com/fillassuncao/denser-models 中找到。 新方法：DENSER 为了促进 ANN 的结构和参数的进化，我们提出了 DENSER：深度进化网络结构表征。DENSER 结合了遗传算法（GA）和动态结构语法演变（DSGE）的基本思想。 表示方法 每个解决方案通过一个有序的前馈层序列及其各自的参数对 ANN 进行编码；学习和任何其他超参数也可以对每个个体进行编码。候选解决方案的表示形式在两个不同的层次上进行： GA 层：对网络的宏观结构进行编码，并负责表示随后用作语法开始符号的指示符的层序列。它要求对网络允许的结构进行定义，例如层的有效序列。 DSGE 层：对与层相关的参数进行编码。参数及其允许的值或范围必须在由用户定义的语法中编码。 交叉 研究者提出了两种交叉操作符，它们基于不同的基因型层次。在当前工作背景下，一个模块并不代表可以多次复制的一组层，而是代表属于相同 GA 结构索引的一组层。 考虑到所有的个体有同样的模块（可能有不同的层数），第一个交叉运算符是一个单点交叉，它在同一个模块内改变两个个体之间的层。 第二个交叉运算符是一个统一的交叉，它改变了两个个体之间的全部模块。 改变 我们开发了一套改变运算符，专门用于促进 ANN 的进化。在 GA 层上，改变旨在操纵网络结构： 添加层：新层是根据要放置层的模块的开始符号的可能性生成的； 复制层：随机选择一个层并将其复制到模块的另一个有效位置。复制是通过引用进行的，即如果参数在层中被改变，所有的副本都被修改； 删除层：选中一个层并将其从给定模块中删除。 上面所说的改变运算符只作用于网络的一般结构，想要改变层的参数需要使用下面的 DSGE 改变： 语法改变：扩展可能性被另一个替代； 整数改变：产生一个新的整数值（在允许的范围内）； 浮动改变：高斯扰动作用于给定的浮点值。 为了测试新的方法，我们对 CNN 的生成进行了实验，以对 CIFAR-10 基准进行分类。CIFAR-10 由 60000 个实例组成，每个实例都是一个 32 × 32 的 RGB 彩色图像，属于十个可能的类别之一。由 DENSER 进化而来的解决方案被映射到 keras 模型中，从而让我们可以测量它们的性能。任务的目标是最大化目标识别任务的准确性。 为了分析进化拓扑的广泛性和可扩展性，我们将采用最好的 CNN 拓扑机构，并对 CIFAR-100 基准的分类进行测试。 用于 CIFAR-10 的 CNN 我们在 CIFAR-10 数据集的分类任务上进行了 10 次生成 CNN 的进化实验。对于生成的网络，我们分析了它们的适应性（即分类任务的准确性）和隐藏层的数量。 图一描述了几代以来最好的 CNN 的平均适应度和层数的进化。对结果进行简要的研究表明，进化正在发生，并且解决方案趋向于在 80 代的周围聚集。可以观察到两种不同的且相互矛盾的行为。从进化的开始到大约第 60 代，性能的增加都伴随着层数的减少；这种现象从第 60 代发生变化并一直持续到最后一代，这期间随着性能的增加，最佳网络的隐形层数量随之增加。这个分析揭示了一个明显的矛盾，它是在第一代随机生成的解决方案中有大量的层，它们的参数是随机设置的这一事实之后解释的。 图 2 展示了在进化过程中发现的最合适的网络（在验证精度方面）。进化网络最令人困惑的特征是在拓扑结束时使用的密集层的重要性和数量。据我们所知，对如此大量的密集层进行顺序使用是前所未有的，可以说人类永远不会想到这样的拓扑结构，这使得这种进化的结果非常有意义。 一旦进化过程完成，在每次运行中发现的最佳网络将被重新训练 5 次。首先，我们用在进化过程中使用的相同的学习速率来训练网络（lr=0.01），但是在 400 个时期中（而不是 10 个）。通过这个设置我们平均获得了 88.41% 的测试集的分类准确度。为了进一步提高网络的准确性，我们采用了 Snoek 等人描述的策略：我们为测试集的每个实例产生 100 个增强图像, 并且预测是 100 个增强图像的平均置信度的最大值。在此验证方法之后，最佳进化网络测试集的平均准确度提高到 89.93%。 为了研究是否有可能提高最佳网络的性能，我们用相同的 CGP-CNN 策略重新训练他们：一种从 0.01 开始变化的学习速率；在第 5 个 epoch，它增加到 0.1；到第 250 个 epoch，下降到 0.01；最后在第 375 个 epoch 下降到 0.001。用之前的训练方式，最适合的网络的平均准确度提高到 93.38%。如果在测试集中进行数据增强，则平均准确度为 94.13%。，即平均误差为 5.87%。 图 3 显示了 DENSER 与用其他方法得到的最佳结果的比较。对结果的分析表明，DENSER 是精度最高的。在我们的方法中可训练参数的数量要高得多，因为我们允许在进化的 CNN 中放置完全连接的层。此外，在进化过程中，没有先验知识被用来使搜索空间发生偏差。 推广到 CIFAR-100 为了测试进化网络的广泛性和可扩展性，我们在 CIFAR-10 数据集上采用由 DENSER 生成的最佳网络，并将其应用于 CIFAR-100 数据集的分类。为了让网络在 CIFAR-100 数据集上工作，我们只将 softmax 层改为有 100 个输出神经元，而不是 10 个。 每个网络的训练是随机的；由于数据增强过程，所以初始条件是不同的，并且使用数据集的不同实例来训练它们。因此，为了进一步改进结果，我们研究了 Graham 提出的测试 fractional max-pooling 性能的方法是否会提高我们网络的性能。简单来说，不是用单一的网络，而是使用一个集合，集合中的每个网络都一个是网络独立训练的结果。使用该方法，训练 5 次的同一网络的集合的测试准确度为 77.51%，训练 10 次的集合的测试准确度为 77.89%，训练 12 次的集合的测试准确度为 78.14%。这些结果比文献中所报道的用相当标准的数据增强方法进行进化的 CNN 的结果更好。 此外, 我们未采用相同的网络结构来组成集合，而是用 DENSER 发现的两个最好的网络拓扑结构来测试建立一个集合的性能，类似于 Real 等人所做的。按照这种方法, 准确度提高到了 78.75%。 图 4 的表格显示了 DENSER 在 CIFAR-100 上获得的结果与其他方法获得结果的比较。 "
365,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736316&idx=1&sn=83b6a5bae8c8a0701c30a5400c98a756&chksm=871ac2c2b06d4bd44c86d662e9a5bd5432b7afda691f4f23588eb7726dc00747883818f4b8c8&scene=27,机器之心年度奖项Synced Machine Intelligence Awards正式发布,"2017 年人工智能继续强势发展，基础研究和产业落地的进程都令人瞩目，既有众多令人兴奋的研究成果，又有持续落地的产业应用案例。科技巨头大力推进技术研究和平台建设，创业公司在摸索场景应用中快速崛起，早期项目也逐渐建立起自己的商业逻辑。 在此背景下，机器之心推出首届 年度奖项评选，希望通过四大奖项来记录过去一年人工智能的发展与进步，更希望最终获奖企业的成绩与经验能够为所有的人工智能从业者带来灵感与启发，以实现人工智能的更大发展。 在奖项征集期间，我们共收到数百家企业的申请，包括行业巨头、垂直领域独角兽和早期创业公司等，覆盖医疗健康、金融、安防、交通、零售、教育等多个热门领域。 机器之心根据研究实力、技术与产品、市场与行业潜力、运营能力、资本与财务状况、创新性与实用性等多项标准，评选出四项大奖：全球三十大最佳 AI 创业公司、中国十大最强 AI 研究实力企业、中国十大最具潜力早期 AI 公司、三十大最佳 AI 应用案例。具体结果如下： 一、全球三十大最佳 AI 创业公司 1、Clarifai Clarifai 成立于 2013 年，专注计算机视觉领域，公司在同年 ImageNet 2013 大赛中获得图像分类组前五名，至今一直处于行业领先地位。公司主要业务是为企业客户和开发人员解决视觉领域的问题与挑战。Clarifai 提供 API 类型的工具，Clarifai 的视觉识别 API 可以识别超过 11,000 种不同内容的照片和视频，公司同时也提供应用于特定领域的识别工具。Clarifai 有着多元化的客户群体，从《财富》500 强公司到小型的开发团队都是他们的服务对象，包括 Buzzfeed、Trivago、500px、StyleMePretty 等。 2、出门问问 出门问问成立于 2012 年，拥有自主研发的语音交互、智能推荐、计算机视觉及机器人 SLAM 等技术，主营业务为 AI 消费电子产品、跨平台语音交互技术提供方。出门问问发布了问问手表 Ticwatch、问问音箱 Tichome、小问音箱 Tichome Mini、与大众汽车集团合资成立的问众智能的问问魔镜 Ticmirror、问问魔眼 Ticeye 等产品。2017 年 4 月出门问问与大众汽车集团成立合资公司问众智能，同年成为 Google Android Wear 中国官方运营伙伴。截至 2017 年 4 月，出门问问共完成六轮融资，累计融资额超 2.55 亿美元。 3、 Darktrace Darktrace  成立于 2013 年，致力于应用 AI 技术应对网络安全挑战。Darktrace 开发了世界上第一个企业免疫系统，该系统能够使企业的安全网络在没有预先知道特定威胁的情况下，检测出绕过传统网络安全工具的威胁和异常行为，并能够对网络攻击进行主动防御。Darktrace 的企业免疫系统已应用于物流、工业生产、教育、金融、健康等多个领域。Darktrace 经历了快速增长，年收入同比增长超过 600％，在全球设有 30 个办事处。目前，Darktrace 已完成 7500 万美元 D 轮融资。 4、Drive.ai Drive.ai 成立于 2015 年，是一家位于硅谷的自动驾驶公司，其主要成员来自斯坦福大学人工智能实验室。公司专注研究深度学习在自动驾驶中的应用，致力于通过工具包将普通汽车变为无人车。2017 年 2 月，Drive.ai 在美国加利福尼亚州山景城街道完成雨夜无人驾驶测试。同年 9 月，Drive.ai 与 Lyft 达成合作协议，在加州推出试点计划。 5、达观数据 达观数据成立于 2015 年，为企业提供完善的文本挖掘、知识图谱、搜索引擎和个性化推荐等大数据服务，是国内唯一一家将自动语义分析技术应用于企业数据化运营的人工智能公司。达观数据目前服务的领域包括媒体、视频、电商、银行以及金融科技等，客户包括华为、中兴、梨视频、酷六、趣头条、京东云等。达观数据在文本挖掘、搜索引擎、个性化推荐系统方面已拥有 30 余项国家发明专利。目前，达观数据已完成 5000 万元 A 轮融资。 6、达闼科技 达闼科技成立于 2015 年，是全球首家云端智能机器人运营商，专注于全球安全通讯网络、人工智能、机器人运营服务。达闼科技正在创建全球独立、高安全级别的高速「天网」，目前，达闼 ADN 已在全球 5 个洲、14 个国家搭建了共计 40 个网络节点。公司产品还有：CoudPepper、Meta 导盲机器人、云端智能连接器 AI Mobile、MCS 等。在知识产权方面，达闼科技已申请专利超过 400 个。目前，达闼科技共完成 2 轮融资，融资总额超过 1.3 亿美元。 7、第四范式 第四范式成立于 2015 年，是人工智能技术与服务提供商，帮助企业实现智能变革、创造商业价值。第四范式的核心产品先知平台 3.0——企业 AI 核心系统覆盖了人工智能落地应用的全过程，帮助企业完成一站式 AI 全系统建设，使企业既可以按需设计 AI 解决方案，又可灵活挖掘全新应用需求。团队已帮助金融、电信、互联网等领域等 100 多家企业成功实现人工智能转型。2016 年 12 月，第四范式荣获中国人工智能界最高奖——「吴文俊人工智能科学技术奖」创新奖一等奖，是首个获得该奖项的企业。 8、Element AI Element AI 成立于 2016 年，是由 Yoshua Bengio 和 Jean-François Gagné 创立的人工智能孵化器，这家被称为「加拿大版 YC」的公司旨在通过硅谷式的产业服务帮助创业者，将人工智能的新技术从实验室转化为实用产品，同时也为其他公司构建研究机构提供解决方案。在 Yoshua Bengio 等人的帮助下，Element AI 拥有学界最强大的技术阵容，虽然其中一些并不是全职，但他们能够为公司和客户提供最好的建议。 9、 Graphcore Graphcore 成立于 2016 年，是开发 IPU-Accelerator 和 IPU-Appliance 产品的硬件系统公司。Graphcore 建立的处理器可以降低在云和企业数据中心应用程序的成本，Graphcore 硬件系统加强了对机器学习模型的处理、训练和推理。目前，Graphcore 已完成 5000 万美元 C 轮融资。 10、H2O.ai H2O.ai 成立于 2011 年，为开发人员和创业公司提供预测分析的深度学习平台。平台可用于智能家电、自驾车、智能助手等领域的应用开发，其产品支持理赔处理、信用评分、欺诈检测、运营智能和预测性维护。2017 年，H2O.AI 发布了 Driverless AI，这项产品可以实现机器学习流程高度自动化，对非技术型的终端用户同样适用。公司客户包括 Capital One、Comcast、AT&T 和 Kaiser，并和 AWS 和 Azure 建立了合作。目前，H2O.ai 已完成 4000 万美元 C 轮融资。 11、寒武纪科技 寒武纪科技成立于 2016 年，是全球智能芯片领域的先行者，旨在打造各类智能云服务器、智能终端以及智能机器人的核心处理器芯片。寒武纪科技是全球第一个成功流片并拥有成熟产品的智能芯片公司，拥有终端和服务器两条产品线。2017 年 11 月，寒武纪科技发布了面向视觉领域的寒武纪 1H8、性能更强的寒武纪 1H16，以及面向智能驾驶领域的寒武纪 1M，同时推出了面向开发者的人工智能系统软件 Cambricon NeuWare。目前，寒武纪已完成 1 亿美元 A 轮融资。 12、汇医慧影 汇医慧影成立于 2015 年，是国家级医学影像人工智能高新技术企业，公司拥有图像深度学习核心技术和多项专利技术，基于云计算、大数据、人工智能技术，构建了智慧影像云平台、数字智能胶片、肿瘤放疗云平台、大数据智能分析云平台和人工智能诊断云平台等五大平台，打造了医学影像的数字化、移动化、智能化，完成了从筛查、诊断、治疗到预后的闭环。截至 2017 年 12 月，汇医慧影已经接入 700 多家医院，包含北医三院、301 医院、北京肿瘤医院等超过 200 家三甲医院。目前，汇医慧影已完成数亿元 B 轮融资。 13、K ensho Kensho 创立于 2013 年，是一家数据分析公司，结合自然语言搜索查询、数据科学和云计算创建新的分析工具。Kensho 帮助商业机构部署机器智能和分析系统，解决当前棘手的分析问题，例如华尔街投资分析面临的三大挑战：速度，规模和自动化，而且 Kensho 的智能系统还能够以简单的英语回答财务问题。公司客户包括金融机构和商业媒体，未来有望拓展至美国政府部门。2017 年 4 月，Kensho 完成 5000 万美元 B 轮融资。 14、氪信 氪信成立于 2015 年，是国内先进的人工智能金融风控服务商，为金融机构和企业提供风险预测和判断能力。氪信利用高维数据处理、机器学习技术和对金融场景落地的经验，帮助金融客户实现数据驱动的效率最大化，取得持续的业绩增长。氪信提供完整的风控闭环服务，在多个金融场景中对信贷违约概率区分度具有 50% 以上的提升。目前，氪信为 50 多家金融机构提供服务，包括招商银行、民生银行、平安证券、中银消费金融、众安保险等。 15、旷视科技 旷视科技成立于 2011 年，致力于计算机视觉技术及产品研发，为行业用户提供基于场景的智能数据服务及行业解决方案。旷视科技深耕金融安全、城市大脑、手机智能三大核心领域，核心合作伙伴包含阿里巴巴、华为、富士康、招商银行、小米、万科等。旷视科技拥有国内外在申及授权专利超过 500 件，在各项国际人工智能顶级竞赛中获得十五项世界技术评测第一。旷视科技旗下的 Face++人工智能开放平台是全球最大的人工智能开发者平台，已服务超过 200 个国家地区的数十万开发者。2017 年 10 月，旷视科技完成 4.6 亿美元 C 轮融资。 16、流利说 流利说由中央「千人计划」专家王翌、胡哲人和林晖在 2012 年共同创立，自主研发了人工智能英语老师、英语口语评测、写作打分引擎和深度自适应学习系统，从听说读写多个维度全面有效提升用户的英语水平。提­供一整套系统性的英语学习解决方案。目前，旗舰 App「英语流利说」注册用户数超过 5000 万人，付费学员超过 60 万人，覆盖全球 175 个国家和国内 379 个城市。流利说是中国在线教育领域第一家实现规模化盈利的公司。2017 年 7 月，流利说完成近亿美金的 C 轮融资。 17、Petuum Petuum 成立于 2016 年，专注于机器学习基础架构平台开发，旨在帮助企业解决企业在人工智能和机器学习的部署过程中遇到的瓶颈问题和其他困难。Petuum 的开发平台和构建 AI 的模块允许使用多个流行语言编程，管理人员和分析人员无需任何编码即可快速高效搭建应用程序，开发人员也可根据实际需求对应用程序进行重新深度编程。Petuum 为应用程序的开发和部署提供了一种跨平台、标准化的方法，使用过程变得高度模块化且易于操控。2017 年 10 月，Petuum 完成 9300 万美元 B 轮融资。 18、Sentient Sentient 成立于 2007 年，创建了世界上规模最大的分布式 AI 平台。公司致力于深度学习和人工智能在电子商务中的应用，解决方案 Sentient Aware 是结合机器学习和计算机视觉技术的商品推荐系统，基于「视觉过滤技术」，针对用户潜在偏好，建构在线商品推荐模型，为零售电商企业提供智能推荐解决方案，改善用户在线购物体验。其他解决方案有 Sentient Ascend、Sentient Investment Management 等。 19、商汤科技 商汤科技成立于 2014 年，专注于计算机视觉和深度学习的原创技术，估值超过 20 亿美金。商汤科技的 DeepID 系列人脸识别算法在国际权威的 LFW 人脸数据库测试中获得了 99.15% 的识别率，是全球首次超过人眼识别率；研发团队在与计算机视觉相关的国际顶尖学术杂志和会议上累计发表论文数超过 400 篇。目前，商汤科技已与国内外多个行业的 400 多家领军企业建立合作，涵盖安防、金融、智能手机、移动互联网、汽车、智慧零售、机器人等诸多行业，为其提供基于人脸识别、图像识别、视频分析、无人驾驶、医疗影像识别等技术的完整解决方案。2017 年 7 月，商汤科技完成 4.1 亿美元 B 轮融资。 20、深鉴科技 深鉴科技成立于 2016 年，定位为深度学习硬件解决方案公司，以自主研发的深度压缩与深度学习处理器（DPU）为核心，打造最好用的解决方案和最高效的整体系统。2017 年 10 月 24 日，深鉴科技宣布完成 4000 万美元 A+轮融资，同时推出六款 AI 产品：人脸检测识别模组、人脸分析解决方案、视频结构化解决方案、ARISTOTLE 架构平台，深度学习 SDK DNNDK、双目深度视觉套件。其中，前四款是针对安防场景的解决方案，深度学习 SDK DNNDK 则是国内首家公开发布的深度学习开发 SDK，能在极大降低 DPU 平台深度学习应用开发门槛和部署难度的同时，显著加速 AI 产品从开发到面市的进程。 21、思必驰 思必驰成立于 2007 年，是国内领先的语音交互人工智能公司，为物联网及相关垂直领域提供自然语言交互解决方案，包括 DUI 全链路智能对话定制平台、人机对话操作系统、人工智能芯片模组等，并在赋能智能硬件终端后，丰富后端服务资源，满足用户的产品体验和任务型需求。2017 年，思必驰与公安部第三研究所在智能语音产品符合性测试技术研究上达成合作，与深圳市沃特沃德股份有限公司共同打造基于智能语音交互的智能硬件产品综合解决方案。知识产权方面，截至 2017 年年底，思必驰共拥有专利 231 项。 22、体素科技 体素科技成立于 2016 年，定位于下一代人工智能医疗产业，致力于提供基于深度学习的精准和个性化的医疗诊断服务。目前公司已经覆盖了肺癌诊断、眼底疾病检查、冠脉增强 CT 分析系统等领域，并根据相应的临床需求提供端到端的解决方案。体素科技的产品形式主要包括三种类型：在 Web 客户端的应用，主要是为医院、体检中心、保险公司提供分析服务；云计算服务接口，主要是与影像器械生产商、医疗影像管理系统软件商合作；为第三方医疗数据分析应用上提供云计算平台。目前，体素科技已完成 1 亿元 A+ 轮融资。 23、图森未来 图森未来成立于 2015 年，专注于自动驾驶货运卡车技术的研发与应用，总部位于北京，在中美多地设有研发中心。图森未来自主研发的以摄像头为主要传感器的 L4 级别自动驾驶技术，能够实现货运卡车在高速公路路段的全无人驾驶。图森未来曾在全球自动驾驶算法评测数据集 KITTI 和 Cityscapes 上刷新 10 项世界纪录，并在美国加州获得路测牌照，成为全球为数不多可在加州公共道路路测的无人驾驶企业之一。目前图森未来 L4 级无人驾驶卡车累计已完成超过一万公里真实环境路测里程，并完成数百万公里的仿真环境测试。目前，图森未来已完成 5500 万美元 C 轮融资。 24、推想科技 推想科技成立于 2015 年，致力于采用人工智能深度学习的方法分析医学影像数据，为影像科医生提供精确、高效的辅助工具，让医生从繁重的重复性工作中得以解放。目前，推想科技的产品有：智能 CT 辅助筛查产品 (AI-CT)、智能 X 线辅助筛查产品 (AI-DR)、智能深度学习科研平台 (AI-Scholar)、智能脑部辅助筛查产品 (AI-Stroke)。截至 2017 年 9 月，推想科技产品已辅助医生完成近 20 万例临床诊断工作。2017 年 9 月，推想科技完成 1.5 亿元 B 轮融资。 25、Vicarious  Vicarious 成立于 2010 年，是一家专注于通用人工智能的科技公司。公司致力于使用计算机神经科学来建造机器人处理各种任务的学习模型，并专注于大脑皮层区域，这个部分在医学上普遍被认为主管视觉和听觉。Vicarious 吸引了 Facebook CEO 扎克伯格、特斯拉 CEO 马斯克、彼得. 蒂尔和亚马逊 CEO 杰夫. 贝索斯等众多大佬的私人投资。2017 年 7 月，Vicarious 完成 5000 万美元 C 轮融资。 26、依图科技 依图科技成立于 2012 年，致力于人工智能创新性研究，主要业务分布于智能安防平台、智慧医疗健康、AI+金融、智慧城市、智能硬件设备等多个领域。依图科技在 2017 年美国国家标准与技术研究院（NIST）主办的人脸识别测试（FRVT）中获得四项测试第一名，在美国国家情报高级研究计划局（IARPA）主办的全球人脸识别挑战赛（FRPC) 获得人脸识别算法检索准确率冠军。依图科技目前的产品有：依图蜻蜓眼人像大平台、依图蜻蜓眼车辆大平台、AICARE® 胸部 CT 智能辅助诊断系统、AICARE® 超声智能辅助诊断系统、AICARE® 儿科门诊智能辅助诊断系统等。目前，依图科技已完成 3.8 亿元 C 轮融资。 27、优必选科技 优必选科技成立于 2012 年，是一家集人工智能和人形机器人研发、平台软件开发运用及产品销售为一体的全球性高科技企业。目前，优必选科技的产品有：Alpha 系列机器人、Jimu 机器人、Cruzr 机器人、Lynx 机器人、星球大战第一军团冲锋队员机器人、Qrobot Alpha 等。除此之外，在 CES 2018 上，优必选还发布了新品双足机器人 Walker。知识产权方面，优必选共有国内外专利 308 项。 28、云从科技 云从科技成立于 2015 年，是从中国科学院孵化的人工智能企业，专注于人脸识别等计算机视觉技术研发。作为中国科学院战略性先导科技专项的唯一人脸识别团队，参与了人脸识别国标、部标、行标的起草与制定，并且拥有红外活体检测、公安部动态人脸识别课题、国家发改委人脸识别系统应用试点等多项成果。2017 年，云从科技成为人脸识别领域银行业和机场的第一大供应商，包括农行、建行、中行、交行等超过 100 家金融机构已采用公司产品。目前，云从科技已完成 25 亿元 B 轮融资。 29、中科虹霸 中科虹霸成立于 2006 年，是目前国内唯一掌握成熟虹膜识别技术的国家级高新技术企业，被列为北京市重大科技成果转化和产业化项目。中科虹霸专业从事生物特征识别、信息安全等领域的技术研究、产品开发与成果转化。目前，中科虹霸的虹膜识别产品已广泛应用于国家安全部门、军队门禁控制、金融安全管理、矿山安全生产等众多领域。2017 年 3 月，中科虹霸与北京市公安局达成合作，进行基于虹膜特征的重点人员生物数据建库及应用服务技术研究；10 月与公安部物证鉴定中心达成合作，建设全国公安虹膜数据和云服务比对平台。 30、追一科技 追一科技成立于 2016 年，主攻自然语言语义理解和对话机器人，通过将人工智能与企业场景深度结合，帮助企业构建人企交互智能化解决方案。其主要产品为 YiBot，YiBot 是一个基于对话机器人的智能服务系统，目前已应用到金融、互联网、出行、零售、地产、教育等多个领域。目前，追一科技已完成 2060 万美元 B 轮融资。 二、中国十大最强 AI 研究实力企业 1、阿里巴巴 阿里巴巴正在大力投入人工智能，基础研究与产业应用并重。2017 年 10 月成立了达摩院，旨在探索基础科学和颠覆式技术创新研究，人工智能是其中之一。另有 iDST（数据科学与技术研究院）、人工智能实验室等专向研究团队。2017年，阿里巴巴有数十篇论文被人工智能顶级学术会议收录。亚洲最大的云计算公司阿里云具备超大规模的计算设施，成为人工智能向各行各业融合的大平台，对外提供飞天、MaxCompute、PAI、AliGenie、AliOS 等服务。ET 城市大脑成为国家新一代人工智能开放创新平台，成为城市级人工智能应用典范。此外，ET 大脑系列（工业大脑、医疗大脑、环境大脑、航空大脑）及 AliOS 智联网操作系统等「产业 AI」方案密集落地真实行业场景，创造新价值。   百度的人工智能以百度大脑和智能云为基础，重点构建 DuerOS 智能语音交互平台和 Apollo 智能驾驶开放平台。在硬件基础方面, 百度从云计算中心到服务器、智能芯片等方面均有布局，并率先推出基于 FPGA 的云服务器。在平台建设方面, 主要为 PaddlePaddle、DuerOS 与 Apollo。在数据方面，作为搜索起家的互联网巨头, 基于互联网积累的线上数据是最大优势, 同时在本地生活、泛娱乐以及游戏、教育等领域的扩张延伸，同样积累起海量的用户数据。在应用方面，百度除了将人工智能技术应用于自身原有的搜索、信息流内容以及广告推荐等核心业务之外, 还在自动驾驶、虚拟助手、金融、医疗、智能硬件等众多领域早有布局。 3、华为 在人工智能上，华为最为人所知的就是 「诺亚方舟实验室」 。华为「诺亚方舟实验室」主要由五大部门组成： 自然语言处理和信息检索部门、 大规模数据挖掘和机器学习部门、 社交媒体和移动智能部门、人机交互系统部门、 机器学习理论部门。实验室的使命是通过数据挖掘、人工智能和相关领域的创新，为公司和社会做出重大贡献。2017 年 9 月 2 日，在德国柏林举行的 IFA 2017 展会上，华为正式发布了全球首款移动端 AI 芯片麒麟 970，填补了移动端对于机器学习任务加速的空白。 4、海康威视   海康威视成立于 2001 年，是以视频为核心的物联网解决方案提供商，在国内设有五大研发中心，全球员工超过 25000 人，其中研发人员占比超过 40%，研发投入占企业销售额的 7-8%。海康威视拥有视音频编解码、视频图像处理、视音频数据存储等核心技术，及云计算、大数据、深度学习等前瞻技术，针对公安、交通、司法、文教卫、金融、能源和智能楼宇等众多行业提供专业的细分产品、IVM 智能可视化管理解决方案和大数据服务。在视频监控行业之外，海康威视基于视频技术，将业务延伸到智能家居、工业自动化和汽车电子等行业，为持续发展打开新的空间。 5、京东 在刚刚过去的 2017 年，京东频频出手，不断挖掘人工智能顶级人才。不久之前，京东宣布 ACM SIGKDD 主席裴健加入京东，负责大数据平台与产品研发部。而在此之前，京东已经从亚马逊、IBM、微软挖走了薄列峰、周伯文、申元庆等顶级人才，分别负责京东金融、AI 平台、云计算等领域。从此布局来看，京东将在 AI 领域持续发力。从首个海外联合 AI 研究计划开始，以人工智能技术为核心，推动成本、效率和用户体验的不断优化，京东的技术驱动转型之路开始加速。 6、今日头条 今日头条成立于 2011 年，「今日头条」是一款基于数据挖掘技术的个性化推荐引擎产品，它为用户推荐有价值的、个性化的信息，AI 在创作、互动和分发三大环节都发挥了巨大作用。目前，今日头条已经在使用基于智能算法的写稿机器人，并且在财经报道、体育赛事报道等领域作，内容完全可以媲美人工编辑。今日头条的研发团队占比 30%，并在 2017 年 MS COCO 竞赛的 Places 项目中获得第二名。 7、科大讯飞 科大讯飞成立于 1999 年，是一家专业从事智能语音及语言技术、人工智能技术研究，软件及芯片产品开发，语音信息服务及电子政务系统集成的国家级骨干软件企业。科大讯飞的语音合成、语音识别、口语评测、机器翻译等智能语音与人工智能核心技术代表了世界最高水平。2017 年，哈工大讯飞实验室 (HFL) 获得斯坦福大学发起的 SQuAD(Stanford Question Answering Dataset) 机器阅读理解挑战赛全球第一名，这是中国本土研究机构首次取得赛事榜首；8 月，在国际医学影像领域的权威评测 LUNA 上，科大讯飞获得平均召回率 92.3% 的检测效果，以显著优势刷新世界记录。10 月，在国际自动驾驶领域权威评测集 Cityscapes 中，科大讯飞获得平均 81.4% 的精度，刷新了世界纪录。 8、搜狗 在搜索、输入法和浏览器之后，人工智能将是搜狗下一阶段的重要战略。语音交互和知识计算是搜狗的发展重心和技术长项，在 2017 年，搜狗机器翻译团队获得 WMT 2017 中英机器翻译冠军。搜狗在人工智能技术战略上有两大产品，知音 OS 和深智引擎（指知识计算）。知音 OS 是构架于知音引擎核心技术上的完整产品解决方案。深智引擎是知识计算平台，集成了包括智能问答、对话服务、机器翻译、图像识别与 OCR、语音识别与合成、智能客服等在内一系列以语言为核心的知识计算技术，帮助软硬件厂商方便快捷地完成产品的智能化升级。 9、腾讯 腾讯在人工智能方面已经先后成立了多个实验室：优图实验室、智能计算与搜索实验室、企业级人工智能实验室腾讯 AI Lab、微信智聆等，另外还和多所高校成立了联合实验室。其中，腾讯 AI Lab 在 WIDER FACE、MegaFace、Knowledge Base Population 等多项竞赛中获得多个项目的第一名。优图实验室国际权威人脸识别数据库 LFW、ICDAR 2015 Born Digital Images、国际权威的计算机视觉算法评测平台 KITTI 等多项竞赛中获得多个项目的第一名，同时， 2017年腾讯有数十篇论文被人工智能顶级学术会议收录。 在数据方面，腾讯在数据方面拥有无可比拟的优势, 同时基于现有的产品和用户, 可以快速找到 AI 落地的场景。在应用场景方面, 腾讯的人工智能技术最主要的依然是加持自有核心业务, 例如社交、游戏、内容推荐和平台型 AI 等，其余在医疗、金融、智能硬件、自动驾驶、工业互联网等方面也都有相应布局，并且首个 AI+ 医疗项目 「 腾讯觅影 」 入选为国家人工智能开放创新平台。 10、微软亚洲研究院 微软亚洲研究院是微软公司在亚太地区设立的研究机构，也是微软在美国本土以外规模最大的一个。目前，微软亚洲研究院共有 200 多名科学家以及 300 多名访问学者和实习生，主要从事五个领域的研究：自然用户界面、 新一代多媒体、以数字为中心的计算、互联网搜索与在线广告、计算机科学基础。SQuAD 竞赛的机器阅读理解项目中长期高居榜首。微软亚洲研究院已与 10 所中国高校建立了多个领域的联合实验室，其中 8 个已通过教育部审批，并被纳入「教育部重点实验室」管理体系，超过 480 项目由微软资助。除了支持联合实验室，微软还资助了来自 64 所大学的 1200 名教授参与的 880 多个涉及各类研究课题的合作项目。 三、中国十大最具潜力早期 AI 公司 1、爱笔智能 爱笔智能（Aibee）由原百度研究院院长林元庆博士于 2017 年 11 月成立。公司名称 Aibee 寓意 AI2B，意在利用 AI 技术深度赋能传统行业。公司将致力于搭建算法与数据闭环，打磨极致 AI 技术，在教育、金融、零售、房地产等领域提供 AI 服务，帮助各行业实现升级。Aibee 提供的 AI 服务跟据行业痛点，将计算机视觉、图像识别、语音识别等多重技术进行全方位融合，提供一整套解决方案。 公司于近日公布 1.65 亿天使融资，加速公司业务落地。 2、合刃科技 合刃科技（Coherent AI）由美国斯坦福大学教授及博士等海归成员回国于 2017 年 8 月成立，致力于全息全频机器视觉系统的商业化，为客户提供新一代机器视觉的核心产品、全套解决方案和服务。区别于传统的基于人类审美的照相机机器视觉，公司颠覆性地通过对 CMOS 图像传感器进行结构升级，软硬件一体化采集包括光强、相位、光谱、入射、偏振方向等多维全息光学信息。 此前在中国光谷 3551 国际创新创业大赛上获得总冠军。目前已获得百度风投、联想之星、长安私人资本、武汉光电工研院育成基金数千万人民币。 3、黑芝麻 黑芝麻智能科技（Black Sesame Technologies）成立于 2016 年 8 月，是一家专注提供图像处理和计算图像的软硬件解决方案及嵌入式感知平台的视觉感知核心技术开发与应用的高科技初创企业，产品及服务适用于自动驾驶、智能手机、智能医疗、安防等业务领域。现已在上海和美国加州硅谷成立研发部门，团队总人数超过 80 人，集合了图像系统领域和汽车领域专家。 公司于近日获得蔚来资本、芯动能投资基金、北极光创投近亿元人民币 A+ 轮融资。 4、小马智行 小马智行（Pony.ai）成立于 2016 年 12 月，是一家专注自动驾驶汽车研发与制造的创新科技企业，主营业务包括自动驾驶技术研发、无人车队改装、运营及增值服务等。其联合创始人兼 CEO 彭军和联合创始人兼 CTO 楼天城曾就职谷歌和百度的自动驾驶部门，担任核心工程师及架构师。公司于 2017 年 6 月获得加州路测许可证，已开展路测工作；并已于 2017 年 10 月成立中国总部及研发中心，计划在 2018 年第一季度在广州正式推出无人驾驶运营车队。 公司于近日获得来自由晨兴资本、君联资本联合领投，种子轮领投方红杉资本中国基金和跟投方 IDG 资本，以及弘泰资本、联想之星、普华资本、启宸资本、DCM、Comcast Ventures 和硅谷未来资本等其他跟投方的 1.12 亿美元 A 轮融资。 5、深动科技  深动科技（Deepmotion.ai）成立于 2017 年 7 月，主要提供无人驾驶 3D 感知、定位和高精地图构建方案。其创始团队来自微软研究院，曾参与过 Hololens 核心算法开发及微软认知服务研发，在立体视觉和深度学习领域拥有多年一线产品的研发经验。公司基于摄像头与 GPS、IMU 等多传感器融合的高精地图构建方案，能实现快速部署，自动处理地理信息并构建高精地图，极大提高了高精地图的采集效率。公司核心团队发表的多篇论文被 ICCV、AAAI 等会议接收。公司目前已接受知名美元基金的投资。 6、ThinkForce ThinkForce（熠知电子）成立于 2017 年 2 月，是一家由来自芯片设计、算法软件、系统开发领域等深厚工业界背景的上海交大校友创立的 AI 芯片创业公司，立志设计融合一流 AI 算法和先进制成工艺的智能芯片，并以此构建人工智能硬件平台，提供一站式行业应用解决方案。公司提出「算法即芯片」的概念，在芯片架构和 Firmware 上使用芯片虚拟化技术，在需要弹性计算的场景成倍提高芯片使用率。 公司在 2017 年完成三轮融资，并于 2017 年 12 月宣布完成由依图科技、云锋基金、红杉资本、高瓴资本参与的 4.5 亿元 A 轮融资，同期成立上海交大-依图-ThinkForce 视觉计算与应用联合实验室，主要研究高性能视觉计算算法、架构、芯片及实际行业应用。 7、芯仑光电 芯仑光电（Celepixel Technology）成立于 2017 年 7 月，是一家掌握集成化传感器系统解决方案和机器视觉平台化技术的科技公司。公司核心技术发明人为新加坡南洋理工教授，已有超过 15 年的图像芯片和混合模式集成电路设计开发经验。公司意旨创造高效的动态视觉图像处理系统，将先进的信号处理技术嵌入传感器芯片并将其与网络甚至是云基础设施相连接，推动机器视觉产业发生根本变化。未来公司将加速百万级像素动态视觉传感器（DVS）平台的搭建，推动与汽车和消费电子领域的应用项目的落地和合作。 此前公司 CeleX 系列动态图像处理传感器项目在 2017 年 ARM Demo Day 上获得安创第四期成长营创新奖，并已于近日获得百度资本领投的四千万人民币 Pre-A 轮融资。 8、一知智科 一知智科（Yiwise.ai）成立于 2017 年 8 月，是一家专注于自然语言处理领域的技术型人工智能公司。目前团队主要精力投入在中文自然语义理解的底层技术开发，包括：阅读理解、多轮对话、精准问答三个领域。同时公司在语义分析与理解、知识库构建与应用及跨媒体推理方向均有一定技术及研发能力积累，分别在知识库构建和阅读理解与语义检索上，与阿里巴巴、网易，以及腾讯、同花顺、微软有合作。未来其技术将应用于语音助手、智能客服、智能导诊等更多应用场景。公司核心成员主要来自浙江大学，并已在 AAAI、WWW、WSDM 等 CCF-A 类会议与期刊发表数十篇论文。公司已于 2017 年 9 月获得来自金沙江创投的千万级天使轮投资。 9、语知科技 语知 NLP 成立于 2017 年 7 月，是一家基于知网语言知识库独有的语义分析技术的 NLP 技术服务公司。公司核心产品「语知自然语言理解技术平台」，提供包括 NLP 概念本体库、NLP 中英文知识库、文本解析器、词语和文本相似度等基础功能，是满足用户对多语种自然语言语义分析需求的开放平台。用户能够基于平台对外提供的 API 接口实现搜索、推荐、舆情、挖掘等语义分析应用，也可以与公司合作定制有特色的语义分析解决方案。公司现已获得来自国家安全机构、司法机关等客户用于针对诈骗信息识别系统、盗窃罪判案辅助系统中自然语言处理技术支持的订单。 10、驭光科技 驭光科技成立于 2016 年 5 月，作为面向 AI 三维感知和信息获取的解决方案提供商，以赋予 AI 更好的感知三维真实世界的能力为目标，致力于智能 3D 视觉和 3D 传感的解决方案、软硬件系统、光学模组、衍射光学元件等的技术研究、产品开发和技术服务，产品广泛应用于智能终端、安防监控、机器人视觉、工业 3D 检测、AR/VR、辅助驾驶等众多领域。驭光是安卓手机人脸解锁方案的唯一国内器件供应商，为手机和其它领域的三维传感应用提供整体定制化解决方案。公司技术实力雄厚，团队成员均毕业于清华、MIT、NTU 等国内外名校，具有在 IBM 等全球领先的公司多年丰富的技术研究和产品研发经验。公司具有全球领先的微纳衍射光学元件设计制造技术、先进的智能三维传感系统及模组设计和集成技术，已申请 10 余项专利，与多家人工智能、智能终端、安防监控等领域的国内外知名企业开展深入合作。公司于 2017 年 11 月获得来自顺为资本和百度风投的千万美元 A 轮投资。 识别下方二维码，关注 「机器之能」 持续关注获奖公司及案例后续报道 "
366,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736316&idx=3&sn=64e629475cd2168f02573925975e8e38&chksm=871ac2c2b06d4bd4d126b0bc47fde14bfea662ec07ab5a482825f85cb9a03b621b5d8978f8a7&scene=27,业界 | 李飞飞、李佳宣布发布Cloud AutoML：AI技术「飞入寻常百姓家」,"李飞飞一直倡导AI民主化，今日谷歌云发布Cloud AutoML，希望帮助ML/AI专业知识和能力有限的企业也能够使用AI技术构建定制化AI模型。目前已有一万多家企业使用Cloud AutoML。 谷歌发布 Cloud AutoML，旨在帮助更多公司构建高质量定制化模型。李飞飞和李佳在相关博客中称：「Cloud AutoML 将帮助 AI 专家更加高产，不断拓展 AI 的新领域，帮助经验不足的工程师构建梦寐以求的强大 AI 系统。」 李佳在朋友圈中称： 今天我们 CloudAI 团队推出了 Cloud AutoML, 自动生成 ML 模型的技术。这是飞飞和我加入谷歌云以来的一个里程碑。迄今为止，我们团队推出了 10 多个 AI 产品，超过一万家公司在使用我们的产品。感恩团队的辛勤工作推动 AI 产品和技术发展！Cloud AutoML 是我们在推广 AI 技术的新尝试，为没有 ML 专业背景的公司量身打造。AI 赋能，愿更多的人能被 AI 惠及！ 李飞飞和李佳发布博客介绍 AutoML，机器之心对博客内容编译如下： 一年前我们加入 Google Cloud 时，就致力于 AI 民主化。我们的目标是降低入门门槛，使尽可能多的开发者、研究者和企业能够使用 AI。 谷歌云 AI 团队一直朝着这个目标前进，也做出了一些成绩。2017 年，我们发布 Google Cloud Machine Learning Engine，帮助具备机器学习专业知识的开发者轻松构建可在任意类型和规模的数据上运行的 ML 模型。我们展示了如何在预训练模型上构建现代机器学习服务，包括视觉、语音、NLP、翻译和 Dialogflow API，为商业应用带来更大的规模和更快的速度。我们的数据科学家和 ML 研究者社区 Kaggle 不断发展，现已有超过 100 万成员。今天，超过一万家企业在使用谷歌云 AI 服务，包括 Box、Rolls Royce Marine、Kewpie 和 Ocado。 但是我们还可以做得更多。目前，只有少数企业具备应用 ML 和 AI 进展的人才和财力。能够创建先进机器学习模型的人非常有限。而且即使你的公司里有 ML/AI 工程师，你仍然必须管控构建定制化 ML 模型所需的时间和复杂流程。尽管谷歌提供可用于多项具体任务的 API，提供预训练机器学习模型，但要实现「AI 人人可用」仍然有很长的路要走。 为了缩小差距，使每家公司都可以使用 AI，我们发布 Cloud AutoML。Cloud AutoML 使用谷歌的 learning2learn 和迁移学习等先进技术，帮助 ML 专业知识有限的公司构建高质量定制化模型。我们相信 Cloud AutoML 将帮助 AI 专家更加高产，不断拓展 AI 的新领域，帮助经验不足的工程师构建梦寐以求的强大 AI 系统。 我们发布的第一个 Cloud AutoML 是 Cloud AutoML Vision，帮助更快、更容易地构建图像识别 ML 模型。可拖放的界面使上传图像、训练管理模型，以及直接在谷歌云上部署训练模型变得更加容易。使用 Cloud AutoML Vision 分类 ImageNet 和 CIFAR 等流行的公开数据集的实践表明它比普通的 ML API 准确率更高，误分类更少。 Cloud AutoML Vision 还具备以下特性： 提高准确率 ：Cloud AutoML Vision 基于谷歌的先进图像识别方法构建，包括迁移学习和神经架构搜索技术。这意味着即使你的公司机器学习能力不足，也可以得到准确率更高的模型。 生产就绪模型的周转时间更快 ：在 Cloud AutoML 的帮助下，你可以在数分钟内创建简单的模型来试运行自己的 AI 应用，或者在一天时间内构建一个完整的生产就绪模型。 易用性 ：AutoML Vision 提供一个简单的用户界面，你可以指定数据，然后将其转换成满足个人需求的自定义高质量模型。 「Urban Outfitters 一直在寻找提升客户购物体验的新方法，」URBN 数据科学家 Alan Rosenwinkel 说，「创建和保持一套完整的产品属性对向客户提供相关产品的推荐、准确的搜索结果以及有效的产品过滤器至关重要；然而手动创建产品属性是很困难和费时的。为了解决这个问题，我们的团队评估了 Cloud AutoML，通过识别细微的产品特征，例如图案和领口风格，以自动化产品属性加工。Cloud AutoML 具有很大的潜力，可以帮助我们的客户发现心仪的产品，获得更好的产品推荐和搜索体验。」 迪士尼消费产品和互动媒体 CTO 和 SVP Mike White 说：「Cloud AutoML 可以帮助我们构建模型，用迪士尼的特征、产品类别和颜色标注我们的产品。这些标注被整合到我们的搜索引擎中，在 shopDisney 上获得了更好的用户体验，包括相关搜索结果、结果发现和产品推荐等方面。」 来自 Zoological Society of London（ZSL）的 Sophie Maxwell 称：「ZSL 是一个国际环保组织，致力于全世界的动物和生态环境保护，其中最重要的环节是追踪野生动物的数量，以了解它们的分布和人类对这些物种的影响。为了达到这个目的，ZSL 在野外部署了很多相机陷阱，在受到热或移动触发的时候拍摄经过的动物。这些设备捕捉到的几百万张照片需要进行手动分析和用相关物种标注，例如大象、狮子和长颈鹿等，这是一项劳力密集和代价昂贵的工作。ZSL 的 Conservation Technology Unit 和谷歌的 Cloud ML 团队有过密切的协作，以帮助发展这项激动人心的技术。ZSL 的目的是将图像标注自动化，以降低成本，进行更广泛的部署，从而更加深刻地理解如何有效地保护全世界的野生动物。」 如果你有兴趣尝试 AutoML Vision，可以在这个网站上申请访问授权：https://services.google.com/fb/forms/cloudautomlalphaprogram/。 AutoML Vision 是我们和 Google Brain 以及其它谷歌 AI 团队密切协作的结果，并且是多个开发中的 Cloud AutoML 产品之一。虽然在使 AI 更加普及的道路上，我们还处于起步阶段，但目前使用 Cloud AI 产品的客户数量已经超过了一万，这令我们深受鼓舞。我们希望 Cloud AutoML 的发布有助于发现 AI 的更多商业用途。 Learning Transferable Architectures for Scalable Image Recognition (https://arxiv.org/pdf/1707.07012.pdf), Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le. Arxiv, 2017. Progressive Neural Architecture Search (https://arxiv.org/abs/1712.00559), Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy, Arxiv, 2017. Large-Scale Evolution of Imfiers (https://arxiv.org/abs/1703.01041), Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc Le, Alex Kurakin. International Conference on Machine Learning, 2017. Neural Architecture Search with Reinforcement Learning (https://arxiv.org/abs/1611.01578), Barret Zoph, Quoc V. Le. International Conference on Learning Representations, 2017. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning (https://arxiv.org/abs/1602.07261), Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. AAAI, 2017. Bayesian Optimization for a Better Dessert (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf), Benjamin Solnik, Daniel Golovin (https://research.google.com/pubs/DanielGolovin.html),Greg Kochanski (https://research.google.com/pubs/105545.html), John Elliot Karro (https://research.google.com/pubs/105483.html), Subhodeep Moitra (https://research.google.com/pubs/SubhodeepMoitra.html), D. Sculley (https://research.google.com/pubs/author38217.html). NIPS, Workshop on Bayesian Optimization, 2017. 原文链接：https://blog.google/topics/google-cloud/cloud-automl-making-ai-accessible-every-business/ "
367,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736316&idx=2&sn=5ff27184029d9f1387a3cffafa931db7&chksm=871ac2c2b06d4bd405a8c2522c14430b34ab18f00dcac69d10ac47588386fe07419aaf55a2c8&scene=27,学界 | AAAI 2018获奖论文提前揭晓：两大奖项花落阿尔伯塔、牛津,"AAAI 2018 大会即将在 2 月 2 日于美国新奥尔良开幕。但在此之前，本届大会的获奖论文已经放出。据机器之心刚刚获得的消息，阿尔伯塔大学 Martin Müller 等人的工作获得了杰出论文（Outstanding Paper），而牛津大学 Shimon Whiteson 等人的研究获得了杰出学生论文（Outstanding Student Paper）奖。 作为人工智能领域的顶级国际会议，AAAI 大会每年举办一次。今年 AAAI 会议将于 2 月 2 日至 2 月 7 日在美国新奥尔良举办。不久之前，AAAI 2018 公布论文接收列表，国内有众多来自学界、产业界的论文被接收。据机器之心了解，阿里巴巴共有 11 篇 AAAI 2018 接收论文，腾讯 AI Lab 11 篇接收论文，之前机器之心也曾介绍过百度的 AAAI 论文《Multi-channel Encoder for Neural Machine Translation》。此外，在机器之心 AAAI 2018 论文专栏中，我们也介绍了来自南京大学、北京理工大学、浙江大学的 AAAI 2018 论文。读者们想要了解更多有关 AAAI 2018 的信息，可按照文末形式交流讨论。 虽然距离 AAAI 2018 大会的召开还有半个多月，但本届大会的获奖论文信息已经陆续出炉。阿尔伯塔大学论文《Memory-Augmented Monte Carlo Tree Search》获得杰出论文奖，牛津大学《Counterfactual Multi-Agent Policy Gradients》论文获得杰出学生论文奖。以下，我们将对这两篇获奖论文进行简单介绍： 杰出论文：Memory-Augmented Monte Carlo Tree Search 据阿尔伯塔大学 Martin Müller 教授的消息，该大学提交的论文《Memory-Augmented Monte Carlo Tree Search》获得了 AAAI 2018 大会的杰出论文奖。该论文作者分别为 Chenjun Xiao、梅劲骋与 Martin Müller。其中，梅劲骋本科毕业于华南理工大学，研究生赴上海交通大学，师从计算机系吕宝粮教授。2015 年起，他来到阿尔伯塔大学攻读博士，师从 Dale Schuurmans 教授。 Chenjun Xiao 研究生与博士阶段均师从于 Martin Müller 教授。 值得一提的是，该论文的导师，阿尔伯塔大学教授 Martin Müller 是计算机围棋顶级专家。Müller 教授所带领的团队在博弈树搜索和规划的蒙特卡洛方法、大规模并行搜索和组合博弈论方面颇有建树。实际上，DeepMind 著名围棋程序 AlphaGo 的设计研发主导人物 David Silver 和黄士杰（Aja Huang）（他们分别是 AlphaGo Nature 论文的第一作者和第二作者，也名列于最近的   论文中）都曾师从于他。 在去年 5 月的机器之心全球机器智能峰会（GMIS 2017）上，Martin Müller 曾以重磅嘉宾的身份在会上发表了主题为《深度学习时代的启发式搜索》的演讲，并 ，做出精彩点评。 目前这篇论文的内容还未公开，我们将持续跟进该研究的进展。 杰出学生论文：Counterfactual Multi-Agent Policy Gradients AAAI 2018 杰出学生论文奖则由牛津大学获得，该论文作者分别为 Jakob N. Foerster、Gregory Farquhar、Triantafyllos Afouras、Nantas Nardelli 与 Shimon Whiteson，主要研究强化学习中的多智能体协同，其提出的中心化评估新方法在《星际争霸》测试基准中获得了很好的效果。以下，我们将简要介绍这篇论文的内容。 许多复杂的强化学习（RL）问题，如自动驾驶汽车的协调（Cao et al. 2013）、网络分组分配（Ye, Zhang, and Yang 2015）和分布式物流（Ying and Dayong 2005）等问题都很自然地建模为多智能体协作系统。然而，针对单个智能体设计的强化学习在这一类问题上通常表现不佳，因为智能体的联合动作空间会随着智能体的增加而呈指数级增长。 为了克服这种复杂性，我们往往有必要采取去中心化策略（decentralised policies），即每个智能体仅根据局部的动作观察历史而选择它们自己的动作。此外，即使在联合动作空间不是特别大的情况，学习期间的局部可观察性和通信约束也可能需要使用去中心化策略。 因此，我们非常有必要构建一种新型的强化学习方法以高效地学习去中心化策略（decentralised policies）。在某些情况下，学习本身也可能需要去中心化。然而在很多情况中，学习可以在模拟器或虚拟实验室中进行，因此智能体能获得额外的状态信息和自由地通信。 去中心化策略的集中式训练是多智能体规划的标准范式（Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016），并且这种方法最近在深度强化学习社区（Foerster et al. 2016; Jorge, Kageb ack, and Gustavsson 2016）获得了广泛的研究。但是，我们该如何更好地利用集中式学习的优势依然是一个开放性问题。 另一个关键的挑战是多智能体间的信度分配（credit assignment；Chang, Ho, and Kaelbling 2003）：在合作的环境下，联合动作通常只能产生全局奖励，因此每个智能体都很难判断它自己对团队的贡献。然而，这些奖励在合作的情况下通常是不可获取的，且往往不能鼓励单个智能体为更大的集体利益而做出牺牲。这种现象通常会在充满挑战的任务中大幅度地阻碍所智能体的学习，即使相对较少的智能体也会产生这种显现。 在本论文中，为了解决这些问题，我们提出了一种新的多智能体 RL 方法，被称作反事实多智能体（COMA）策略梯度。COMA 采用 actor-critic 方法（Konda and Tsitsiklis 2000），其中 actor（即策略）按照由 critic 评估的梯度而训练。COMA 主要基于以下三个想法。 首先，COMA 使用一个中心化的 critic（只用于学习期间），同时执行期间只需要 actor。由于学习是中心化的，因此我们可以使用中心化的 critic 限定联合动作和所有可用的状态信息，同时每个智能体的策略只限定在其自己的动作观察历史上。 第二，COMA 使用反事实基线（counterfactual baseline）。这一想法的灵感来自差异奖励（difference rewards；Wolpert and Tumer 2002; Tumer and Agogino 2007），其中每个智能体从目标奖励中学习，目标奖励将比较全局奖励与当那个智能体的动作被默认动作替代时接收的奖励。尽管差异奖励是执行多智能体信度分配（credit assignment）的有效方法，它们需要访问模拟器或者评估的奖励函数，并且总体上不清楚如何选择默认动作。COMA 通过使用中心化的 critic 计算一个特定智能体的优势函数而解决了这一问题，该函数对比了当前联合动作的评估回报与边际化单一智能体动作的反事实基线，同时保持其他智能体的动作不变。 这类似于计算一个贵族效用（aristocrat utility；Wolpert and Tumer 2002），但避免了策略和效用函数之间的递归性相互依赖问题，这主要是因为反事实基线（counterfactual baseline）对策略梯度的期望贡献为零。因此，COMA 并不依赖额外的模拟、近似或关于适当默认行动的假设，它会依赖于集中式的 critic 为每一个智能体计算一个分离的基线，以推理特定智能体动作改变的反事实。 第三，COMA 使用一个 critic 表征以高效地计算反事实基线。在单次前向传播中，以所有其它智能体的动作为条件，它会为给定智能体的所有不同动作计算 Q-values。因为单个集中式的 critic 可用于全部的智能体，所有智能体的所有 Q-values 都能在一个批量的单次前向传播中计算。 我们在星际争霸单位微操的测试平台上评估了 COMA，近来它已经因为较高的随机性、大型状态动作空间和延迟奖励成为了挑战性的强化学习基准任务。以前的研究工作（Usunier et al. 2016; Peng et al. 2017）利用集中控制策略而对整个状态进行调控，因此其在宏观动作上有非常强劲的表现，这种策略会利用星际争霸的内置 planner 而结合移动与攻击动作。产生一个有意义的去中心化基准即使对于较少智能体也被证明是非常有挑战性的，因此我们提出了一种大规模减少每个智能体的视野以移除访问宏观动作的变体方法。 我们在这个新基准上的经验结果表示，COMA 相比于其它 actor-critic 方法和 COMA 本身的切除版可以显著地提升性能。此外，COMA 的最佳智能体可以和集中控制器的顶尖结果相匹配，因此它也能获得全部的状态信息和宏观动作。 论文：Counterfactual Multi-Agent Policy Gradients 论文链接：http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf 很多现实世界的问题，如网络分组路由和自动驾驶汽车协调，都是被自然地建模为多智能体合作系统的。我们急需一种新的强化学习方式来有效学习这种系统的去中心策略。对此，我们提出了一种新的反事实多智能体（COMA）策略梯度方法。COMA 使用中心化评论来估算 Q 函数和去中心参与者的行为来优化智能体策略。此外，为了解决多智能体信度分配的挑战，它使用了一个反事实基线以边缘化单个智能体的行为，同时其它智能体的行为保持固定。COMA 还使用了一种关键的表征方法，允许单次前向传播就可以高效地计算反事实基线。我们使用具备显著的局部可观测性的去中心变体，在星际争霸单位微操的测试平台上评估了 COMA。实验证明，COMA 显著地提升了平均性能，在相同的设定下超越了其它多智能体 actor critic 方法，且最佳性能的智能体可与当前最佳的中心化控制器相比较，并能获得全部状态的信息访问。 "
368,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736276&idx=5&sn=115d32f42c5848b9313c4d0fbe6d044b&chksm=871ac2eab06d4bfc4bde5ccf2b6e0c2a4fd7a800d26092375ce089a80145a39cf6ed483d04fe&scene=27,AAAI 2018 | 双流束网络：北理工提出深度立体匹配新方法,"作者：Lidong Yu 等 由北京理工大学贾云得教授研究组提出的立体匹配新方法着重于成本聚合问题，在 KITTI 和 Scene Flow 基准测试中超越了此前业内的最佳水平。本文已提交今年 2 月举行的 AAAI 2018 大会，并成为 Spotlight poster 论文。 立体匹配（Stereo matching）是计算机视觉社区研究的一个基础问题。立体匹配的研究目标是计算由立体照相机收集的图像的视差图（disparity map）。这种视差图在 3D 场景重建、机器人、自动驾驶领域都有普遍的应用。随着大规模数据集的出现、计算力的快速发展，深度神经网络经验证在立体匹配中有所成效。目前，许多顶尖的方法通过学习稳健地局部特征或者相似性测量作为成本计算来提升表现（(Zbontar and LeCun 2015; Luo, Schwing, and Urtasun 2016; Shaked and Wolf 2016)。然而，这些方法在少纹理区域和遮挡边界区域问题上仍有很大的困难，这主要是因为在成本计算过程中看到的视野受限。 为了处理成本计算结果的不匹配值（成本体积），成本聚合在传统的立体匹配方法中是不可或缺的。成本聚合应用于成本体积，用于改正聚合匹配成本时出现的不匹配值。它通常是通过在一个支持区域中的常数差异上加和或平均匹配成本完成的（Yang 2012；Min，Lu and Do 2011；Tombari et al. 2008）。然而，传统的成本聚合方法受到浅层手工设计的限制，无法进行聚合。它们无法在保持局部适值的情况下有效形成全局引导。在本论文中，北京理工大学的研究人员提出了一种基于学习的成本聚合方法，可在全局视野和局部适值之间做出有效平衡，其方法使用了全新的双流束神经网络。 本论文提出的成本聚合能够以端到端的形式整合其他深度立体匹配流程，因为它是以整个网络子架构的形式构建的。通过基于学习的成本聚合，立体匹配流程的端到端训练不仅可以学习成本计算的特征和相似性，也可以表现为成本聚合。新方法的架构和典型的深度立体流程之间的对比如图 1 所示。我们可以看到，基于学习的成本聚合是通过一个双流束网络（twostream network）以明确方式实现的。 成本聚合过程被重述为一个生成潜在成本聚合结果（提出建议，选择其中最佳）的学习机制。因此，基于学习的成本聚合由双流束网络执行：一个流用于生成建议，另一个流用于评估建议。第一个流根据成本体积（由成本计算算出）计算出的潜在聚合结果得出局部适值。生成是通过卷积操作沿着成本体积的三个维度来执行的，其中结合了空间和深度上的信息。第二个流通过评估每个建议来为成本聚合带来全局视角引导，这个过程是通过轻卷积网络将低阶结构信息作为建议的评估标准来获得的。由于结构信息只包含独立于深度的 2D 信息，因此引导（guidance）的深度维度不变。因此，对每个建议的不同评估遵循相同的引导。在对每个建议评估之后，模型使用胜者通吃策略来选出最佳聚合值，形成聚合成本值。 研究人员提出的架构在 Scene Flow（Mayer 等人，2016）和 KITTI 基准测试上（Menze & Geiger，2015；Geiger，Lenz and Urtasun，2012）展示了很高的准确度。该研究主要贡献为以下三点。 该研究是第一个在基于学习的立体匹配方案中明确对成本聚合建模的方法。研究人员使用生成学习过程重新形式化成本聚合，并让算法在成本聚合建议中选择。 研究人员提出了一种全新的双流束网络来生成和选择成本聚合建议。这种双流束网络保留了丰富的语义信息，同时带来了低级结构信息，证明了将高级特征与低级特征融合的能力。 新提出的基于学习的成本聚合是端到端可训练的深度立体匹配流程子架构。它适合于没有成本聚合的流程，进一步提升了准确率。 图 2：北理工提出的立体匹配管道与基于学习的成本聚合。 不同的颜色代表不同数据的大小：蓝色对应 W × H × C，橘色对应 D × H × W × C，绿色对应 H × W × G，紫色对应 D × H × W × G。成本计算步骤分为三个部分。A1 是使用残差 Siamese 网络的特征提取子网络。A2 是特征体积构建部分，细节如图 3 所示。A3 计算使用 3D 卷积网络计算特征体积的相似，输出初始成本体积。基于学习的成本聚合是由一个双流束网络座位整个流程的子架构实现的。指导流程在 B1。建议流程在 B2。成本聚合过程是通过赢者通吃的策略来选择最佳建议。最后，使用 soft-argmin 函数来计算视差图。 网络架构 作为像素级的匹配任务，立体匹配需要计算左图像中每个像素与右图像中 D 个对应像素之间的相似性，其中 D 是最大视差。计算匹配成本可形成成本体积 C0(h,w,d)。立体匹配管道和其建议的成本聚合由端到端可训练网络执行。与使用黑箱网络的方法相比，我们从经典立体匹配算法（Scharstein & Szeliski，2002）中获取灵感，提出了双流束网络的成本聚合方法。 论文：Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture 论文链接：https://arxiv.org/abs/1801.04065 深度神经网络在立体匹配（stereo matching）上表现出色。很多研究着重于特征提取和相似度测量，却忽视了至关重要的成本聚合。在本文中，我们提出了一种基于学习的成本聚合方法，通过在端到端训练流程中的新型子体系结构进行立体匹配。我们将成本聚合重新形式化为生成和选择建议，并表明聚合结果的成本聚合学习过程。成本聚合的子架构是通过一个双流束网络实现的，一个用于生成成本汇总建议，另一个用于选择建议。选择的标准由轻卷积网络的低阶信息获取结构确定。双流束网络提供了成本聚合的全局视图指导，避免了由于匹配计算成本造成视角限制出现的不匹配值。在 KITTI 和 Scene Flow 中的测试表明我们的方法超越了此前业内的最佳水平。 "
369,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736276&idx=4&sn=7918365b4b7de6544944189ee43f5fab&chksm=871ac2eab06d4bfc1987834ce2827d26fd0f7a32735d4846567dc93f6c3cc84db9b2bf717d4b&scene=27,入门 | 从遗传算法到强化学习，一文介绍五大生物启发式学习算法,作者：Luke James 本文是作者献上的一部「野外纪录片」，介绍了五个直接受大自然启发而产生的人工智能算法：人工神经网络、遗传算法、集群智能、强化学习和人工免疫系统。 在当今技术背景之下，人工智能的发展催生出很多美好事物。人类花费数十年研究如何优化数学计算以使复杂的学习算法运转起来，此外，我们还已经超越自身的物种，正努力创造新一代智能体。大自然及其所包含的一切，深深地植根于人工智能的运作之中，而这正是本文的主题。 David Attenborough 的野生动物纪录片令人震撼，他们通过高清晰的细节记录了地球上诸多物种的行为和特征，如何融入自然生态系统，并协同共存使得自然生机勃勃——使其成为「地球」。我虽然不是 David Attenborough，但是也要献上一部「野生动物纪录片」，介绍那些直接受大自然启发而产生的人工智能算法。在此之前，我首先介绍两个算法概念：搜索/路径寻找和预测建模。 搜索（路径寻找）算法 搜索算法本质上是一种程序，被设计用来发现通往目标的最优/最短的路径。例如，旅行推销员问题是一个典型的搜索优化问题，其中包含给定的一系列城市及其之间的距离。你必须为推销员找到最短路径，同时每个城市只经过一次，从而最小化旅行时间和开销（确保你回到起点城市）。这一问题的真实应用是运货车。假设伦敦有 100 个人在线下单，所有箱子要装进货车，快递员现在必须计算最高效的路线（平衡距离/所花费的时间），以便从仓库交付这些包裹（最终还要返回仓库），确保公司把时间和金钱消耗降到最低。 预测建模算法 如今，有关预测建模的炒作是最多的。全世界的数据科学正在强烈呼吁「神经网络」，而像谷歌这样的大公司也正努力通过人工智能及其各种不同变体解决世界上的难题。预测建模本质上借助统计学来预测结果。你经常听到数据科学家试图解决两类预测建模问题：回归和分类。回归是找到两组变量关联性的暗黑艺术；分类是确定数据集属于不同组的概率的过程。 1. 人工神经网络 算法类型：预测建模 生物启发：认知脑功能（神经元） 用例：情感分析、图像识别/检测、语言修正、机器人 让我们从最基础的人工智能算法开始。神经网络是人工智能子范畴机器学习的一部分。神经网络的设计目的是在神经元层面上模拟大脑功能，通过轴突和树突的交互在系统之中把信息传递过一系列的层，生成一个预测性的输出。每个层提供一个数据表征的额外层，并允许你建模最复杂的问题。 神经网络很可能是使用最为广泛的机器学习算法，并且是目前为止数据科学和深度学习的最热趋势。这一概念最初起始于 1958 年的感知机，后来 Geoffrey Hinton 完善了它，并在谷歌、Facebook 等公司中大为流行。神经网络可用于解决一系列问题，比如自然语言处理、视觉识别。这一监督式学习算法可以解决回归和分类问题，其实例可在常规的消费产品中发现，比如智能手机和智能家居设备。 2. 遗传算法 算法类型：搜索/路径寻找 生物启发：适者生存/进化（细胞繁殖） 用例：数据挖掘/分析、机器人、制造/设计、流程优化 遗传算法在连续的一代代个体之间采取适者生存的进化方法，以期解决搜索问题。每一代包含一群模拟 DNA 染色体的字符串。群体中的每个个体表征搜索空间中的一点，因此每个都是可能的候选方案。为了提升方案数量，我们使个体经历一次进化过程。 群体之中的每个个体将会竞争资源和配偶。 相比于表现差的个体，每次竞争中的最成功个体将（通常）产生更多个体。 来自更多「理想」候选的基因在群体中传播，因此这些优秀的父母往往会产生潜力更大的后代。 3. 群集／集群智能（SWARM/COLLECTIVE INTELLIGENCE） 蚁群优化算法示例——一种集群智能算法 算法类型：搜索／路径寻找 生物启发：蚁群／鱼群／鸟群 用例：机器人、视频游戏 AI、制造业、路径规划 蚁群优化（Ant Colony Optimisation）和粒子群优化（Particle Swarm Optimisation）是两种最广为人知的「集群智能」算法。从基础层面上来看，这些算法都使用了多智能体。每个智能体执行非常基础的动作，合起来就是更复杂、更即时的动作，可用于解决问题。 蚁群优化（ACO）与粒子群优化（PSO）不同。二者的目的都是执行即时动作，但采用的是两种不同方式。ACO 与真实蚁群类似，利用信息激素指导单个智能体走最短的路径。最初，随机信息激素在问题空间中初始化。单个智能体开始遍历搜索空间，边走边洒下信息激素。信息激素在每个时间步中按一定速率衰减。单个智能体根据前方的信息激素强度决定遍历搜索空间的路径。某个方向的信息激素强度越大，智能体越可能朝这个方向前进。全局最优方案就是具备最强信息激素的路径。 PSO 更关注整体方向。多个智能体初始化，并按随机方向前进。每个时间步中，每个智能体需要就是否改变方向作出决策，决策基于全局最优解的方向、局部最优解的方向和当前方向。新方向通常是以上三个值的最优「权衡」结果。 4. 强化学习 算法类型：预测建模 生物启发：经典条件反射 用例：视频游戏、自动驾驶汽车、生产线软件、财务系统 强化学习受到心理学和经典条件反射的启发，为智能体的积极动作给予正值反应。学习强化学习的概念通常比学习流行的经典条件反射示例「巴甫洛夫的狗」更加简单。该示例是 1890 年代俄国心理学家伊万·巴甫洛夫执行的研究，旨在观察狗对食物的唾液分泌。详细解释可参阅：https://www.simplypsychology.org/pavlov.html。本质上，如果强化学习智能体执行了一个好的动作，即该动作有助于完成要求任务，则它会得到奖励。智能体将使用策略来学习在每一步中最大化奖励。将原始输入应用到算法中使得智能体开发出自己对问题的感知，以及如何以最高效的方式解决问题。 RL 算法常常与其他机器学习技术（如神经网络）一同使用，通常称为深度强化学习。神经网络通常用于评估 RL 智能体作出某个决策后所获得的奖励。DeepMind 在这方面取得了很大成果，它使用深度 Q 学习方法解决更通用的问题（如利用算法的能力玩 Atari 游戏、战胜围棋世界冠军）。DeepMind 现在在研究更复杂的游戏，如星际争霸 2。 Q 学习是强化学习算法的无模型版本，可用于对任意有限马尔可夫决策过程寻找最优的动作选择策略。程序初始化时，每个动作-价值对的 Q 值由开发者定义，并由 RL 算法在每个时间步进行更新。下图展示了 Q 值的更新公式。 5. 人工免疫系统 人工免疫系统组件 算法类型：预测建模 生物启发：免疫系统 用例：安全软件、自动导航系统、调度系统、故障检测软件 免疫系统通过免疫应答机制保护身体免受病原体等的侵袭。人工免疫系统（AIS）是一种适应性系统，受启发于理论免疫学和免疫功能在问题求解中的应用。AIS 是生物启发计算和自然计算的分支，与机器学习和人工智能联系紧密。以下算法常用于 AIS： 克隆选择 树突状细胞 负选择 人工免疫识别 和生物免疫系统一样，AIS 能够将所有「细胞」分类为「自己」或「非己」细胞。智能的分布式任务组（distributed task force）用于对所有细胞执行动作。免疫系统中最重要的两种细胞是 T 细胞和 B 细胞。T 细胞有三种类型：激活 B 细胞、摧毁入侵者、调节机体免疫问题。B 细胞生成抗体。人工免疫系统通常用于监控入侵检测，从而抵御网络攻击，通常被整合进企业级软件中。与上文提到的其他算法不同，这方面的在线免费学习资料较少，而且可能也是发展最慢的。 本文介绍了 5 种受生物启发的技术。影响 AI 系统的生物启发算法还有很多，欢迎分享。 
370,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736276&idx=3&sn=a8210f8f2fbc9e5472d5ec7fc6c9de6e&chksm=871ac2eab06d4bfc6d77c33bf44df5b801bd5d447674a844e7ee90ba56d3976fbdd1a9dff6b9&scene=27,业界 | 百度成立数据可视化实验室，发布深度学习可视化平台 Visual DL,据百度技术经理祖明的知乎文章介绍，2018 年 1 月 16 日，百度发布开源产品 ECharts（echarts.baidu.com）的最新大版本 4.0，新版本在产品的性能、功能、易用性等各个方面进行了全面提升。此外，百度还一起发布了 ECharts GL 1.0 正式版，ZRender 4.0 全新版本，WebGL 框架 ClayGL、深度学习框架 VisualDL 等数据可视化产品。祖明还提到，随着这些产品的发布，百度正式公布全新升级的数据可视化品牌----「百度数据可视化实验室」。本文对深度学习可视化工具 VisualDL 做了介绍。 VisualDL 项目地址：https://github.com/PaddlePaddle/VisualDL VisualDL 是一个深度学习可视化工具，可以帮助开发者设计深度学习方面的项目，它目前囊括了标量特征、参数分布、模型结构和图像可视化等特征。该项目的 GitHub 介绍表示这个可视化工具正在快速发展，新的功能也将不断添加到里面。 据祖明介绍，从去年开始，团队和百度 PaddlePaddle 深度合作，打造了一款深度学习可视化平台 VisualDL，也在这次一同发布，希望通过可视化的方法将模型训练过程中的各个参数以及计算的数据流图实时地展现出来，以帮助模型训练者更好的理解、调试、优化模型。用户只需要通过 Visual DL 提供的接口将模型相关的各种参数数据写入日志中，然后 VisualDL 会读取日志中的模型相关数据将其展示出来，这些数据包括模型训练过程中的各种定量的度量、用户传入的或者中间训练过程生成的各种图片、以及神经网络训练过程中的计算数据流图。 目前 VisualDL 提供四个组件，包括计算图、标量、图像和直方图，这和 TensorFlow 中的 TensorBoard 有一些相似。它们都希望追踪神经网络在整个训练过程中的信息，例如迭代的过程中每一层参数是如何变化与分布、损失函数在迭代中的变化与整个计算图的可视化等。以下是 VisualDL 四个组件的简要介绍： 计算图（Graph） VisualDL 提供的可视化计算图是与 ONNX（Open Neural Network Exchange）相兼容的，通过与 Python SDK 协作，VisualDL 能与最主要的 DNN 框架相匹配，包括 PaddlePaddle、PyTorch 和 MXNet。下图展示了 VisualDL 的计算图可视化： 标量（Scalar） 标量可用来展示训练迭代过程中损失函数的变化趋势。 图像（Image） 图像可用来可视化任何张量或中间生成的图像。 直方图（Histogram） 直方图可用来可视化参数的分布或任何张量的变化趋势等。 VisualDL 的使用 VisualDL 同时提供了 Python SDK 和 C++ SDK，因此我们可以快速地可视化数据，如下展示了如何用 Python 创建一个简单的标量可视化，并且我们从不同的时间戳中插入数据。 在训练过程生成了一些日志后，我们可以登录控制面板并查看实时的数据可视化效果，登录的方式也和 TensorBoard 类似，只需要在命令窗口中输入以下命令： 当然，我们也能更改登录端口或远程登陆的主机的 IP 地址。读者若需要了解这一部分的具体过程和安装方法等内容，可详细查阅该 GitHub 项目的说明。 
371,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736276&idx=2&sn=c17a17bb1da728a7816ca3a831ed7651&chksm=871ac2eab06d4bfca1ace8b09aeb5d61a568d7ac026ad945618cb1b1672bca152de4cceab4d3&scene=27,专访 | 张胜誉转身业界，希望找到符合腾讯特点的量子研究发展模式,「国内外企业的量子计算团队各有各的模式，我们希望能找到适合自己特点的发展模式，」腾讯量子实验室杰出科学家、原香港中文大学教授张胜誉说。针对腾讯量子实验室的搭建与规划、量子计算与人工智能基础研究的机遇与挑战等问题，他向机器之心进行了深入分享。 又有一家中国互联网巨头挺进量子计算的基础研究。 不久前，在腾讯举办的学术&工业交流会议（TSAIC）上，公司筹建中的量子实验室首次正式亮相。 近年来，传统计算机的性能增长越来越困难。探索全新物理原理的高性能计算技术的需求应运而生。科学家认为，量子计算机会成为未来科技的引擎，极大改变人类日常生活。 尽管目前世界上还没有一台真正意义上的大型通用量子计算机，通往梦想之路仍然漫长也充满挑战，但这并不影响一些高科技公司「未雨绸缪」。 过去的 2017 年，我们见证了各国将量子计算列入国家战略并大力投入量子计算机研究的蓬勃态势，同时也目睹谷歌、IBM、英特尔、微软等科技巨头以及 Rigetti、IonQ、Quantum Circuits 等量子计算创业新秀在这场举世瞩目的「量子霸权」争夺赛上的激烈角逐，量子计算的潜能和重要性正逐步被更多人认识并关注。 同时，正如谷歌量子人工智能实验室的初衷之一——将机器学习与量子计算机进行整合，不少着眼量子机器学习的企业级量子研究室对这一研究方向表示看好，计划打造量子神经网络，或量子化的机器学习算法，用来缩短训练现有经典神经网络所需的时间，甚至开发新型的机器学习算法。 量子物理与机器学习的结合给突破经典机器学习模型的技术瓶颈带来了新的可能，受量子力学原理启发进行改造的经典计算模型则有望在经典计算机上就能实现运算的指数级加速，使用量子计算进行机器学习则可能是下一代「杀手级技术」。 「国内外企业的量子计算团队各有各的模式，我们希望能找到适合自己特点的发展模式，」腾讯量子实验室杰出科学家、香港中文大学教授张胜誉说。 针对腾讯量子实验室的搭建与规划、量子计算与人工智能基础研究的机遇与挑战等问题，张胜誉向机器之心进行了深入分享。 以下为采访实录，机器之心进行了不改变原意的整理。 机器之心：有观点认为国内企业做量子计算研究，在某种层面上是「赶时髦」，效仿国外科技巨头。你怎样看？ 张胜誉 ：腾讯以企业的长期健康发展为目的，不必赶时髦，也不必避时髦。 市值上，中国互联网公司已经具有举足轻重世界地位。其中腾讯市值最大，位居世界前五。但在底层基础研究投入上，中国互联网公司和一些国外公司相比还有一定差距。 这其实也很自然，在成长和快速发展的初期阶段，首先要保证好生存。随着逐步进入稳健期，腾讯越来越重视科技和文化投入。事实上，中国几家大的互联网公司都开始尝试一些基础科学研究。 重要的是怎么做。每个公司都需要结合自己不同业务场景，但是基本的一点是，做研究不是做秀，需要长期认真，踏实地去做。 而这一点，我对腾讯特别有信心。 机器之心：与这个领域的其他「参赛者」相比，腾讯会在多大程度上对量子计算的基础研究表示重视和支持？ 张胜誉 ：我也有很直接地问过 一位集团高层，「腾讯是否真正能在不管商业情况的前提下，坚持支持小规模高质量的基础研究工作？」 他的回答让我很赞赏。他认为这个事情我并不用担心，他个人和公司在看中量子计算这个方向后，都很愿意支持。腾讯的风格是比较踏实和容忍的。他甚至和我说，你招人的话，更需要担心的是另外一个方面——如果招来一个很聪明且非常努力工作的人，他在看到腾讯内部其他很聪明也很努力工作的员工做的产品能很快落地、很快被应用，并获得了巨大的商业回报以及个人回报后，还能否继续沉得住气做他的基础研究。 所以我们需要担心的是，候选者是否真正对基础和应用研究感兴趣，尤其是具备做研究工作的深入程度，并能够持续保持这个优秀品质。这样的回答让我相信，腾讯有这个决心和耐心。我想时间会慢慢告诉我们，哪些公司有更长远的视野，做一些更具战略布局的基础研究投入。 机器之心：能够获得真正意义上的支持吗？ 张胜誉 ：这是肯定的。 虽然腾讯在研发产品上是快速出结果，而量子方向从研究到最后有可能出现落地的应用的周期会非常长，但我个人对这方面顾虑不是很大。 我以前做量子算法，研究哪些问题有很好的量子算法进行加速运算，我知道这里面有很多困难，甚至即使你有好的量子计算机作为硬件算力平台，面临的困难依旧很多。所以腾讯高层是充分了解这些困难，没有期望在短期内就得到商业回报，而是愿意提早布局，长期投入。 机器之心：会对标国外量子实验室吗？ 张胜誉 ：我们希望能找到符合自身特点的发展模式。 国内外企业的量子计算团队都有各自模式。其中，一个很大的分野在于是否构建自己的硬件团队。如果构建硬件团队，全部投到一个方案，还是有一个主导方案和一个辅助方案。 我个人并不认为，国内互联网公司现在就应该赌其中某一个量子计算实现的硬件方案。因为，某些方案虽然进度较快，但技术突破往往难以预料，现在认定某个方案最终胜出，还为时过早。但我们会密切关注硬件团队发展并及时跟进。 至于理论方面，我相信我们会发展成国际一流的团队。 理论不止需要扎实的功底，更需要超越时代的视野。否则一时会产生很多文章，但是十年过后发现那些文章在历史上都留不下什么痕迹。我希望腾讯量子实验室的理论方面从小做起，但是心存高远的目标。 欢迎有在亚洲工作意向的各个级别的学者，也包括聪明而有抱负的学生和博士后加入，共同努力。 机器之心：产业界给实验投钱的前提是什么？是不是需要先从理论上证明，存在具有杀手级应用的量子算法？ 张胜誉 ：其实，过去二十年里发现了很多量子算法，早已不仅仅 Shor 和 Grover 的那两个。 只是目前发现的量子算法总数少，不如经典算法那样成体系。有很多量子算法，外界知道的不多。 当然，算法科学家也一直在寻找在解决哪些问题上，特别是些实用性强的问题，量子算法比经典算法效率更高。如有重要理论突破，相信会加速产业界对相应研发的投入。 另一方面，除了有理论证明的传统算法，暂时没有理论支持的算法也重要。 就像深度学习，人类对其成功原因的理解还停留在比较初级的层面，但这并不妨碍落地应用。对于这一类量子算法，需要理论和实验科学家精诚合作，共同努力。 机器之心：会与其他「参赛选手」展开竞争吗？ 张胜誉 ：目前来看，行业合作远远大于竞争。 因为大家有一个共同的敌人——实现量子计算机中遇到的噪声问题。我们希望受控的量子比特之间有比较好的纠缠和交互，但也不希望它们与环境的相干性太厉害。这很不容易控制。需要学界和业界的积极合作和共同努力。 目前待合作的问题包括量子计算硬件层面的制造、纠错机制与纠错码的研究、噪声来源和形成的原因研究、如何消除噪声，以及根据现有的噪声能否设计一些有趣的问题和算法，使得即时在有噪声的情况下运行该算法依旧能获得有效的结果等。 希望高校和研究机构都能以开放的心态来共同推动基础研究。这个阶段过多考虑利益之争，可能会严重限制量子计算发展。看似得到一些短期利益，但是实际上变成固步自封。 机器之心：张教授本科就读于复旦大学数学系。1999-2002 年期间，在清华大学计算机系人工智能实验室读硕士，师从应明生。当时所在团队是不是国内最早从事量子编程语言、量子复杂性等量子计算软件方向研究的团队？后来到普林斯顿大学师从姚期智，研究课题上有没有什么变化？ 张胜誉 ：本科数学系毕业。研究生阶段，对量子物理和量子计算的接触并不多，也没有计算复杂性背景。初入清华，研究方向也不是量子。 直到第二学年末（2001 年中），才开始跟随应老师学习量子信息知识，偏重量子状态和量子测量的一些基本数学性质。那时候，应老师的团队确实是国内最早从事量子信息处理研究的团队之一。 博士阶段来到普林斯顿之后，发现北美学术界更多地在进行量子算法和量子计算复杂性研究，而姚先生也是这方面的大师，所以，个人研究也更多地转到相应领域。 机器之心：近年比较关注哪些方向的学术研究？ 张胜誉 ：除了继续关注量子算法和量子复杂性之外，还做过一些量子博弈的研究。发现在很多场景下，使用量子技术会在一个博弈中取得很大优势。 更加近期的关注点还包括机器学习基础研究，以及使用量子物理改进经典的机器学习算法。 量子机器学习是一个有趣的领域，现有的量子计算机都存在运算规模小、噪声大、难以控制等问题。而量子机器学习算法可能受到的噪声对性能的影响相对小一些。 我本人也关注经典机器学习算法，因为它在近年发展得特别快。这在一定程度上说明，人类对自身理解有了很大进步。希望量子物理中的一些模型和想法能够进一步推进机器学习发展。 机器之心：进入产业界后，学术研究方向会有变化吗？ 张胜誉 ：一方面，腾讯充分理解量子计算研发的基础性和前瞻性，所以给了量子实验室比较大的自由，包括学术自由。 另一方面，作为一个大平台，腾讯有多方面的业务和场景，这些会给学术研究带来很多新的思考和问题。 具体到领域，在学术界时，我觉得很多方向都很有趣，比如量子算法、计算复杂性、量子物理基础、量子信息、量子容错纠错、量子博弈、量子密码等。 现在身份转变后，可能会更多去关注能先被产业界应用的研究方向，比如量子算法中的一些有潜在应用前景的问题。 机器之心：量子机器学习领域有不同的细分研究方向，比如使用量子物理原理构建新的机器学习模型、使用量子算法加速经典机器学习运算效率、通过经典机器学习解决量子物理问题等。你最近有接触过这领域哪些比较有趣的研究工作？ 张胜誉 ：你说的这几种都是很有意义的研究方向。其中，用经典机器学习帮助解决量子物理问题可能短期内比较容易实验实现，直接应用主要在科学研究上，商业应用价值暂时还不明显。 经典机器学习问题的量子算法这些年有一些了，不过用的技术还可以更丰富一些，其中有一些工作的严格性还需要进一步夯实。 其实，还有一类你没提到的，就是用量子的想法来启发经典机器学习的新算法。 最近，听到一名腾讯员工参与的工作，即将发表在 AAAI 2018。 研究使用了受量子物理原理启发、处理 NLP 问题的经典算法。在模型和计算环境上都是经典状态，有直接应用前景。 腾讯内部有一个量子学习小组，对量子物理+人工智能感兴趣的小伙伴们可以互相学习，氛围非常好。 机器之心查阅到，其论文名为：End-to-End Quantum-like Language Models with Application to Question Answering 机器之心：你曾被香港中文大学学生评为最好的计算机系教授，但也有学生很不理解您的教学方式。上课不带教案、考试是开卷考、会经常问学生想学什么等。如何评价自己的教学模式？ ：最好是过奖了。博士毕业后，我下定决心教书的一个原因是，内心还抱有一点小情怀。除了学术产出以外，也希望能通过更直接的方式，把自己的经验和教训分享给更多人，而教书显然是更好的方式。所以我希望找到合适的教学方式。 我教研究生课程的时候，上讲台尽量不带任何笔记和 ppt，所有算法细节从头到尾都在现场证明。可能有的学生会认为这个老师备课不认真，因为我都是现场用公式现推，有的时候也会卡住。但我希望这是一个比较自然的过程，能让大家体会到哪些推理过程比较重要，哪些是不太重要的细节。 大多数计算机算法没有那么复杂，如果能化繁为简，并且在面对很困难的问题时，传递给学生一种感觉：「我承认这个问题很难，我也搞不懂，也很头疼。但突然有一天，我想到一个角度来理解它时，会突然明白这东西也没有那么复杂。」 至于开卷考试，多用于本科课程，是希望学生把注意力用在去理解不同算法具体有什么用、适用于哪些问题、如何设计算法、复杂性怎么分析等，而不是死记硬背知识点。毕竟将来工作后面临的实际问题就是可以查阅资料的情况下，需要你设计有效算法快速解决问题。这和开卷考试比较像。 机器之心：进入工业界后，你的工作重心会有哪些转变？ 张胜誉 ：在腾讯量子实验室，研究还是实验室日常工作的一个非常重要的组成部分。组里会按兴趣分工，一部分人更偏纯粹基础研究，另一部分会偏产业结合。 除了研究，还要做的事情包括团队建设、管理、业务合作、外联、宣传甚至涉及投资，市场等，比以前单纯做老师要丰富很多。这也会帮助拓宽我们的视野，能力得到全面锻炼。 同时，我还是希望能继续培养量子人才。腾讯就有几万员工，借由腾讯大平台对外做一些科普教育和讲座，还能把量子科技传播到更多地方。 团队还可以与高校研究院联合做一些暑期班，为公司也包括整个行业长期培养人才。 
372,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736276&idx=1&sn=6fc23652c35b573e9a5a857e157d49f5&chksm=871ac2eab06d4bfc2e0a6f76f032ba92d74d36d3ac6b553d54ac59f3745ab5b48506d35a91df&scene=27,从零开始，教初学者如何征战Kaggle竞赛,作者：Oren Dar 在学习过深度学习的基础知识之后，参与实践是继续提高自己的最好途径。本文将带你进入全球最大机器学习竞赛社区 Kaggle，教你如何选择自己适合的项目，构建自己的模型，提交自己的第一份成绩单。 本文将介绍数据科学领域大家都非常关心的一件事。事先完成一门机器学习 MOOC 课程并对 Python 有一些基础知识有助于理解文本，但没有也没关系。本文并不会向大家展示令人印象深刻的成果，而是回顾基础知识，试图帮助初学者找到方向。 文章结构： 介绍 Kaggle 综述 建立自己的环境 预测房价竞赛简介 加载和检查数据 我们的模型：决策树介绍、偏差-方差权衡、随机森林 预处理数据 整合并提交结果 目前，我们能在网上找到很多高质量的免费机器学习教程，如 MOOC。一年以前，我在 Udacity 接触了「机器学习入门」课程，我认为它对于新手来说非常友好。在这里，我学到了机器学习基础概念、很多流行算法，以及 scikit-learn 的 API。在完成课程以后，我非常希望学到更多，但陷入了短暂的迷茫。 在做完一番研究后，我认为下一步最优的选择是进军 Kaggle，它是谷歌旗下的一个预测模型竞赛平台。没什么比自己动手进行实践更好了！ 初次尝试 Kaggle 竞赛是很紧张刺激的，很多时候也伴随着沮丧（得到好成绩之后这种感觉似乎还加深了！），本文将着重介绍如何入门并开始你的第一场 Kaggle 竞赛，在这个过程中尽快成长。 Kaggle 综述 （如果你已经熟悉 Kaggle 网站了，本段可以跳过） Kaggle 上有两个最适合新手的竞赛（某种程度上已成为 Kaggle 的「入门教程」）： Titanic（预测生存：一种二元分类问题）：https://www.kaggle.com/c/titanic 房价（预测价格：回归问题）：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 我强烈建议你两项都尝试一下，本文主要介绍后者。不过，其中需要的知识大部分是通用的，所以你完全可以看完本文，然后尝试其他 Kaggle 竞赛或者数据科学问题，所以选择挑战其他竞赛也没有问题！ 在每个竞赛的「Overview」（概览）选项卡上，你可以看到关于比赛及其数据集的一些信息、提交有效结果的评估标准（每个竞赛都略有不同），以及该竞赛的 FAQ。 在「Data」（数据）选项卡上，你可以看到数据的简要说明。我们需要的是这三个文件：train.csv、test.csv 和 data_description.txt（这是至关重要的，因为其中包含数据的详细描述），请将它们放在你可以快速访问的文件夹里。 「Discussions」（讨论）选项卡就像竞赛的专属论坛——不过不要低估它！在流行的竞赛中，这些讨论中经常包含非常有价值的信息，因为竞赛条款有时会要求参与者必须在讨论版上公开他们所使用的任何信息。例如，数据泄露是很难避免和处理的，偶尔也会发生在竞赛中。一方面，充分利用数据才能得到更高的分数赢得竞赛；但另一方面，结合了数据泄露的模型通常对于实践来说是无用的，所以也不被竞赛支持——因为它们使用了「非法」信息。勤奋的参与者经常会在讨论版上分享数据泄露以帮助竞赛环境变得更好。此外，Kaggle 的成员也会经常在其上分享一些信息，努力维护这个社区。在排行榜上名列前茅的参与者有时也会在其中分享自己的成功经验（通常会在竞赛结束前后）。 「Kernel」选项卡基本上是「讨论」版块的应用、代码版，我认为这是对于初学者而言最重要的一个版块。任何人都可以在其中分享自己的脚本或笔记，链接任何数据集与竞赛，形式可以是文档、注释、可视化和输出，每个人都可以观看、投票、复制这些内容，甚至也可以在浏览器上直接运行它们！我刚才提到的两个竞赛（Titanic、房价竞赛）都形成了有趣、漂亮、成功的 Kernel，强烈推荐进行过自己的尝试之后浏览这个版块。Kaggle 正在不断提升 Kernel 的功能，现在甚至有一个「仅限 Kernel」、奖金为 10 万美元的竞赛。不过，Kernel 中的讨论往往是硬核的，缺乏有关概念的解释，或者说预先认为你已具备相关知识，所以有时了解起来会有些许困难。 我强烈推荐使用 Python3.6 在 Jupyter Notebook 环境中处理任何数据科学相关的工作（其中最流行的发行版称为「Anaconda」，包括 Python、Jupyter Notebook 和很多有用的库）。然后你就可以通过在终端（或者 Anaconda GUI）输入 Jupyter Notebook 随时启动该环境。除此之外，本文展示的内容也可以在 Kaggle 网站上的私人 Kernel 上完成（完全在浏览器上工作），这和 Jupyter Notebook 是等价的。 在开始之前，先介绍使用 Jupyter Notebook 的几个基本要点： 你可以输入任意的方法名，然后按 Tab 键查看所有可能选项； 类似地，选择任意方法，按 Shift-Tab 键几次可以在你的 notebook 中打开它的相关文档； 在任意语句之前输入%time 并执行该 cell，可以输出所需执行时间； 类似地，在任意语句之前输入%prun 并执行该 cell，可以令其在 Python 的代码分析器中运行，并输出结果。 可以在这里查看更多有用的命令：http://ipython.readthedocs.io/en/stable/interactive/magics.html 目标概览 这是一个监督学习问题，意味着训练集中包含一系列的观察数据（行）和相关的多种信息（列）。其中一列是我们感兴趣并能够预测的信息，通常称其为目标变量或者因变量，在分类问题中称为标签、类。在我们的案例中，目标变量是房价。其它的列通常称为独立变量或特征。我们还有一个测试集，也包含一系列的观察数据，其中的列与训练集相同，除了目标变量，因为我们的目标就是预测目标变量的值。因此，完美情况下，我们要建立一个模型，该模型可以学习训练集中因变量和独立变量之间的关系，然后使用学习到的知识在测试集中尽可能准确地预测因变量（目标变量）的值。由于目标变量（房价）是连续的，可以取任意的值，因此这个问题属于回归问题。 加载和检查数据 现在我们已经成功启动了 Jupyter Notebook，首先要做的事情就是加载数据到 Pandas DataFrame 中。Pandas 可以处理 Python 中所有数据分析相关的工作，是很强大和流行的库，DataFrame 是它用于保存数据的对象名称。 最后一行使用了 Python 3.6 的字符串格式将从 Kaggle 下载的 CSV 文件（『comma-separated-values』，一种常用格式，可使用任何标准软件打开，例如 Excel）加载到 Pandas DataFrame 中。我们之后将频繁使用 read_csv，因此建议先浏览它的文档（这是一个好习惯）。加载数据并查看 DataFrame，可以发现数据集中的第一列是 Id，代表数据集中该行的索引，而不是真实观察值。因此，我修改了代码，加上 index_col=『Id』作为参数，从而在加载数据到 DataFrame 的时候，确保 Pandas 将其作为索引而不是列，并在它之前添加一个新的索引列。 现在，我们来看看训练集的样子。 训练集总共有 80 列（除 Id 以外），其中 79 列是独立变量，1 列是因变量。因此，测试集应该只有 79 列（独立变量）。大多数的数字和字符串都没有什么意义，其中 Alley 列甚至全都是『NaN』，即值的丢失。别担心，我们之后会处理这个问题。下一步是考虑需要使用的模型。我们先讨论一下决策树（有时在应用到回归问题的时候称为回归树）。 决策树介绍 其基本思想是很简单的，当学习（拟合）训练数据的时候，回归树搜索所有独立变量和每个独立变量的所有值，以寻找能将数据最佳地分割为两组的变量和值（从数学角度来说，树总是选择能最小化两个节点的加权平均方差的分割），然后计算分数（最好是选定指标上的分数），以及每个组因变量的平均值。接着回归树递归地重复该过程，直到无法进一步分割（除非设置了具体的 max_depth，如下图所示）。树最后一级的每个节点都被称为『叶』，每一个都和因变量（在该叶相关的所有观察数据）的平均值相关。 旁注：这是一个『贪婪』算法的很好示例，在每一次分割中，算法检查了所有选项，然后选择了在该点的最佳选项，以期望最终得到全局最佳结果。当树拟合了训练数据之后，使用任何观察数据预测因变量的值时，只需要遍历树，直到抵达一个叶节点。 我们数据集的可视化示例，其中 max_depth 设为 3。 在树的每个节点，第一个元素是节点的分割规则（独立变量及其变量值），第二个元素是在该节点的所有观察数据的均方差（MSE），第三个元素是该节点的观察数据的数量（samples），即这一组的规模。最后一个元素 value 是目标变量（房价）的自然对数。该过程和贪婪算法类似，在每个节点局部地进行最佳分割，确实可以随着树的扩展减少均方差的值，并且每个叶节点都有一个相关的「SalePrice」值。 偏差-方差权衡 我们回忆一下监督学习的目标。一方面，我们希望模型可以通过拟合训练数据捕捉独立变量和因变量的关系，从而使其可以做出准确的预测。然而，模型还需要对（未见过的）测试数据进行预测。因此，我们还希望模型捕捉变量之间的普遍关系，从而可以进行泛化。该过程称为『偏差-方差权衡』。 如果模型没有充分拟合训练数据，它将会有高偏差（通常称为欠拟合），因此它的训练误差较大。然而，如果模型过于拟合训练数据，它会捕捉到变量之间的特殊关系（偶然的），导致高方差（通常称为过拟合），因此它的测试误差较大。所以，我们需要在偏差和方差之间进行权衡。 决策树过拟合 假定我们将一个回归树拟合到训练数据中。这个树将是什么结构？实际上，它将持续分割直到每个叶节点只有一个观察数据（无法再继续分离）。换种说法，回归树将为训练集的每一个观察数据建立一个独特路径，并根据观察数据在路径末端的叶节点上给出因变量的值。 如果将训练集中因变量的值删除，并用训练过的树预测因变量的值，结果如何？可以猜到，它将表现得很完美，达到基本 100% 的准确率和 0 均方差。因为它已经学习了训练集中每个观察数据的相关因变量值。 然而，如果我打算让树预测未见过的观察数据的因变量值，它将表现得很糟糕，因为任何未见过的观察数据都会在原来的树构建一个独特的叶节点。这正是一个过拟合的例子。可以通过调整树的参数减少过拟合，例如，限制树的 max_depth，但实际上还有更好的方法。 解决方案：随机森林 在机器学习中，我们通常会设计「元学习」以结合小模型的多个预测而生成更好的最终预测，这种方法一般可称为集成学习。特别的，当我们结合一些决策树为单个集成模型，我们可以将其称之为「Bootstrap Aggregating」或简单地称之为「Bagging」。通过这种方法构建的「元模型」是一种较为通用的解决方案，因此随机森林可以适用于广泛的任务。 随机森林简单而高效，当我们用这种方法拟合一个数据集时，就会像上文所述的那样构建许多决策树，只不过每个决策树是在数据的随机子集中构建，且在每一次分割中只考虑独立变量「特征」的随机子集。然后为了生成新的观察值，随机森林会简单地平均所有树的预测，并将其作为最终的预测返回。 现在我们所做的的就是构建许多弱分类器或弱决策树，然后取它们的平均值，为什么要这样做呢？ 简单的回答就是它们确实工作地非常好，如果读者对随机森林的统计解释感兴趣的话，可以阅读更多的技术细节。但我不擅长于统计，但我会尽可能地给出一个基本的解释：bootstrap 采样和特征子集可以使不同的决策树尽可能地去相关（即使它们仍然基于相同的数据集和特征集），这种去相关能允许每一棵树在数据中发现一些不同的关系。这也就使它们的均方差要比任何单颗树都少的多，因此减少过拟合后它们能在总体上获得更好的预测和泛化结果。 简单来说，对于未见的观察结果，每个决策树预测该观察结果结束时所处叶节点的因变量值，即特定树空间中最类似的训练集观察结果。每棵树都是在不同的数据上构建的不同树，因此每棵树用不同的方式定义相似性，预测不同的值。因此对于给定未见观察结果，所有树的平均预测基本上就是训练集中与之类似的观察结果的值的平均值。 此特性的影响之一是：尽管随机森林在测试集与训练集相似度较高时（值属于同样的范围）非常擅长预测，但当测试集与训练集存在根本区别时（不同范围的值），随机森林的预测性能很差，比如时序问题（训练集和测试集不属于同样的时间段）。 不过我们的案例中测试集和训练集具备同样范围的值，因此这对我们没有太大影响。 预处理数据 我们在让随机森林运行起来之前还有一件事要做：随机森林虽然理论上可以应对分类特征（非数据形式：字符串）和数据缺失，scikit-learn 实现却并不支持这两种情况。所以我们需要使用 pd.interpolate() 来填充缺失的值，然后使用 pd.get_dummies() 的『One-Hot Encoding』来将分类特征转换为数字特征。这个方法非常简单，让我们假设一个分类变量有 n 个可能值。该列被分为 n 个列，每一列对应一个原始值（相当于对每个原始值的『is_value?』）。每个观察值（以前有一个分类变量的字符串值），现在在旧字符串值对应的列上有一个 1，而其他所有列上为 0。 我们现在准备构建一个模型，使用数据进行训练，并用它来预测测试集，然后将结果提交到 Kaggle 上。 整合结果并提交 这就是我们的模型提交 Kaggle 所需的所有代码——大约 20 行！我们运行这些代码，随后继续向 Kaggle 提交结果——得分为 0.14978，目前排行约为 63%。对于五分钟的代码编写来说，结果不错！在这里我们可以看到随机森林的力量。 说明 在将训练集和测试集分别加载进 DataFrame 之后，我保存了目标变量，并在 DataFrame 中删除它（因为我只想保留 DataFrame 中的独立变量和特征）。随后，我在训练集和测试集中添加了一个新的临时列（'training_set'），以便我们可以将它们连接在一起（将它们放在同一个 DataFrame 中），然后再将它们分开。我们继续整合它们，填充缺失的数值，并通过独热编码（One-Hot Encoding）将分类特征转换为数字特征。 正如之前所述的，随机森林（以及其他大多数算法）都会在训练集和测试集有差不多数值的情况下工作良好，所以在修改内容的时候我希望对两个数据集进行同样的修改。否则，interpolate 可能会在训练集和测试集上填入不同的数值，而 get_dummies 可能会以两种不同的方式对相同的分类特征进行编码，从而导致性能下降。随后我在将其分开，去掉临时列，构建一个有 100 个树的随机森林（通常，树越多结果越好，但这也意味着训练时间的增加），使用计算机的所有 CPU 核心（n_jobs=-1），使用训练集进行拟合，用拟合的随机森林来预测测试集的目标变量，把结果和它们各自的 Id 放在一个 DataFrame 中，并保存到 一个 CSV 文件中。随后登陆 Kaggle 页面提交 CSV 文件，大功告成！ 原文地址：https://towardsdatascience.com/machine-learning-zero-to-hero-everything-you-need-in-order-to-compete-on-kaggle-for-the-first-time-18644e701cf1 
373,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736406&idx=5&sn=10abf0d216feefc95c112bd91f3e14c2&chksm=871ac368b06d4a7e526ca57adcacd6dea4d3aa7775f067ffc9811161bf69b21064a255d0525b&scene=27,学界 | 谷歌提出机器对话Self-Play框架M2M，提高自动化程度,"受最近 AI 游戏研究的启发（self-play），谷歌提出了 M2M 的机器对话框架，其结合了众包模式和聚焦任务特定经验的方法，并通过增加自动化程度，以快速引导智能体进行目标导向的对话，并可生成高质量对话数据集。 1. 介绍 使用监督学习方法训练的目标导向的智能体，通常在使用相同任务的对话训练的时候才能得到最佳表现。然而，当开发对话智能体帮助用户完成新任务的时候，例如通过在线网站进行医生预约，可能不存在该任务的人类-智能体对话数据集，因为目前还没有和该特定 API 进行交互对话的智能体。一个常用的方法是通过众包模式使用 Wizard-of-Oz 设置（Wen et al. (2016); Asri et al. (2017)）以收集和标注自由格式的对话。然而这种处理方式很昂贵，并存在损耗，因为从众包人员收集的自由格式的对话：（i）可能没有覆盖智能体需要处理的所有交互；（ii）可能包含不适合用作训练数据的对话（例如众包人员使用的对话可能过分简化或复杂）；（iii）可能在对话行为标注中存在错误，需要对话开发者进行昂贵的手动修改。 另一种方法在面向消费者的语音助理中应用很广泛，它允许第三方开发者建立聚焦于单独任务（例如，DialogFlow1 、Alexa Skills2 、wit.ai3）的对话「经验」或「技能」。这为对话开发者提供了对特定任务处理的完全控制，使其能递增地添加新的特征到经验中。然而，这种方法非常依赖于开发者设计对话式交互的所有层面，以及预期用户和智能体交互以完成任务的所有方式。将这种方法扩展以使其更加数据驱动化是很有价值的，提高其在对话研究社区中的流行度（相比 Wizard-of-Oz 方法）。 作者在本文中提出了 Machines Talking To Machines（M2M，机器对话机器）的框架，这是一个功能导向的流程，用于训练对话智能体。其主要目标是通过自动化任务无关的步骤以减少建立对话数据集所需的代价，从而对话开发者只需要提供对话的任务特定的层面。另一个目标是获得更高质量的对话，「高质量」指的是：（i）语言和对话流的多样性，（ii）所有预期用户行为的覆盖范围；以及（iii）监督标签的准确性。最后，这个框架的目标是引导对话智能体，使其被部署去服务实际的用户，并达到可接受的任务完成率，之后，该框架应该能使用强化学习通过用户反馈直接提升自身性能。 之前建立语义解析器（Wang et al. (2015)）、把自然语言问题映射到结构化问题 (Zhong et al. (2017)) 解析器的方法，都依赖众包形式把自动生成的结构化表征映射到 single-shot 自然语言表述中。然而，以这种方式生成多轮对话需要多个参与智能体的协作。受最近 AI 游戏研究的启发 (Silver et al. (2016, 2017))，我们引入了「dialogue selfplay，自对话」的概念，也就是两个或者多个对话智能体通过选择离散对话行为进行交互，以尽可能地生成对话历史。在此研究中，作者部署了一个基于日程的用户模拟器智能体 (Schatzmann et al. (2007)) 和一个基于有限状态机器的系统智能体，来进行自对话步骤。 论文地址：https://arxiv.org/abs/1801.04871 摘要： 我们在本文中提出了 Machines Talking To Machines（M2M，机器对话机器）的框架，该框架结合了自动化和众包模式以快速地引导端到端对话智能体在任意范畴内进行目标导向的对话。M2M 只需要来自对话系统开发者的一个任务纲要（task schema）和一个 API 客户端就可以扩展到新的任务中去，但它也可以通过客户定制进行特定任务的交互。在数据收集方面，和 Wizard-of-Oz 相比，M2M 有更丰富的多样性和更广泛的重要对话流的覆盖范围，同时保持了个人言辞的自然性。在第一阶段，一个模拟用户机器人和一个领域不可知的系统机器人进行交谈，以尽可能生成对话「轮廓」（大纲），即模板对话和它们的语义解析。在第二阶段，众包人员对对话进行上下文重写，以使对话更加自然，同时保持原来的含义。整个过程可以在数小时内完成。我们用 M2M 收集了一个跨越两个领域的包含 3000 个对话的新语料库，并和流行的对话数据集在表层句子形式和对话流的质量、多样性上进行了比较。 "
374,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736232&idx=4&sn=3332215f55785b316965f1c004d74508&chksm=871ac216b06d4b006805a43097b9d94f6c2ed3e247817370de03fde0ffd835a50996b3594158&scene=27,教程 | 如何优雅而高效地使用Matplotlib实现数据可视化,"Matplotlib 能创建非常多的可视化图表，它也有一个丰富的 Python 工具生态环境，很多更高级的可视化工 具使用 Matplotlib 作为基础库。因此本文旨在提供一种高效的 Matplotlib 使用方法，并希望该方法可以帮助大家理解如何更有效地进行日常数据分析工作。 简介 对新手来说 Python 可视化实在有些令人挫败。有很多不同的选项，如何选择正确的选项是一个挑战。例如，两年前这篇文章《Overview of Python Visualization Tools》仍然吸引了大量读者。在那篇文章中，我否定了 Matplotlib。但是，在使用过 pandas、scikit-learn、seaborn 和其他 Python 数据科学包之后，我觉得之前否认 Matplotlib 的行为有点不成熟。坦白讲，当时我不是很了解 Matplotlib，也不懂如何在我的工作流中高效使用 Matplotlib。 现在我学习了一些工具，了解了如何基于 Matplotlib 使用这些工具，Matplotlib 逐渐变成了可视化工具的核心。本文将展示如何使用 Matplotlib。我坚定地认为 Matplotlib 是 Python 数据科学包必不可少的一部分，希望这篇文章可以帮助大家了解如何使用 Matplotlib 进行 Python 可视化。 为什么大家都在否定 Matplotlib？ 我认为，Matplotlib 对于新手来说比较难存在几个原因。首先，Matplotlib 有两个界面。第一个界面基于 MATLAB，使用基于状态的接口。第二个界面是面向对象的接口。本文就不展开介绍 Matplotlib 有两个界面的原因，但了解这两种方法在使用 Matplotlib 绘图时会很重要。两个界面会引起混淆的原因可以通过 Stack Overflow 和谷歌搜索查找一些信息。此外，新用户将发现混淆问题有多个解决方案，但是这些问题看起来类似却不完全相同。从我的个人经验来讲，我们从以前的代码中可以看出有一些 Matplotlib 代码的混杂。 关键点 Matplotlib 新手应该学习和使用面向对象的接口。 使用 Matplotlib 的另一个历史性挑战是一些默认的样式缺乏吸引力。在 R 使用 ggplot 就可以生成相当不错的图，而 Matplotlib 相对来说有点丑。好消息是 Matplotlib 2.0 中的样式好看了很多，你可以用最小的努力生成可视化。 第三个挑战是你不确定什么时候该使用 Matplotlib，什么时候该使用基于 Matplotlib 构建的工具，如 pandas 或 seaborn。大部分时候做一件事都有多种选择，但是对于新手来说选择正确的道路有些困难。 为什么使用 Matplotlib？ 尽管 Matplotlib 有这么多问题，我还是喜欢用它。因为它很强大，这个库允许你创建几乎所有的可视化图表。此外，围绕 Matplotlib 有一个丰富的 Python 工具生态环境，很多更高级的可视化工具使用 Matplotlib 作为基础库。因此如果你想在 Python 数据科学工具包中进行任何操作，你需要对如何使用 Matplotlib 有一些基础了解。这就是本文其余部分的重点，提供一种高效使用 Matplotlib 的基础方法。 前提 推荐以下步骤学习如何使用 Matplotlib： 1. 学习 Matplotlib 的基本术语，具体来说就是什么是 Figure 和 Axes。 2. 一直使用面向对象的界面，养成习惯。 3. 用基础的 pandas 绘图开始可视化。 4. 使用 seaborn 进行稍微复杂的数据可视化。 5. 使用 Matplotlib 自定义 pandas 或 seaborn 可视化。 下图非常重要，有助于理解图的不同术语。 大部分术语很直接易懂，需要牢记的是 Figure 是可能包含一或多个 axes 的最终图像。Axes 代表单个图。一旦你理解这些是什么以及如何通过面向对象的 API 评估它们，其余步骤就很简单了。 了解这个知识还有一个好处，就是当你在网络上看东西的时候有一个出发点。如果你花时间了解了这个点，那么其他的 Matplotlib API 才有意义。此外，很多高级 Python 包，如 seaborn 和 ggplot 依赖于 Matplotlib 构建，因此理解了基础，学习更强大的框架才更加容易。 最后，我不是说你应该逃避其他优秀选项，如 ggplot（又名 ggpy）、bokeh、plotly 或 altair。我只是认为你需要对 matplotlib + pandas + seaborn 有一个基础的了解。了解基础可视化栈之后，你就可以探索其他优秀工具，根据需求做出合适的选择。 开始 下面主要介绍如何在 pandas 中创建基础的可视化以及使用 Matplotlib 定制最常用的项。了解基础流程有助于更直观地进行自定义。 我主要关注最常见的绘图任务，如标注轴、调整图形界限（limit）、更新图标题、保存图像和调整图例。 开始，我打算设置输入，读取一些数据： 数据包括 2014 年的销售交易额。为简短起见，我将总结这些数据，列出前十名客户的采购次数和交易额。绘图时我将对各列进行重命名。 下图是数据。 现在数据以简单的表格形式呈现，我们再来看一下如何将数据绘制成条形图。如前所述，Matplotlib 具备多种不同风格，可用于渲染图表。你可以使用 plt.style.available 查看你的系统可用的风格。 使用如下简单风格： 现在我们有了好看的风格，第一步就是使用标准 pandas 绘图函数绘制数据： 推荐使用 pandas 绘图的原因在于它是一种快速便捷地建立可视化原型的方式。 自定义图表 如果你对该图表的重要部分都很满意，那么下一步就是对它执行自定义。一些自定义（如添加标题和标签）可以使用 pandas plot 函数轻松搞定。但是，你可能会发现自己需要在某个时刻跳出来。这就是我推荐你养成以下习惯的原因： 生成的图表和原始图表基本一样，不过我们向 plt.subplots() 添加了一个额外的调用，并将 ax 传输至绘图函数。因此，通过 ax 或 fig 对象可以执行任何自定义。 我们利用 pandas 实现快速绘图，现在利用 Matplotlib 获取所有功能。通过使用命名惯例，调整别人的解决方案适应自己的需求变得更加直接简单了。 假设我们想调整一些轴标签，且 ax 变量中有多个轴，可以进行一些操作： 这是另一种改变标题和标签的简单方式： 为了进一步展示该方法，我们还可以使用 plt.subplots() 函数可以定义图像尺寸，一般以英寸为单位。我们还可以使用 ax.legend().set_visible(False) 移除图例。 要想修改这个图像，你可能需要执行很多操作。图中最碍眼的可能是总收益额的格式。Matplotlib 可以使用 FuncFormatter 解决这一问题。该函数用途多样，允许用户定义的函数应用到值，并返回格式美观的字符串。 以下是货币格式化函数，用于处理数十万美元区间的数值： 现在我们有了格式化程序函数，就需要定义它，并将其应用到 x 轴。完整代码如下： 这张图美观多了，非常好地展示了自定义问题解决方案的灵活性。最后要说的自定义特征是向图表添加注释。你可以使用 ax.axvline() 画垂直线，使用 ax.text() 添加自定义文本。就以上示例，我们可以画一条表示平均值的线，包括代表 3 个新客户的标签。以下是完整代码（包括注释）： 图表 目前，我们所做的所有改变都是针对单个图表。我们还能够在图像上添加多个表，使用不同的选项保存整个图像。 如果我们确定要在同一个图像上放置两个表，那么我们应该对如何做有一个基础了解。首先，创建图像，然后创建轴，再将它们绘制成图表。使用 plt.subplots() 可以完成该操作： 在这个例子中，我使用 nrows 和 ncols 指定大小，这对新用户来说比较清晰易懂。我还使用 sharey=True 以使 y 轴共享相同的标签。 该示例很灵活，因为不同的轴可以解压成 ax0 和 ax1。现在我们有了这些轴，就可以像上述示例中那样绘图，然后把一个图放在 ax0 上，另一个图放在 ax1。 现在，我已经在 jupyter notebook 中用 %matplotlib inline 展示了很多图像。但是，在很多情况下你需要以特定格式保存图像，将其和其他呈现方式整合在一起。 Matplotlib 支持多种不同文件保存格式。你可以使用 fig.canvas.get_supported_filetypes() 查看系统支持的文件格式： 我们有 fig 对象，因此我们可以将图像保存成多种格式： 结论 该版本将图表保存为不透明背景的 png 文件。我还指定 dpi 和 bbox_inches=""tight"" 以最小化多余空白。最后，希望该方法可以帮助大家理解如何更有效地使用 Matplotlib 进行日常数据分析。 "
375,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736364&idx=3&sn=9317c03bdcee1d94255e87d4ea8deb46&chksm=871ac292b06d4b84a628c1d520f722d36357d376c6ca251297029a4b1859cb6c8981205935b4&scene=27,业界 | 反GAN传统，Petuum自动驾驶新研究提出从复杂真实图像生成简单虚拟表征以预测驾驶指令,"不久之前，机器之心推出了介绍 AI 创业公司 Petuum 在医疗领域的一系列研发成果的文集。而除了医疗领域，Petuum 也在自动驾驶等多个领域启动了研发项目。本系列我们将介绍 Petuum 在自动驾驶研发方向的一系列成果。我们在此以［用于端到端公路驾驶的无监督真实域到虚拟域的域统一］这一开创性论文来开始这一系列。在获取用于训练自动驾驶系统的数据时，常见的做法是使用对抗生成模型（GAN）根据来自模拟器的虚拟图像生成接近真实的图像。在这篇论文中，Petuum 团队则反其道而行之，直接以真实驾驶图像为起点，利用无监督去除其中对驾驶行为预测无关的细节而使之简化为虚拟域中的精炼规范表征，并据此预测车辆指令，形成一种更加高效准确的全新的训练方案。 引言   开发基于视觉的自动驾驶系统是一个长期以来一直存在的研究问题 [27, 24, 10, 9]。在已经提出来的各种解决方案中，将单个正面相机的图像映射成汽车控制命令的端到端驾驶模型得到了很多研究关注 [5, 33, 18]，因为这没有繁琐的特征工程过程。研究者们进行了很多尝试，试图通过利用中间表征来提升单纯的端到端模型的表现（如图 1 所示）。例如，[33] 将语义分割作为次要任务来提升模型的表现，而 [8] 则首次训练了一个检测器来检测周围的车辆，之后再进行驾驶决策。但是，当我们迈向更大的规模时，收集驾驶数据和标注中间表征的成本可能会高得不切实际。 此外，由于真实世界很复杂，驾驶场景的一般图像和中间表征中包含了很多多余的细节。这些细节中很多都是带来麻烦的信息，它们要么与预测任务无关，要么就根本没帮助。比如，在公路上驾驶的典型人类驾驶员不会根据前一辆车的品牌或道路之外的景观而改变自己的行为。在完美的情况下，模型应该只通过观察人类驾驶数据就能学习到关键的信息，但由于深度神经网络本质上是黑箱的，所以我们难以分析模型是否是根据正确的信号做出预测的。[6] 对神经网络的激活进行了可视化，结果表明其模型不仅学习了车道标记等对驾驶至关重要的信息，而且还学到了非典型车辆类别等不需要的特征。[18] 给出了经过因果过滤（causal filtering）优化的注意图（attention map）结果，其中似乎包含了相当多随机的注意点。我们很难证明学习这样的信息对驾驶有帮助，而且我们相信从驾驶图像中有效地提取最少充分信息（minimal sufficient information）的能力对提升预测任务的表现而言至关重要。   相对而言，来自模拟器的驾驶数据当然能避免这两个问题。一方面，只需简单地设置一个机器人汽车，我们就可以轻松获得无限量的标注了控制信号的驾驶数据。另一方面，我们可以控制该虚拟世界的视觉外观，还能通过最小化多余的细节来构建一个规范的驾驶环境。 这促使我们开发了一个可以有效地将真实驾驶图像变换成它们在虚拟域中的规范表征的系统，从而有助于执行车辆指令预测任务。很多已有的研究工作利用虚拟数据的方式是使用生成对抗网络（GAN）将虚拟图像变换成看起来很真实的图像 [12]，同时在辅助目标的帮助下保持标注的完整 [34,7]。我们的方法虽然也是基于 GAN，但却有几方面的不同之处。首先，我们的目的是将真实图像变换成它们在虚拟域中的规范表征，而不是反过来。我们所说的规范表征（canonical representation）是指从背景中分离出了对该预测任务而言最少充分信息的像素级表征。因为任何图像都只能有一个规范表征，所以我们不会在生成过程中引入任何噪声变量。其次，我们不会试图直接保留标注，因为我们不清楚到底是图像中的哪些信号确定了车辆指令。相反，我们提出了一种全新的联合训练方案，可以将对预测而言关键的信息逐渐地提取到生成器中，这还能使训练稳定化以及防止驾驶关键信息的模式崩溃（mode collapse）。   我们的工作有三大全新的贡献。第一，我们引入了一种无监督的真实域到虚拟域统一框架，可以将真实驾驶图像变换成它们在虚拟域中的规范表征，然后可以据此预测车辆指令。第二，我们开发了一种全新的训练方案，该方案不仅能将对预测而言关键的信息逐渐地提取到生成器中，还能选择性地防止 GAN 的模式崩溃。第三，我们给出的实验结果证明了为端到端驾驶任务在虚拟域中使用统一的规范表征的优越性。 网络设计和学习目标 DU-Drive 的学习目标。给定真实域中一个标注了车辆指令的驾驶图像数据集和虚拟域中一个相似的数据集，我们的目标是将真实图像变换成其在虚拟域中的规范形式，然后再在变换后的假虚拟图像上运行预测算法。我们的模型与条件 GAN [22] 密切相关，其中生成器和判别器都有一个条件因子（conditional factor）作为输入；但仍有两个细微的差异。其一，在我们的模型中，判别器并不依赖于该条件因子。其二，我们的生成器的输入中不包含任何噪声向量。不同于将简单的虚拟图像映射成信息丰富的真实图像（可能存在多种可行方案），将真实图像映射成仅包含足以完成预测任务的最少充分信息的规范形式的方式应该是唯一的。因此，我们可以移除传统 GAN 中的噪声项，使用确定性的生成网络作为我们的生成器。 论文： 用于端到端公路驾驶的无监督真实域到虚拟域的域统一（Unsupervised Real-to-Virtual Domain Unification for End-to-End Highway Driving） 论文链接：https://arxiv.org/abs/1801.03458 在基于视觉的自动驾驶的范围内，单纯的端到端模型是不可解释的且只有次优的表现，而居中的感知模型需要分割掩码或检测边界框等额外的中间表征，在我们向更大的规模发展时，其标注成本可能会高得无法实现。原始图像和现有的中间表征还充满了与车辆指令预测无关的麻烦细节（比如前面车辆的风格或道路之外的景观）。更重要的是，之前的所有研究都无法应对臭名昭著的域转移（domain shift）问题——如果我们要将收集自不同来源的数据融合到一起，那就会极大地阻碍模型的泛化能力。在这项研究中，我们通过利用收集自驾驶模拟器的虚拟数据而解决了上述限制；我们还提出了 DU-drive，这是一种用于端到端驾驶任务的无监督真实域到虚拟域的域统一框架。它可以将真实驾驶数据变换成其在虚拟域中的规范表征，然后可以据此预测车辆控制指令。我们的框架有几个优点：1）可以将收集自不同源分布的驾驶数据映射进一个统一的域；2）可以利用可以免费获取的有标注的虚拟数据；3）可以学习到驾驶图像中专用于车辆指令预测的可解释的规范表征。我们在两个公开的公路驾驶数据集上进行了大量实验，结果清楚地表明了 DU-drive 的表现优越性和解释能力。  "
376,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736232&idx=5&sn=466a31fd21af0fe9fa91b0e1dc6fa3f7&chksm=871ac216b06d4b00c1b80907a7e2d8450e060420ab156fd5c6a46f6dd44b4767cc4aeb8f617c&scene=27,学界 | 谷歌大脑提出Adversarial Spheres：从简单流形探讨对抗性样本的来源,"Nurhachu Null 近日， Ian  Goodfellow 等人提出对抗性同心高维球，他们利用数据流形的维度来研究输入维度的改变对神经网络泛 化误差的影响，并表明神经网络对小量对抗性扰动的脆弱性是测试误差的合理反应。 已经有大量工作证明，标准图像模型中存在以下现象：绝大多数从数据分布中随机选择的图片都能够被正确分类，但是它们与那些被错误分类的图片在视觉上很类似（Goodfellow et al., 2014; Szegedy et al., 2014）。这种误分类现象经常被称作对抗样本。这些对抗的错误在角度、方向和缩放方面有着很强的鲁棒性（Athalye & Sutskever, 2017）。尽管已经有了一些理论工作和应对的策略 (Cisse et al., 2017; Madry et al., 2017; Papernot et al., 2016)，但是这种现象的成因仍然是很难理解的。 目前有一些针对对抗样本而提出的假设：一个比较常见的假设就是神经网络分类器在输入空间中不同区域的线性特征太强了 (Goodfellow et al., 2014; Luo et al., 2015)。另一个假设认为对抗样本不是数据的主要部分 (Goodfellow et al., 2016; Anonymous, 2018b,a; Lee et al., 2017)。Cisse 等人则认为，内部矩阵中较大的奇异值会让分类器在面临输入中的小波动时变得更加脆弱（2017）。 在尽力解释对抗样本背后的原因时，还有一些工作为增加模型的鲁棒性提出了一些应对方法。有的工作通过改变模型所用的非线性变换来增强鲁棒性 (Krotov & Hopfield, 2017)，将一个大型的网络提炼成一个小型网络 (Papernot et al., 2016)，或者使用正则化 (Cisse et al., 2017)。其他的工作探索使用另一个统计模型来检测对抗样本（(Feinman et al., 2017; Abbasi & Gagné, 2017; Grosse et al., 2017; Metzen et al., 2017)）。然而，很多这种方法都被证明是失败的 l (Carlini & Wagner, 2017a,b)。最终，很多例子中出现了使用对抗训练来提升鲁棒性的方法 (Madry et al., 2017; Kurakin et al., 2016; Szegedy et al., 2014; Goodfellow et al., 2014)。尽管对抗训练使得模型在面临对抗扰动时有所进步，但是在超越对抗训练所设计的范围时，局部误差还是会出现（Sharma & Chen，2017）。 这种现象特别有趣，因为这些模型在测试集上具有很高的准确率。我们假设这种现象本质上是由数据流形的高维度造成的。为了着手研究这些假设，我们定义了一个简单的合成任务，来区分两个同心的（concentric）高维球。这使得我们可以研究具有良好数学定义的数据流形中的对抗样本，我们还可以对模型学到的决策边界进行定性地描述。更重要的是，我们可以自然地改变数据流形的维度来研究输入维度的改变对神经网络泛化误差的影响。我们在多个数据集上的实验和理论分析证明以下几点： 与图像模型中类似的现象出现了：大多数从数据分布中随机选择的点被正确分类了，然而未被正确分类的点和不正确的输入很「相近」。即使在测试误差小于百万分之一的时候，这种现象仍然会发生。 在这个数据集中，泛化误差和最近误分类点之间的平均距离存在一个权衡。尤其是，我们证明，任何一个能够将球体的少量点误分类的模型都会在面临 O(1 square root d) 大小的对抗扰动时表现得很脆弱。 在这个数据集上训练得到的神经网络顺理成章地逼近误差集合最近误差平均距离的理论最优的权衡。这表明，为了线性地增加到最近误差的平均值，模型的错误率必须指数降低。 我们还证明，即使忽略掉大部分的输入，在在这个数据集上训练的到的模型也能够达到极高的准确率。 下面，我们探讨一下高维球中的对抗样本和图像模型中的对抗样本之间的联系： 论文：Adversarial Spheres 论文链接：https://arxiv.org/abs/1801.02774 摘要： 目前最先进的计算机视觉模型表现出了对微弱对抗性扰动的脆弱性。换句话说，数据分布中的绝大多数图像能够被模型正确分类，而且这些被正确分类的图像与被误分类的图像在视觉上特别相似（人眼无法察觉）。尽管这个现象目前已经存在大量的研究，但是这个现象的成因仍然是很难被理解的。我们假设这个反直觉的现象本身是由于输入数据流形的高维几何特征造成的。作为探索这个假设的第一步，我们研究了在一个简单的合成数据集上对两个高维同心球的分类。我们对这个数据集展示了测试误差和最近误差的平均距离之间的权衡。尤其是，我们证明，任何一个能够将球体的少量点误分类的模型都会在面临 O(1 square root d) 大小的对抗扰动时表现得很脆弱。意外地，当我们在这个数据集上训练几个不同结构的网络时，它们的所有误差都达到了这个理论边界。理论结论是，神经网络对小量对抗扰动的脆弱性是观察到测试误差数量的必然结果。希望我们对这个简单例子的理论分析能够推动这种探索：现实世界中复杂数据集的复杂几何结构是如何导致对抗样本的。 "
377,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736232&idx=1&sn=6c427e322346aea8fb7556fc4a8e5222&chksm=871ac216b06d4b0075676a4e79218c31b70ef86a49b78ac9d29dd8a91c25ef55ee9516c101d2&scene=27,1.65亿天使融资背后，是时候把「百度」标签从林元庆身上摘下了,如果说「技术难度」是单点技术公司的重要门槛，那么「快」就是林元庆为自己团队设定的门槛。 是时候把百度的标签从林元庆身上撕下来了。 现在，他与所有皱眉思索如何让公司迅速跑起来的创业者一样，从百度离开的 3 个月里，马不停蹄地拜访潜在客户，接受各种投资人热情地轰炸，利用各种人脉召集人才，甚至连公司的选址，都经过了一番考量。 「大概只有公司内部的装饰与布置是家人替我安排的，因为这对我来说可能真的是个挑战。其他事情，基本都会经过我的手。」他终于在 1 月初的某个下午挤出一点时间，向我们敞开了办公室大门。 很显然，他正在经历一个创业者最应该焦头烂额的时间段。 爱笔智能（Aibee），这家由林元庆一手创办的 AI 公司，终于在上个月有了自己的「实体店面」——上地三街的某栋商务大楼内，似乎距离老东家百度并不远； 而公司的室内陈设，也严格秉承了「一桌一椅一盆栽」的办公室格局，从表面上来看，并没有太多新意。 不过，大多空置着的工位既印证了这家公司成立时间不长，也能暗示出林元庆短时间内期望实现的团队规模——我们数了一下，在国内，至少应该有 50 余人。 「虽然我离开百度没多久，但现在身份变了，时间对我来说很紧迫，」他承认，自己在这段时间的工作量急剧膨胀， 「除了融资，我在这段时间拜访了将近 40 家传统企业，上周六还跟一个同事刚刚去上海踩点看项目，当天往返。」 然而，这种「紧迫性」，只能让我们从他已经完成的任务清单上获得些许实感；而面对他本人时所呈现出的状态，实际上恰恰相反：气定神闲，还捎带着一些兴奋与跃跃欲试。 这就很容易让人产生一种错觉，好像他正站在一个 2000 米高的台子上，即将完成一项蹦极挑战。 的确，融资的「异常顺利」给他的这场创业博弈增加了不少筹码。毕竟，在短时间内完成 1.65 亿人民币的天使轮融资，显然是投资方踊跃登门而促成的一项结果。 「融资前后花了大概不到 1 个月左右的时间就已经敲定。」林元庆称，从钱的层面上，他并没有经历大部分创业者所面临的那种生与死考验。 而站在投资者角度，从国内技术巨头走出来的顶级人才，「无论如何都是要见一面的」。 在这 10 家参与此轮投资的基金中，领投方昆仲资本与跟投方之一的华创资本都没有否认，他们在看一家 AI 创业公司时，「百度」等标签或多或少会让创业者有一些拿到融资的优先级优势。 「通常情况下，我们并不是某个项目出来了再去了解。如果看某个领域，一些大的平台里我们会通过各种方式去了解这是一个什么人，在干什么。」昆仲资本创始合伙人梁隽樟关注了林元庆很长一段时间，「百度」，无疑是让后者大幅提升关注度的一个重要前缀。 「他宣布创业那天，应该很多人找他，他自己都开玩笑说『手机被打爆了』。不过，你投不投他，他接不接受你的投资，这是一个经过仔细考量的双向选择，」他话锋一转，「我们在认识的基础上，针对他要做的事情聊了很久才做决定。我们几乎每晚都会聊到凌晨，他一直都在非常拼命地工作。」梁隽樟把林元庆形容成一个「标准的工作狂」，经常大半夜狂发微信。 「我们不是没有放弃过类似的从大平台出来的技术创业者，有些人专业能力远远不够。」他以自己的衡量标准给了林元庆一个高度评价， 「其实选择创业公司的过程，就像过日子一样，人要聊的来，还能一起走的下去。我们想要商业回报，然后我们从他身上看到了这种可能性。」 「怎么说呢，你接触他，就会发现他是一个非常有特点的人，对，非常有意思，有特点。」 科学家的另一个隐秘身份 坦率讲，即使坐在林元庆面前，我们也找不到一些合适的词来形容他的外貌。 眼镜、平头、微胖的身材……他可以是任何一个面朝电脑在代码里神游的工程师，也可以是午休时间在公司楼前空地上吸烟晒太阳的互联网技术男。 以至于当我问一个曾经采访过他的老媒体人对他的印象时，这位记者沉默了好一会儿，最后蹦出了一个词：「憨态可掬？」 当然，他的确也是一个没有什么架子的人。 在前后接受我们两次的采访过程中，几乎是有问必答，哪怕同样的问题再问一遍；而浓浓的福建口音同时又频繁夹杂英文单词的说话习惯，几乎成了他为数不多的「个人特色」之一。 就是这样一个「普通中年男人」，在人工智能领域，特别是计算机视觉技术方面取得的成绩与头衔，可以列一份长长的清单（参考我们此前对他的报道：  ； ），而在 NEC 美国实验室媒体分析部门与百度研究院任过的负责人职位，也是对他在 AI 领域专业能力最直接的一种认可。 不过，梁隽樟所说的「有意思，有特点」，当然不是林元庆这些被公众认证过的能力。 譬如，你可能会接受「一个在技术领域成绩突出的人也同时拥有很强的销售能力」这个命题；但你是否会轻易相信，一个还不错的销售可以成为一个了不起的科学家？ 大概只有林元庆比较亲近的几个朋友，了解他有段波澜起伏的经历。 很多人都知道，林元庆拥有清华大学光学硕士与宾夕法尼亚大学的电子工程博士学位，但就在 1993 年读本科时，他其实还兼职做了广告销售； 而在1997 年清华读书期间，他还创立过一家广告公司。至今已经过了 20 年，这家公司依然存在。 1999 年底，当他还正在考虑申请留学的时候，一个朋友找上他，想让他帮忙卖掉中关村的一批广告牌。 20 世纪初，尽管正值互联网创业持续发酵期，但那时候的中关村还远没有现在一副「车水马龙」、「地价连城」的景象。 特别是户外广告牌买卖，还处于萌芽阶段，大多数 IT 客户仍然喜欢在《计算机世界》《联合商情》等平面媒体上登广告，没有什么人能看到户外广告牌的价值。 因此，尽管林元庆的朋友通过海淀工商局在中关村一些路口比较顺利地立了一批 1.5 米左右的广告牌，但几乎没有什么客户上门，而他们也不知道到底哪种客户喜欢这种广告形式。 「当时我朋友可能跟黑妹牙膏的人比较熟，所以那时候中关村几个牌子上只有黑妹牙膏的广告。」 林元庆觉得这是一段非常有意思的回忆，「但我觉得这（户外广告）是一个很棒的东西，就帮他去卖牌子，基本上一个月左右，就把大部分牌子卖掉，挣到了 6 万多块钱。」他虽然看似轻描淡写，但还是有些小得意， 「大部分都卖给了联想电脑，当然还有其他客户。这些广告牌应该是中关村地区第一批广告牌。」 「我当时在中关村跑广告也算是名气蛮大的。」他最后终于忍不住咧开嘴笑起来。因此，当我们把「曾经的中关村广告一哥」这个调侃性称号送给他时，他欣然接受： 「当时好多人因为这个知道我，但实际上，我做了 7 年的兼职销售只是想多点收入，来支持我的学业，当然，也希望能看看学校外面的世界。」 然而，这段在广告牌界「横着走」的经历并没有让他有多少成就感。 2000 年，当他坐上飞往美国的飞机时，还跟旁边的妻子反复唠叨——「这辈子咱们不再搞商业的东西了，好好做学术，做研究。」 但现在，好像又转回来了。当然，纯粹卖广告与技术落地是两种性质完全不同的商务，林元庆把这个转变称为「一种螺旋式的上升」。 「我们那个时候去美国的目的性很强，态度也很坚决，想在技术方面做出一些成绩。不过，后来在 NEC、在百度又经历了一些东西才发现，技术，特别是 AI ，绝对绝对不能脱离应用，脱离传统行业。」 这一次，他与传统行业站在了一起 在梁隽樟与林元庆接触的过程中，后者身上强烈的商业嗅觉是很打动他的一个层面。 当然，这不单单是指那些在学生时期更偏向「玩票儿」性质的销售经历，而是在百度期间实打实主导 AI 商业项目落地的大量实战经验。 在他担任百度深度学习研究院（IDL）院长的 2 年时间里，曾主导很多百度与外界客户的商业项目谈判。对于百度能够谈妥一些落地项目，林元庆功不可没。 「虽然百度研究院听起来非常学术，但其实，我在 IDL 的每个季度会上都会有一张 slide，就是一定要做算法、数据和应用的闭环。这是我在百度技术研发时一直非常强调的。」 通俗点说，就是技术一定要落地；而率先成型的产品和服务，也一定会反哺技术，继而进行更加快速高效的升级与迭代。 举个例子，林元庆在 2017 年翻看 AI 技术论文时发现，关于「行人再识别」这项技术的论文突然多了起来。过去几年可能也就每年一两篇的样子，但今年 CVPR 突然就有了十几篇文章。 这是为什么呢？因为这项技术在安防应用领域变得非常重要。 「这项技术简单来说就是有两个摄像机，这边一个摄像头，走廊那头还有一个摄像头，你从这边走到那边，然后两个摄像头的图像传输对比一下最后确认是同一个人。就是这项技术，但以前没什么人研究的，而去年，AI 在安防领域额的应用突然变得火爆了，对技术的研究也就跟上来了。」 因此他认为，由技术商业落地中发现的新问题，去循环推动技术的进一步研究，是所有 AI 商业公司不断对自有核心技术进行升级与迭代的关键步骤。 这样的模式思维，也成为林元庆创立爱笔智能的一个核心思想。只是，他对于技术应用的思考维度已经不再局限于某个简单的产品与服务，而是汇集成了几个问题： 1. 用这项 AI 技术/产品/服务，我的公司能节省多少金额的成本？ 2. 成本不变，用了这个技术/产品/服务，我的经济效益相对之前提升了多少倍？ 3. 那些年你用过的 AI，真的为业务带来了质的提高了吗？ 4. 对于各种行业来说，AI 的能力边界在哪里？ 5. 客户真的关心自己用的是哪些技术吗？ 机场的人脸识别闸机？收银台上的人脸识别摄像头？离开百度后，林元庆把自己从这些曾经很引以为傲的、单一场景中的产品与服务完全剥离了。 他背上包，揣上自己针对传统行业列出的问题清单，一家一家拜访教育、房地产以及零售行业的龙头老大们，向他们请教所在领域究竟存在什么痛点。 虽然说是「请教」，但对于那些懂行业却不懂 AI 的企业来说，林元庆的登门与其说是「取经」，不如说是「讲经」——「 AI 很有意义，但我真的不知道它对我业务上的意义究竟在哪里」，这是一个传统教育行业从业者告诉我们的困惑。 在林元庆 2 个月内对近 40 家百亿企业的密集调研中，CEO 或 CTO 们通常会兴高采烈地把技术部门的人叫出来，让这些工程师把公司做的技术改造一项一项拿出来给他展示： 「看，我们也有 AI，既做了这个，又改进了那个。」 然后，他们会向林元庆表示：「 AI 非常有意思，我们已经做了这么多尝试，你能不能把我们目前的这些技术做得更好？」 但很遗憾，他很快发现，这些所谓「利用 AI 进行的赋能」对他们本身的业务并没有太大作用，甚至有时候就是『鸡肋』： 有挺好，但没有也死不了。 看完技术人员的展示，他通常会问这么一个问题：假定你们已经做的，加上规划的都完成了，你们觉得这些技术能给你们的核心业务带来多大提升？ 基本上问出这个问题，现场就凝固了。 「我看多了才知道，原来他们觉得这个东西有 20% 帮到忙的可能性，他们可能会做，但却不知道做出来到底多有用，没有人真正回答这个问题。」 气氛冷下来后，林元庆又会继续提出另一个问题： 假设历史只给你们这家公司一次 AI 升级的机会，就一次，你打算做什么？ 常常就是从这个问题开始，他所拜访的公司开始「敞开天窗跟他说亮话」，全部放开来跟他进行多轮深入对谈。 最后，他基本总结出了目前传统行业在与 AI 进行磨合过程中的「痛点」——想出来的场景常常过于单一，改造工程划分得太细。 他给我们举了一个具体的例子：麦当劳的薯条可能 7 分钟就要换一拨，过了 7 分钟，之前剩下的薯条就要扔掉。可以想象，这样 1 年会浪费掉多少吨粮食，耗费多大的食材成本。 那么问题来了： 如果这个问题可以靠 AI 解决，那么什么样的数据需要被收集？移动支付与摄像头应该如何进行连结？供应链管理与客户信息系统是否也需要被赋能？ 「他们感到痛苦的地方在于，很多东西被撕得特别碎，今天一个人觉得这里语音识别可以用得上，明天又觉得人脸识别挺好，后天又有一个人跑来说『我给你弄个AR』吧，来自三家供应商的东西根本融不成一个整体。」林元庆似乎对单点技术供应商的业务模式并不买账，「你给这家付了点钱那家付了点钱，但最后这些技术也没帮你解决太大的问题，都是些小修小补。」 说到这里，林元庆新公司的业务模式已经不难猜了——与单点技术相对立，做一份「结果导向型」的多点技术整合方案。 在确定「自己的公司究竟能做到什么」这个问题上，林元庆的思路异常清晰，且有「自知之明」。他让我们必须先明确三个事实： 1. AI 有边界，很多你想要的结果，现有技术还达不到； 2. 你的客户不 care 你在中间到底用了什么技术，他们希望用最后达到的经济效益来衡量； 3. 不要期望方案一下子能达到 100% 的效果，「迭代」与「升级」在 AI 改造过程中非常重要。 在这些条件的基础上，第一步，他会采取与 IBM 相似的业务模式，先帮这家传统企业做一个类似于 AI 咨询的服务，梳理他们的整个业务板块，进而分析哪些环节能够靠 AI 进行有效改造。 第二步，他将组建一支囊括大数据分析师、语音识别专家、计算机视觉专家等 AI 工程师组成的综合性团队，一个项目里，每种技术维度的工程师大约有2~5 人。 用一个不算恰当的比喻，这就像一个「豪华版」毛坯房设计团队一样，里面会囊括室内布局设计师、橱柜设计师、水电改造师等等角色，最终给住户出一个最优设计解决方案。 「我们会先只做一个行业里的一家代表性公司，非常仔细地彻底地去帮他们做改造，然后整理出一个完整方案后，再帮行业里其他家去做，从点到线，等做的行业多了，也就变成了面。」 从点到线到面。这种路径与当下的单点技术供应商恰恰相反，他们选择的是 从面到点 ：先打造一个通用型技术，然后再将技术用到一些更加垂直的场景中。 譬如科大讯飞，就是语音识别领域的单点技术厂商；而今年融资热情高涨的商汤与 Face++，就是人脸识别领域的单点技术供应商。 很多时候，我们不能单纯去依靠业务模式来评判两种路径的好坏，但很明显，两者必有自己的优势与缺陷。 而单点技术商业路径的最大挑战，林元庆认为就在于「门槛」。通常来说，一个领域只要出现了一家卖单点技术的公司，就会紧接着出现第二家、第三家、第 n 家。 「这是一个再自然不过的趋势，单点技术的门槛从出现到最后，只会越来越低，竞争越来越激烈，除非也转型争取打透某些行业。」 化解质疑 当然，这并不是说像爱笔智能这种技术综合解决方案供应商就能活得更为轻松。 在 1 个月前林元庆正式宣布创立爱笔智能后，其公开的业务模式也引起了一些争议。 譬如，一位在金融行业拿下不小份额的人脸识别技术供应商这样抨击林元庆推崇的这种模式： 「虽然理论上可行，但是实行起来却很难。你如果是一家百亿级别的企业，你会把一整套系统改造方案交给一家刚刚成立不久的初创公司去做吗？」 而一位同样做量化技术解决方案的技术公司则向我们表示，这种模式前期要投入大量资源，毕竟这是一个人才资源密集型公司。当然，还要建立行业威信，甚至有数据分析方面的积累性经验： 「你不能仅仅靠跟企业的研发部门深入聊天来获取痛点，很多时候，只有数据会告诉你痛点到底在哪里。」 也就是说，如果不拿下行业里的标杆企业，不能率先拿出合格的「整体赋能方案」，其生存艰难程度将有过之而无不及。 林元庆听到这些不一样的声音似乎非常开心，他还开了个玩笑：「我个人的行业专业度与知名度或许会对建立信任有点帮助。」 不过他并没有反对这家人脸识别技术工程师的观点，对于创业公司来说，拿下第一家客户确实很艰难，更何况是「推销」自己的一整套改造方案。 因此，他从两方面来应对这个问题：慎重选择进入的行业，优中选优；用「融入」消除企业顾虑。 「线下零售」是林元庆与行业客户及投资方们反复推敲确定的第一个改造领域。原因很简单——这是一个对新技术接受度更高，改造起来相对容易的领域。 「这个领域的数据虽然很零散，但并不意味着不好收集。相反，在为一家连锁餐饮做软硬件改造方案的时候，麦克风、摄像头在店里更容易安装，侵犯隐私的风险较低，也就意味着数据更容易获取。」林元庆首先提到了数据获取这个敏感话题，「此外，零售行业也是一个技术更迭较快，离钱相对较近的领域，你看中国对移动支付的接收度，对人脸识别支付的接收度以及新零售的方兴未艾，就知道这个行业的开放程度与改造潜力的确很大。」 而「融入」，则意味着「变成客户的一部分」。 按照林元庆对团队的规划，如果拿下一个企业的项目，项目将由双方研发团队共同组成，比如规模大概为 60 人左右，其中 20 余人来自爱笔智能，而另外 50% 的技术人员则来自客户的研发部门。 「一个项目通常需要 3~6 个月的封闭开发，在这段时间内，我们就相当于这家企业研发部门的一部分，技术与工程细节是完全开放的，同时也会帮企业去培养他们自己的 AI 技术团队。」 任何企业的商业模式都不可避免会被别人借鉴和复制，这是业界常态。如果说「技术难度」是单点技术公司的重要门槛，那么「快」，就是林元庆为自己团队设定的门槛。 「成立公司以来我好像没对团队有什么要求，但唯有一个字是我一直强调的，就是『快』。」 这是林元庆为公司研发任务与执行任务确立的基调：一旦一纸合约拿到，就一定要在 6 个月之内做出完整解决方案。这一点上，他坚称不会设置「备选项」： 「我们的确是一家人才密集型企业，那么如果陷入一个项目两年，会出现什么情况呢？一个是团队工作的节奏会变慢，收益周期变长；另一个更为致命的问题是，客户可能会觉得合作不够顺畅，磨合成本太高，干脆自己慢慢做。」 「没有一家公司不会受到死掉的威胁，即便对 BAT 而言，『快』也非常重要。」他最后，对所有的外界质疑似乎全盘接受，并认为， AI 创业公司正走在一个历史性的窗口期： 「我非常相信现在的 AI 发展期就像 2000 年左右的互联网时代，是个名副其实的机会窗口，但这个窗口只有 1~3 年，3 年左右，格局就会定下来。2018，将是一个重要分水岭。」 「对我来说，紧迫感随时都在。因此，在这 3 年里，我们要脚踏实地地往前跑。」他哈哈大笑起来，「你会看到，2018 年上半年，大概 2 个月后吧，我们的第一份方案就会拿出来。」 号召力与好人缘 如果从「 6 个月就要拿出一套合格的 AI 整体技术解决方案」这个公司硬性 KPI 指标进行反推，就意味着林元庆一定要在短时间内建立一个强大的技术团队。 虽然在创业不到 3 个月的这段时间内，他已经组建起了包括硅谷与北京两地在内的小 20 人团队，但他在 2018 年上半年的目标，却是50~60 人。就在采访当天，公司还新入职了一位来自硅谷的员工。 「我们一直保持着紧迫感，连招人也是，不过我一点都不担心没人招。」 他很自信，认为在自己的好朋友、斯坦福实验室知名科学家 Silvio Savarese（李飞飞的先生）确认加入他的团队并担任要职后，公司的号召力将会继续向全球扩散。 「在我创业这段时间，Silvio 与飞飞给了我很多建议。Silvio 还强烈要求让公司位于硅谷的团队与国内总部采取无缝式工作模式，」他指了指在相对朴素的办公室内「唯一值钱的设备」——思科会议系统，笑着告诉我们：「只要两地的工程师一转头，就能看到对方办公室的模样。」 「我本身就是一个很 open 的人，虽然也奉行工程师文化，但更喜欢比较随意的工作氛围，不会去设立什么『高级』等没有多大意义的头衔。当然，项目肯定会有个 leader 。」 林元庆这种相对扁平化的团队设置偏好，也得到了一些前百度员工的「积极认证」。 一位前研究院实习生告诉我们，在团队开会的时候，林元庆完全允许实习生去会议室旁听：「我觉得这样做很难得，特别是在等级相对森严的大公司，我很感激。」 这个评价与我们自开头所提到的「对林元庆的初印象」有了高度重合——没架子，极具亲和力。除了好朋友，同时也是地平线机器人创始人余凯已经象征性地入股爱笔智能外，另一位不愿透露姓名的前 IDL 员工也用一个事实印证了林元庆的「好人缘」： 「 2017 年春节的时候，林元庆给 IDL 的员工发完年终奖，年后 200 多人的团队没一个人离职。」 然而，就是这种平易近人的性格，配上其雷厉风行的做事风格，给人一种充满矛盾但却没有丝毫违和的奇妙感觉，就像梁隽樟在谈到跟林元庆进行业务交流时，得出这样一番感慨： 「有时候半夜，他突然在群里甩过一个方案来，说『这个搞定了』，我一看时间，凌晨两点，自己还迷迷瞪瞪的，有点哭笑不得。但是很奇怪，跟他一起做事，你会觉得很安定，就是觉得『啊，这是个挺靠谱的人。』」 实际上，与其说是「创业」让林元庆有了紧迫感和使命感，倒不如说他每天活的都像是在创业。 无论是在 NEC、百度还是当下，他表示，自己的作息几乎没有改变过： 「像 2017 年在百度，每天 8 点半上班，11 点多下班。周天上午会打个篮球，其他时间照旧。」 而实际上，刚才那位实习生曾经这样「吐槽」领导作息： 「什么鬼，我自己是因为习惯晚上工作，所以凌晨 2~5 点不睡觉，但技术大神为何早上来得早，连晚上回家都只比我早一点点？」 「并不只是在百度，在 NEC 以及现在出来创业，我的行事风格一直都很一致：要做就马上坚决地去执行，不拖泥带水。」林元庆说。 最后，在被问及「是否能够保证在 3 年这个关键期限内做出成绩」时，他没有直接给我答案： 「我们会跑得很快，也确信我们正走在一条给传统行业创造巨大价值的路上，不要急。」 
378,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736232&idx=3&sn=eaeb824fee91ace8f70352ccdfbd6450&chksm=871ac216b06d4b0025fcac5a8fd2b7bddf4f334e72debca99202ecd008468e90119a5b340221&scene=27,资源 | 十倍模型计算时间仅增20%：OpenAI开源梯度替换插件,"训练一个非常深度的神经网络需要大量内存。通过由 OpenAI 研究员 Tim Salimans 和 Yaroslav Bulatov 联合开发的工具包，你可以权衡计算力和内存的使用，从而使你的模型更合理地占用内存。对于前馈模型，我们能够借助该工具把大 10 多倍的模型放在我们的 GPU 上，而计算时间只增加 20%。 项目链接：https://github.com/openai/gradient-checkpointing 通过梯度检查节约内存 深度神经网络训练的内存密集部分是通过反向传播计算损失的梯度。通过查看由你的模型定义的计算图，并在反向传播中重计算这些结点，有可能在减少内存成本的同时计算对应结点的梯度。当训练的深度前馈神经网络包含 n 个层时，你可以这种方式把内存消耗降至 O(sqrt(n))，这需要执行一个额外的前馈传递作为代价（可参见 Training Deep Nets with Sublinear Memory Cost, by Chen et al. (2016)）。通过使用 TensorFlow graph editor 自动重写反向传递的计算图，该库提供了 TensorFlow 的一个功能实现。 使用一般 tf.gradient 函数和我们的内存优化的梯度实现训练一个大批量的 ResNet 模型时占用的内存比。 工作原理 对一个简单的 n 层前馈神经网络，获取梯度的计算图如下所示： 神经网络的层级激活值对应于 f 标记的结点，且在正向传播过程中，所有这些结点需要按顺序计算。损失函数对激活值和这些层级参数的梯度使用 b 结点标记，且在反向传播过程中，所有这些结点需要按逆序计算。计算 f 结点的激活值是进一步计算 b 结点梯度的前提要求，因此 f 结点在前向传播后会保留在内存中。只有当反向传播执行地足够远以令计算对应的梯度不再需要使用后面层级的激活值或 f 的子结点时（如下图所示），这些激活值才能从内存中清除。这意味着简单的反向传播要求内存与神经网络的层级数成线性增长关系。下面我们展示了这些结点的计算顺序，紫色的结点表示在给定的时间内需要储存在内存中。 如上所述，简单的反向传播已经是计算最优的了，因为每个结点只需要计算一次。然而，如果我们愿意重新计算结点，那么我们可以节省大量的内存。当我们需要结点的激活值时，我们可以简单地重计算前向传播的结点激活值。我们可以按顺序执行计算，直到计算出需要使用激活值进行反向传播的结点。 使用这一策略，需要令计算梯度的内存在神经网络层的数量 n 上是稳定的，且 n 在内存方面是最优的。但是要注意，结点的计算数量现在扩展了 n^2，相比于之前的 n。n 个结点中的每一个被再计算 n 次。因此计算图变得很慢以计算深度网络，使得这一方法不适用于深度学习。 为了在内存与计算之间取得平衡，我们需要一个策略允许结点被再计算，但是不太经常。这里我们使用的策略是把神经网络激活的一个子集标记为一个结点。 这些检查点结点在前向传播后保留在内存中，而其余结点最多只会重新计算一次。在重新计算后，非检查点结点将保留在内存中，直到不再需要它们来执行反向传播。对于简单的前馈神经网络，所有神经元的激活结点都是由正向传播定义的连接点或图的分离点。这意味着我们在反向传播过程中只需要重计算 b 结点和最后检查点之间的结点，当反向传播达到了我们保存的检查点结点，那么所有从该结点开始重计算的结点在内存中都能够移除。计算和内存使用的顺序如下所示： 图 3：Checkpointed backprop 对于例子中的简单前馈网络，最好的选择是将每 qrt(n)-th 个结点作为 checkpoint。这样，checkpoint 结点的数量和 checkpoint 之间的结点数目都是 sqrt（n）的倍数，这意味着所需的内存现在也与我们网络中层数的平方根成比例。由于每个结点最多只能重算一次，因此该策略所需的额外算力相当于整个网络的单次正向传递。 OpenAI 的工具包实现了 checkpointed backprop，如图 3 所示。这是通过标准反向传播（图 1 所示）和 TensorFlow 图编辑器的自动重写实现的。对于包含关结点的图（单结点图分隔符），我们选择自动选择 checkpoints 的策略，使用 sqrt(n)，提供 sqrt(n) 给前馈网络。对于只包含多结点分割的一般计算图，我们的 checkpointed backprop 实现仍然有效，但目前仍需使用者手动选择 checkpoint。 更多的计算图、内存用量和梯度计算策略说明可以在这篇文章中找到：https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9。 设置需求 在运行测试的时候，保证能建立 CUDA Profiling Tool Interface（CUPTI），例如，通过运行 export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH}:/usr/local/cuda/extras/CUPTI/lib64""。 使用 本项目提供了一个 TensorFlow 中 tf.gradients 的插入式替换。载入此函数需要： 随后使用 gradients 函数，就像你正常使用 tf.gradients 来计算梯度损失参数一样。（这里假设你明确地调用 tf.gradients，而不是将其隐藏在 tf.train.Optimizer 中。） 除了 tf.gradients 的常规参数以外，OpenAI 的 gradients 函数还有一个额外的参数 checkpoints。Checkpoints 参数告诉 gradients 函数计算图中的哪个结点在前向传播中需要检查。检查点之间的结点会在反向传播时计算。你可以为 checkpoint 提供一个张量列表，gradients(ys,xs,checkpoints=[tensor1,tensor2])，或使用以下关键词： ‘collection（默认）’：这个 checkpoint 的所有张量返回 tf.get_collection('checkpoints')。你随后需要确认自己在定义自己的模型时是使用 tf.add_to_collection('checkpoints', tensor) 来加入张量的。 ‘memory’：它使用启发式机制来自动选择 checkpoint 的结点，从而达到我们需要的内存用量 O(sqrt(n))。启发式方法是通过自动识别图中的「关结点」来实现的，即移除时将计算图分成两个断开的张量，然后对这些张量进行检查点确定，找到一个合适的数量。这种方式目前在很多模型上运行良好（但不是所有）。 ‘speed’：这个选项试图通过检查所有操作的输出来最大化运行速度，这通常非常耗费算力，特别是在卷积和矩阵乘法上。 覆盖 TF.GRADIENTS 直接使用 gradients 新函数的另一个方法是直接覆盖 Python 上注册的 tf.gradients 函数名。就像这样： 这样，所有 tf.gradients 的调用就会使用节约内存的版本作为代替了。 测试 在 GitHub 资源的测试文件夹中包含用于测试代码准确性，并分析各类模型内存使用情况的脚本。修改代码后，你可以从该文件夹运行./run_all_tests.sh 来进行测试。 下图展示了在 CIFAR10 上运行不同层数 ResNet 的内存用量和时间，Batch-size 为 1280，GPU 为 GeForce GTX 1080： 限制 目前提供的代码在运行模型之前全部使用 Python 进行图操作，这会导致大型图处理速度缓慢。当前用于自动选择 checkpoint 的算法是纯启发式的，预计在已有测试之外的一些模型上可能会失败。在这种情况下，我们应该使用手动选择 checkpoint 的方式。 参考内容 Academic papers describing checkpointed backpropagation: Training Deep Nets with Sublinear Memory Cost, by Chen et al. (2016) (https://arxiv.org/pdf/1604.06174.pdf), Memory-Efficient Backpropagation Through Time, by Gruslys et al. (2016) (https://arxiv.org/abs/1606.03401v1) Explanation of using graph_editor to implement checkpointing on TensorFlow graphs: https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-269241038, https://github.com/yaroslavvb/stuff/blob/master/simple_rewiring.ipynb Experiment code/details: https://medium.com/@yaroslavvb/testing-memory-saving-on-v100-8aa716bbdf00 TensorFlow memory tracking package: https://github.com/yaroslavvb/chain_constant_memory/blob/master/mem_util_test.py Implementation of ""memory-poor"" backprop strategy in TensorFlow for a simple feed-forward net: https://github.com/yaroslavvb/chain_constant_memory/ "
379,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736206&idx=2&sn=0b762099170a3f465ce9a7aa8dc26ae7&chksm=871ac230b06d4b261829e34dd281ed887a77ca7434c07140a216bb96babfcc26bc33e095c1d5&scene=27,业界 | 斯坦福博士李纪为成立AI金融公司香侬科技，获红杉中国数千万天使投资,"李纪为博士是近期涌现出的著名青年 AI 学者之一，在 2017 年 7 月份博士毕业之后不久，机器之心就对他进行了独家专访，并 。历经半年，李纪为博士在大公司招揽与创业之间，毅然选择了创业，加入了 AI 创业大军。   香侬科技官方网站：http://shannon.ai/ 李纪为博士期间研究方向是人工智能的一个重要分支——自然语言处理（NLP），他是第一位仅用三年时间就获得了斯坦福大学计算机科学博士学位的人。在三年的博士生涯中，他的多篇论文被各类顶级会议接收，他也是自然语言处理领域引用量最高的博士生之一。据统计，ICLR 2017 上李纪为有三篇论文被大会接收，其中两篇为第一作者；在 EMNLP 2017 上，他有两篇论文被大会接收，同样均为第一作者。 李纪为此前在接受机器之心专访时曾经表示，「如果有合适的机会，创业当然非常有可能。」如今看来，李博士最终还是选择创业，他告诉机器之心，经过半年的准备，他与两名大学同学共同创立了 AI 金融公司——香侬科技，专注于利用 NLP 技术提取、分析金融信息，为广大投资者提供有效、准确、全面的投资信息。据了解，香侬科技已经获得红杉中国数千万元人民币天使轮投资。 在投资消息发布前，机器之心对李纪为博士进行了专访，了解其创立的香侬科技、AI 金融、NLP 等方面的问题。 机器之心：恭喜公司天使轮融资成功，目前你的感觉如何？ 李纪为： 创业是一种全新的体验。在创业时，你能遇见到不同的人和事儿，无时无刻不在刷新着你对周围世界的认识。不管是金融从业者（一级市场、二级市场、投行、券商等等），投资人，还是科研界学霸、工程大牛、产品经理等，他们所带有的与生俱来的职业嗅觉与理念，都值得我们学习很久很久。这是一个非常有意思的过程，每天都有新的东西需要学习、新的挑战。 这有一点像我刚开始读博士的感觉——每天早上都是带着对一天的憧憬起床。我非常愿意用「有意思」这个词来形容现在的生活。团队每天都在学习和成长过程中，这也是最令人鼓舞的。 机器之心：能否简单介绍一下新成立的公司？ 李纪为： 香侬科技-名称来自于信息论创始人克劳德·香侬（Claude Shannon）。 这来源于一种情怀，在 CMU 学习的时候仔细拜读过他在 50 年代那篇划时代的论文「Prediction and entropy of printed English」. 当时我在 CMU 的老师 Eduard Hovy 一直想用这篇文章里面的概念来定义不同 NLP 问题的难度，所以就让我去读这篇文章。这是一篇具有时代意义的文章，香侬与他的太太 Betty Shannon 完成了重要的实验：给定英文一段话，告诉前面出现的字母，然后去猜下一个字母是什么。这是一个划时代的研究，它在语言上定义了 的概念：语言的不确定性决定于熵的大小。现代很多的 NLP 的理论都是和它有关的。在 NLP 的问题中，你能猜得多准，基本定义了这个问题的难度。我也很崇拜他，这就是为什么我们将创业公司取名为 Shannon。 注册公司时我惊奇地发现，Shannon.AI 这个域名竟然还没有被注册，当时真的高兴手舞足蹈。 同时，这个名字听起来也给人一种温暖的感觉，它有点像吴语里的「想你」（此处大笑）。 公司成立于北京，目前位于中关村。筹备方面，是从今年 11 月份开始正式推进这家公司的成立。从我的角度来讲，我也在加入大公司与创业之间犹豫了很久，直到去年九月份，随着几位合伙人陆续到位，我们就开始了各项事务的筹备。 机器之心：可否透漏首轮融资得到了哪些投资机构的投资？ 李纪为： 本轮天使融资为红杉中国基金独家，融资金额为数千万人民币。 机器之心：这是一家 AI 金融领域的创业公司，它将面向哪些类型的业务？为什么会选择金融这个方向？ 李纪为： 我们希望人们能够快捷、简单、准确全面地获取自己想要的金融信息。让信息为大众所用是我们的愿景。 我们是这样思考的：互联网 1.0 时代让世界更公平了，它就像广播、电视一样，是一个里程碑。互联网让人们获取信息的机会变得平等了，曾经只有少数人可以接触到的信息现在可以为大众所有。但从另一个角度来看，互联网时代的来临也制造了巨量的信息。现在，在很多事情上使用简单的搜索方法已经很难找到你想要的信息了。这又把互联网 1.0（以门户网站、搜索为典型）为我们带来的这份平等给部分抵消了——信息太多，无从查找，只有某些特定的人群才知道去哪里找到有用的信息。 在金融领域中，让信息能更容易地为大众所获取是一个很有意义的方向。在国内，随着国民生活水平的提高，投资需求正在逐渐升温，越来越多的人们正在将眼光伸向股票、基金，甚至债券、期货等二级市场方向上的投资。 然而，目前无论对于机构投资者，还是我们这样的个人而言，传统方式的信息渠道能够解决的问题还是太少了。最简单的一件事：现在我想搜索一下「最近有哪些上市公司的董事长正被证监会调查？」这是一个很明确的问题，也是对于市场行情非常重要的事情。还有很多其他事，我们是无法找到现成信息的，如果必须要找的话，我们必须一个一个地从不同信息源之中的信息里进行提取出来，自己进行分析。 我们希望能够构建一个面对投资者的智能解决方案，来解决投资上对于信息的需求。能够通过用算法来解决这些问题。 机器之心：对你们来说，大陆的金融市场与海外（尤其是美国）有哪些不同？ 李纪为： 美国的二级市场中，机构资金占比 90%，个人投资者习惯于把钱给机构托管；而在国内的股市里，散户占据一大部分，由于信息的不对称，甚至由于无法找到有效的信息获取渠道，很多投资者会处于相对不公平的劣势位置。我们所做的就是解决这一问题，让所有人都能够获得信息。 机器之心：香侬科技的创始团队构成是什么样的？ 李纪为： 我们的核心创始团队为三人，我的博士专业是计算机科学（自然语言处理），另两人均为金融领域背景, 都是我大学同学。沈盛杰本科毕业于北京大学物理学院，硕士毕业于光华管理学院，曾在对冲基金 Magnetar Capital 和中信证券等公司工作，主要负责金融数据处理，衍生品定价和交易策略研究等工作。另一位创始人何豪杰是国内二级市场资深研究员，长期从事行业和上市公司研究，曾就职于招商基金和私募基金星石投资等公司。 机器之心：选择这个方向创业，是否意味着金融行业已经到了进入 AI 时代的节点？ 李纪为： 我坚信 AI 会革命化地改变很多行业，金融便是其中的一个。但这并不意味这这次革命会在一夜之间发生，它可能需要一段很长的时间，可能 3-5 年，也可能是 5-10 年，甚至更长。深度学习从爆发到今天仅仅经历了五、六年的时间，而纵观人类科技的发展历史，具有重要意义的新科技都经历了更长的发展。技术和传统行业结合，慢慢地改变一个行业，会是一个漫长的过程。这个过程不仅仅是解决曾经的旧需求，而是在迭代中创造新的需求，新需求的创造恰恰是个慢慢发掘的过程。 我坚信人工智能逐渐会给很多行业带来变革。我认为，并不是 AI 造就了如今的时代，而是如今的时代催生了 AI 的发展：我们现在的工作正变得更加复杂，信息量变得更大。随着海量数据的产生，我们有了更多的需求去分析、理解，去提取我们想要的东西。我们每天都在用头条、滴滴、饿了么、美团这样的应用，大量的需求意味着我们需要一种高效的工具来分析、整合其中产生的数据，让人们能够简便地运用其中的知识。所以其实是社会需求推动了 AI，AI 反过来再推动了社会的变革。 站在金融的角度上，我们需要用新的方法来满足用户的需求，同时也可能会不断发现、甚至改变用户的需求，这是一个长期的过程。 机器之心：本轮融资过后，公司是否会开始进一步的团队建设，主要会招募哪些方向的人才？ 李纪为： 当然需要人工智能和深度学习方面的人才。不过，如何让实验室的深度学习技术落地，真正走进千家万户，这是一个全维度的任务。公司也希望招募更多前后端、数据库等方面有经验的工程师，以及产品经理、运维、UI 设计方面的大牛，大家把各自的特长相结合，互相促进，互相学习，得到一个真正可以造福行业的 AI 工具。 机器之心：此前有关人工智能学界与金融行业的一条重磅新闻是邓力加盟对冲基金巨头 Citadel，我们知道，邓力也研究自然语言处理（NLP）。NLP 与金融领域的契合点在哪些方面上？ 李纪为： 邓力老师去 Citadel  是机器学习最近的一个大新闻了。 NLP 与金融领域契合点主要是两点，一种是帮助你收集信息。一种是帮你做决策。 帮你做决策就是帮你做交易。第二步要比第一步更难。我们目前专注于收集信息方面，做投资决策还需要时间和技术的积淀，它将是未来可能发展的方向。 机器之心：机器学习作为量化交易方法的方向之一，在交易中有哪些应用？ 李纪为： 举一个例子，美国最著名的对冲基金——文艺复兴的两位 Co-CEO，Bob Mercer 与 Peter Brown。他们在自然语言处理（NLP）领域里也曾是风云人物。他们在上世纪 90 年代初把统计机器学习引入到了 NLP，革命性地改变了这个领域。直到深度学习火热之前，他们提出的方法一直是 NLP 里的主流。他们在 1994 年提出的 IBM model 在机器翻译领域盛行了 20 年，直到最近两年才被神经网络的 seq2seq+attention 超越。有人笑谈，那个年代，他们把 NLP 玩儿坏了（此处大笑）。 后来他们都加盟了文艺复兴对冲基金，成为了用机器学习做量化投资的风云人物。 值得一提的是，Peter Brown 是深度学习先驱 Geoffrey Hinton 门下第一个毕业的博士，1987 年博士毕业于 CMU。 所以说机器学习在量化投资里的应用由来已久，很多量化投资都在应用机器学习、统计学习的算法。随着原微软研究院资深研究员邓力老师这样的学者加入金融领域，神经网络作为一种人工智能中流行的方法也被金融界人士拿来开始尝试于投资（量化模型）。但由于金融界是一个倾向于不公开新成果的行业，所以目前我们了解的不多。  "
380,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736206&idx=3&sn=20a019ce95f61904e5ef8255bf0c3743&chksm=871ac230b06d4b265254c8d535142407851a311e07bed74bde828752f25441a6ccc859c3773a&scene=27,入门 | 从感知机到深度神经网络，带你入坑深度学习,"机器学习工程师 Adi Chris 最近学习完吴恩达在 Coursera 上的最新课程后，决定写篇博客来记录下自己对这一领域的理解。他建议通过这种方式有效地深入理解一个学习主题。除此之外，也希望这篇博客可以帮助到那些有意入坑的朋友。 言归正传。在我正式介绍深度学习是什么东西之前，我想先引入一个简单的例子，借以帮助我们理解为什么需要深度神经网络。 同时，本文附有使用深度神经网络模型求解异或（XOR）问题的代码，发布在 GitHub 上。 GitHub 地址：https://github.com/chrisbangun/medium-post/tree/master/Perceptron-to-DNN 何为异或问题？对于给定的两个二进制输入，我们通过异或逻辑门得到一个预测输出，这一过程即为异或问题。注意，输入不相等时输出为 1，否则为 0。表 1 展示了异或函数的所有可能的输出结果： 那么现在我们就画出数据分布图来探究它的本质。 看到上图后，我们或许会反思，这真的是一个简单问题么？ 如你所见，我们的数据并非线性可分的，因此，一些常用的线性模型，例如 logistic 回归可能就不太适合分类我们的数据了。为了给你一个更直观的理解，我用一个简单的线性模型画出了如下图的决策边界。 微调 logistic 回归模型构造决策边界 上面的图清楚的告诉我们，我们需要一个更好的分类器来分离非线性数据。SVM 结合它的核心技巧就是一个不错的选择。但是，在本文中，我们打算重新构建一个神经网络而非 SVM，并带你领略下神经网络解决异或问题的风采。 神经网络就是就是找到一个可以模拟人脑工作行为的表现良好的近似函数。图 1 对人类神经元与人工智能网络作了类比。 图 1：（a）人脑神经元结构（b）从生理神经网络类比得出的人工智能网络—图片摘自 cs231n.github.io 你不需要了解太多生物学知识，我会从高度形象的角度来解释人体神经元如何处理信息。 人体的神经元通过树突接受信号。这些信息或信号随后被传递到脑细胞或细胞体。在细胞体内部，所有的信息将被加工生成一个输出。当该输出结果达到某一阈值时，神经元就会兴奋，并通过轴突传递信息，然后通过突触传递到其他相连的神经元。神经元间传输的信号量取决于连接的强度。 前面提到的整个流程某种程度上适用于人工智能网络。你可以把树突想象成人工智能网络中基于突触感知器的被赋予了权重的输入。然后，该输入在人工智能网络的『细胞体』中相加。如果得出的输出值大于阈值单元，那么神经元就会『兴奋』，并将输出传递到其它神经元。 这样你就可以理解人工智能网络就是借鉴基本的生物神经元工作原理建模的吧。 为了了解神经网络是如何工作的，我们先来看看一个叫感知机的简单的人工神经网络。 对我而言，感知机是我见过的机器学习中最优雅的算法之一。它于 1950 年代被提出，尽管很简单，但它可以说是很多重要的机器学习算法的起点了，例如 logistic 回归、支持向量机甚至深度神经网络。 那么感知机怎么工作昵？我们以图 2 展开讨论。 图 2：感知机 图 2 展示了给定三个输入 x_1、x_2 和 x_3 以及一个可以计算输出值的神经元的感知机算法。Rosenblatt 通过引入权重的概念介绍这一简单的规则，用于生成输出值。权重通常是表示输入对应于输出的重要性的实数。上图中的神经元将会得到两个可能的结果，0 或 1，是由每个输入的加权和 ∑wjxj 决定的大于或小于阈值的结果。因此，感知机的主要思想就是去学到一些可以决定神经元兴奋还是抑制的与输入特征相乘的权重 w。我们可以写出一个如下的数学表达式： 我们现在可以做两件事来修改上述公式：第一，我们把权重相加操作转变成两个向量的点乘。 w (权重) 和 x (输入), 其中 w⋅x ≡ ∑wjxj。接下来，我们可以把阈值移到不等式的另一端并取个新变量名为偏置 b，b 恒等于阈值的负数。通过这些改动，感知机公式重写如下： 现在我们把这些公式套进我们的感知机架构，这样就有了如下所示的完整的单层感知机架构： 图 3：单层感知机架构 通常情况下，单层感知机模型都会使用阶跃函数作为激活函数将结果转化成 0 或 1，因此将输入归到 0 或 1 类。如图 4 所示，负数的输出为 0，正数的输出为 1。 对于输入数据线性可分的分类任务，阶跃函数十分有用。但是，由于我们的目的是找到一个用于分离非线性数据的分类器，单层感知机与阶跃函数就毫无意义了。稍后几节，我们将会看到使用非线性激活函数的多层感知机网络模型。 关于我们为什么不用使用阶跃函数，这里有两个主要原因： 1. 目前，结合反向传播使用梯度下降算法是训练一个多层神经网络的有效方法之一（我们稍后会简短的介绍一下）。反向传播的必要条件是使用的激活函数必须可微。然而阶跃函数在 x=0 处不可导，且其它位置导数均为 0。如此一来就无法运用梯度下降法更新权重了。 2. 回想下，神经网络的主要目的就是学习到使预测尽可能接近真实值的权重和偏置。为了达到这一目的，如同很多优化问题，我们希望对权重或偏置上作一个小的改变，在网络输出中只产生一个相对小的变化。而这是一个仅能生成 0 或 1 的函数难以企及的。 激活函数是神经网络的一个重要组成部分。一般来说，我们最少有三个需要激活函数的理由： 它帮助神经元学习和理解一些非常复杂的东西。 它们为网络引入非线性属性 我们希望对权重或偏差上作一个小的改变，以便在网络输出中只产生一个相对小的变化。 我们已经看到以阶跃函数作激活函数的例子，然而，在这一节，我们将要探讨一些深度学习中常用的非线性激活函数。顺便提一下，若要深入了解激活函数，包括每一个激活函数的利弊，你可以参考 Avinash Sharma 和 Karpathy 写的文章。  Avinash Sharma ：https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0 Karpathy：https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b Sigmoid 函数 sigmoid 函数，也即 logistic 函数，对于任意输入，它的输出范围都是 (0,1)。公式如下： sigmoid 的数学公式 图 5：sigmoid 函数图 图 5 画出了 sigmoid 函数的图形。如你所见，它很像平滑版的阶跃函数。但是，sigmoid 有很多好处，例如： 1. 它是非线性的 2. 不同于二值化输出，sigmoid 可以输入 0 到 1 之间的任意值。对，跟你猜的一样，这可以用来表示概率值。 3. 与 2 相关，sigmoid 的输出值在一个范围内，这意味着它不会输出无穷大的数。 但是，sigmoid 激活函数并不完美： 梯度消失。如前面的图片所示，当输入值 z 趋近负无穷时，sigmoid 函数的输出几乎为 0 . 相反，当输入 z 趋近正无穷时，输出值几乎为 1 . 那么这意味着什么？ 在这两个极端情况下，对应的梯度很小，甚至消失了。梯度消失在深度学习中是一个十分重要的问题，我们在深度网络中加了很多层这样的非线性激活函数，这样的话，即使第一层的参数有很大的变化，也不会对输出有太大的影响。换句话讲，就是网络不再学习了，通常训练模型的过程会变得越来越慢，尤其是使用梯度下降算法时。 sigmoid 的另一个弊端就是实际运用中指数运算开销太大。尽管有人说，与矩阵乘法或卷积相比，激活函数在深度网络的计算是非常小的一部分，所以这可能不会成为一个大问题。不过，我认为这值得一提。 Tanh 函数 Tanh 或双曲正切是另一个深度神经网络中常用的激活函数。类似于 sigmoid 函数，它也将输入转化到良好的输出范围内。具体点说就是对于任意输入，tanh 将会产生一个介于 -1 与 1 之间的值。 Tanh 函数的数学公式 图 6：tanh 函数图 如前面提及的，tanh 激活函数有点像 sigmoid 函数。非线性且输出在某一范围，此处为 (-1, 1)。不必意外，它也有跟 sigmoid 一样的缺点。从数学表达式就可以看出来，它也有梯度消失的问题，以及也需要进行开销巨大的指数运算。 ReLU(修正线性单元) 终于讲到了 Relu，人们起初并不觉得它的效果会好过 sigmoid 和 tanh。但是，实战中它确实做到了。事实上，cs231n 课程甚至指出，应该默认使用 Relu 函数。 ReLU 从数学表达式来看，运算十分高效。对于某一输入，当它小于 0 时，输出为 0，否则不变。下面是 ReLU 的函数表达式。 图 7：ReLU 函数图 那么你可能会问，「它是线性函数吧？为何我们说它是非线性函数？」 在线代中，线性函数就是两个向量空间进行向量加和标量乘的映射。 给定上面的定义，我们知道 max(0, x) 是一个分段线性函数。之所以说是分段线性，是因为它在 (−∞, 0] 或 [0,+∞) 上符合线性函数的定义。但是在整个定义域上并不满足线性函数的定义。例如 f(−1) + f(1) ≠f (0) 所以 Relu 就是一个非线性激活函数且有良好的数学性质，并且比 sigmoid 和 tanh 都运算得快。除此以外，Relu 还因避免了梯度消失问题而闻名。然而，ReLU 有一个致命缺点，叫「ReLU 坏死」。ReLu 坏死是指网络中的神经元由于无法在正向传播中起作用而永久死亡的现象。 更确切地说，当神经元在向前传递中激活函数输出为零时，就会出现这个问题，导致它的权值将得到零梯度。因此，当我们进行反向传播时，神经元的权重将永远不会被更新，而特定的神经元将永远不会被激活。 还有件事值得一提。你可能注意到，不像 sigmoid 和 tanh，Relu 并未限定输出范围。这通常会成为一个很大的问题，它可能在另一个深度学习模型如递归神经网络（RNN）中成为麻烦。具体而言，由 ReLU 生成的无界值可能使 RNN 内的计算在没有合理的权重的情况下发生数值爆炸。因此反向传播期间权重在错误方向上的轻微变化都会在正向传递过程中显著放大激活值，如此一来学习过程可能就非常不稳定。我会尝试在下一篇博客文章中详细介绍这一点。 参考阅读： 资源 | 从 ReLU 到 Sinc，26 种神经网络激活函数可视化 图 8：多层感知机 图 8 所示架构叫多层感知机（MLP）。从名字我们就可以推知，我们只是简单地堆积多层感知机而已。上图是一个三层感知机模型：一个输入层、一个隐藏层，以及一个输出层。然而，在深度学习或神经网络领域，人们并不叫它三层神经网络。通常，我们只统计隐藏层或其加上输出层的层数，因此，上图的网络也叫两层神经网络。隐藏层并不单单指输入层或输出层。现在，如你所猜，所谓的深度学习，就意味着有更多的隐藏层。 那么神经网络如何进行预测昵？ 当所有的输入通过所有隐藏层到输出层后，神经网络就会产生一个预测。这一过程叫前馈。如图 8 所示，网络接受输入 X，然后计算激活函数并逐层传递，直到输出。在监督任务中，对于此类分类任务，我们通常在输出层使用一个 sigmoid 函数，以便将预测值转化为概率值。在图 8 中，我们可以看到输出值为 0.24，由于它小于 0.5，我们可以说预测值 y_hat 为 0 . 跟一般的分类任务一样，我们有一个代价函数，用于评估我们的模型拟合真实标签的程度。事实上，训练神经网络可以简单地看作尽可能最小化代价函数的过程。我们可以定义如下的代价函数： 均方误差 所以我们的目的就是找到最佳 w 和 b，使代价函数 J 尽可能的小。为了达到这一目的，我们得靠两大重要的算法，梯度下降和反向传播。 梯度下降 对那些已经接触过机器学习的人来说，你们已经很熟悉梯度下降法了。训练神经网络与训练任何其它使用梯度下降法的机器学习模型没有多大区别。唯一明显的区别是网络中的非线性效应使得我们的代价函数非凸。 为了帮助你理解，我们假设有一个如下图 9 所示的凸代价函数： 图 9：梯度下降法图解 上表中，水平坐标表示参数空间，权重和偏置，代价函数 J(w, b) 就是水平轴上面的抛物面。图中的红色圆圈代表初始权重 w 和 b 对应的代价。为了最小化这一代价，我们需要走到这个抛物面底。那么问题来了，我们怎么知道沿哪个方向走昵？是参数变大的方向还是变小的方向？我们可以做一个随机搜索，但这显然耗时且开销过大。 通过处理可学习的权重和偏置可以找到最佳方向。微积分告诉我们，在给定的点上，梯度方向就会指向函数值改变最快的方向。因此，我们将使用代价函数对权重和偏置的梯度。 现在，我们简单地看看图 10 所示的代价-权重变化。 图 10：梯度的形象化表示 图 10 描绘了代价函数对应权重的函数值。你可以把图上的黑色圆看作初始代价。考虑到函数或变量的梯度可负可正可 0。负梯度意味着该线反向倾斜，反之亦然。现在，我们的目的是最小化代价函数，我们就必须沿着梯度的反方向更新权重。这一更新过程可以用以下公式表示： 图 11：梯度下降法的参数更新 其中α是步长或学习率，我们将它与可学习参数 w 的偏微分相乘。所以α有啥用昵？ 梯度告诉我们哪个方向函数值改变的最快，但是它并未告诉我们应该沿这一方向跨多大步。我们需要一个超参数去控制步长的大小，例如，我们沿某一方向该移动多远，这就是 α 存在的意义。选取正确的学习率十分重要，因为它对两方面有很大的影响：算法的学习速度和我们是否收敛到局部极小值。实际运用中，你可能会运用一个自适应学习率算法，例如动量算法、RMSProp、Adam 等等。AYLIEN 的一个大佬写了一篇关于学习率算法的很棒的文章（如下参考第一篇）。 参考阅读： 技术 | 深度解读最流行的优化算法：梯度下降 深度 | 从修正 Adam 到理解泛化：概览 2017 年深度学习优化算法的最新研究进展 反向传播 我们在前一节讲述了梯度下降算法，它是深度学习的学习问题的一个优化算法。考虑到我们需要计算关于可学习参数 w 和 b 的偏微分才能使用梯度下降法。换句话说，我们需要计算 w 和 b 的偏微分。 但是，如果我们仔细看看代价函数 J，下图 12 所示，就会发现 J 和 w 、 b 并没有直接关系。 图 12：均方误差 只有从得到 y_hat 的输出层追溯到输入层，我们才会发现 J 与 w 、b 的间接关系，如下图 13 所示： 图 13：反向传播图解 你现在应该明白，为了得到代价函数的参数关于 w 和 b 的梯度，我们需要计算所有参数的偏微分，例如前面层的*a* (激活函数) 和 *z* (线性运算: wx + b)，这就是反向传播存在的意义。反向传播其实就是反复运用微积分的链式法则，这可能是神经网络中最有效的计算可学习参数梯度的方法了。 接下来，我手把手带你算一下代价函数对第二层神经网络权重 w2 的梯度，简单起见，我们使用图 8 的结构，一个包含三个神经元的隐藏层。 为了得到 y_hat 对 z2 的变化率，我们需要对 sigmoid 激活函数的 z 求微分。 一旦我们得到 J 对 W2 的偏导值，就可以使用图 11 中的公式更新 W2 的值。 我们通常对所有可学习参数重复这一过程，直到得到尽可能小的代价函数值。 不错！我想我们已经了解了如何构建一个神经网络模型甚至深度学习模型的所有知识，这些知识将帮助我们解决异或问题。 写这篇博客时，我顺便搭了一个简单的单隐藏层神经网络模型。图 14 是我使用的样例网络。我画出了一些由我的模型生成的不同数量的神经元的决策边界。如你后面将看到的，包含更多的神经元会使网络模型变得更加复杂，从而创造一个更复杂的决策边界。 图 14：3 隐藏神经元的两层神经网络 图 15：由一个包含多个神经元（2 个、3 个、4 个）的隐藏层生成的决策边界 但是，到底怎样才是最佳选择？包含更多的神经元还是更多的隐藏层？ 理论上讲，网络深的主要好处就是可以表示更复杂的函数。具体而言，通过使用更深层次的网络结构，我们可以学习许多不同抽象层次的特征，例如，从边缘（较底层）到非常复杂的特征（较深层）。 然而，实际中使用深度网络并非总是有用。我们训练深度网络时最常遇到的就是梯度消失问题：一个非常深的网络通常会发生某个梯度迅速变为零的状况，因此使得梯度下降非常缓慢。 再具体点说，使用梯度下降时，因为反向传播是从输出层传播到输入层，而从输入到输出的每一步都是权重矩阵的相乘，因此梯度可能呈指数衰减到 0，某些情况下甚至会发生梯度爆炸。 参考阅读： 学界 | Andrej Karpathy：你为什么应该理解反向传播 为了结束这篇冗长的博文，简要总结的要点如下： 神经网络直观地引入了可以用来解决一个复杂的非线性可分的数据的非线性模型。 感知机算法为之后的很多高级机器学习甚至是深度学习算法提供了思路。 深度学习直观上就是使用很多隐藏层来搭建一个网络，当然有很多版本，例如卷积网络、循环网络等。 激活函数是神经网络中的重要一环，你必须理解。 目前反向传播搭配梯度下降法是训练神经网络的最佳方案。 使用更多的隐藏层并不一定能提高我们的模型的表现。事实上，深度网络饱受梯度消失之苦。   原文链接：https://becominghuman.ai/from-perceptron-to-deep-neural-nets-504b8ff616e "
381,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736188&idx=2&sn=03793343fabba546cafd9540be0f8277&chksm=871ac242b06d4b54283e7926a91dd4aff8a9480acd4f79a57785822c080dc8e027da96b1d04f&scene=27,业界 | 机器阅读理解打破人类记录，解读阿里iDST SLQA技术,几乎在同一时间，微软和阿里巴巴的机器阅读理解系统在最新的 SQuAD 数据集测评结果中取得了并列第一的成绩。这是历史上第一次，机器阅读理解的精准匹配分数超越了人类的评测结果。 这两天 NLP 圈炸锅了，焦点围绕着微软阿里到底是谁先打破了机器阅读理解的人类记录。 事情是这样的。两家的 PK 发生在 SQuAD 数据集上，这是行业内公认的机器阅读理解标准水平测试，也是该领域顶级赛事，被誉为机器阅读理解界的 ImageNet（图像识别领域的顶级赛事）。和 ImageNet 一样，SQuAD 的成绩排名也会定时更新。 微软和阿里先后于 1 月 3 日 和 1 月 5 日在 SQuAD 官方平台提交模型，但阿里的结果在 1 月 11 日先于微软产出并被公布，微软的结果在 1 月 12 日紧随其后发布。 阿里发布结果后，主办方斯坦福向阿里发了贺信，表彰其机器阅读理解系统首次超越人类。微软结果发布后，鉴于双方在更细分维度上的评测结果各有千秋，官方给出了排名并列第一的说法。 机器阅读理解的评测维度分为 EM（Exact Match，精准匹配分数）和 F1（精确率和召回率的平均，模糊匹配分数）。 下图可见，阿里巴巴在 F1 分数上略胜一筹，微软的 EM 分数优于阿里巴巴。无论如何，我们可以欣喜地看到包括阿里，微软亚洲研究院，腾讯，哈工大和讯飞等中国的研究人员走在了世界的前列。 抛开「谁是第一」事件本身，机器之心采访到此次破纪录的阿里巴巴 iDST NLP 团队，希望回归技术，对其模型进行解读。 机器阅读理解作为 QA 问答技术中的全新领域，允许用户输入非结构化文本及问题，机器在阅读理解基础上，从文本中寻找答案回答用户问题。 对于阿里巴巴来说，机器阅读理解技术的最直接应用就是阿里小蜜现在能直接阅读说明书回答用户问题了。 本次阿里巴巴参与测评的系统名为 SLQA，即 SLQA，即 Semantic Learning for Question Answering，是 iDST NLP 团队提出的「基于分层融合注意力机制」的深度神经网络系统。评测证明，相比传统方法，SLQA 的效果取得了显著的提升。 采用传统方法解决机器阅读理解问题，一般会将该过程分为以下几个步骤： 1）对问题、篇章分别进行词法、句法分析，针对分析结果进行特征提取： 2）基于特征采用诸如 LR、CRF 等模型进行答案边界预测； 3）采用梯度下降类算法在训练集上进行优化，拟合数据分布。 在此过程中，基础语言模型、依存分析等模块的准确率在一定程度上会影响训练效果，特征工程的优劣也同样左右着是否能训练得到可用的模型。 随着深度学习在 NLP 领域的大量应用，很多场景如切词、词性标注、翻译、命名实体识别等 End2End 模型逐渐取得接近并超越传统模型的效果。在机器阅读理解场景，iDST NLP 团队设计了 Semantic Learning Net，即 SLQA 背后的算法模型。 该模型模拟人类在做阅读理解问题时的一些行为，包括结合篇章内容审题、带着问题反复阅读文章、避免阅读中遗忘而进行相关标注等。   团队总结，人类在进行阅读理解时，常见思维顺序如下： 1）通读篇章，理解文章主题和大体内容；读题，了解提问内容及关注点。 2）带着问题找答案，将问题同篇章做关联，并结合篇章主题，理解问题重点。 3）定位可能的答案范围，并再次重点阅读附近文字。 4）为避免忘记问题，再次审题，并结合 3）中重点区域进行答案圈选。 5）针对挑出的答案候选进行精筛，确定最正确的答案。 结合以上思路，团队构建模型的主要思想是在捕捉问题和文章中特定区域关联的同时，借助分层策略，逐步集中注意力，使答案边界清晰。 同时，为了避免过于关注细节，团队采用融合方式将全局信息加入注意力机制，进行适度纠正，确保关注点正确。这种逐步聚焦并兼顾全局的方式与其他参赛者已经公布的的做法不太相同，也是团队此次刷榜登顶的关键所在。 目前业界主流的基于 End2End 学习的机器阅读理解模型主要为 Encode-Interaction-Pointer 框架。基于上述分析，SLQA 系统包含如下基本结构：Encoder Layer（文本表征），Attention Layer（注意力机制），Match Layer（问题篇章匹配）以及 Output Layer（答案预测）。 Encoder Layer 用于表示学习，可以理解为语言模型层，用以将篇章及问题从离散字符转变为蕴含语义的表征向量。团队采用了多层双向 LSTM 并分别对篇章和问题进行主题和重点词关注。 Attention Layer 得到有效的问题及篇章表征后，为表达依据问题定位答案过程，缩小备选答案查找范围，将搜索空间通过注意力机制约束，主要进行多层融合注意力表示，对问题和篇章进行相关性对齐（Align），并不断补充全局信息（Fusion），每一次对齐都基于下层信息并在此基础上更加细化（paragraph→sentence→phrase→word），采用的方式分别为 Co-Attention（篇章到问题，问题到篇章），Self-Attention（问题自身，篇章自身）。 Match Layer 用于做融合信息后的问题和篇章匹配，团队采用双线性矩阵来学习经过多层信息过滤后的篇章和问题匹配参数，由于在前一阶段无关信息已经被过滤，最后的匹配可完成答案的定位工作。 Output Layer 结合匹配信息对篇章中词汇进行标注，预测相应词汇是答案开始位置或结束位置的概率。之后，模型会抽取可能性最高的一段连续文本作为答案。 团队采用的技术就是基于以上四个Layer的深度神经网络模型，重点探索和研究的Layer是第三层（Hierarchical Attention Fusion Network）。 iDST NLP 团队负责人司罗表示，本次 SQuAD 评测登顶得益于其 NLP 团队自身的完善性。「NLP 领域内的很多技术方向可以互相借鉴，例如机器阅读理解任务，我们就借鉴了机器翻译的一些技术。应该说我们机器阅读理解的技术是建立在我们更广阔的自然语言处理能力上的。」 据司罗介绍，本次登顶只是阿里巴巴相关技术研发的一个侧面，其所指向的「创新的问答系统」的落地应用才是团队的重要发展方向。 由于团队支持阿里大生态下的问答技术业务（如与阿里小蜜合作的智能客服等），因此团队的着眼点其实一直放在业务应用场景而非单纯的技术突破上。 「阿里小蜜是我们所知的第一个真正把机器阅读理解应用在大规模客服场景下的产品。」司罗说。 除阿里小蜜外，SLQA 系统在售前咨询场景也能发挥作用。 面向商家的智能客服「店小蜜」是阿里售前咨询场景的典型案例。顾客在购物时，往往会对商品信息进行询问确认后才会下单购买，例如「荣耀 5c 的双摄像头拍照效果有什么特点？」而这些信息往往已经存在于商品的详情描述页。 店小蜜通过机器阅读理解技术，让机器对详情页中的商品描述文本进行更为智能的阅读和回答，在降低卖家服务成本的同时提高购买转化率。 更广泛地，着眼整个社会，机器阅读理解也有着巨大的价值。试想机器自动阅读病历回答病人关心的问题、自动阅读古今名著帮人们写论文、自动阅读旅游场景的说明书来回答旅行者的问题、自动阅读繁复的法律税务条款来帮助人们解决法律报税的问题。 「机器阅读理解让知识获取不受人脑的限制。」司罗说。 尽管社会价值巨大，但目前的机器阅读理解技术还在面临很大的挑战。例如该技术对于解决 WIKI 类客观知识问答已经取得比较好的结果，但对于复杂问题来说仍处于比较初级的阶段。 司罗认为，这是由于 WIKI 场景的数据相对较为充分、文档结构也清晰、内容描述较为正规。而其他广大的应用场景常常存在训练数据不足、文档知识不明确、描述不完整等问题，有不少甚至要通过多步推理才能得到答案。 这不仅对阿里巴巴，也对整个业界提出了挑战。司罗表示，阿里希望建立自己完善的自然语言技术能力，在领域自适应、多步推理、知识自动抽取和归纳等方面进行全面且充分的准备，迎接这些挑战。 
382,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736188&idx=4&sn=1401f52ddc55d6a6d0d0891e34e7197c&chksm=871ac242b06d4b54dd854d1d07d14014d0331ca0bcd96b0992e25cd24454566b8704678e0882&scene=27,学界 | 有趣的研究奥巴马Net：从文本合成真实的唇语口型,"结合语音合成模型、视频生成模型等，本论文研究了如何使用原始文本生成人读随机文本的虚拟视频，且口型完全对照，更加自然逼真。 目前存在大量关于使用机器学习方法生成图像的研究（Isola et al.，2016）。同样，语音合成方面也有显著进展（Sotelo et al.，2017）。不过，将两种模式同时建模的研究并不多。本论文展示了结合多个近期开发的模型生成人读随机文本的虚拟视频。我们的模型可在人说话的任意近景（close shot）视频集合（带对应的转录文本）上进行训练。结果就是构建了一个系统，可利用任意文本生成语音，并根据现有视频中嘴型区域进行修改，以使其更加自然逼真。视频示例：http://ritheshkumar.com/obamanet。我们以 Barack Obama 为例展示了该方法，因为他的视频常用于对唇同步方法进行基准测试，但是我们的方法还可用于生成任意人的视频（在可获取数据的前提下）。 2. 相关研究 近期，生成照片级真实感视频领域出现了显著进展（Thies et al., 2016）。具体来说，Karras et al. (2017) 尝试基于音频生成人脸动画。Suwajanakorn et al. (2017) 的研究与我们的研究最接近，但是存在两个重要差异：一，我们用神经网络，而不是传统的计算机视觉模型；二，我们添加了一个文本转语音合成器以构建完整的文本转视频系统。 3 模型描述 3.1 文本转语音系统 我们使用 Char2Wav 架构从输入文本中生成语音，我们使用从视频中提取的音频，加上对应的转录文本，来训练语音合成系统。 3.2 关键点生成 给定输入音频，该模块会预测口型的表达。我们使用谱特征（spectral features）来表示音频。为了计算口型表示，我们需要从面部提取的嘴部关键点，并做归一化处理从而不受图像大小、面部位置、面部旋转、面部大小的影响。归一化在此过程中非常重要，因为它能使生成的关键点兼容于任何视频。然后，我们在归一化处理过的嘴部关键点上利用 PCA 降维，对特征去相关（decorrelate）。我们只使用最重要部分作为口型的表征。   至于网络架构，我们采用 Suwajanakorn 等人在 2017 年论文中使用的同样架构：也就是给定音频特征输入的情况下，使用一个带有时间延迟的 LSTM 网络来预测口型表示。 3.3 视频生成 我们选择视频生成方法背后的动机是近年来 pix2pix（Isola 等人 2016）的成功，它成为了图像到图像转译任务的通用解决方案。该任务属于我们的文章范围，因为我们的目标是，基于嘴部表征，将输入的面部图像（对嘴部做过剪裁的）转译为嘴部区域重绘的输出图像。 为了避免直接以嘴部特征作为 U-Net 架构的表征，我们通过在输入的剪裁图像上绘制嘴部轮廓隐含地作为条件。网络学习利用这种轮廓决定输出的嘴部生成图像。 我们注意到，循环网络生成的关键点随时间没有变化。在给定嘴部关键点信息的情况下，这使得我们能够通过独立合成视频的每一帧，并行完成视频生成。在生成视频帧上，我们不需要任何机制来保持时间上的一致性。 我们在像素空间中，只使用 L1 损失函数来训练该网络，发现该目标足够学习嘴部图像的重绘（in-painting）且不像 pix2pix 原论文中提到的那样需要额外的 GAN。 论文： ObamaNet: Photo-realistic lip-sync from text  链接： https://arxiv.org/abs/1801.01442v1 摘要：我们展示了 ObamaNet，首个利用任意新文本生成音频和照片级真实感唇同步视频的架构。与其他已公开的唇同步方法相反，我们的方法仅使用完全训练的神经模块，不依赖传统的计算机绘图方法。更准确地说，我们使用了三个主要模块：基于 Char2Wav 的文本转语音网络、用于生成与音频同步的嘴特征点的时间延迟 LSTM，和基于 Pix2Pix、用于生成基于特征点的视频帧的网络。 "
383,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736206&idx=1&sn=9dc96e4545767fd065946121321fb669&chksm=871ac230b06d4b26d7f6f83c5f6c502ea69d28b11637591f1650d9ea57d68bd096b35b5b5eef&scene=27,「我是可微分编程的粉丝」，Gary Marcus再回应深度学习批判言论,"近日，Gary Marcus 针对各研究者与开发者的评论作出了回应，他从什么是通用人工智能开始回应了常见的 14 个质疑或问题，其中就包括 LeCun 所说的「mostly wrong」。此外，Marcus 还重申了他对深度学习的观点，并继续补充了一些他在上一篇文章所没提到的局限性。 所有真理必经过三个阶段：第一，被嘲笑；第二，被激烈反对；第三，被不证自明地接受。——叔本华（德国哲学家，1788-1860） 在上篇文章中（参见： ），我列出了深度学习的十大挑战，并认为深度学习本身尽管很有用，但不太可能独自实现通用人工智能。我认为深度学习「并非一种通用的解决方案，而只是众多工具之一」。 取代纯粹深度学习的将是混合模型，它不仅具有深度学习的监督形式，还包含其他技术，比如符号处理（symbol-manipulation）和无监督学习（也可能会被重新概念化）。我同样敦促社区将更多的先验结构（innate structure）纳入 AI 系统。 文章一推出，引来数千人在 Twitter 上讨论，有些人非常认同（比如「这是很多年来我读过的最好的深度学习与 AI 文章」），有些人反对（「很有想法… 但大部分是错的」）。 我认为清晰地认识这些问题很重要，因此我编辑了一个包含 14 个常见问题的清单：无监督学习的用武之地在哪儿？为什么我没有描述有关深度学习的更美好事情？是什么给了我权利首先讨论这一事情？让神经网络实现从偶数到奇数的泛化有何意义？（这是最重要的问题）以及更多其他问题。我没有涉及所有问题，但是我尽量做到有代表性。 1. 什么是通用智能？ 机器学习著名教授 Thomas Dietterich，我目前最彻底而明确的反对者，给出了一个让我非常满意的回答： 「通用智能」是一个系统，可在一系列的目标和环境中智能地执行动作。可参见 Russell 与 Norvig 的教科书《人工智能：一种现代方法》，其中把智能定义为「理性地行动」。 2. Marcus 不喜欢深度学习。他应该多说些深度学习的大量应用所取得的实际成果。 上面提及的 Thomas Dietterich 教授写到：Gary Marcus 的文章令人失望。他很少述及深度学习的成就（比如自然语言翻译），并小看了其他成果（比如包含 1000 个类别的 ImageNet 是小型的／「非常有限」）。 对于第一点，我当然可以说出更多成果，但是却没有，就像我忘记提及 Dietterich 的最佳实例；不过我在《Deep Learning: A Critical Appraisal》第一页就提到： 深度学习取得了大量的当前最佳结果，比如在语音识别、图像识别和语言翻译领域，并在当前广泛的 AI 应用中发挥着重要作用。 稍后我将在文中引用若干个很不错的文本和博文，其中包含大量的实例。但是其中大多数不能被称为 AGI，这是我的论文主旨。（比如，谷歌翻译非常赞，但不通用，它无法像人一样回答关于其翻译内容的问题。） 第二点更加真实。1000 个分类真的很有限吗？是的，如果与认知的灵活性相比的话。认知科学家通常认为个体所知道的概念数量是 50000 个，并且人类可以轻易地把这些概念组合成数量更多的复杂想法。宠物（pet）和鱼（fish）很可能在这 50000 个概念之中；而一个不同的概念宠物鱼（pet fish）很可能不在。并且我可以轻易地接受「a pet fish that is suffering from Ick」这个概念，或者注意到「it is always disappointing to buy a pet fish only to discover that it was infected with Ick」（我小时候得过 Ick 这种病，至今依然很排斥）。我可以表达多少像这样的想法？明显超过 1000。 我并不确切知道人类可识别多少视觉范畴，但我的猜测大体不差。试着谷歌一下有关「pet fish」的图像，没问题；然后再试下「pet fish wearing goggles」，你得到的大多是带着眼镜的狗，错误率超过 80%。 依我看，ImageNet 限定为 1000 个类别本身对机器学习就是一种危害，它取得了短期的成功，却避开了更难、更开放、必须最终解决的问题（比如场景和语句理解）。相较于本质上我们可以看到和理解的无限的语句、场景，1000 个类别实在是太少了（参见文末 Note 2）。 3. Marcus 说深度学习没有什么用，但它对很多任务都有实际意义 当然深度学习是有用的，我从没这样说过它没有用。只是说在目前监督学习的形式下，深度学习可能正接近它的极限；这些极限将阻碍我们通往通用人工智能的进程——除非我们开始结合符号运算和先天经验等。 我的核心观点是：尽管我提出了这么多问题，但我不认为我们需要放弃深度学习。相反，我们需要对其进行重新概念化：它不是一个普遍的解决办法，而仅仅只是众多工具中的一个。我们有电动螺丝刀，但我们还需要锤子、扳手和钳子，因此我们不能只提到钻头、电压表、逻辑探头和示波器。 4. Gary Marcus 说 DL 对于层级结构来说并不够好，但是 LeCun 表明深度学习非常擅长利用这种层级结构 这是 Ram Shankar 提出的非常机敏的问题，我应该更清楚地回答：我们能考虑的层级结构有很多种。对于层级特征，深度学习是非常好，也许是有史以来效果最好的。就像 LeCun 所说的那样能高效处理特征层面的层级结构，我们通常把它表示为层级的特征检测。这就像我们用点构建线、用线构建字、用字构建句子那样。Kurzweil 和 Hawkins 也着重强调这一点，且这种层级结构真的可以追溯到 Hubel 和 Wiesel（1959）的神经科学试验和 Fukushima 在 AI 上的观点（Fukushima, Miyake, & Ito, 1983）。Fukushima 在他的神经认知学模型中手动构建了许多抽象特征的层级表示，而 LeCun 和很多研究者表示我们不需要手动完成这件事，让机器执行就行了。 但是这种方式并不需要追踪所遇到的子成分，顶层系统不需要明确地编码整个输出的结构，这也是为什么深度学习系统会出现对抗样本的挑战。例如在典型的图像识别深度网络中，没有完全认知到校车由轮子、底盘、车窗等构成，因此它们会认为黄色和黑色的条纹就是校车（(Nguyen, Yosinski, & Clune, 2014），且这种条纹的模型与校车的输出单元激活值密切相关。[Note 3] 我所讨论的层级结构是不同的，它围绕着可以被明确解释的整体和部分之间的关系而构建。经典的证明是乔姆斯基的层级观点，在一个有复杂语法单元组成的句子中，如使用新颖的短语「the man who mistook his hamburger for a hot dog with a larger sentence like The actress」代替「she would not be outdone by the man who mistook his hamburger for a hot dog」。 我不认为深度学习在理解上述的女演员、男人、和热狗之间的关系，尽管它会尝试着去理解。 即使在视觉上，问题也会存在，例如 Hinton 最近关于 Capsule 的研究（Sabour, Frosst, & Hinton, 2017），他们试图通过使用更多结构化的网络在编码方向上构建更鲁棒的图像识别模型。我认为这是一个很好的趋势，也是一个潜在能解决对抗样本问题的方法。 5. 在通用人工智能的环境下讨论深度学习是有问题的，因为深度学习的目标就不是通用人工智能！ 这个问题最好的回答是魁北克教授 Daniel Lemire 所说的：「Oh! Come on! Hinton, Bengio… are openly going for a model of human intelligence」。 其实有很多学界领军人物都表明深度学习是很难找到它的极限的，它能面对超乎我们想象的困难。DeepMind 最近的 AlphaGo 论文 [见 Note 4] 有类似的定位，Silver 等人（Silver et al., 2017）表明： 「我们的研究结果全面地证明了，一个纯粹 [深度] 强化学习方法是完全可行的，即使在最具挑战的领域。」 总之而言，人们持续对人类水平和 AI 系统的性能进行基准测试，很大的原因就是因为 AGI 就是我们的目标。 6. Marcus 认为有问题的是监督学习，并非深度学习。 Yann LeCun 在我的 Facebook 主页中发表了以下评论： 我没有时间做出完整的回应，但总而言之：（1）我认为文章的大部分观点都有错误。如果文中所有「深度学习」的实例都被「监督学习」取代，那么错误就会大大减少。（2）寻找一种将深度学习的概念拓展到无监督学习和推理中的方法，正是过去 2.5 年来我一直倡导的。我不只是在倡导它，实际上我一直在努力...... 你对这件事很了解，但是你没在论文中写明。 上述评论中，所谓我不承认 LeCun 最近的工作这一部分有些奇怪。诚然，我没能找到一篇能在我文中引用的总结性文章（当我问 LeCun 时，他通过邮件告诉我还没有这样一个文章），但是我明确地提到了他的兴趣： 最近深度学习先驱 Geoffrey Hinton 和 Yann LeCun 都表明无监督学习是超越有监督、少数据深度学习的关键方法。 我同样在文中指明：我们要清楚，深度学习和无监督学习并不是逻辑对立的。深度学习主要用于带标注数据的有监督学习，但是也有一些方法可以在无监督环境下使用深度学习。 我的结论也是积极的。虽然我对目前建立无监督学习系统的方法表达了保留意见，但我最终的结语是乐观的：如果我们建立了能设定自身目标的系统，并在更抽象的层面上进行推理和解决问题，那么人工智能领域将会有重大的进展。 LeCun 的评论中正确的部分是，我提到的许多问题是监督学习中的普遍问题，而非深度学习所特有的问题。我本可以更清楚地阐明这一点。许多其他的监督学习技术面临类似的挑战，例如泛化以及对海量数据集的依赖；而在我所说的问题中，深度学习所特有的问题相对较少。 但是，其他监督学习技术同病相连，无法真正帮助深度学习。如果有人能提出一个真正令人钦佩的、以无监督的方式进行深度学习的方法，可能对深度学习需要重新进行评估。然而，我没有看到那种无监督学习，至少是目前所追求的。目前，它们都无法对于我提出的挑战（例如推理、层级表征、迁移、鲁棒性和可解释性）进行补救。现在看来，这只是一个金融期票而已。[Note 5] 正如波特兰州立大学和圣达菲研究所教授 Melanie Mitchell 迄今为止在一条无答复的 tweet 中提到的：……LeCun 说 Gary Marcus 文章是「大部分错误」，但是如果限制在监督学习，那么「错误大大减少」。我很愿意听到（现有的）无监督学习项目的例子，希望有这种例子能说明 Gary Marcus 的说法是错误的。 我也很愿意听到这样的消息。 同时，我认为没有原则性的理由能让人相信无监督学习可以解决我提出的问题，除非我们首先加入更加抽象、象征性的表述。 7. 深度学习不仅包括卷积网络（Marcus 所批评的那种），它「本质上是一种新的编程风格——『可微分编程』——而且该领域正试图用这种风格实现可重用构造。我们已有一些方向：卷积、池化、LSTM、GAN、VAE、记忆单元、路由单元等。」——Tom Dietterich 这似乎（在 Dietterich 的更长的一系列推文中）作为一种批评被提出。但我对此感到困惑，因为我是一个可微分编程的粉丝，而且我也是这么说的。也许关键在于，深度学习可以采取更广泛的方式。 无论在什么情况下，我都不会将深度学习和可微分编程（例如我所引用的、像神经图灵机和神经编程这样的方法）等同起来。深度学习是许多可微分系统的组成部分。但是这样的系统也完全插入了从符号处理技术中提取的重要元素，我也一直在敦促这个领域对此整合（Marcus，2001; Marcus，Marblestone，＆Dean，2014a; Marcus，Marblestone，＆Dean，2014b）。这其中包括记忆单元、变量操作以及其他系统（比如近两篇论文所强调的路由单元）。如果把这所有都融合到深度学习中能让我们得到通用人工智能，那么我下面的结论将会完全正确：对于扩展来说，大脑可能被视为由「一系列可重复使用的计算基元组成 - 基本单元的处理类似于微处理器中的一组基本指令。这种方式在可重新配置的集成电路中被称为现场可编程门阵列（FPGA）」，正如我在其它地方（Marcus，Marblestone，＆Dean，2014）所论述的那样，逐步丰富我们的计算系统所建立的指令集会有很大的好处。 8. 现在 vs 未来。也许深度学习现在不起作用，但我们的子孙后代可能实现 AGI。 有可能。我认为深度学习在将我们导向 AGI 上可能扮演着重要的角色，如果首先添加一些关键的东西（许多还没有被发现）的话。 但是，补充哪些因素至关重要？这个未来的系统，应该称为深度学习本身，还是更为合理地称之为「使用深度学习的某某某」？这取决于深度学习在终极解决方案的哪个部分起作用。例如，也许在真正充分理解自然语言的系统中，操作符号的方法将扮演深度学习的同样重要的角色，或者更重要的角色。 当然，术语学是这个问题的一部分。最近一个好朋友问我，为什么我们不能将包括深度学习在内的任何东西都称为深度学习，即使它包含操作符号的方法？深度学习的优化处理应该很关键。对此我作出回应：为什么不把包含符号操作在内的任何东西都称为符号操作，即使它包含深度学习呢？ 基于梯度的优化应该得到应有的效果，但符号处理也应该是这样。符号处理是系统地表示和实现高级抽象的唯一已知工具，它基本上覆盖了世界上所有复杂的计算机系统，包括电子表格、编程环境、操作系统等。 最后，我猜想，最后的成功也将归因于神经网络和符号处理之间的不可避免的联姻，混合系统将把这两个同样于 20 世纪 50 年代初发展起来的 20 世纪人工智能的伟大思想汇集在一起。其他尚未发明的新工具也可能至关重要。 对于一个深度学习的真正追随者而言，任何东西都是深度学习，无论它如何与其他技术融合，无论它与现有技术有多么不同。（帝国主义万岁！）如果你用一个神经元代替了经典的、符号性微处理器中的每一个晶体管，但是保持芯片的逻辑完全不变，一个真正的深度学习追随者仍然会宣告胜利。但是，如果我们把所有技术混在一起，我们就无法理解推动（最终）成功的关键法则。[Note 6]  9. 没有机器可以推断。因此期望神经网络可以从偶数中生成奇数并不公平。 这里有一个以二进制位为表达式的函数。 f(110) = 011； f(100) = 001； f(010) = 010。 那么 f(111) 等于多少？ 普通人或许会猜测 f(111) 等于 111。但如果你是上文讨论的那种神经网络，你的答案或许并非如此。 如果你曾数次听说过神经网络中的隐藏层可以「抽象化函数」，那么你应该对此感到惊讶。 如果你是人类，你可能认为该函数就是某种「逆转」，可以用一串计算机代码轻松表达。如果你是某种神经网络，那么学习以从偶数扩展到奇数的方式将这种逆转抽象化非常困难。不过是否有可能做到呢？如果你没有对整数的先验知识，则不可能。试试另一种情况，这次是十进制位：f(4) = 8；f(6) = 12，f(5) 等于多少？人类读者不会关心这个问题需要从偶数扩展到技术，而大量神经网络却会产生困惑。 当然，由于示例较少，该函数并不确定，但是大部分人会认为 f(5)=10，这一点非常有趣和重要。 同样有趣的是，大部分标准多层感知机（代表二进制位数字）给出的答案并非如此。这给了我们一些启示，但是神经网络社区的很多人对此不以为然，François Chollet 是一个例外。 重要的是，识别一个可以应用到所有整数的规则就相当于神经网络识别在一个语境中使用的新名词可以在大量其他语境中使用。我第一次听说 blicket 这个词表示物体时，猜测它可用于多种情况，如 I thought I saw a blicket、I had a close encounter with a blicket，以及 exceptionally large blickets frighten me 等等。我就可以生成和解释此类句子，而无需特殊的训练。blicket 是否与我听到的其他词语音相近并不重要。如果大部分机器学习系统处理该问题时遇到问题，那么我们理应认为大部分机器学习系统有问题。 那么我「公平」吗？公平，也不公平。我确实让神经网络做一些违反它们假设的事情。 神经网络拥护者或许会说「等一下，在你的『逆转』示例中，输入空间有三个维度，分别代表最左边的二进制位、中间的二进制位和最右边的二进制位。训练过程中最右边的二进制位只能是零，如果那个位置的数字是 1 的话，网络就不知道该怎么做了。」比如，康奈尔大学的一位博后 Vincent Lostenlan 说：「我不理解你在 3.11 中想证明什么。f 是输入空间中（n-1）维超立方体顶点的恒等函数。你为什么对 DNN，或者说任何 ML 模型感到震惊，而不是「泛化」至第 n 维？」 Dietterich 也持相同观点，只不过更为准确：「Marcus 抱怨深度学习无法推断，但是『没有』方法可以推断。」 但是尽管对于深度学习难以解决奇偶数的问题（本文语境下）二者都说得对，但是在更大的问题上，他们的观点都是错误的，理由有三： 一，人类可以推断。在上述两个示例中，人类都可以推断出正确答案。你打算相信谁，我还是你自己的眼睛？ 对于在当代机器学习中浸淫已久的人来说，我的奇偶数问题似乎并不公平，因为训练过程中并没有说明特定的维度（限制最右的二进制位的值为 1）。但是当人类看到上述示例时，你不会被训练数据中的这一差距阻挠，你甚至不会注意到它，因为你的注意力处于更高级的规律。 人们通常用我刚才描述的方式进行推断，比如从上文给出的三个训练示例中识别出字符串逆转。从技术角度看，这是推断，而你恰好做到了。我在《The Algebraic Mind》中认为这种推断是在训练示例空间以外泛化全称量化一对一映射（universally quantified one-to-one mapping）。如果我们想要赶上人类学习，则找到该问题的解决方案非常重要，即使这意味着动摇原有的假设。 现在，很可能有人用这个理由认为这不公平：人类泛化此类映射时，明显依赖于先验知识。 确实如此。但是重点是：某种神经网络缺乏好的方式来整合合适的先验知识。准确地说是因为这些网络缺乏好的方式来整合先验知识，如「很多泛化适用于无界类别的所有元素」或「奇数除以 2，余数为 1」，神经网络缺乏对变量的运算时就会失败。合适的先验知识允许神经网络获取和表示全称量化一对一映射。标准的神经网络无法表示此类映射，除了使用有限的一些方式（比如卷积）。 二，当前没有任何系统（深度学习或其他）可以用我上文描述的方式进行推断，这样说并非没有理由。其他架构可能「处于险境」，但是这不意味着我们应该放弃游向岸边。如果我们想实现通用人工智能，就必须解决这个问题。 三，当前没有系统可进行推断的论断是错误的；已经存在很多 ML 系统至少能够推断出我描述的部分函数，你或许就拥有一个：Microsoft Excel，具体来说是它的快速填入（Flash Fill）函数（Gulwani, 2011）。支持它的方法与机器学习大相径庭，但是它可以进行某种推断，尽管是在比较狭窄的领域中。 它甚至可以用你所用的方式，即使在百位数的训练维度中没有正例。该系统从你想要的函数示例中学习，并进行推断。小菜一碟。深度学习系统可以用三个训练示例做到吗？即使有在其他小型计数函数上的大量经验。 也许吧，但这样做的唯一方法可能就只是变量运算的混合，这与大多数深度学习中典型的卷积神经网络所采用的方法是不同的。 为了把所有这一切都变得不同，一个粗略的方法就是考虑目前大多数机器学习系统所处的阶段是什么 [Note 7]，即它们并不是考虑被设计为「outside the box」，它们被设计为在黑箱内完美的嵌入器。对于一些目标来说，这是没什么问题的，但并不是所有。人类比目前的 AI 更擅长于站在黑箱外思考，我不认为有人会反驳这一点。 但是没有机器能处理类似人那样广度的问题，如果机器学习工程师希望为 AGI 努力，那么他们真应该朝这个方向发展。 10. 你所论述的事实该领域中的每个人都已知道了，它并没有新意。 当然，并不是所有人都知道。正如前所述，很多评论者都表明我们还不知道深度学习的极限在哪，还有人认为极限会有一些，但是我们目前还没有发现。也就是说，我从来没有说过我的观点是全新的，我引用了很多学者的研究结果，他们都独立地得出了相似的观点。 11. Marcus 没有引用 XXX。 是的没错，文献引用是不完整的。我未引用的论文中最重要的是 Shanahan 的 Deep Symbolic Reinforcement（Garnelo，Arulkumaran & Shanahan，2016）；我也漏掉了 Richardson 和 Domingos（2006）的 Markov Logic Networks。如果现在来看，我还希望引用 DeepMind 的 Evans 和 Edward Grefenstette 2017 年的一篇论文，以及 Smolensky 有关张量计算的文章（Smolensky 等人 2016）。以及多种形式归纳编程的研究（Gulwani 等人，2015），以及概率编程（Goodman、Mansinghka、Roy、Bonawitz & Tenenbaum，2012）。所有这些研究都努力在将网络与规则联系在一起。 此外还有 Jordan Pollack 等先驱者们的早先研究（Smolensky 等人，2016）。以及 Forbus 和 Gentner（Falkenhainer，Forbus & Gentner，1989）以及 Hofstadter 和 Mitchell 1994 年进行的类比工作，还有很多。还有很多文献是需要引用的。 总之，我试图找出其中具有代表性的研究，而非全面引用，但我承认还是应该做得更好…… 12. Marcus 不是站在业内的角度思考问题的，他不是推动者，他只是一个批评者。 关于是否列出这个问题，我有些犹豫，但我看到有很多人都持有这种观点，其中甚至包括一些知名专家。正如 Ram Shankar 提到的，「作为一个社区，我们必须把批评限制在科学和价值的层面上。」真正重要的不是我的资历（事实上我认为自己有资格写这样的文章），而是论证的有效性。 要么我的论点是正确的，要么不是。 不过，对于那些希望了解我的背景的人，在本文附录中有一些可以参考的资料。 13. Re：层次结构，Socher 的 tree-RNN 如何呢？ 我已写邮件向作者问询，希望进一步了解这项技术。我也在推动其他一些研究团队尝试 Lake 与 Baroni（2017）这类的研究。 Pengfei 等人（2017）也提出了一些有趣的讨论。 14. 你对深度学习的批判应该更强烈。 明面上还没有人这么说，但有一些类似的话已经出现了，大多是在私下里。 例如有些人就指出：深度学习可能会在未来预测上出现一些严重错误。 目前，对于深度学习成功的感觉正以指数级的速度快速发展……这就像鸟儿低空掠过树枝，看到大量果实，一旦飞过果树，进行深度推理的速度就会变慢了。此外，我不明白在识别猫正确率刚刚达到 95% 的今天，为什么人们对于通用人工智能、伦理、道德有这么多的思考。后一类的问题应该存在于更复杂的空间之上。 这位同事还补充说：[研究者们] 在宣布在某些领域取得胜利的速度过快了。比如图像处理：我们已经发现了一类计算机更加擅长解决的图像处理问题，确实如此，但同样这些算法仍然会被对抗攻击迷惑。此外，当它们出错时，错误往往非常离谱。与之相对的，当我在街道上驾驶汽车时，我可能会把一棵树误认为是路灯柱，但我不会有那些深度学习网络犯的那些奇怪错误（这是因为我对含义和背景信息有深入的理解）。人们确实通常知道这些局限性，但 ImageNet 的结果给人们带来了一个基本观点：计算机比人类更擅长图像识别。 另一位同事、机器学习研究者和作者 Pedro Domingos 指出了一些我没有提到的当前深度学习方法的其它短板： 和其它灵活的监督学习方法类似，深度学习系统可能不稳定——训练数据的少许改变可能会导致所得模型发生巨大变化。 即使更少量的数据就足够了，但它们还是需要大量数据。（数据增强的成本非常高，而在人类看来，这应该不是必需的。） 它们可能很脆弱：数据上的微小变化可能会导致灾难性的失败（比如将数字数据集中的黑白像素翻转（Hosseini, Xiao, Jaiswal, & Poovendran, 2017））。 它们的准确度往往比我们推断的更低（比如 Ribeiro, Singh and Guestrin (2016) 发现在从 ImageNet 提取出的一个数据集上实现的狼与狗辨别的高准确度主要是通过检测狼图像中的白色雪堆得到的。） 在机器学习的历史中，到目前为止，每一种范式在失势之前往往会主导大约十年的时间（比如神经网络主导了八十年代，贝叶斯学习主导了九十年代，核方法主导了 2000 年代）。 正如 Domingos 指出的那样，我们不能保证这种起起伏伏还会重复。神经网络之前已经经历过几次起伏了，一直可以追溯到 1957 年 Rosenblatt 的第一个感知器。我们不应该将这种周期性的热情误认为是智能的完全解决方案——在我看来，这仍然还需要数十年的时间。 如果我们想实现 AGI，我们自己必须清晰地认识到我们成功路上所面临的挑战。 备注 1. 感谢 Amy Bernard、Josh Cohen、Ernie Davis、Shlomo Shraga Engelson、Jose Hernandez-Orallo、Adam Marblestone、Melanie Mitchell、Ajay Patel、Omar Uddin 和 Brad Wyble 给出的评论。 2. 依赖这 1000 个图像集还存在其它问题。比如，在阅读本论文的草稿时，Melanie Mitchell 向我指出了 Loghmani 及其同事 (2017) 最近在评估深度学习在真实世界中的表现的重要成果。该论文的摘要写道：「分析深度表征从网络图像到 [现实中的] 机器人数据的可迁移性。尽管 [使用网络图像开发的表征] 得到了出色的结果，但实验表明在真实机器人数据上的物体分类还远未得到解决。」 3. 而且文献正在快速增长。12 月底有一篇关于欺骗深度网络使其将两位滑雪者误认为狗的论文（https://arxiv.org/pdf/1712.07113.pdf）以及另一篇关于用于构建真实世界对抗补丁的通用工具的论文（https://arxiv.org/pdf/1712.09665.pdf，也可参阅 https://arxiv.org/abs/1801.00634。）(https://arxiv.org/abs/1801.00634%E3%80%82%EF%BC%89) 深度学习在真实环境中竟如此脆弱，想想都可怕。 对于这个问题，可以查阅 Filip Pieknewski 的博客了解为什么使用照片训练的深度学习系统难以将它们所学到东西迁移到线条图画上：https://blog.piekniewski.info/2016/12/29/can-a-deep-net-see-a-cat/。视觉并不像很多人以为的那样已经得到了解决。 4. 正如我将在即将到来的论文中会解释的那样，AlphaGo 实际上并不是一个纯粹的（深度）强化学习系统，尽管引用的段落看起来好像是这样。这实际上是一个混合系统——其中包含由操作符号的算法驱动的组件以及一个经过精心设计的深度学习组件。 5. 随便一提，AlphaZero 并不是无监督的，而是自监督的——使用了自我对弈和模拟作为生成监督数据的方式；我会在接下来的论文中更详细地讨论该系统。 6. 比如谷歌搜索以及理解它的可能方式。谷歌最近已经在其用于搜索的大量算法中加入了深度学习算法 RankBrain。而且谷歌搜索肯定是在输入数据和知识后以分层的方式处理它们（按 Maher Ibrahim 的说法，这就是算作深度学习所需的一切）。但是，深度学习实际上只是众多算法中的一个；比如知识图谱组件则主要基于遍历本体（traversing ontology）的经典人工智能概念。从任何合理的角度看，谷歌搜索都是一个混合系统，深度学习只是其中众多方法中的一种。 将谷歌搜索整体上称为「一个深度学习系统」是一种严重的误导，就好像是因为木工活涉及到螺丝刀就把木工活称为「螺丝刀」。 7. 归纳逻辑编程、归纳函数编程（微软 Flash Fill 背后的大脑）和神经编程等是重要的例外。这些全部都取得了一些进展；其中一些甚至包含了深度学习，但在它们原本的操作运算之外还包括对变量的结构化表征和操作；这就是我要求的一切。 8. 我的人工智能实验始于青少年时期，其中包括用 Logo 编程语言编写的一个拉丁语-英语翻译器。在研究生学院，我与 Steven Pinker 一起研究探索了语言习得、符号规则和神经网络之间的关系。（我也要感谢我的本科导师 Neil Stillings）。我为我的论文（Marcus et al., 1992）收集的儿童语言数据已经被引用了数百次 在 20 世纪 90 年代末，我发现了多层感知器中一些特定的可复现的问题（Marcus, 1998b; Marcus, 1998a）；基于这些观察，我设计了一个被广为引用的实验。该研究发表在 Science 上（Marcus, Vijayan, Bandi Rao, & Vishton, 1999），表明年幼的婴儿可以提取代数规则，而 Jeff Elman（1990）的当时流行的神经网络则无法做到。所有这些在 MIT Press 2001 年出版的一本书（Marcus, 2001）中达到了高潮，其中包含了各种代表性的原语，其中一些已经开始出现在最近的神经网络中；尤其是在新的差分编程领域（Daniluk, Rocktäschel, Welbl, & Riedel, 2017; Graves et al., 2016）中对变量的操作，相关研究应该在某个位置引述这本书。使用记忆记录也得到了重点强调，在 Facebook（Bordes, Usunier, Chopra, & Weston, 2015）等的关于记忆网络的研究中可以看到相关思想。接下来的十年我研究的其它问题包括遗传性（innateness，Marcus, 2004）（我会在接下来关于 AlphaGo 的论文中详细讨论）和进化（Marcus, 2004; Marcus, 2008），我最后又回到了人工智能和认知建模。我在 2014 年发表在 Science 上的一篇关于皮质计算的文章（Marcus, Marblestone, & Dean, 2014）也预测了一些当前差分编程领域正在发生的情况。 最近我离开了学术界，在 2014 年创立了并领导着一家机器学习公司；从任何合理的角度看，这家公司都很成功——在成立大约两年之后被 Uber 收购了。作为联合创始人和 CEO，我组建了一个包含世界上一些最好的机器学习人才的团队，其中有 Zoubin Ghahramani、Jeff Clune、Noah Goodman、Ken Stanley 和 Jason Yosinski；该团队在开发我们的核心知识产权和塑造我们在智能上的使命方面发挥了关键性作用（Zoubin Ghahramani 和我本人联合撰写的一个专利正在申请中）。 尽管我们做的很多工作都是机密，现在也归 Uber 所有，而不是我，但我可以说我们工作中的很大一部分都是为了将深度学习整合进我们自己的技术中，这让我很大程度上熟悉了 TensorFlow 和梯度消失（爆炸）问题的乐趣和艰难。我们的目标是让我们日常就能在稀疏数据上使用混合深度学习系统来得到当前最佳的结果（有时候成功了，有时候没成功）。 "
384,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736206&idx=4&sn=eac29cfd2ec102ca2912079ecbfb32fc&chksm=871ac230b06d4b26f0eaff29ef811c6efee4f20305bce7374bb25bf7340bc431672ee9ccc723&scene=27,学界 | 为代码自动添加注释，让 Java 程序的阅读和开发更高效,"在阅读代码时，准确适当的注释能够给开发者提供很有价值的帮助。但并不是每一个需要阅读的代码都包含注释，这可能会让开发者白白花费更多用于理解代码的时间。特拉华大学软件分析实验室的一项研究试图通过数据驱动的技术来解决这一问题，为与对象相关的语句序列自动生成自然语言描述，从而帮助开发者更有效率地理解阅读代码。该论文也是 SANER 17(IEEE 软件分析演进与逆向工程国际大会) 会议接收的论文之一。注：为了区分代码 method 和本论文中提出或提到的解决方案，本译文中将 method 译为「方法」，将 approach 译为「办法」。 软件维护工作中的很大一部分时间花费是理解已有的代码。通过帮助开发者快速理解代码和减少他们的阅读代码量，可以减少开发者理解程序的时间。当程序员书写注释时，这些注释的目的是描述该方法（method）。不幸的是，程序员常常不写注释，而且有时候注释是过期的。使用空行隔开不同方法的代码可以帮助阅读者理解代码；但是，他们仍然需要阅读代码。另一种办法是 method refactoring，以使用方法调用（method call）来替代它们，这样也能压缩阅读量；但是，这种转换需要繁琐的分析才能维持原有的语义。 为了解决程序员写注释的问题，以帮助提升对程序的理解，之前 Sridhara（特拉华大学软件分析实验室）通过人工的方式为少量已知的多语句动作编写了一套模板，比如用于「计算最大值」的循环结构。这样所得到的注释可以方法（method）的注释。为了避免手动开发模板，我们提出了一种机器学习的方法自动识别 Java 方法中的循环动作。我们的方法不需要手动构建模板，自动的特征提取方法可以应用到大数据中。 我们在之前的研究成果中将动作单元（action unit）的概念定义为：逻辑上实现了一个高层面动作（即一个高层面算法步骤）的由连续语句序列构成的代码块。一个方法通常包含多个动作单元，其中动作单元是很小的而不足以构成单个方法，但却需要多个语句来实现。尽管之前的研究都使用了结构信息和语言信息来识别用于实现高层面算法步骤（动作单元），但是这他们都只解决了程序中的循环语句。 还有一种未被考虑过的，同时也是非常重要的动作单元，是与对象相关的动作单元。我们将与对象相关的动作单元定义为仅由非结构化连续语句构成的动作单元且这些非结构化连续语句是通过对象互相关联在一起的。非结构化语句是变量声明/赋值或方法调用语句。比如，在 Listing 1 中，第 11-14 行表示了一个与对象相关的动作单元，这是通过 mappingRow 对象关联的。与对象相关的动作单元通常是实现一个方法的中间步骤，而且在代码中非常常见。在从 GitHub 随机选择的 1000 个开源项目中，我们发现用空行隔开的代码块中有 23.03% 都是与对象相关的动作单元。 Listing 1：阐释动作单元 除了识别实现高层面动作的代码，我们还想通过简短的英语短语来描述它。对于 Listing 1，我们之前的技术 [3] 会将第 2-7 行的第一个动作单元识别为 if 循环并将其描述为「确定一个元素是否存在于该比特流中」。对于第 11-14 行的动作单元，这篇关于与对象相关的动作单元的论文会生成「将新创建的映射行添加到数据库中」。对于第 9 行单行的与对象相关的动作单元，我们会生成「将指定比特流添加到比特流」。 与对象相关的动作单元的概念催生出了一种用于生成方法总结注释的新方式，尤其是对于较长的方法。Sridhara et al. 提出的之前最佳方法是根据它们的特点选择单独的语句，然后为这些单独的语句生成用于方法总结的英语短语。对于 Listing 1，用于内容选择的 Sridhara 方法会选择第 9、12、13、14 行，其中第 12 和 13 行之后会被过滤为普遍动作。第 9 行被选择的原因是这是一个 void return 语句，第 14 行被选择的原因是这是一个 void return 和结束语句。尽管最终选择的第 9 和 14 行内容能够得到一个合理的方法总结，但应当指出选择它们的原因并不是为了识别高层面动作。因此，在 Listing 2 中，Sridhara 的方法会选择第 4-6、8 和 10 行。第 4-6 行会被看作是一个高层面动作序列（因为它们都是一样的动作）。 Listing 2：用于总结注释生成的动作单元 通过本论文提出的方法识别与对象相关的动作单元，我们可以将 Listing 2 中的第 2-8 行识别为一个与对象相关的动作单元，第 10 行识别为另一个与对象相关的动作单元。因为我们能识别出第 8 行终止了一个完整的与对象相关的动作单元，我们为该动作单元生成「为新创建的 Json 解析器解析坐标」，这从第 8 行和第 3 行中提取了内容。更重要的是，第 4-6 行没有被识别为一个需要通过总结描述的单独的动作单元。由于这种差异，Sridhara et al. 会在总结中包含「获取下一个 token」，而这并不适合用在该总结语句中。 除了用于方法总结注释生成，自动识别和描述动作单元还有其它应用。我们生成的描述可以用作任何被识别出的动作单元的内部注释。被识别出的动作单元可以用作用于提取方法重构的源，并且生成的描述还可以在调整之后用于为重构代码生成方法名。在生成 API 用途的代码范例上存在一些研究努力，比如 [5]；而我们观察到这些代码范例中很多都是动作单元。因为与对象相关的动作单元出现得很频繁，所以本研究成果可用于为识别出的范例提供描述。另一个潜在的应用是帮助没有头绪的程序员快速理解代码段，否则这件事做起来就会很繁琐和困难。 本论文的主要贡献包括： 一个用于识别实现了单个高层面动作的方法内的与对象相关的动作单元的算法 用于合成简洁且准确的表达与对象相关的动作单元的高层面动作的自然语言描述的规则 在为与对象相关的动作单元自动生成自然语言描述上进行了评估研究，表明人们对我们的方法的有效性有很强的正面的整体看法 我们的自动系统仅涉及源代码分析，不需要任何执行信息，因此可以用于不完整的和不可执行的遗留系统。我们的系统可以轻松地集成到 IDE 中，从而可在软件开发者开始开发 Java 方法时提供最新的描述。 问题描述 我们在这篇论文中解决的问题是：给定一个 Java 方法 M，自动发现每个与对象相关的动作单元，这些与对象相关的动作单元实现了组成 M 的整体算法的高层面动作；然后用简洁的自然语言描述准确地表达每个动作单元。 方法概述 图 1 描绘了为与对象相关的动作单元进行自动识别和自然语言描述生成的主要步骤。给定一个 Java 方法作为输入，我们构建一个抽象句法树（AST：abstract syntax tree），并同时使用 AST 和在相关对象上执行的操作来识别方法中的与对象相关的动作单元。对于每个与对象相关的动作单元，我们要识别出表示该动作单元的主要动作的语句，我们将其称为该动作单元的焦点语句（focal statement）。焦点语句能为该动作单元的自然语言描述提供主要内容。我们会识别焦点语句中应该被包含进生成的描述中的动作和参数，然后用词汇描述这些动作和参数以创建一个自然语言短语。 图 1：为与对象相关的动作单元生成描述的整体过程 焦点语句、动作和参数的识别以及词汇化都是通过我们开发的一组规则执行的，这些规则来自对大量 Java 方法的一项数据驱动的研究。我们的大部分工作都是开发这些规则。我们通过人工的方式分析了大量从开源项目中随机选择的语句序列，并开发了与对象相关的动作单元的模板。在我们分析中，我们使用了空行来为我们提供学习这些模板的样本，因为 Java 开发者传统上会使用空行来将方法分割成逻辑上相关的各个部分 [7,8,9]。因此，在学习开发者如何将一个方法分成多个算法步骤方面，空行是一个重要的来源。尽管我们使用了空行来学习用于识别与对象相关的动作单元的模板，但我们的基于模板的识别方法并不依赖于方法中的空行。如果空行存在，该算法会使用空行；但就算没有空行，它也能识别与对象相关的动作单元。 动作单元和焦点语句识别 如图 2 所示，一个动作单元通常包含 3 个部分。（1）是对对象引用 o 的声明或赋值。（2）是一个或多个语句，其中每个语句都是在对象 o 上的一次方法调用。（3）是使用对象 o 的一个语句。具体来说，本研究中的「使用」是指 o 在有声明/赋值的情况下出现在「=」的右侧。如果没有声明/赋值，那么 o 将作为一个方法调用的参数出现，并且该方法调用不是在 o 上调用的。这 3 个部分中每个部分都是可选的。 图 2：与对象相关的动作单元的一般格式 对于焦点语句的位置，我们分析得到了 3 种主要情况。注意这里无需执行数据流分析，因为我们只需要确定对象名的复用。 情况 1：（3）部分存在 情况 2：（3）部分不存在 情况 3：多个对象 识别动作和参数 Listing 11 中的动作单元及其焦点语句给出了一个完整的示例；根据识别动作和参数的规则，动作是「set」，主题是「information」，辅助参数为「config」。 Listing 11：示例代码段 动作和参数的词汇化 我们可以根据焦点语句识别动作和参数。接下来，我们生成包含与之相关的动作和参数为内容的自然语言描述。我们利用了之前基于源代码得到词汇的研究成果 [13,2]。有了动作和参数之后，我们按以下模板生成一个动词短语：动作 主题（辅助参数） 评估 我们的评估设计的目的是回答这三个研究问题： 研究问题 1：动作和参数选择的效果如何？ 研究问题 2：文本生成的效果如何？ 研究问题 3：描述生成器会消耗多少时间？ 论文：为与对象相关的语句序列自动生成自然语言描述（Automatically Generating Natural Language Descriptions for Object-Related Statement Sequences） 论文地址：https://www.eecis.udel.edu/~xiwang/saner17.pdf  当前驱动软件维护工具的源代码分析要么将方法（method）当作是单个单元，要么就将其当作是单独的语句或词构成的集合。它们往往会利用方法名（method name）或任何已有的内部注释。但是，内部注释很少见，而方法名通常又不能体现该方法的多个高层面的算法步骤——这些步骤太小了以至于不能成为一个单独的方法，但却需要多个语句才能实现。之前的研究已经证明了自动确定循环（loop）的高层面动作（action）的可行性；但是，很多高层面的动作都仍未得到解决和记录，尤其是主要通过对象引用互相关联的连续语句序列。我们将其称为与对象相关的动作单元（object-related action unit）。 我们在这篇论文中提出了一种办法，可用于自动生成方法内的与对象相关的动作单元的自然语言描述。我们的办法是利用大量可用的高质量开源项目来学习与对象相关的动作的模板，确定可以代表主要动作的语句，然后生成这些动作的自然语言描述。我们在 100 个与对象相关的语句序列构成的集合上进行了评估研究，结果表明我们的办法有希望自动识别动作和参数以及生成自然语言描述。 "
385,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736188&idx=3&sn=4ae5773f347af3278fb5847cfdc175d5&chksm=871ac242b06d4b549a2786757e2145486c7c964efb9d2c7939f98fef5668866fd42052ca4ee2&scene=27,资源 | Synonyms：一个开源的中文近义词工具包,"近日，Hai Liang Wang 和胡小夕在 GitHub 开放了一个中文近义词工具包 Synonyms，它可用于如文本对齐、推荐算法、相似度计算、语义偏移、关键字提取、概念提取、自动摘要、搜索引擎等很多 NLP 任务。该工具包目前能搜索近义词和比较语句相似度等任务，且词汇量达到了 125,792。机器之心也尝试使用 Synonyms 搜索一段中文的近义词，并有非常不错的反馈。 项目地址：https://github.com/huyingxi/Synonyms 该中文近义词工具包采用的基本技术是 Word2vec，因此在介绍该工具的同时我们会简要介绍词嵌入方法。此外，Synonyms 的安装十分便捷，我们可以直接使用命令 pip install -U synonyms 完成。该工具包兼容 Python 2 和 Python 3，且目前的稳定版为 v2.0，以下是使用 Synonyms 工具的效果： 如果我们想把单词输入机器学习模型，除非使用基于树的方法，否则需要把单词转换成一些数值向量。一种直接的方法是使用「one-hot encoding」方法将单词转换为稀疏表示，如下所示向量中只有一个元素设置为 1，其余为 0。 这种方法的缺点在于一个词的向量长度等于词汇表的大小，且非常稀疏。不仅如此，这种方法剥离了单词的所有局部语境，我们不能通过向量表示这个词的概念。因此，我们需要使用更高效的方法表示文本数据，而这种方法可以保存单词的上下文的信息。这是 Word2Vec 方法的初衷。 一般来说，Word2Vec 方法由两部分组成。首先是将高维 one-hot 形式表示的单词映射成低维向量。例如将 10，000 列的矩阵转换为 300 列的矩阵，这一过程被称为词嵌入。第二个目标是在保留单词上下文的同时，从一定程度上保留其意义。Word2Vec 实现这两个目标的方法有 skip-gram 和 CBOW 等，skip-gram 会输入一个词，然后尝试估计其它词出现在该词附近的概率。还有一种与此相反的被称为连续词袋模型（Continuous Bag Of Words，CBOW），它将一些上下文词语作为输入，并通过评估概率找出最适合（概率最大）该上下文的词。 对于连续词袋模型而言，Mikolov 等人运用目标词前面和后面的 n 个词来同时预测这个词。他们称这个模型为连续的词袋（CBOW），因为它用连续空间来表示词，而且这些词的先后顺序并不重要。 CBOW 可以看作一个具有先知的语言模型，而 skip-gram 模型则完全改变将语言模型的目标：它不像 CBOW 一样从周围的词预测中间的词；恰恰相反，它用中心语去预测周围的词： 在加载 Synonyms 中，我们可以看到会打印出「loaded (125796, 100) matrix from...」，因此 Synonyms 采用的词向量维度为 100。 输出近义词向量： synonyms.nearby(WORD) 会返回一个包含两项的列表： [[nearby_words], [nearby_words_score]]，nearby_words 是 WORD 的近义词向量，也以列表的方式存储，并且按照距离的长度由近及远排列，nearby_words_score 是 nearby_words 中对应词的距离分数，分数在 (0-1) 区间内，越接近于 1，代表越相近。比如: 在出现集外词的情况下，返回 [[], []]，目前的字典大小: 125,792。 机器之心尝试将一整段关于 Word2vec 的中文分割为一个个单词，再使用 Synonyms 工具对分词的结果取近义词，以下是试验结果： 两个句子的相似度比较： 其中，参数 seg 表示 synonyms.compare 是否对 sen1 和 sen2 进行分词，默认为 True。返回值：[0-1]，并且越接近于 1 代表两个句子越相似。 在 SentenceSim 上进行测试： SentenceSim 地址：https://github.com/fssqawj/SentenceSim/blob/master/dev.txt 评测结果： 以友好的方式打印近义词，方便调试，display 调用了 synonyms#nearby 方法： 最后，Synonyms 项目的作者胡小夕是北京邮电大学研究生，目前实习于今日头条 AI LAB。从事自然语言处理方向研究，在智能客服，知识图谱等领域都有相关研究开发经验。研发模型在文体分类权威数据集 TREC 上达到目前最优精度，申请深度学习与自然语言处理结合的国家发明专利 5 项。 @online{Synonyms:hain2017,
  author = {Hai Liang Wang, Hu Ying Xi},
  title = {中文近义词工具包Synonyms},
  year = 2017,
  url = {https://github.com/huyingxi/Synonyms},
  urldate = {2017-09-27}
} "
386,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736232&idx=2&sn=04b1deed56d718decdf271f166d209e2&chksm=871ac216b06d4b004614ec20979bec27b458578eb5fb3255af1d0c9451581528594b19608038&scene=27,入门 | 想实现DCGAN？从制作一张门票谈起！,生成对抗网络因为优雅的创意和优秀的性能吸引了很多研究者与开发者，本文从简洁的案例出发详解解释了 DCGAN，包括生成器的解卷积和判别器的卷积过程。此外，本文还详细说明了 DCGAN 的实现过程，是非常好的实践教程。 热身 假设你附近有个很棒的派对，你真的非常想去。但是，存在一个问题。为了参加聚会，你需要一张特价票——但是，票已经卖完了。 等等！难道这不是关于生成对抗网络（Generative Adversarial Network）的文章吗？是的，没错。但是请先忍忍吧，这个小故事还是很值得一说的。 好的，由于派对的期望值很高，组织者聘请了一个有资质的安全机构。他们的主要目标是不允许任何人破坏派对。为了做到这一点，场地的入口安排了很多警卫，检查每个人门票的真实性。 你并没有什么武打天赋能硬闯进去。所以，唯一的途径是通过一张非常有说服力的假票瞒天过海。 不过，这个计划存在一个很大的问题——你没见过真票长什么样。 即使根据自己的创造力设计了一张票，你是不可能在第一次尝试时能骗过警卫的。此外，如果没有一张足够真实的派对假票，带着自己做的假票进门无异于自投罗网。 为了解决这个问题，你决定打电话给你的朋友 Bob 帮你点忙。 Bob 的任务非常简单。他将用你做的假票尝试混进派对中去。如果他被拒之门外，他将为你带回有关票面样式的有用提示。 基于这个反馈，你可以再试着做一张新版假票交给 Bob，让他再试一次。这个过程不断重复，直到你能伪造一张完美的假票。 这个派对非去不可！实际上，上图是从一个假票生成器网站上复制下来的！ 撇开「假票事件」，这几乎是生成对抗网络（GAN）所做的全部工作。 目前，GAN 的大部分应用都在计算机视觉领域。其中的一些应用包括训练半监督分类器，并利用低分辨率的图像生成高分辨率的图像。 本文通过亲手处理生成图像的问题来介绍 GAN。你可以在以下地址找到本文的 Github 代码。 项目地址：https://github.com/sthalles/blog-resources/blob/master/dcgan/DCGAN.ipynb 生成对抗网络 GAN 是由 Goodfellow 等人设计的生成模型（参见论文 Generative Adversarial Networks，2014，Ian J. Goodfellow et al.）。在 GAN 的设计中，由神经网络表示的两个可微函数被锁定在这场极大极小博弈中。这两个参与者（即生成器和判别器）在这个框架中扮演着不同的角色。 生成器（generator）试图产生来自某种概率分布的数据。换句话说，它代表着上述故事中的你——企图生成派对的门票。 判别器（discriminator）像一个法官，它可以判定输入是来自生成器还是真正的训练集。这也就代表着故事中的警卫——将你的假票和真票进行对比，找出设计的缺陷。 我们用带有批归一化的 4 层卷积网络构建生成器和判别器，训练该模型将生成 SVHN 和 MNIST 图像。 总而言之，游戏规则如下： 生成器试图使判别器发生错误判断的概率最大化。 判别器引导生成器产生更逼真的图像。 在理想的平衡状态中，生成器将捕获训练数据的一般性分布。因此，判别器将总是不能确定其输入是否真实。 在 DCGAN 论文（Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks）中，作者描述了深度学习技术的结合，这是训练 GAN 的 关键。这些技术包括：（i）全卷积网络和（ii）批归一化（BN）。 前者强调 Strided Convolutions（以替代池化层）：增加和减少特征空间的维度。而后者归一化特征向量，从而显著地减少多层之间的协调更新问题。这有助于稳定学习，并能帮助处理糟糕的权重初始化问题。 不必多说，让我们深入实施细节，在细节中同时多谈谈 GAN。下面，我们展示了深度卷积生成对抗网络（DCGAN）的实现方法。我们遵循 DCGAN 论文中描述的实践方法，使用 Tensorflow 框架进行实现。 生成器 生成器网络有 4 个卷积层。除输出层外，其他所有层后都紧接着批归一化（BN）和线性修正单元（ReLU）进行激活。 它将随机向量 z（从正态分布中抽取）作为输入。把向量 z 进行四维重塑后，将其送入生成器，启动一系列上采样层。 每个上采样层都代表一个步长为 2 的转置卷积运算。转置卷积运算与常规卷积运算类似。 一般而言，常规卷积运算的层从宽而浅到窄而深。而转置卷积运算恰好相反：其层从窄而深到宽而浅。 转置卷积运算操作的步长定义了输出层的大小。在使用'same'填充、步长为 2 时，输出特征图的尺寸将是输入层大小的两倍。 这是因为，每当我们移动输入层中的一个像素时，我们都会将输出层上的卷积核移动两个像素。换句话说，输入图像中的每个像素都被用于在输出图像中绘制一个正方形。 将 3x3 的卷积核在 2x2 的输入上进行步长为 2 的转置卷积运算，相当于将 3x3 的卷积核在 5x5 的输入上进行步长为 2 的常规卷积运算。对于二者，均使用不带零填充的「VALID」。 简而言之，窄而深的的输入向量是生成器的开始。在每次转置卷积之后，z 变得更加宽而浅。所有转置卷积运算都使用 5x5 大小的卷积核，其深度从 512 逐渐降到 3——此处的 3 代表 RGB 彩色图像的 3 个通道。 最后一层输出一个 32x32x3 的张量并使用 tanh 函数将值压缩在 -1 和 1 之间。 最终的输出尺寸由训练集图像的大小定义。在这种情况下，如果对 SVHN 进行训练，生成器将产生 32x32x3 的图像。但是，如果对 MNIST 进行训练，则会生成 28x28 的灰度图像。 最后，请注意，将输入矢量 z 传送到生成器前，需要将其缩放到 -1 到 1 的区间，以遵循 tanh 函数的使用规则。 判别器 判别器也是一个带有 BN（输入层除外）的 4 层 CNN，并使用 leaky ReLU 进行激活。在基本的 GAN 结构中，有许多激活函数能正常工作。但是 leaky ReLU 尤为受欢迎，因为它可以使得梯度在结构中更容易传播。 常规 ReLU 函数通过将负值截断为 0 起作用。这可能有阻止梯度在网络中传播的效果。然而，在输入负值时，leaky ReLU 函数值不为零，因此允许一个小的负值通过。也就是说，该函数计算的是输入特征和一个极小因子之间的最大值。 Leaky ReLU 试图解决 ReLU 的梯度消失问题。如果神经元陷入这种情况，即对任意输入，ReLU 单元总是输出为 0，就会出现梯度消失。对于这些情况，梯度完全消失，网络无法进行反向传播。 这对于 GAN 来说尤其重要。这是因为，生成器学习的唯一方式是接收判别器的梯度。 一开始，判别器会收到一个 32x32x3 图像张量。与生成器相反，判别器执行一系列步长为 2 的常规卷积运算。每经过一次卷积，特征向量的空间维度就会减少一半，而训练的卷积核数量会加倍。 最后，判别器需要输出概率。为此，我们在最后一层使用 Sigmoid 激活函数。 请注意，在此框架中，判别器的角色是一个常规二元分类器。在一半时间里，它从训练集接收图像，另一半时间从生成器接收图像。 现在再回到我们的派对门票事件。为了伪造假票，唯一的信息来源是朋友 Bob 的反馈。换句话说，在每次尝试中，Bob 提供的反馈质量对于完成工作至关重要。 同样地，每当判别器注意到真实图像和虚假图像之间的差异时，它就向生成器发送一个信号。该信号是从判别器向生成器反向传播的梯度。通过接收它，生成器能调整其参数，从而接近真实数据的分布。 这就显示了判别器的重要性。事实上，生成器生成的数据有多棒，判别器区分它们的能力就有多强。 损失函数 现在，让我们来描述这个结构中最棘手的部分——损失函数。首先，我们知道，判别器从训练集和生成器中接收图像。 我们希望判别器能区分真实和虚假的图像。每当我们通过判别器运行一个小批量值时，我们都会得到 logits。这些是来自模型未经缩放的值。 不过，我们可以将判别器接收的小批量分成两种类型。第一种仅由来自训练集的真实图像组成，第二种仅由生成器创造的虚假图像组成。 由于两个网络同时训练，GAN 也需要两个优化器。它们分别用于最小化判别器和生成器的损失函数。 我们希望，判别器对真实图像输出接近 1 的概率，对虚假图像输出接近 0 的概率。为了做到这一点，判别器需要两类损失，其总损失函数是两部分损失函数之和。其中之一用于最大化真实图像的概率，另一个用于最小化虚假图像的概率。 比较实际（左）和生成（右）的 SVHN 样本图像。虽然有些图像看起来很模糊，有些图像很难辨认，但显而易见的是，数据分布是由模型捕获的。 训练开始时，会出现两个有趣的情况。其一，生成器不知如何创建和训练集类似的图像。其二，判别器不知如何将其接收的图像进行分类为「真」或「假」。 因此，判别器接收两类有显著差异的批数据。一个由训练集的真实图像组成，另一个则包含高噪声的信号。随着训练的进行，生成器开始输出更接近训练集图像的图像。这是因为生成器不断训练，学习了组成训练集图像的数据分布。 与此同时，判别器开始越来越好，它变得很擅长将样品分类为真或假。结果，这两种小批量数据在结构上开始变得相似。因此，判别器无法识别图像的真假。 我们使用原版的交叉熵作为损失函数，且 Adam 作为该函数的优化器也是一个不错的选择。 总结 GAN 是机器学习中目前最热门的话题之一。这些模型也许可以打开无监督学习的大门，将机器学习扩展到新的视野中。 自 GAN 创立以来，研究人员已经开发出许多用于训练 GAN 的技术。在这些用于训练 GAN 的改进技术中，作者描述了用于图像生成和半监督学习的最新技术。 如果你想深入了解这些主题，我推荐你阅读生成模型（Generative Models）相关的内容：https://blog.openai.com/generative-models/#gan。 同时，你也可以看看半监督学习与 GAN（https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e），以此获得半监督学习上的应用。 
387,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736180&idx=5&sn=dedf3d1d30ae35e2d3f269daa0cb02ff&chksm=871ac24ab06d4b5c0bef1b526df216c81095d2eb473fbdc5a69cb925e16f648d16ec1d2448a5&scene=27,学界 | 循环神经网络自动生成程序：谷歌大脑提出「优先级队列训练」,"arXiv Daniel A. Abolafia、Mohammad Norouzi、Quoc V. Le 由谷歌大脑 Quoc V. Le 团队提交的论文提出了一种使用循环神经网络进行程序合成的新方法——优先级队列训练（PQT）。目前，该论文已提交 ICLR 2018 大会，正在接受评议。 GitHub 链接：https://github.com/tensorflow/models/tree/master/research/brain_coder 自动程序合成是一项具备广泛应用潜力的任务。传统方法（如 Muggleton & de Raedt (1994); Angulin (1987)）通常不使用机器学习，因此需要编程语言和手动启发式方法方面的专业知识来加速底层组合搜索。为了创建无需领域专业知识的更通用的编程工具，近期大量研究开始开发神经模型，可促进某种形式的内存访问和符号推理（如 Reed & de Freitas (2016); Neelakantan et al. (2016); Kaiser & Sutskever (2016); Zaremba et al. (2016); Graves et al. (2016)）。尽管这些研究取得了很大成果，但是没有一种方法能够用富于表达性的编程语言合成源代码。 近期出现了多项使用神经网络从输入-输出示例导出程序的成功尝试（Riedel et al., 2016; Bunel et al., 2016; Balog et al., 2017; Parisotto et al., 2016），甚至从非结构化文本生成程序（Parisotto et al., 2016），但是它们通常使用限制性编程语法，需要真正程序或正确输出形式的监控信号。与之相反，本论文提倡使用富于表达性的编程语言 BF，其语法简单，且图灵完备。此外，本论文旨在使用强化学习合成程序，只需要一个 solution checker 就可以计算奖励信号。你可以将代码长度罚分或执行速度纳入奖励信号，进而搜索高效的短程序。因此，相比训练过程中需要程序或正确输出的其他方式，基于奖励的程序合成具备更高的灵活性。   为解决基于奖励信号的程序合成，本论文研究了两种不同方法。第一种方法是策略梯度（PG）算法（Williams, 1992），训练一个循环神经网络（RNN）来生成程序，每次生成一个 token。然后执行该程序并打分，奖励反馈将返回至 RNN 以更新参数，这样随着时间，生成的程序将越来越好。第二个方法是一种欺骗性的简单优化算法——优先级队列训练（priority queue training，PQT）。训练过程中保持 K 个最优程序的优先级队列可见，对队列中前 K 个程序使用对数似然目标函数来训练 RNN。然后从 RNN 中采样新程序，更新队列，进行迭代。本论文还对比了遗传算法（GA）基线（之前证明其可生成 BF 程序，Becker & Gottschlich (2017)）。令人惊讶的是，论文发现 PQT 算法显著优于 GA 和 PG 方法。 论文在 BF 编程语言上评估了提出方法的有效性。BF 语言是图灵完备的语言，仅有 8 条指令。BF 语言的极简语法使之易于生成语法正确的程序，这与比较高级的语言相反。本论文考虑了不同字符串操作、数值和算法任务。结果显示文中考虑的所有搜索算法在大部分任务中都能够找到正确的程序，而论文提出的方法是最可靠的，因为它是在最随机的种子上找到解决方案的，且可以解决的任务也是最多的。   本论文的主要贡献： 提出一个程序合成学习框架，训练过程中仅需要奖励函数（无需真正程序或正确输出）。此外，论文提倡使用简单且富于表达性的编程语言 BF 作为程序合成的基准环境（还可以参考 Becker & Gottschlich (2017)）。 受 Liang et al. (2017) 论文的启发，本论文使用优先级队列和 RNN 的搜索算法进行实验，并展示了将优先级队列作为 RNN 的训练目标是一种高效、稳定的方法。 提出一种实验方法来对比不同的程序合成方法，包括遗传算法和策略梯度方法。该方法评估了每个合成方法的平均成功率，并提供了一种调整超参数的标准方法。使用该方法，研究者发现使用优先级队列训练的循环神经网络优于基线模型。 方法 研究者实现了一个程序生成模型，即每次输出一串 BF 语言字符串的 RNN。图 2 展示了 RNN 模型，允许用自回归方式采样 BF 字符串，即将前一个预测作为下一步的输入。第一步的输入是特殊的 START 符号。RNN 生成特殊的 EOS 符号时终止，该符号表示字符串终结，或者程序长度超出预先指定的最大长度。每个时间步的预测通过多项式分布（一种多个时间步共享权重的 softmax 层）进行采样。程序序列的联合概率是所有 token 概率的积。 图 1：合成器图示。合成器是一个循环神经网络，以自回归的方式生成程序。 表 3 展示了相同的算法加上均匀随机搜索的成功率，以及训练和评估测试案例的成功率。研究者还在最后一行列出平均值，进行各列之间的整体对比。从表中可知，根据训练和评估平均值，PQT 要明显优于 PG 和 GA。PG+PQT 仅与 PQT 相当。由于过拟合，在很多情况下评估成功率要低于训练成功率。 表 3：当执行程序的最大数（max NPE）是 20M 时，在所有任务上合成方法成功的数量（25 个之中）。单元格中有斜杠分割的两个数。第一个数是训练测试案例中的成功数量，第二个数是留出评估测试案例中的成功数量。对于无法合成满足某个任务训练案例的方法，用短线进行标记。 论文：Neural Program Synthesis with Priority Queue Training 论文链接：https://arxiv.org/abs/1801.03526 摘要：我们认为程序合成任务是在程序的输出中存在一个奖励函数，目标是找到奖励最大的程序。我们采用了一种迭代优化方案，在这种方法中，我们在目前已生成的 K 个最佳程序数据集上训练一个 RNN 模型。随后，我们合成新的程序，并通过 RNN 采样将它们添加到优先级队列中。我们使用一种简单但富于表达性的图灵完备语言 BF，对该算法（优先级队列训练，PQT）与遗传算法和强化学习基线进行了对比。实验结果证明简单的 PQT 算法显著优于基线。通过在奖励函数中添加程序长度罚分，我们就可以生成简短、人类可读的程序。 "
388,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736188&idx=1&sn=05c4841eca8e72525dac6f6aff3b7690&chksm=871ac242b06d4b54381d9f7c6f11cfb8a44c803809edcd3d9c5fb84f3a5f2586022ae17b90d3&scene=27,从香农熵到手推KL散度：一文带你纵览机器学习中的信息论,"信息论与信息熵是 AI 或机器学习中非常重要的概念，我们经常需要使用它的关键思想来描述概率分布或者量化概率分布之间的相似性。在本文中，我们从最基本的自信息和信息熵到交叉熵讨论了信息论的基础，再由最大似然估计推导出 KL 散度而加强我们对量化分布间相似性的理解。最后我们简要讨论了信息熵在机器学习中的应用，包括通过互信息选择决策树的特征、通过交叉熵衡量分类问题的损失和贝叶斯学习等。 信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。它最初被发明是用来研究在一个含有噪声的信道上用离散的字母表来发送消息，例如通过无线电传输来通信。而本文主要探讨信息熵在 AI 或机器学习中的应用，一般在机器学习中，我们可以将信息论应用在连续型变量上，并使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。 因此在机器学习中，通常要把与随机事件相关信息的期望值进行量化，此外还要量化不同概率分布之间的相似性。在这两种情况下，香农熵都被用来衡量概率分布中的信息内容。香农熵是以信息论之父 Claude Shannon 的名字命名的，也称为信息熵或微分熵（连续）。 自信息 香农熵的基本概念就是所谓的一个事件背后的自信息（self-information），有时候也叫做不确定性。自信息的直觉解释如下，当某个事件（随机变量）的一个不可能的结果出现时，我们就认为它提供了大量的信息。相反地，当观察到一个经常出现的结果时，我们就认为它具有或提供少量的信息。将自信息与一个事件的意外性联系起来是很有帮助的。例如，一个极其偏畸的硬币，每一次抛掷总是正面朝上。任何一次硬币抛掷的结果都是可以完全预测的，这样的话我们就永远不会对某次结果感到惊奇，也就意味着我们从这个实验中得到的信息是 0。换言之，它的自信息是 0。如果硬币的偏畸程度稍微小一些，这样的话，尽管看到正面朝上的概率超过了 50%，每次抛掷还会有一些信息。因此，它的自信息大于 0。如果硬币的偏畸程度是导致反面朝上的结果，我们得到的自信息还是 0。在使用一个没有偏畸的硬币做实验时，每次抛掷得到正面朝上和反面朝上的概率都是 50%，我们会得到最大的意外性，因为在这种情况下硬币抛掷的结果的可预测性是最小的。我们也可以说，均匀分布的熵最大，确定事件的熵最小。 基于以上的非正式需求，我们可以找到一个合适的函数来描述自信息。对于一个可能取值为 x_1,x_2,...,x_n 的离散随机变量 X，它的概率质量函数 P（X），以及任何正的取值在 0 到 1 之间的单调递减函数 I(p_i) 都可以作为信息的度量。此外，还有另外一个关键的属性就是独立事件的可加性；两次连续硬币抛掷的信息应该是一次单独抛掷的 2 倍。这对独立变量而言是有意义的，因为在这种情况下意外性或者不可预测性会增大为之前的两倍。形式上，对于独立事件 x_i 和 x_j 而言，我们需要 I(p_i * p_j) = I(p_i) + I(p_j)。满足所有这些要求的函数就是负对数，因此我们可以使用负对数表示自信息： 图 1 所示是自信息 I(p)。 图 1：函数 I(p) 的自信息。小概率对应着较高的自信息，反之亦然。 我们继续回到简单的硬币抛掷实验中。在信息论中，1bit（也叫做 Shannon）信息代表一次单独硬币抛掷的两种可能结果。相似地，对于两次连续抛掷而言，就需要 4 bit 来描述 4 中可能的结果。通常，用 log_2(n)（2 的对数）bit 来描述 n 个连续的独立随机事件的结果，或者是自信息。下面我们来验证一下一次连续三次的实验中自信息的计算：总共有 2^3=8 种可能的结果，每种结果的概率都是 0.5^3=0.125。所以，这次实验的自信息就是 I（0.125）= -log_2(0.125) = 3。我们需要 3bit 来描述这些所有可能的结果，那么，任何一次连续三次的硬币抛掷的自信息等于 3.0。 我们也可以计算连续随机变量的自信息。图 2 展示了三种不同的概率密度函数及其对应的信息函数。图 2（A）所示的 Dirac delta 对应着很强的偏差，总是同一面朝上的偏畸硬币对应着零熵。所有 p(x)= 0 的地方都对应着无限高的信息量。然而，由于这些零概率的事件永远不会发生，所以这只是一个假设。图 2（B）中的高斯概率密度函数就是对那种经常同一面朝上，但不总是同一面朝上的情况的模拟。最后，图 2（C）描述的是一个均匀分布概率密度函数，它对应着均匀的信息量，和我们没有偏畸的硬币是类似的。 图 2. [-3,3] 上的三种不同的概率密度函数及其自信息 I（p）。(A)Dirac δ函数（完全确定）；（B）μ = 0,σ = 0.5 的高斯分布；（C）均匀分布 熵 到目前为止我们只讨论了自信息。在正常的硬币实验中，自信息实际上都等于香农熵，因为所有的结果都是等概率出现的。通常，香农熵是 X 的所有可能结果的自信息期望值： 其中 b 是对数的底数。上面我们使用的是 b=2，其他常见的选择还有 b=10，以及 e。其实这个影响不大，因为不同底数的对数之间存在一个常数的关系。我们这里仍然假设底数为 2，所以我们将省略下面公式中的 b。 如果仔细注意的话，你可能会疑惑，当 p(x_i) = 0 的时候会发生什么，因为这种情况下我们必须计算 0 · log(0)。事实上，我们需要计算的是一个极限：lim_(p→0) p*log(p(x_i))=0。使用洛必达法则或泰勒展开式求解的过程读者可以查阅书籍自行完成。 当香农熵泛化到连续域的时候，通常它指的是一种微分熵，对于连续的随机变量 x 及其概率密度函数 p(x)，它的香农熵定义如下： 我们上述三个分布的熵分别是 0（狄拉克δ分布），174（高斯分布）以及 431（均匀分布）。在我们的实验中出现的模式是：越宽广的分布对应着越高的信息熵。仔细观察图 2（B）和图 2（C）有助于你的理解。尽管高斯分布中 I（p）曲线下面的面积要远大于均匀分布，然而它的信息熵要远小于均匀分布，因为信息熵 I（P）是按照概率密度 p 加权的，在高斯分布的两侧，p 接近于 0。更广的概率密度对应着更大的信息熵，有一个很好的比喻帮助记住这个：想象某种气体充满了一个储罐。从物理学中我们可以知道，一个封闭系统中的熵会随着时间增加，而且从来不会减少。在我们从储罐的另一侧注入气体之后，气体粒子的分布会收敛于一个均匀值。低熵意味着高密度的气体粒子聚集在某个特定的区域，而这是永远不会自发发生的。很多气体粒子聚集在某个小面积区域对应的还早呢故事我们的高斯概率密度函数，在狄拉克δ分布中是一个极端粒例子，所有的气体都被压缩在一个无限小的区域。 交叉熵 交叉熵是一个用来比较两个概率分布 p 和 q 的数学工具。它和熵是类似的，我们计算 log(q) 在概率 p 下的期望，而不是反过来： 在信息论中，这个量指的是：如果用「错误」的编码方式 q（而不是 p）去编码服从 q 分布的事件，我们所需要的 bit 数。在机器学习中，这是一个衡量概率分布相似性的有用工具，而且经常作为一个损失函数。因为交叉熵等于 KL 散度加上一项信息熵，即 D_KL(p||q) = H(p, q) - H(p)。而当我们针对 Q 最小化交叉熵时，H(p) 为常量，因此它能够被省略。交叉熵在这种情况下也就等价于 KL 散度，因为 KL 散度可以简单地从最大似然估计推导出来，因此下文详细地以 GAN 为例利用 MLE 推导 KL 散度的表达式。 KL 散度 与交叉熵紧密相关，KL 散度是另一个在机器学习中用来衡量相似度的量：从 q 到 p 的 KL 散度如下:D_KL(p||q)。在贝叶斯推理中，DKL(p||q) 衡量当你修改了从先验分布 q 到后验分布 p 的信念之后带来的信息增益，或者换句话说，就是用后验分布 q 来近似先验分布 p 的时候造成的信息损失。例如，在训练一个变分自编码器的隐藏空间表征时就使用了 KL 散度。KL 散度可以用熵和交叉熵表示： 交叉熵衡量的是用编码方案 q 对服从 p 的事件进行编码时所需 bit 数的平均值，而 KL 散度给出的是使用编码方案 q 而不是最优编码方案 p 时带来的额外 bit 数。从这里我们可以看到，在机器学习中，p 是固定的，交叉熵和 KL 散度之间只相差一个常数可加项，所以从优化的目标来考虑，二者是等价的。而从理论角度而言，考虑 KL 散度仍然是有意义的，KL 散度的一个属性就是，当 p 和 q 相等的时候，它的值为 0。 KL 散度有很多有用的性质，最重要的是它是非负的。KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 『几乎 处处』 相同的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常 被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的：对于某 些 P 和 Q，D_KL(P||Q) 不等于 D_KL(Q||P)。这种非对称性意味着选择 D_KL(P||Q) 还是 D_KL(Q||P) 影响很大。 在李弘毅的讲解中，KL 散度可以从极大似然估计中推导而出。若给定一个样本数据的分布 P_data(x) 和生成的数据分布 P_G(x;θ)，那么 GAN 希望能找到一组参数θ使分布 P_g(x;θ) 和 P_data(x) 之间的距离最短，也就是找到一组生成器参数而使得生成器能生成十分逼真的图片。 现在我们可以从训练集抽取一组真实图片来训练 P_G(x;θ) 分布中的参数θ使其能逼近于真实分布。因此，现在从 P_data(x) 中抽取 m 个真实样本 {𝑥^1,𝑥^2,…,𝑥^𝑚}，其中符号「^」代表上标，即 x 中的第 i 个样本。对于每一个真实样本，我们可以计算 P_G(x^i;θ)，即在由θ确定的生成分布中，x^i 样本所出现的概率。因此，我们就可以构建似然函数： 其中「∏」代表累乘、P_G(x^i;θ) 代表第 i 个样本在生成分布出现的概率。从该似然函数可知，我们抽取的 m 个真实样本在 P_G(x;θ) 分布中全部出现的概率值可以表达为 L。又因为若 P_G(x;θ) 分布和 P_data(x) 分布相似，那么真实数据很可能就会出现在 P_G(x;θ) 分布中，因此 m 个样本都出现在 P_G(x;θ) 分布中的概率就会十分大。 下面我们就可以最大化似然函数 L 而求得离真实分布最近的生成分布（即最优的参数θ）： 在上面的推导中，我们希望最大化似然函数 L。若对似然函数取对数，那么累乘 ∏ 就能转化为累加 ∑，并且这一过程并不会改变最优化的结果。因此我们可以将极大似然估计化为求令 log[P_G(x;θ)] 期望最大化的θ，而期望 E[logP_G(x;θ)] 可以展开为在 x 上的积分形式：∫P_data(x)logP_G(x;θ)dx。又因为该最优化过程是针对θ的，所以我们添加一项不含θ的积分并不影响最优化效果，即可添加 -∫P_data(x)logP_data(x)dx。添加该积分后，我们可以合并这两个积分并构建类似 KL 散度的形式。该过程如下： 这一个积分就是 KL 散度的积分形式，因此，如果我们需要求令生成分布 P_G(x;θ) 尽可能靠近真实分布 P_data(x) 的参数θ，那么我们只需要求令 KL 散度最小的参数θ。此外，我们可以将 KL 散度的积分形式转换为我们熟悉的 KL 散度表达式： 在离散型变量的情况下，KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号消息时，所需要的额外信息量。 你或许疑问，这里的熵和机器学习是如何相关的。下面我们看一下一些具体的领域。 贝叶斯学习 首先，上面描述的高斯分布的例子是很重要的，因为在机器学习应用中，高斯分布是一个很常见的建模选择。机器学习的目标就是减少熵。我们希望做一些预测，而且我们必须对自己的预测比较确定。而熵正好可以用来衡量这个置信度。在贝叶斯学习中，经常假设一个先验分布具有较宽广的概率密度函数，这反映了随机变量在观测之前的不确定性。当数据来了以后，熵会减小，并且让后验分布在最可能的参数值周围形成峰值。 决策树学习 在决策树的学习算法中，一般包含了特征选择、决策树的生成与决策树的剪枝过程。决策树的特征选择在于选取对训练数据有分类能力的特征，而通常特征选择的准则是信息增益或信息增益比。 在李航的统计学习方法中，一般熵 H(Y) 与条件熵 H(Y|X) 之差可以称为互信息（Mutual Information），决策树学习中的信息增益等价于训练数据中类与特征的互信息。若给定训练数据集 D 和特征 A，经验熵 H(D) 表示对数据集 D 进行分类的不确定性。而经验条件熵 H(D|A) 表示在特征 A 给定的条件下对数据集 D 进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征 A 而使得对数据集 D 的分类的不确定性减少的程度。显然，对于数据集 D 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。 根据信息增益准则的特征选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 因此在决策树学习中，熵被用来构建树。通过将数据集 S 根据可能的「最佳」属性分成一些子数据集，从根节点开始构建决策树，「最佳」属性也就是能够将得到的子数据集的熵最小化的属性。这个过程被递归地重复，直到没有更多的属性来分割。此过程被称为 ID3 算法，由此可见 ID3 算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。 分类 不管是在二分类问题还是多分类问题中，交叉熵是 logistic 回归和神经网络中的标准损失函数。通常，p 是真实分布，q 是模型描述的分布。让我们来看一个二分类 logistic 回归中的一个例子。两个类别的标签分别是 0 和 1，logistic 模型给每一个输入赋予以下概率：q_(y=1) =y_hat，q_(y=0) = 1- y_hat。这个可以简写为 q ∈ {y_hat, 1 − y_hat}。尽管真实标签是精确的 0 和 1，但是这里还是写成 p ∈ {y, 1 − y}，因此不要被这个表达方式搞混。在这个标记下，每个样本的真实值和估计分布之间的交叉熵如下： 当它被作为一个损失函数使用的时候，我们用的是 N 个样本的交叉熵均值， 结语 以上基本上来说就是机器学习中所涉及的信息论基础，虽然我们并不怎么使用信息论中关于消息长度的解释，但机器学习主要使用用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。信息论是我们构建损失函数随必须要考虑的，而且它的对数形式很容易与输出单元一般采用的指数形式相结合而提高学习的效率。此外，现代深度网络的成功与最大似然估计的流行，很大程度是因为有像信息熵那样对数形式的损失函数而取得极大的提升。 原文链接：https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32 "
389,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736180&idx=2&sn=f7ee0d4df620dd6dd0a9e503dff0135b&chksm=871ac24ab06d4b5c019d30f3025c4f3fa89a21c088a31c3c7e01e40d33ff4de4c35911e67d7b&scene=27,前沿 | 简述脉冲神经网络SNN：下一代神经网络,TowardsDataScience Devin Soni 脉冲神经网络（SNN）属于第三代神经网络模型，实现了更高级的生物神经模拟水平。除了神经元和突触状态之外，SNN 还将时间概念纳入了其操作之中。本文将简要介绍这种神秘的神经网络形式。 所有对目前机器学习有所了解的人都听说过这样一个事实：目前的人工神经网络是第二代神经网络。它们通常是全连接的，接收连续的值，输出连续的值。尽管当代神经网络已经让我们在很多领域中实现了突破，但它们在生物学上是不精确的，其实并不能模仿生物大脑神经元的运作机制。 第三代神经网络，脉冲神经网络（Spiking Neural Network，SNN），旨在弥合神经科学和机器学习之间的差距，使用最拟合生物神经元机制的模型来进行计算。脉冲神经网络与目前流行的神经网络和机器学习方法有着根本上的不同。SNN 使用脉冲——这是一种发生在时间点上的离散事件——而非常见的连续值。每个峰值由代表生物过程的微分方程表示出来，其中最重要的是神经元的膜电位。本质上，一旦神经元达到了某一电位，脉冲就会出现，随后达到电位的神经元会被重置。对此，最常见的模型是 Integrate-And-Fire（LIF）模型。此外，SNN 通常是稀疏连接的，并会利用特殊的网络拓扑。 LIF 模型中膜电位的微分方程 脉冲期间的膜电位形态 三神经元网络的脉冲训练 脉冲神经网络图示 乍一看，脉冲神经网络的方法像是一种倒退。我们从连续输出移动至二进制输出，这些脉冲训练的可解释性不强。但是，脉冲训练增强了我们处理时空数据（或者说真实世界感官数据）的能力。空间指神经元仅与附近的神经元连接，这样它们可以分别处理输入块（类似于 CNN 使用滤波器）。时间指脉冲训练随着时间而发生，这样我们在二进制编码中丢失的信息可以在脉冲的时间信息中重新获取。这允许我们自然地处理时间数据，无需 RNN 添加额外的复杂度。事实证明脉冲神经元是比传统人工神经元更强大的计算单元。 既然理论上 SNN 比第二代网络更强大，那么我们很自然会想到为什么它们没有得到广泛应用。主要问题在于 SNN 的训练。尽管我们有无监督生物学习方法，如赫布学习（Hebbian learning）和 STDP，但没有适合 SNN 的有效监督训练方法能够 i 通过提供优于第二代网络的性能。由于脉冲训练不可微，我们无法在不损失准确时间信息的前提下使用梯度下降来训练 SNN。因此，为了正确地使用 SNN 解决真实世界任务，我们需要开发一种高效的监督学习方法。这是一项艰巨的任务，因为它涉及到，给定这些网络的生物现实主义，确定人类大脑如何学习。 另一个问题是在正常硬件上模拟 SNN 需要耗费大量算力，因为它需要模拟微分方程。但是，神经形态硬件，如 IBM TrueNorth，旨在使用利用神经元脉冲行为的离散和稀疏本质的专门硬件模拟神经元，进而解决该问题。 今天看来，SNN 的未来依然不甚清晰。一方面，它们是我们当前神经网络的天然继承者；但是另一方面，对大多数任务来说它们还远不是实践工具。目前在实时图像和音频处理中有一些 SNN 实际应用，但相关文献仍然很少。绝大多数 SNN 论文或者是理论的，或者在一个简单的全连接第二代网络之中展示性能。然而，很多团队正致力于开发 SNN 监督式学习规则，并且我对 SNN 的未来充满乐观。 原文链接： 
390,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736180&idx=4&sn=eabde70bab7a056b1f7ced0e29f6cf88&chksm=871ac24ab06d4b5cedddb68e05f3af36bb860a2242f40503a9add56ae404678f23900a043452&scene=27,入门 | 无需双语语料库的无监督式机器翻译,"Medium Harshvardhan Gupta 去年，Facebook 发表论文《Unsupervised Machine Translation Using Monolingual Corpora Only》，提出使用单语语料库的无监督式机器翻译。近日 Medium 上一篇文章对该论文进行了解读，机器之心对此进行了编译介绍。 深度学习广泛应用于日常任务中，尤其擅长包含一定「人性」的领域，如图像识别。或许深度网络最有用的功能就是数据越多性能越好，这一点与机器学习算法不同。 深度网络在机器翻译任务中做得不错。它们目前在该任务中是最优的，而且切实可行，连 Google Translate 都在使用。机器翻译需要语句级别的平行数据来训练模型，即对于源语言中的每个句子，目标语言中都有对应的译文。难点在于某些语言对很难获取大量数据（来使用深度学习的力量）。 机器翻译的问题 如上所述，神经机器翻译最大的问题是需要双语语言对数据集。对于英语、法语这类广泛使用的语言来说，这类数据比较容易获取，但是对于其他语言对来说就不一定了。如能获取语言对数据，则该问题就是一个监督式任务。 解决方案 论文作者指出如何将该任务转换成无监督式任务。在该任务中，所需的唯一数据是两种语言中每种语言的任意语料库，如英语小说 vs. 西班牙语小说。注意两部小说未必一样。 也就是说，作者发现如何学习两种语言之间共同潜在空间（latent space）。 自编码器简单回顾 自编码器是用于无监督任务的神经网络的一种宽泛类别。它们可以重新创建与馈送的输入相同的输入。关键在于自编码器中间有一个层，叫作 bottleneck 层。该层可以捕捉所有输入的有趣信息，去除无用信息。 简言之，bottleneck 层中的输入（这里经过编码器转换）所在的空间就是潜在空间。 去噪自编码器 如果自编码器可以学会完全按照接收的馈送来重建输入，那么它或许什么都不用学了。这种情况下，输出可以被完美重建，但是 bottleneck 层中并没有有用特征。为了弥补，我们可以使用去噪自编码器。首先，向输入添加一些噪声，然后构建网络用来重建原始图像（不带噪声的版本）。用这种方式，通过让网络学习什么是噪声（以及真正有用的特征）使其学习图像的有用特征。 为什么要学习共同潜在空间？ 潜在空间捕捉数据特征（在机器翻译中，数据是句子）。如果可以学习对语言 A 和语言 B 馈送的输入输出相同特征的空间，那么就可以实现这两种语言之间的翻译。由于该模型已经学会了正确的「特征」，那么利用语言 A 的编码器来编码，利用语言 B 的解码器解码就可以使该模型完成翻译。 如你所料，该论文作者利用去噪自编码器学习特征空间。他们还指出如何使自编码器学习共同潜在空间（作者在论文中称之为对齐潜在空间），以执行无监督机器翻译。 语言中的去噪自编码器 作者使用去噪编码器以无监督的方式学习特征。其中定义的损失函数为： 式 1.0 去噪自编码器损失函数 式 1.0 的解释 l 是语言（按其设置，应该有两种可能的语言），x 是输入，C(x) 是将噪声加到 x 之后的结果。e() 是编码器，d() 是解码器。等式末尾的 Δ(x_hat ,x) 项是 token 级别的交叉熵误差总和。由于是通过输入序列得到输出序列，我们需要确保每个 token 都以正确的顺序排列。因此最终得到了上式中的损失函数。可以将其视为多标签分类问题，其中输入中的第 i 个 token 和输出中的第 i 个 token 对比。一个 token 就是一个单元，不能再继续分解。在机器翻译中，一个单词就是一个 token。 因此，式 1.0 的作用是使网络最小化它的输出（给定带噪输入）和原始语句之间的差异。 如何添加噪声 图像处理可以通过在像素中添加浮点数来添加噪声，而在语言中添加噪声的方式是不同的。因此，论文作者开发了自己的噪声生成系统。他们用 C() 表示噪声函数。C() 以输入语句为输入，然后输出该语句的带噪声版本。 有两种添加噪声的方法。 一种是，以 P_wd 的概率从输入中删除一个单词； 另一种是，每个单词以下式中的约束从初始位置偏移： 这里，σ是第 i 个 token 偏移后的位置。因此，上式的含义是「一个 token 最多可以偏离原来位置 k 个 token 的距离」。 作者使用的 k 值为 3，P_wd 值为 0.1。 跨域训练 为了学习两种语言的互译，需要构建将输入序列（语言 A）映射到输出序列（语言 B）的过程。作者称该学习过程为跨域训练。首先，采样一个输入语句 x，然后使用前一次迭代后的模型 M() 生成翻译后的输出 y，即 y=M(x)。之后，使用上述的噪声函数 C() 应用到 y 上，得到 C(y)。语言 A 的编码器将 C(y) 编码，然后由语言 B 的解码器将其解码，重构出 C(y) 的无噪声版本。训练模型时使用的是相同的交叉熵误差总和，类似式 1.0。 通过对抗训练学习共同潜在空间 论文中并没有提到如何学习共同潜在空间。上述跨域训练可能在某种程度上有助于学习类似的空间，但要使模型学习类似的潜在空间需要添加一个更强的约束。 作者使用了对抗训练。他们使用了另一个称为鉴别器的模型，以每个编码器的输出为输入，预测被编码的语句所属的语言。然后，编码器也要学习欺骗鉴别器。这在概念上和标准的 GAN 并没有什么区别。鉴别器通过每个时间步（由于使用了 RNN）的特征向量预测输入所属的语言种类。 整合所有部分 将上述的三个不同的损失（自动编码器损失、翻译损失和鉴别器损失）加在一起，所有的模型权重在一个步骤内更新。 由于这是一个序列到序列问题，作者使用了 LSTM 网络，结合注意力机制，即有两个基于 LSTM 的自编码器，每种语言使用一个。 训练该架构时有三个主要步骤。训练过程是迭代进行的。训练循环分为以下三步： 1. 使用语言 A 的编码器和语言 B 的解码器进行翻译； 2. 给定一个带噪语句，训练每个自编码器重新生成一个去噪语句； 3. 给步骤 1 中得到的翻译语句添加噪声然后重新生成，以提升翻译能力。这一步中，语言 A 的编码器和语言 B 的解码器（以及语言 B 的编码器和语言 A 的解码器）需要一起训练。 注意虽然步骤 2 和步骤 3 是分开的，但权重是同步更新的。 如何快速启动该框架 如上所述，该模型使用了之前迭代的译文来提升自己的翻译能力。因此，在训练循环开始之前，事先具备某些类型的翻译能力是很重要的。作者使用了 FastText 学习词级双语词典。注意这种方法是很粗糙的，只在模型开始时使用。 框架的完整结构如下流程图所示： 结论 本文介绍了一种非常新的无监督机器翻译技术。它使用多种不同的损失函数来提升各个单独任务，同时使用对抗训练为架构行为添加约束。 原文链接： "
391,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736115&idx=5&sn=1000a67166423aa98b8d11025bb369cb&chksm=871ac18db06d489b0e919af21c04456f5730aa0fdcda74ffdef2278f1ec726d39d2382f21cdf&scene=27,学界 | 伯克利吴翼&FAIR田渊栋等人提出强化学习环境Hourse3D,"构建虚拟 3D 环境对于强化学习研究非常重要。近日，UC Bekerley 博士生吴翼、FAIR 研究工程师吴育昕、博士后 Georgia Gkioxari 和研究科学家田渊栋共同提交了一篇论文，提出一种基于 SUNCG 数据集构建的丰富、可扩展的高效环境 House3D。研究者用连续和离散动作空间训练强化学习智能体，改善了它们在新环境中的泛化能力。该论文目前已提交至 ICLR 2018 大会。 项目链接：https://github.com/facebookresearch/House3D 近期，深度强化学习已在多种游戏上显示了自己的能力，如 Atari 游戏（Mnih et al., 2015）和围棋（Silver et al., 2016），在其中都展示了超越人类的水平。这些巨大成就的根基在于高效明确、可进行自由学习与探索的智能体模拟环境。目前为止，很多研究人员提出的环境已经编码了人类智能的某些方面，其中包括 3D 感知（DeepMind Lab（Beattie et al., 2016）和 Malmo（Johnson et al., 2016））、实时决策（TorchCraft（Synnaeve et al., 2016）和 ELF（Tian et al., 2017））、快速反应（Atari（Bellemare et al., 2013））、长期计划（围棋、国际象棋）、语言和交流（ParlAI（Miller et al., 2017）和（Das et al., 2017b））。 尽管如此，深度强化学习在这些模拟环境中的进展是否能够以及如何迁移到真实世界仍然是一个开放性问题。对于这个方向，最重要的事情就是构建模拟真实世界的环境，该环境需要具备真实世界丰富的结构、内容和动态性。为了加快学习，这些环境应该实时响应，并提供大量多样化的复杂数据。尽管我们很需要这些特性，但它们仍然无法保证达到泛化目标。泛化是智能体在新场景中成功完成任务的能力，这对实际应用非常重要。例如在很多房子里训练的家用机器人或在很多城市中训练的自动驾驶汽车，应该能够轻松部署到与训练场景完全不同的新房子或新城市。 尽管人们往往认为泛化与学习有关，但是毫无疑问泛化与训练智能体的环境的多样性有关。为了促进泛化，环境需要提供大量数据，允许智能体测试其在新条件下识别和动作的能力。要验证智能体是否开发出智能技巧，而不是仅凭记忆（过拟合），具备无偏、无限制本质的新型生成环境是必要的。注意：泛化和大型数据集之间的结合带来了图像识别和目标识别领域的近期进展（Russakovsky et al., 2015; He et al., 2015）。 与泛化被正式定义和充分研究的监督学习相反，强化学习中的泛化可以用多种方式来理解。DeepMind 实验室（Beattie et al., 2016; Higgins et al., 2017）通过像素级颜色或纹理变化和迷宫布局引入环境多样性。Tobin et al. (2017) 通过引入随机噪声改变物体颜色，来探索像素级的泛化。Finn et al. (2017b) 研究在奖励仅提供给任务的部分子集时，智能体在类似任务配置环境中的泛化能力。Pathak et al. (2017) 测试智能体在同样游戏更困难级别的泛化能力。 但是，环境中的像素级变化（如物体的颜色和纹理）或难度级别变化在智能体那里却经常得出非常近似的视觉观察结果。在真实世界中，人类感知和理解复杂的视觉信号。例如，一个人到了朋友家，即使装饰和设计都是新的，他也能轻松识别出哪是厨房。而智能体要想在真实世界中成功，需要理解新的结构布局和多样的物体外表。智能体需要能够将语义与新场景联系起来，在视觉变化场景中实现泛化。这篇论文研究了语义级别的泛化，其中的训练环境和测试环境在视觉上不相同，但共享同样的高级别概念属性。具体来说，研究者提出一个包括数千个室内场景（具备不同的场景类型、布局和物体）的虚拟 3D 环境 House3D，见图 1a。House3D 利用 SUNCG 数据集 (Song et al., 2017)，该数据集包含 4.5 万个人类设计的真实世界 3D 房子模型，其中的物体全部标注成不同类别。研究者将 SUNCG 数据集转换至 House3D 环境，这对于不同任务都是有效且可扩展的。在 House3D 中，智能体可自由探索空间，感知大量不同视觉外表的物体。 House3D 展示了智能体确实能够在新基准任务 RoomNav 中学习高级别概念，并泛化至未见的新场景中。在 RoomNav 中，智能体开始位于房子的随机位置，并被要求去往高级语义概念指定的目的地（如厨房）。RoomNav 示例见图 1b。研究者从 House3D 中手动选取了 270 个房子，并将其分割成两个训练集（20 个房子和 200 个房子）和一个用于评估的留出集（50 个房子）。这些房子足够大，且适合导航任务。研究者展示了：使用用标准深度强化学习方法（如 A3C 和 DDPG）训练的 gated-CNN 和 gated-LSTM 策略，大型多样化训练集可以改善智能体在 RoomNav 中的泛化能力。这与小型训练集的结果相反，其中出现了显著的过拟合。此外，深度（depth）信息和语义信号（如分割）带来更好的泛化效果。前者（深度）促进即时动作，后者（分割）帮助在新环境中的语义理解。实验结果证明了为真实世界机器人构建实际视觉系统的意义，同时引出在处理复杂的真实世界任务时，分离视觉和机器人学习等方向。 （a）研究者基于 SUNCG 数据集构建了一个高效的交互式环境，该数据集包含 4.5 万个不同的室内场景，从单间到带游泳池和健身房的两层楼房。所有 3D 对象都被完全标注成 80 多个类别。环境中的智能体可获得多种模式的观察结果（如 RGB 图像、深度、分割掩码、自上而下的 2D 视角等。（b）研究者专注于基于语义的导航任务。给定一个高级任务描述，智能体自行探索环境，到达目标房间。 论文： Building Generalizable Agents with a Realistic and Rich 3D Environment 论文链接：https://arxiv.org/abs/1801.02209 摘要： 要缩小机器智能与人类之间的差距，引入视觉真实、内容充实的环境至关重要。在此类环境中，你可以评估和改善实际智能系统的关键特性，即泛化。本研究构建了一种丰富、可扩展的高效环境 House3D，包含 45622 个人工设计的 3D 房屋场景，从单人间到楼房应有尽有；且该环境具备一套完整标注的 3D 物体、材质和场景布置，基于 SUNCG 数据集（Song et al., 2017）。我们重心关注语义级别的泛化，使用 House3D 的子集研究概念驱动的导航任务 RoomNav。在 RoomNav 中，智能体根据语义概念的指定导航至目的地。为了成功，该智能体通过开发感知来理解所处的场景，通过将场景映射到正确的语义来理解概念，通过观察底层物理规则来导航至目的地。我们用连续和离散动作空间训练强化学习智能体，展示了它们在新环境中的泛化能力。具体来说，我们观察到（1）在大型房屋集合上进行训练的难度显著增加，但泛化能力也要好得多；（2）使用语义信号（如分割掩码）提升泛化性能；（3）语义输入信号的门网络（gated network）可以改善训练性能和泛化能力。我们希望 House3D（包括对 RoomNav 任务的分析）能够对设计实际的智能系统有所帮助，也希望社区广泛使用该环境。  "
392,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736115&idx=3&sn=d8a6ec663875b4a4dceff9e4ce5db050&chksm=871ac18db06d489b0902d4568adc0af67c679c025c7db82a72d14e9dac944dfa500f64df0c46&scene=27,业界 | 超越英伟达Pascal五倍？揭秘英特尔深度学习芯片架构,在被英特尔收购两年之后，深度学习芯片公司 Nervana 终于准备将代号为「Lake Crest」的架构转化为实际的产品了。 对于英特尔来说，现在入局或许有些迟到，英伟达已经占据深度学习芯片市场很长一段时间了，后者有充分的时间通过新的深度学习专用 GPU 和软件加速器来维持并扩张自己的市场。换句话说，即使技术优秀，价格合理，英特尔想要把 Nervana 推上深度学习训练市场并占据一席之地也需要大量的努力——现在看来，英特尔似乎正把筹码押注在 Nervana 的激进路线图上。 现在，我们可以在架构上对它一窥究竟了——它究竟与 GPU 有多大区别，存在哪些优势，以及它的性能优势主要在哪里——确切地说，是功耗效率优势。据 Nervana 元老，现任英特尔 AI 硬件负责人 Carey Kloss 介绍，Nervana Intel 目前的芯片与初代 Nervana 芯片非常相似，但由于英特尔提供了更多专业技术，新的深度学习芯片可以以每年一次的频率快速更新换代。 「加入英特尔之后，我们并没有改变很多，但我们确实应用了母公司很多技术资源，包括封装、电路板设计、电力传输、实验室和接口，在很多事情上一切如常，但在很多技术方面上我们也有了很大进步，」Kloss 告诉 The Next Platform。相比他看到的其他半导体公司，Nervana 的产品先进很多，他们很有信心让新款英特尔集成产品保持稳定的性能曲线。 现在，英特尔已经非常接近推出「Lake Crest」，或它的新名称：英特尔神经网络处理器（Intel Nervana Neural Network Processor，NNP），越来越多的硬件架构细节正逐步被披露。我们从英特尔收购 Nervana 之后几年里陆续获知了这款芯片是如何在模型训练上优于 GPU 的，现在更了解了新的芯片通过改变内存带宽瓶颈获得了更加高效的表现。 虽然目前我们还没有得到任何英特尔 NNP 的性能基准测试结果，Kloss 表示在未来几个月内我们将会看到重大的结果发表。当然，将 NNP 的性能以 GPU 常用的浮点运算速度来测量并不容易，因为两种架构（尽管有一些结构相同）在浮点依赖上的运行方式非常不同。换句话说，即将出现的基准测试将展示 NNP 的高性能，但并不是简单地将其映射到「TFLOPS」数字上。 如果想了解这种新架构的潜力，我们需要先回到 2016 年 8 月，英特尔收购 Nervana 前夕。那时该公司披露了一些内存技巧之外，有关架构方面的细节。时任 Nervana 首席执行官 Naveen Rao 曾告诉我们，在仅握有 2800 万美元资金的情况下，想要推出 28nm 制程的芯片还需要相当大的努力。他简要解释了公司产品在连接和内存构造的特点，并指出它将成为英伟达 NVlink 技术的有力竞争者。Rao 表示，在纯每秒处理速度上，第一块 Nervana 芯片的速度将是英伟达 Pascal 架构的 5-6 倍。 「我们知道目前人们都在使用 Pascal 架构来做深度学习，所以我们在数据上很有信心，他们（英伟达）引入了 NVLink，但我们从底层开始开发了自己的多芯片解决方案。我们的芯片拥有更快的速度，芯片之间拥有专用的串行链路。我们也有相应的软件，让很多芯片运行起来就像一块芯片。我们抛弃了累赘——而不是在已有内存层级之上分层，构建只能解决单一问题的结构。」Naveen Rao 表示。 今天看来，所有这些都仍然适用，除了日益增长的 FP16 运算速度，低精度训练已经成为了深度学习硬件上的一个热门话题，而英伟达的新一代 GPU 架构 Volta 也加入了专门用于处理矩阵乘法的 TensorCore。英特尔 Nervana 也会在今年的晚些时候推出自己的强力产品，但其性能、功耗和先进性如何，还有待考量。 Nervana 芯片是如何处理大规模低精度训练的，以内存连接策略是什么机制，这些特性都是人们期待已久的。其中的一些问题已经在 2017 年底的论文《Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks》，及其在 NIPS 2017 上的演讲中得到了解答。除了论文之外，Flexpoint 也有一个更加简单的展示页：https://ai.intel.com/flexpoint-numerical-innovation-underlying-intel-nervana-neural-network-processor/ 实际上，与更标准的 FP16 方法相比，使用 16 位整数乘法器和加法器树，NNP 更加节省功耗与面积。这是一个有趣的选择，因为 FP16 有更小的乘法器，但是也有加法器树和移位的需要，这抹去了 FP16 的优势，Kloss 解释道。「我们相信 Flexpoint 是神经网络性能与功耗的最佳平衡。我们已经看到大量神经网络在其之上运行良好，所以 16 位动态范围是足够的，同时我们也通过更少的移位器和低位运算降低了浮点运算量的需求，提高了效率。」 「在单个芯片上的神经网络计算极大地受到能源与带宽的限制。为了让神经网络负载获得更高的吞吐量，除了前面提到的存储创新之外，我们还创造了一种新的数字格式 Flexpoint。Flexpoint 能让标量计算（scalar computation）部署为固定点相乘和相加，同时可使用共享的指数做大型动态范畴。因此，每个回路变得更小，这可带来芯片上并行处理的增加，同时可同步降低每次运算的能耗。」 对 Nervana 和英特尔而言，神经网络优秀表现的另一面是其存储和网络优势带来的带宽提升。如果你观察 NIPS 上展示的芯片，其片上高速网格网络都带有高频宽存储（HBM）。Nervana 的目标是最大化芯片上处理矩阵乘积和卷积运算的区域，而不是让这些区域做其他事，例如像 GPU 这样的芯片会设置做常见工作负载的区域。面对大型的神经网络，这种设计能使用外部网络，加上有足够频宽的多个芯片进行扩展，从而使组合的芯片能像单个大型计算节点一样做运算。 「矩阵相乘和卷积在深度学习中都是非常重要的计算。这些计算不同于常用工作负载，因为这些计算和数据移动大部分都可以提前知道。如此情况下，NNP 芯片没有标准的缓存层级，片上存储也直接由软件管理。更好的存储管理能让芯片在每个单元裸片（die）上都能获得最大的计算量，也就是对深度学习模型更快的训练。」 这些芯片上的连接性是架构宽带的另一个重要指标，它通过 SerDes（串行器/解串器，被拆分为四个 100GB 的「quads」），具有 1.2Terabits 的双向带宽。不幸的是，对此机制的性能我们没有看到图示。然而，就像 Kloss 告诉我们的，非常明显，能够把多个芯片组合成单个大型虚拟芯片，从而满足更大的模型。 再次强调，这些启示并不令人惊讶，因为 Nervana 几年前就曾介绍过这是一种快速地、定制化的、互相连接、高宽带内存、高精准的芯片。然而，最终能看到已经很好了，因为我们能把已有方法与巨头的训练方法、深度学习芯片创业公司的方法做对比了。 英特尔在深度学习上的目的是做出一款能够面相所有实现大小的产品。除了以上对此芯片的讨论，他指出最近的论文只强调 Xeon CPU 和 FPGA 的训练时间，以及 Movidius 对大规模机器学习推理的有利条件。英特尔和 Nervana 的未来将会如何发展？时间将会给我们答案。  
393,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736115&idx=2&sn=62f5482c7b706fbc6fc0ace859cd9c98&chksm=871ac18db06d489bfb90b15ade8013fe5408108148e65848e7508a201a4a767c692637ea31e4&scene=27,盘点 | Jeff Dean撰文回顾谷歌大脑2017：从基础研究到新硬件,继 之后，谷歌大脑负责人 Jeff Dean 近日撰文回顾了 2017 年的工作，内容包括基础研究工作，机器学习的开源软件、数据集和新硬件。本文是这次盘点的第一部分，谷歌将稍后推出第二部分，介绍机器学习在医疗、机器人等不同科学领域的应用与创造性，以及对谷歌自身工作带来的影响。 谷歌大脑团队致力于拓展人工智能在研究和系统工程方面的进展。去年，我们盘点了 2016 年的工作。2017 年，我们继续研究机器智能，并与谷歌、Alphabet 的多个团队合作，利用我们的研究成果改善人们的生活。本文将对 2017 年的工作进行盘点，包括基础研究工作，机器学习的开源软件、数据集和新硬件。 核心研究 谷歌大脑团队的研究重心是提升对机器学习的理解，提高解决该领域问题的能力。以下是 2017 年研究的几个主题。 AutoML 自动化机器学习的目标是开发新技术，使计算机可以自动解决新的机器学习问题，无需人类机器学习专家对每个新问题进行干预。如果要开发真正智能的系统，那么这就是我们所需要的基础能力。我们开发了使用强化学习和进化算法设计神经网络架构的新方法，在 ImageNet 分类和检测任务中实现了先进结果，同时也展现了如何自动学习新的优化算法和高效的激活函数。 语音理解和生成 开发新技术的另一个主题是提升我们的计算系统理解和生成人类语音的能力，包括与谷歌语音团队合作开发的 ，比谷歌之前的语音识别系统的性能提升了 16%。 我们还与谷歌机器感知（Machine Perception）团队合作开发了 的新方法，可以大幅提升生成语音的质量。该模型的平均评价计分（MOS）为 4.53，而有声书中的专业录音 MOS 为 4.58，之前最优的计算机生成语音系统的 MOS 为 4.34。 新型机器学习算法和方法 我们继续开发新型机器学习算法和方法，包括  、 （sparsely-gated mixtures of experts）、超网络（使用一个模型的权重生成另一个模型的权重）、新型多模态模型（在同一个模型中执行音频、视频、文本输入的多任务学习）、 、符号和非符号学习优化方法、通过离散变量进行反向传播的技术，以及多个新的强化学习算法改进。 用于计算机系统的机器学习 我们对在计算机系统中使用机器学习替换传统的启发式方法也很感兴趣。我们展示了如何 ，效果胜过人类专家。我们还在论文《 》中展示了神经网络学习索引比 B-Trees、哈希表和布隆过滤器速度更快、模型更小。我们在将机器学习应用到核心计算机系统方面仍需努力，正如谷歌在 NIPS 2017 上所展示的那样。 隐私性和安全性 机器学习及其与安全性和隐私性之间的相互作用仍然是我们的主要研究重点。我们展示了多种提供隐私性保证的机器学习技术，详见 ICLR 2017 最佳论文《 》。我们还继续研究对抗样本的特性，包括展示真实世界中的对抗样本、如何在训练过程中大规模控制对抗样本使模型对对抗样本具备更强的鲁棒性。 理解机器学习系统 我们已经看到深度学习的优秀结果，但理解其工作原理非常重要。在 ICLR 2017 另一篇最佳论文《 》中，我们展示了当前的机器学习理论框架无法解释深度学习方法的结果。我们还发现优化方法导致的极小值平坦度与优化泛化能力的相关性并没有我们之前认为的那么紧密。为了更好地理解深层架构中的训练过程，我们发布了多篇论文分析随机矩阵，因其是大部分训练方法的起点。我们在 中展示了优秀实验设计和数据严密性的重要性，研究发现目前很多 GAN 的衍生算法实际上并没有提升性能。我们希望该研究为其他研究者进行鲁棒性实验研究提供一个示例。 我们正在开发方法，使机器学习系统具备更好的可解释性。三月，我们与 OpenAI、DeepMind、YC Research 联合发布 。该平台的文章清晰地阐述了机器学习概念和精彩的交互可视化工具，并因此赢得声望。第一年，Distill 发布了大量富有启发性的文章，旨在理解不同机器学习技术的内在工作原理，我们期待它在 2018 年有更大进步。 用于机器学习研究的开源数据集 开源数据集如 MNIST、CIFAR-10、ImageNet、SVHN 和 WMT 极大地推动了机器学习的发展。我们的团队和谷歌研究团队在过去一年中一直致力于开放式机器学习研究的新数据集开源工作，开放了大量大型标注数据集，包括： YouTube-8M ：超过 700 万 YouTube 视频，带有 4716 种不同类别的注解 YouTube-Bounding Boxes ：21 万 YouTube 视频，500 万边界框 Speech Commands Dataset ：数千个讲话者说的命令式短句 AudioSet ：200 万时长 10 秒的 YouTube 音频片段，标注有 527 个不同的音频事件 Atomic Visual Actions (AVA) ：57000 视频片段，21 万动作标签 Open Images ：900 万创作通用的许可图像，6000 个标注类别 Open Images with Bounding Boxes：600 个分类的 120 万边界框 TensorFlow 与开源软件 在整个团队的历史中，我们创建了一些工具，帮助我们自己开展机器学习研究，并将其部署到诸多谷歌产品之中。2015 年 11 月，我们开源了第二代机器学习框架 TensorFlow，希望整个机器学习社区可以从中受益。2017 年 2 月，我们发布了 TensorFlow 1.0，11 月，我们推出了包含 ： ，用于交互式命令风格的编程；XLA，TensorFlow 程序的一个优化编译器；TensorFlow Lite，移动端和嵌入式设备的一个轻量级方案。预编译的 TensorFlow 二进制文件现在已被来自超过 180 个国家的用户下载 1000 万次之多，GitHub 上的源代码也有了超过 1200 个贡献者。 2 月，我们 ，超过 450 人前来山景城参会，超过 6500 人观看了视频直播。会议主题涵盖新特征、TensorFlow 使用技巧等。我们将在 2018 年 3 月 30 号在湾区举办下一届大会。 11 月，TensorFlow 庆祝其推出两周年。TensorFlow 是 GitHub 上的最大机器学习平台，也是五大 repo 之一。TensorFlow 被大大小小很多公司和组织使用，GitHub 有超过 24500 个独立 repo 与 TensorFlow 相关。很多研究论文的发布带有 TensorFlow 实现的研究结果，使得社区更容易理解所使用方法以再现或继续扩展工作。 TensorFlow 同样受益于其他谷歌研究团队的相关开源工作，包括 TF-GAN（用于生成对抗模型的一个轻量级库）、TensorFlow Lattice（用于使用 lattice 模型工作的一系列评估器）和 TensorFlow Object Detection API。TensorFlow 模型 repo 也随着模型的发展而不断增长。 除了 TensorFlow，我们还 ，一个用于浏览器的深度学习 API 的开源硬件加速实现（无需任何下载或安装）。deeplearn.js 主页上有很多不错的实例，比如 Teachable Machine 和 Performance RNN。2018 年我们将会继续努力，实现 TensorFlow 模型在 deeplearn.js 环境中的直接部署。 TPU 大约五年之前，我们认识到深度学习将会极大地改变我们所需的硬件。深度学习计算是密集型的，并有两个特性：它们主要由密集的线性代数运算（矩阵倍数、向量运算等）组成，可以忍受精度的降低。我们意识到可以利用这两个特性构建高效运行神经网络计算的专门硬件。我们为谷歌平台提供设计，由此诞生了第一代 TPU：一个单芯片 ASIC，可以加速深度学习模型的推理。第一代 TPU 部署在我们的数据中心已有三年，在谷歌搜索，在谷歌翻译，在谷歌图片，在 AlphaGo 对战柯洁和李世石中，以及在许多其他研究和产品应用中，都有 TPU 在发挥作用。六月，我们 ，表明第一代 TPU 比现今的 GPU 或 CPU 快 15X - 30X，性能/功耗提升 30X - 80X。 推理很重要，但是加速训练进程更为重要，并且更难。我们的第二代 TPU 是在 上推出的，它是一个可同时加速推理和训练的全系统，会上展示了一个单设备配置和一个被称作 TPU Pod 的多机架深度学习超级计算机配置。我们还宣布第二代 TPU 将会以云端 TPU 的形式出现在谷歌云平台上。我们同样推出了 TensorFlow Research Cloud (TFRC)，该项目可为致力于共享其成果的顶级机器学习研究者免费提供 1000 块云端 TPU。十二月，我们在 TPU Pod 上用时 22 分钟完成了 ResNet-50 ImageNet 模型的高精度训练，相比之下传统方法则需要数天甚至更长。我们认为以这种方式缩短研究周转时间将会极大地提升谷歌机器学习团队及所有使用云端 TPU 团队的产出效率。  
394,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736180&idx=3&sn=5e3c2d3fab70dcb632a58b9b62a91669&chksm=871ac24ab06d4b5ccdeda3977cdc1bdc2fe112e9a442dba12c84dfce568b29bce8d486003b56&scene=27,教程 | 用数据玩点花样！如何构建skim-gram模型来训练和可视化词向量,"本文介绍了如何在 TensorFlow 中实现 skim-gram 模型，并用 TensorBoard 进行可视化。 GitHub 地址：https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb 本教程将展示如何在 TensorFlow 中实现 skim-gram 模型，以便为你正在处理的任意文本生成词向量，然后用 TensorBoard 进行可视化。我发现这个练习 1）有助于理解 skim-gram 模型是的工作原理；2）在 CNN 或 RNN 中使用词向量之前，先熟悉词向量捕获文本的关系。 我在 text8 数据集上训练了一个 skim-gram 模型，该数据集是英文维基百科文章的集合。我用 TensorBoard 来可视化这些嵌入。TensorBoard 允许使用 PCA 选择 3 主轴来投影数据，进而看到整个文字云。超级酷！你可以输入任何单词，它会显示相邻的单词。你也可以隔离最靠近它的 101 个点。 看看下面的片段。 完整代码：https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb 为了可视化训练过程，我还持续跟踪一组随机单词在模型中最靠近的预测单词。在第一次迭代中，最接近的预测单词看起来非常随机。这很合理，因为所有词向量都是随机初始化的。 训练结束时，该模型已经能更好地找到单词之间的关系。 Word2Vec 和 Skip-Gram 模型 创建词向量是基于大型文本语料库，为每个单词创建向量的过程，且语料库中语境相似的单词所对应的向量在向量空间中非常接近。 这些词向量可以很好地捕捉单词之间的上下文关系（例如，黑色、白色和红色的示例向量会紧密地结合在一起），而且使用这些向量（而不是单词本身）来完成文本分类或新文本生成等自然语言处理（NPL）任务，会得到更好的结果。 有两个主要的模型来生成这些词向量——连续词袋（CBOW）和 Skip-Gram 模型。CBOW 模型试图根据给定语境词预测中心词，而 skip-gram 模型试图根据给定中心词预测语境词。我们可以看一个简化的例子： CBOW: The cat ate _____. Fill in the blank, in this case, it's「food」. CBOW：这只猫吃了________。（句子填充）本例中，应该填「食物」。 Skip-gram: ___ ___ ___ food. Complete the word's context. In this case, it's「The cat ate」 Skip-gram：_______________食物。（句子填充）本例中，可以填「这只猫吃了」 如果你对这两种方法的详细对比感兴趣，请参见此链接：https://iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/。 大量论文发现，skip-gram 模型能产生更好的词向量，所以我将重点放在实现这个模型上。 在 Tensorflow 中实现 Skip-Gram 模型 这里我只列出构建模型的主要步骤。详情请查看我的 GitHub repo。 1. 数据预处理 首先清理数据，删除标点、数字，并将文本分割成单个单词。比起单词，程序能更好地处理整数，因此我们创建一个「词汇转整数」字典，将每个单词映射到一个整数上。代码如下： 2. 子采样 经常出现的单词，如「the」、「of」和「for」，并没有给附近的单词提供太多的语境。如果丢弃一些，我们就可以消除数据中的的部分噪声，实现更快的训练和更好的表示。这个过程被 Mikolov 称为子采样（subsampling）。 3. 创建输入和目标 skip-gram 模型的输入是每个单词（编码为整数），而目标是围绕该窗口的单词。Mikolov 等人发现，如果这个窗口的大小是可变的，同时更接近中心词的单词被采样次数较多时，性能会更好。 「由于距离更远的词通常不如距离更近的词与目标单词的关系那么紧密，我们从远距离的词中采样较少的单词作为训练样本，以降低其权重……如果选择窗口大小= 5，那么我们将为每一个训练词随机选择一个 1 和窗口大小 5 之间的数字 R，然后将目标单词在句子中的前后 R 个单词纳入训练，作为正确的标签。」 4. 构建模型 下图展示了我们将要构建网络的一般结构（图片来源：http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/）。 我们把一个输入词如「ants」（蚂蚁）表示为独热向量。这个向量有 10000 个分量（每个分量都对应于词汇表中的一个单词），我们将单词「ants」对应的分量设为「1」，所有其他分量都为 0。网络的输出也是一个单向量（也包含 10000 个分量）。 训练结束时，隐藏层将会有经过训练的词向量。隐藏层的大小相当于向量中的维数。在上面的例子中，每个单词都有一个长度为 300 的向量。 你可能已经注意到，skip-gram 神经网络包含大量的权重……在我们的例子中有 300 个特征和包含 10000 个单词的词汇表，也就是说在隐藏层和输出层都有 3 百万个权重数！在大型数据集上进行这样的训练令人望而却步，因此 word2vec 的作者引入了一些调整来使训练变得可行。详情请查看：http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/；Github 代码：https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb。 5. 用 TensorBoard 进行可视化 使用 TensorBoard 中的「嵌入投影机」可视化嵌入。要实现这个功能，你需要完成以下步骤： 在检查点目录的训练结束时保存你的模型 创建一个 metadata.tsv 文件包含每个整数转换回单词的映射关系，这样 TensorBoard 就会显示单词而不是整数。将这个 tsv 文件保存在同一个检查点目录中 运行这段代码： 打开 TensorBoard，将其指向检查点目录 大功告成！ 原文链接：https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8 "
395,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736115&idx=4&sn=cd4642b7b1c6f9b739b98ab80722864e&chksm=871ac18db06d489b2389a35d14d0b05a14a0e624f16b438454298772a73bb1f6131cc902dded&scene=27,入门 | GPU是如何优化运行机器学习算法的？,"在机器学习中，绝大多数任务会涉及到耗费时间的大量运算，而且随着数据集的增加，运算量会越来越大。解决这个问题的一个方法就是使用多线程。在这篇文章中，我要结合代码介绍一下 GPU 加速，它是如何完成的，以及用于 GPU 任务的简单 API。下面以一个矩阵乘法开始全文内容。 矩阵乘法 上面给出了两个矩阵，一个 3×6 的，一个 6×6 的。乘积的结果将会是一个 3×6 的矩阵。完成这个运算总共需要 3×6×6 次乘法运算。那么，我们可以得到这样的结论：这个任务的时间复杂度是 O（mn^2）。这也就意味着，2000×2000 的矩阵运算将会需要 8,000,000,000 次乘法运算。这会花费大量的 CPU 计算时间。 引入 GPU 通常 GPU 会包含大量的处理核心。核心数目从 384 个到几千个。下面是 NVIDIA 几款消费级 GPU 的比较（https://www.nvidia.com/en-us/geforce/products/10series/compare/）：  CUDA 是统一计算设备架构（Compute Unified Device Architecture）的缩写。它们以相对稍慢的速度运行，但是能够通过使用大量运算逻辑单元（ALU）来提供很大的并行度。更详细内容请参考链接（http://www.nvidia.com/object/what-is-gpu-computing.html）。 这张图展示了 CUDA 的线程模型（这个和市场上其他的架构几乎是相同的，例如 AMD）。简单起见，我们假设一每个 CUDA 核一次只能运行一个线程。如果我们的数据集比较大，我们可以将它分成块。上图中的一个 Grid 包含多个 Block。Block 则是另一个包含与它维度相同个数的线程的矩阵。总之，由于这是一个简介，所以我们要以一个用 Java 开发的简单 API 来聚焦更大更复杂的结构。 GPU 的思考 正如我们讨论到的，每个 GPU 核心都能运行一个独立的线程。开始这个模拟的最简单的方式就是假设最终结果数组中的每个元素都由一个 GPU 核来计算。因为所有的核都是并行运行的，所有矩阵的所有元素也会被并行的计算。所以，我们现在的时间复杂度就变成了 O(n)。现在，对于 2000×2000 的矩阵乘法，我们只需要 2000 次运行，这对计算机而言是容易计算的。通常我们之前所说的每一个线程都知道自己的身份，也就是它所属于的 block 和 Grid。或者，说得简单一些就是元素在矩阵中的位置。此外，矩阵会被加载到 GPU 中共享它的内存，我们可以通过索引直接访问元组中的数据。是不是很容易？我们对着代码来看一看吧。 使用 APARAPI 进行 GPU 编程 APARAPI（A-PARallel-API）是一个基于 OpenCL 的用于 GPU 编程的 wrapper。它既支持 CUDA 架构，也支持 AMD 架构。此外，这个 API 还引入了 Java 中的伟大的面向对象思想，如果我们直接用 C++来完成这个任务的话也许会有些混乱。上手非常容易。虽然其中有内在依赖项，但是要确保你正确地设置了 OpenCL 或者 CUDA。简单的 Google 一下会帮助到你。大多数设备都是自带的（OSX 或者 windows 设备）。 pom.xml MatrixMultiplication.java 上述代码的精简化 Kernel 就是在 GPU 上运行的代码部分。Kernel 可见的变量将会被拷贝到 GPU 的 RAM 中。我们因为 GPU 支持线性数组，所以我们不能以 2D 数组的形式输入数据。GPU 不能处理 2D 数组，但是它们是通过维度的概念来处理的（此处暂且不讨论这个内容）。 上述代码在 GPU 中分配了小于等于 SIZE × SIZE 个线程。 上述代码从私有内存中得到了线程的 ID。我们可以通过这个 ID 来区分这个线程单元的位置。对每个线程我们做以下处理： 这是两个矩阵对应单元相乘相加的最简单的形式。我们只为使用线程索引的单个线程定义了 Kernel，它将会在所有的线程上并行运行。 结果 运算是很快的，但是有多快呢？这个是上述代码的输出： 1200 × 1200 由于下面的矩阵比较大，所以我们只在 GPU 上运行以下的运算。 2000 × 2000 的矩阵运算耗时 3757ms 5000 × 5000 的矩阵运算耗时 5402ms 自己也尝试一下？  "
396,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736180&idx=1&sn=4e26c6a0b3c72fd82809ce94b476c55a&chksm=871ac24ab06d4b5c38d08eeb6236c30e8c905ae02dbb7b6356022a97e7cc9cd468ad90ffa8d7&scene=27,每个Kaggle冠军的获胜法门：揭秘Python中的模型集成,"集成方法可将多种机器学习模型的预测结果结合在一起，获得单个模型无法匹敌的精确结果，它已成为几乎所有 Kaggle 竞赛冠军的必选方案。那么，我们该如何使用 Python 集成各类模型呢？本文作者，曼彻斯特大学计算机科学与社会统计学院的在读博士 Sebastian Flennerhag 对此进行了一番简述。 在 Python 中高效堆叠模型 集成（ensemble）正在迅速成为应用机器学习最热门和流行的方法。目前，几乎每一个 Kaggle 冠军的解决方案都使用了集成，很多数据科学 pipeline 也使用集成。 简单来说，集成把不同模型的预测结果结合起来，生成最终预测，集成的模型越多，效果就越好。另外，由于集成结合了不同的基线预测，它们的性能至少等同于最优的基线模型。集成使得我们几乎免费就获得了性能提升！ 本文介绍集成的基础知识：定义及工作原理，并提供构建基础集成的实践教程。阅读本文，你将： 理解集成的基础知识； 了解如何编写集成； 理解集成的主要优缺点。 预测共和党和民主党的捐款 在本文中，我们将使用美国政治捐款数据集来解释集成的工作原理。该数据集最初由 FiveThirtyEight 的 Ben Wieder 制作，他查阅美国政府的政治捐款记录，发现科学家向政治家们捐款时，通常会选择民主党。 该论断基于向共和党和民主党捐款数额的比例。但是，我们还可以从中看出更多事情：比如，哪个学科最可能捐款给共和党，哪个州最可能捐款给民主党。我们将更进一步，预测捐款更可能的流向。 此处使用的数据经过稍微修改（https://www.dataquest.io/blog/large_files/input.csv）。我们删除了捐款给共和党和民主党之外的其他党派的捐款信息，以使过程更加清晰，并且还去除了重复信息和不太有趣的特征。数据脚本地址：https://www.dataquest.io/blog/large_files/gen_data.py。数据如下： 上图是 Ben 的论断的数据依据。确实，75% 的捐款是给民主党的。我们来看一下可以使用的特征，我们具备捐款人、交易详情和捐款接受者的数据信息： 我们使用 ROC-AUC 来评估模型性能。如果你之前没用过该指标，随机猜测可以是 0.5 分，完美的召回率和精确率是 1.0。 什么是集成？ 想象一下你在玩常识问答游戏。一个人玩时，可能会有一些题你完全不了解。如果我们想获得高分，就需要组建一个团队来覆盖所有相关主题。这就是集成的基本概念：结合多个模型的预测，对特异性误差取平均，从而获得更好的整体预测结果。 一个重要问题是如何结合预测。以常识问答游戏为例，我们很容易想象到团队成员可能会使用多数投票的方式确定选择哪个答案。机器学习的分类问题也是一样：作出最常见的类别标签预测相当于多数投票规则。但是也有很多其他方式可以结合预测，通常我们会使用一个模型来学习如何最好地结合预测结果。 通过决策树理解集成 我们用一个简单的可解释性模型来解释集成：使用 if-then 规则的决策树。决策树越深，可以捕捉的模式就越复杂，不过也更有可能出现过拟合。因此，我们需要另一种方式来构建决策树的复杂模型，而不同决策树的集成就是这样一种方式。 我么使用下列辅助函数来可视化决策树： 在训练数据上用决策树拟合一个节点（决策规则），查看它在测试集上的性能： 每个叶节点记录它们在训练样本中的比例、类别分布和类别标签预测。我们的决策树根据捐款金额是否超过 101.5 进行预测：它竟然作出了同样的预测！鉴于 75% 的捐款都给了民主党，这个结果并不令人惊讶。但是这没有充分利用我们已有的数据，下面我们使用三层决策规则，看看会得到什么： 该模型并不比简单的决策树好太多：预测到的共和党捐款金额比例只有 5%，远远低于 25%。仔细观察会发现该决策树使用了很多不确定的分割规则（splitting rule）。观察结果中高达 47.3% 的结果在最左边的叶节点中，而 35.9% 在右二的叶节点中。因此大量叶节点没有关联。使该模型更深只会导致过拟合。 在深度固定的情况下，决策树可以通过增加「宽度」的方式来增加复杂度，即创建多个决策树，并将其连接起来。也就是决策树的集成。想了解这个集成模型为什么会起作用，先要考虑我们如何让决策树探索出比上层树更多的其他模式。最简单的解决方案就是删除树中较早出现的特征。假如我们删除了转账金额特征（transaction_amt），树的根节点，则新的决策树如下： ROC-AUC 得分与上树得分类似，但是共和党捐款比例增加至 7.3%。还是很低，但比之前稍高一些。重要的是，与第一个树相反（第一个树大部分规则与转账本身相关），这棵树更专注于候选人的居住地。现在我们有两个模型，二者预测能力相近，但基于不同的规则运行。因此，它们可能出现不同的预测误差，我们可以使用集成方法取其平均数。 为什么平均预测有作用 假如我们要基于两个观察结果生成预测。第一个观察结果的真正标签为共和党，第二个是民主党。在该示例中，假设模型 1 预测结果是民主党，模型 2 预测结果是共和党，如下表所示： 如果我们使用标准的 50% 分割规则（50% cutoff rule）进行类别预测，每个决策树的预测结果都是一个正确一个错误。我们对模型的类别概率取平均来创建一个集成。在该示例中，模型 2 对观察结果 1 的预测是确定的，而模型 1 相对来说不那么确定。集成对二者的预测进行衡量，然后支持模型 2，正确地预测了共和党。至于第二个观察结果，局面扭转过来，集成正确地预测到民主党： 如果集成包含两个以上决策树，则它根据多数原则进行预测。因此，集成对分类器预测结果取平均又叫作多数投票分类器（majority voting classifier）。当集成基于概率取平均时，我们称其为软投票，而对类别标签预测结果取平均被成为硬投票。 当然，集成不是万能的。你可能注意到上述示例中，取平均有效的前提是预测误差必须不相关。如果两个模型都作出了错误的预测，则集成无法作出进行修正。此外，在软投票机制中，如果一个模型作出了错误的预测，但概率值较高，则集成可能会作出错误的判断。通常，集成无法使每个预测都正确，但是预计其性能优于底层模型。 森林是树的集成 回到我们的预测问题，看看我们是否可以用两个决策树构建一个集成。首先检查误差关联性：高度关联的误差会造成差的集成。 有一些关联性，但不过分：预测方差仍有很大的利用空间。为了构建该集成，我们简单地平均了两个模型的预测。 确实，集成步骤导致分值增加。但是如果我们有更多不同的树，我们甚至可以得到更大的分值。在设计决策树时，我们应该去除哪些特征？ 一个快速有效的实践方法是随机地选择一个特征子集，在每个 draw 上拟合一个决策树并平均其预测。这一过程被称为自举平均（bootstrapped averaging，通常缩写为 bagging），它应用于决策树所产生的模型是随机森林。让我们看看随机森林能为我们做什么。我们使用 Scikit-learn 实现构建了 10 个决策树的集成，每一个拟合包含 3 个特征的子集。 随机森林极大改进了我们之前的模型。但是只使用决策树可以做的事情比较有限。是时候扩展我们的视野了。 目前为止，我们看到了集成的两个重要方面： 1. 预测误差的关联性越低，效果越好 2. 模型越多，效果越好 出于这一原因，尽可能使用不同模型不失为一个好方法（只要它们表现良好）。目前为止我们一直在依赖简单的平均，但是稍后我们将了解如何使用更复杂的结合。为了记录进程，我们把集成公式化为如下： 涵盖的模型没有限制：决策树、线性模型、核模型、非参数模型、神经网络，或者甚至其他集成！记住我们包含的模型越多，集成的速度就会越慢。 为了构建不同模型的集成，我们首先在数据集上对一组 Scikit-learn 分类器进行基准测试。为了避免代码重复，我们使用下面的辅助函数： 我们现在正准备创建一个预测矩阵 P，其中每个特征对应于由给定模型做出的预测，并根据测试集为每个模型评分： 这是我们的基线。梯度提升机（Gradient Boosting Machine/GBM）效果最好，其次是简单的 logistic 回归。对于我们的集成策略来说，预测误差必须是相对不关联的。 误差明显关联，这对于表现良好的模型是可以预期的，因为它是典型的异常值，很难纠正。然而，大多数关联性在 50-80％的范围内，所以还有很大的改进余地。事实上，如果我们从类别预测的角度看误差关联性，事情看起来会更有希望： 为了创建集成，我们继续并进行平均预测，正如我们所期望的，集成的性能要好于基线。平均化是一个简单的过程，如果我们存储模型预测，我们可以从一个简单的集成开始，并在训练新模型时随时增加其大小。 可视化集成的工作过程 我们已经对集成的误差关联机制有所了解。这意味着集成通过平均谬误可以平滑决策边界。决策边界向我们表明评估器如何将特征空间分割成邻域，其中所有的观察结果被预测为具有相同的分类标签。通过平均基学习器决策边界，集成被赋予更平滑的边界，泛化也更自然。 下图展示了这一点。这里的实例是鸢尾花数据集，其中评估者试图对三种花进行分类。基学习器在其边界都有一些不良的特性，但是这个集成有一个相对平滑的决策边界，与观察结果一致。令人惊讶的是，集成既增加了模型的复杂度，也起到了正则化项的作用！ 当任务是分类时，另一种理解集成的方式是检查 ROC 曲线（Receiver Operator Curve），它向我们展示了评估者如何进行精确率和召回率之间的权衡。通常，不同的基学习器做出不同的权衡：一些通过牺牲召回率实现更高的精确率，另一些则相反。 另一方面，对于每个训练点，非线性元学习器可以调整其依赖的模型。这意味其可以极大地减少不必要的牺牲，并在增加召回率的同时保持高精确率（反之亦然）。下图中，集成在精确率上做了一个更小的牺牲，以增加召回率。 但是在预测误差变动一定的情况下，你不会期望更多的提升吗？一些模型表现要比其他模型相对糟糕，但是其影响一样大。这对不平衡数据集来说是毁灭性的：如果一个模型做出极端预测（即接近于 0 或 1），则通过软投票召回，因为其对预测平均值有极大的影响。 对我们来说，一个重要的因素是模型是否可以捕捉到共和党所收捐款的全部比例。一个简单的检查表明所有模型对共和党捐款比例的预测都过低，其中一些相对更糟。 我们尝试通过去除最糟糕的来提升集成，比如说多层感知机（MLP）： 实际上不算提升：我们需要一个更聪明的方式来排定模型之间的优先顺序。很明显，从一个集成中删除模型是相当猛烈的，因为有可能删除带有重要信息的模型。我们真正想要的是学习平均预测时使用的一组合理的权重。这把集成变成了一个需要训练的参数化模型。 学习结合预测 学习加权平均值意味着对于每个模型 f_i 都有一个权重参数 ω_i∈(0,1) 将权重分配给该模型的预测。加权平均值需要所有的权重总和为 1。现在，集成的定义如下： 这与之前的定义有一些小的改变，但是很有趣的一点是：一旦模型生成预测 p_i=f_i(x)，则学习权重就相当于基于预测来拟合线性回归： 权重有一些约束项。然后，我们不用仅拟合线性模型了。假设我们拟合最近邻模型。集成会基于给定观察结果的最近邻取局部平均值，这样集成就可以适应模型性能随着输入变化而产生的改变。 实现集成 要构建这种类型的集成，我们需要： 1. 用于生成预测的基学习器库； 2. 学习如何最佳结合预测结果的元学习器； 3. 在基学习器和元学习器之间分割训练数据的方法。 基学习器采用原始输入，生成一系列预测。如果我们的原始数据集是形态为 (n_samples, n_features) 的矩阵 X，那么基学习器库输出新的预测矩阵 P_base，形态为 (n_samples, n_base_learners)，其中每一列代表一个基学习器的预测结果。元学习器在 P_base 上训练。 这意味着恰当地处理训练集 X 至关重要。尤其是，如果我们在 X 上训练基学习器，用它们预测 X，则元学习器将在基学习器的训练误差上训练，但在测试时元学习器将面对基学习器的测试误差。 我们需要一种策略来生成反映测试误差的预测矩阵 P。最简单的策略是将完整的数据集 X 分割成两部分：在其中一半上训练基学习器，然后让它们预测另一半，然后将预测输入元学习器。这种方法又简单又快，不过会丢失一些数据。对于中小型规模的数据集，信息丢失可能会比较严重，导致基学习器和元学习器性能不好。 为了保证覆盖完整的数据集，我们可以使用交叉验证法。有很多方式可以执行交叉验证，在那之前，我们先来一步一步地实现集成。 第一步：定义基学习器的库 它们是处理输入数据并生成预测的模型，可以是线性回归，也可以是神经网络，甚至可以是另一个集成。和往常一样，多样性是强大的！唯一需要注意的是，我们加入越多的模型，集成运行的速度就会越慢。在这里，我会使用此前的模型集合： 第二步：定义一个元学习器 应该使用哪个元学习器，人们并没有统一看法，但目前流行的选择是线性模型、基于核的模型（支持向量机和 KNN 算法）以及基于决策树的模型。你也可以使用另一个集成作为「元学习器」：在这种特殊情况下，你最终会得到一个两层的集成，这有点类似于前馈神经网络。 在这里，我们会使用一个梯度提升机。为了确保 GBM 能够探索局部特征，我们需要限定每 1000 个决策树在 4 个基学习器的随机子集和 50% 的输入数据上进行训练。这样，GBM 就会表达每个基学习器在不同近邻输入空间上的预测内容。 第三步：定义步骤，生成训练和测试集 为简单起见，我们将完整训练集分为基学习器的训练集和预测集。这种方法有时被称为「混合（Blending）」。不过，不同社区之间的术语是不同的，所以知道集成使用了哪种类型的交叉验证有时并不容易。 我们现在有一个为基学习器准备的训练集（X_train_base,y_train_base）和一个预测集（X_pred_base,y_pred_base），准备好为元学习器生成预测矩阵了。 第四步：在训练集上训练基学习器 为在训练数据上训练基学习器，我们照常运行： 为训练基学习器，我们需要执行： 第五步：生成基学习器预测 基学习器拟合后，我们现在可以生成一系列预测用于训练元学习器。注意，我们生成的基于观测值的预测并不会用于基学习器的训练，对于每个观测： 在基学习器预测集中，我们生成了基学习器预测结果的集合： 如果你实现自己的集成，请特别注意如何索引预测矩阵的行和列——将数据分成两个部分并不难，但对于后来的交叉验证就很有挑战性了。 为生成预测，我们需要执行： 第六步：训练元学习器 预测矩阵 P_base 反映了测试时间的性能，可被用于训练元学习器： 就是这样！我们现在有了完全训练的集成，可用来预测新数据了。为生成观测值 x(j) 的预测，我们先要将其输入基学习器。它们输出一系列预测 我们再将其输入元学习器。元学习器会给我们集成的最终预测 现在我们对于集成学习有了一个明确的认识，是时候看看它能对政治捐款数据集做出怎样的预测了： 为生成预测，我们需要执行： 正如我们所料，集成击败了此前基准的最佳估计，但它仍然无法击败简单的平均集成。这是因为我们只对一半的数据进行基学习器和元学习器的训练，所以大量的信息丢失了。为了防止这点，我们需要使用交叉验证策略。 利用交叉验证训练 在交叉验证训练基学习器时，每个基学习器的备份都进行了 K-1 fold 的拟合，并进行了剩余 fold 的预测。这一过程不断重复，直到每个 fold 都被预测。我们指定的 fold 越多，每次训练过程中的数据就越少。这使得交叉验证的预测在测试期间噪声更小，性能更好。但这显著增加了训练时间。通过交叉验证拟合一个集成经常被称为堆叠（stacking），而集成本身会被称为超级学习器（Super Learner）。 为理解交叉验证是如何运作的，我们可以把它想象为之前集成的一个外循环（outer loop）。外循环在个格不同的测试 fold 上迭代，而其余的数据用于训练；内循环训练基学习器并产生预测数据。这是一个简单的堆叠实现： 让我们来看看这里涉及的步骤。首先，我们在所有数据上拟合基学习器：这与之前的混合集成相反，基学习器在测试时间内训练了所有数据。随后我们遍历所有 fold，随后遍历所有基学习器生成交叉验证预测。这些预测堆叠在一起构成了元学习器的训练集——它也训练了所有数据。 混合和堆叠的基本区别在于，堆叠允许基学习器和元学习器在全部数据集上进行训练。使用双重交叉验证，我们可以测量这种情况下的差异： 堆叠带来了可观的性能提升：事实上，它得到了目前的最佳分数。对于中小型数据集来说，这种成绩是非常典型的，其中混合的影响比重很大。随着数据集体量的增大，混合与堆叠的表现会逐渐趋同。 堆叠也有自己的缺点，特别是速度。通常，在交叉验证的情况下，我们需要知道这些问题： 1. 计算复杂度 2. 结构复杂度（信息泄露的风险） 3. 内存用量 理解它们对于高效使用集成方法来说非常重要，让我们一一道来。 1. 计算复杂度 假设我们需要使用 10 折堆叠。这需要在 90% 的数据上训练所有基学习器 10 次，然后在所有数据上再训练一次。若有 4 个基学习器，集成需要花费的时间大约是最佳基学习器的 40 倍。 但每个 cv-fit 都是独立的，所以我们不需要依次拟合模型。如果我们能够平行拟合所有 fold，集成就只会比最佳基学习器慢 4 倍——这是一个巨大的提升。集成是并行化的最佳受益者，能够充分利用这一机制对它来说至关重要。为所有模型拟合所有 fold，集成的时间惩罚就可以忽略不计了。为了介绍这一点，下图是 ML-Ensemble 上的一个基准，它展示了 4 个线程上依次或并行堆叠或混合拟合所花费的时间。 即使有了这种程度的并行性，我们也可以减少大量计算时间。然而，并行化与一系列潜在的棘手问题有关，如竞态条件、锁死和内存爆炸。 2. 结构复杂度 当我们决定在元学习器上使用整个训练集时，我们必须关注「信息泄露」问题。当错误地预测训练期间使用的样本时，就会出现这种现象，例如混合了不同的 fold，或使用了错误的训练子集。当元学习器在训练集上出现信息泄露时，预测错误就会产生：garbage in、garbage out。发现这样的 bug 是非常困难的。 3. 内存用量 并行化的最后一个问题，特别是在 Python 中多任务处理时经常会碰到的问题。在这种情况下，每个子进程都有自己的内存，同时需要复制父进程中所有的数据。因此，一个未做优化的实现会复制程序的所有数据，占用大量内存，浪费数据序列化的时间。为阻止这种情况，我们需要共享数据存储器，而这反过来又容易导致数据损坏。 结果：使用工具包 这一问题的结果是需要使用一个经过测试、且为你的机器学习方法构建的软件包。事实上，一旦你用上了集成工具包，构建集成就会变得非常简单：你需要做的仅仅是选定基学习器、元学习器，以及训练集成的方法。 幸运的是，今天每个流行的编程语言里都有很多可用的工具包——虽然它们有着不同的风格。在本文的末尾我会列举其中的一些。现在，让我们选用其中的一个，看看集成方法是如何处理政治捐款数据集的。在这里，我们使用 ML-Ensemble 来构建我们之前提到的广义集合，但现在使用 10 折交叉验证。 就是这么简单！ 观察超级学习器简单平均集合的 ROC 曲线，其中展示了超级学习器如何利用全部数据仅牺牲少量召回率即可获得给定精确率。 除了本文介绍的几种集成外，目前还有很多其他的集成方式，不过它们的基础都是一样的：一个基学习器库、一个元学习器，以及一个训练程序。通过调整这些组件的配合，我们可以设计出各种特定的集合形式。有关更高级集合的内容请参阅这篇文章：https://mlwave.com/kaggle-ensembling-guide/。 当我们谈到软件时，每个人都有自己的喜好。随着集成方法的流行，集成工具包的数量也越来越多。实际上集成方法是先在统计学社区中流行起来的，所以 R 语言中有很多为此设计的库。近年来，Python 和其他语言中也出现了更多相关工具。每个工具包都能满足不同的需求，处于不同的成熟阶段，所以我建议大家选用前先浏览一番再做决策。  下表列出了其中一些工具： "
397,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736115&idx=1&sn=f09f01e162a82f8778c6fb5fdb127872&chksm=871ac18db06d489bf2936ba7b1ba795bb5d9e97ed5807e12f0a6c8e294f1fd42cff132eb876a&scene=27,前端慌不慌？用深度学习自动生成HTML代码,"如何用前端页面原型生成对应的代码一直是我们关注的问题，本文作者根据 pix2code 等论文构建了一个强大的前端代码生成模型，并详细解释了如何利用 LSTM 与 CNN 将设计原型编写为 HTML 和 CSS 网站。 项目链接：https://github.com/emilwallner/Screenshot-to-code-in-Keras 在未来三年内，深度学习将改变前端开发。它将会加快原型设计速度，拉低开发软件的门槛。 Tony Beltramelli 在去年发布了论文《pix2code: Generating Code from a Graphical User Interface Screenshot》，Airbnb 也发布了 Sketch2code（https://airbnb.design/sketching-interfaces/）。 目前，自动化前端开发的最大阻碍是计算能力。但我们已经可以使用目前的深度学习算法，以及合成训练数据来探索人工智能自动构建前端的方法。在本文中，作者将教神经网络学习基于一张图片和一个设计模板来编写一个 HTML 和 CSS 网站。以下是该过程的简要概述： 1）向训练过的神经网络输入一个设计图 2）神经网络将图片转化为 HTML 标记语言 3）渲染输出 我们将分三步从易到难构建三个不同的模型，首先，我们构建最简单地版本来掌握移动部件。第二个版本 HTML 专注于自动化所有步骤，并简要解释神经网络层。在最后一个版本 Bootstrap 中，我们将创建一个模型来思考和探索 LSTM 层。 代码地址： https://github.com/emilwallner/Screenshot-to-code-in-Keras https://www.floydhub.com/emilwallner/projects/picturetocode 所有 FloydHub notebook 都在 floydhub 目录中，本地 notebook 在 local 目录中。 本文中的模型构建基于 Beltramelli 的论文《pix2code: Generating Code from a Graphical User Interface Screenshot》和 Jason Brownlee 的图像描述生成教程，并使用 Python 和 Keras 完成。 核心逻辑 我们的目标是构建一个神经网络，能够生成与截图对应的 HTML/CSS 标记语言。 训练神经网络时，你先提供几个截图和对应的 HTML 代码。网络通过逐个预测所有匹配的 HTML 标记语言来学习。预测下一个标记语言的标签时，网络接收到截图和之前所有正确的标记。 这里是一个简单的训练数据示例：https://docs.google.com/spreadsheets/d/1xXwarcQZAHluorveZsACtXRdmNFbwGtN3WMNhcTdEyQ/edit?usp=sharing。 创建逐词预测的模型是现在最常用的方法，也是本教程使用的方法。 注意：每次预测时，神经网络接收的是同样的截图。也就是说如果网络需要预测 20 个单词，它就会得到 20 次同样的设计截图。现在，不用管神经网络的工作原理，只需要专注于神经网络的输入和输出。 我们先来看前面的标记（markup）。假如我们训练神经网络的目的是预测句子「I can code」。当网络接收「I」时，预测「can」。下一次时，网络接收「I can」，预测「code」。它接收所有之前单词，但只预测下一个单词。 神经网络根据数据创建特征。神经网络构建特征以连接输入数据和输出数据。它必须创建表征来理解每个截图的内容和它所需要预测的 HTML 语法，这些都是为预测下一个标记构建知识。把训练好的模型应用到真实世界中和模型训练过程差不多。 我们无需输入正确的 HTML 标记，网络会接收它目前生成的标记，然后预测下一个标记。预测从「起始标签」（start tag）开始，到「结束标签」（end tag）终止，或者达到最大限制时终止。 Hello World 版 现在让我们构建 Hello World 版实现。我们将馈送一张带有「Hello World！」字样的截屏到神经网络中，并训练它生成对应的标记语言。 首先，神经网络将原型设计转换为一组像素值。且每一个像素点有 RGB 三个通道，每个通道的值都在 0-255 之间。 为了以神经网络能理解的方式表征这些标记，我使用了 one-hot 编码。因此句子「I can code」可以映射为以下形式。 在上图中，我们的编码包含了开始和结束的标签。这些标签能为神经网络提供开始预测和结束预测的位置信息。以下是这些标签的各种组合以及对应 one-hot 编码的情况。 我们会使每个单词在每一轮训练中改变位置，因此这允许模型学习序列而不是记忆词的位置。在下图中有四个预测，每一行是一个预测。且左边代表 RGB 三色通道和之前的词，右边代表预测结果和红色的结束标签。 在 Hello World 版本中，我们使用三个符号「start」、「Hello World」和「end」。字符级的模型要求更小的词汇表和受限的神经网络，而单词级的符号在这里可能有更好的性能。 以下是执行预测的代码： 10 epochs: start start start 100 epochs: start <HTML><center><H1>Hello World!</H1></center></HTML> <HTML><center><H1>Hello World!</H1></center></HTML> 300 epochs: start <HTML><center><H1>Hello World!</H1></center></HTML> end 在收集数据之前构建第一个版本。在本项目的早期阶段，我设法获得 Geocities 托管网站的旧版存档，它有 3800 万的网站。但我忽略了减少 100K 大小词汇所需要的巨大工作量。 训练一个 TB 级的数据需要优秀的硬件或极其有耐心。在我的 Mac 遇到几个问题后，最终用上了强大的远程服务器。我预计租用 8 个现代 CPU 和 1 GPS 内部链接以运行我的工作流。 在理解输入与输出数据之前，其它部分都似懂非懂。输入 X 是屏幕的截图和以前标记的标签，输出 Y 是下一个标记的标签。当我理解这一点时，其它问题都更加容易弄清了。此外，尝试其它不同的架构也将更加容易。 图片到代码的网络其实就是自动描述图像的模型。即使我意识到了这一点，但仍然错过了很多自动图像摘要方面的论文，因为它们看起来不够炫酷。一旦我意识到了这一点，我对问题空间的理解就变得更加深刻了。 在 FloydHub 上运行代码 FloydHub 是一个深度学习训练平台，我自从开始学习深度学习时就对它有所了解，我也常用它训练和管理深度学习试验。我们能安装它并在 10 分钟内运行第一个模型，它是在云 GPU 上训练模型最好的选择。若果读者没用过 FloydHub，可以花 10 分钟左右安装并了解。 FloydHub 地址：https://www.floydhub.com/ 复制 Repo： 登录并初始化 FloydHub 命令行工具： 在 FloydHub 云 GPU 机器上运行 Jupyter notebook： 所有的 notebook 都放在 floydbub 目录下。一旦我们开始运行模型，那么在 floydhub/Helloworld/helloworld.ipynb 下可以找到第一个 Notebook。更多详情请查看本项目早期的 flags。 HTML 版本 在这个版本中，我们将关注与创建一个可扩展的神经网络模型。该版本并不能直接从随机网页预测 HTML，但它是探索动态问题不可缺少的步骤。 概览 如果我们将前面的架构扩展为以下右图展示的结构，那么它就能更高效地处理识别与转换过程。 该架构主要有两个部，即编码器与解码器。编码器是我们创建图像特征和前面标记特征（markup features）的部分。特征是网络创建原型设计和标记语言之间联系的构建块。在编码器的末尾，我们将图像特征传递给前面标记的每一个单词。随后解码器将结合原型设计特征和标记特征以创建下一个标签的特征，这一个特征可以通过全连接层预测下一个标签。 设计原型的特征 因为我们需要为每个单词插入一个截屏，这将会成为训练神经网络的瓶颈。因此我们抽取生成标记语言所需要的信息来替代直接使用图像。这些抽取的信息将通过预训练的 CNN 编码到图像特征中，且我们将使用分类层之前的层级输出以抽取特征。 我们最终得到 1536 个 8*8 的特征图，虽然我们很难直观地理解它，但神经网络能够从这些特征中抽取元素的对象和位置。 标记特征 在 Hello World 版本中，我们使用 one-hot 编码以表征标记。而在该版本中，我们将使用词嵌入表征输入并使用 one-hot 编码表示输出。我们构建每个句子的方式保持不变，但我们映射每个符号的方式将会变化。one-hot 编码将每一个词视为独立的单元，而词嵌入会将输入数据表征为一个实数列表，这些实数表示标记标签之间的关系。 上面词嵌入的维度为 8，但一般词嵌入的维度会根据词汇表的大小在 50 到 500 间变动。以上每个单词的八个数值就类似于神经网络中的权重，它们倾向于刻画单词之间的联系（Mikolov alt el., 2013）。这就是我们开始部署标记特征（markup features）的方式，而这些神经网络训练的特征会将输入数据和输出数据联系起来。 编码器 我们现在将词嵌入馈送到 LSTM 中，并期望能返回一系列的标记特征。这些标记特征随后会馈送到一个 Time Distributed 密集层，该层级可以视为有多个输入和输出的全连接层。 和嵌入与 LSTM 层相平行的还有另外一个处理过程，其中图像特征首先会展开成一个向量，然后再馈送到一个全连接层而抽取出高级特征。这些图像特征随后会与标记特征相级联而作为编码器的输出。 标记特征 如下图所示，现在我们将词嵌入投入到 LSTM 层中，所有的语句都会用零填充以获得相同的向量长度。 为了混合信号并寻找高级模式，我们运用了一个 TimeDistributed 密集层以抽取标记特征。TimeDistributed 密集层和一般的全连接层非常相似，且它有多个输入与输出。 图像特征 对于另一个平行的过程，我们需要将图像的所有像素值展开成一个向量，因此信息不会被改变，它们只会用来识别。 如上，我们会通过全连接层混合信号并抽取更高级的概念。因为我们并不只是处理一个输入值，因此使用一般的全连接层就行了。 级联图像特征和标记特征 所有的语句都被填充以创建三个标记特征。因为我们已经预处理了图像特征，所以我们能为每一个标记特征添加图像特征。 如上，在复制图像特征到对应的标记特征后，我们得到了新的图像-标记特征（image-markup features），这就是我们馈送到解码器的输入值。 解码器 现在，我们使用图像-标记特征来预测下一个标签。 在下面的案例中，我们使用三个图像-标签特征对来输出下一个标签特征。注意 LSTM 层不应该返回一个长度等于输入序列的向量，而只需要预测预测一个特征。在我们的案例中，这个特征将预测下一个标签，它包含了最后预测的信息。 最后的预测 密集层会像传统前馈网络那样工作，它将下一个标签特征中的 512 个值与最后的四个预测连接起来，即我们在词汇表所拥有的四个单词：start、hello、world 和 end。密集层最后采用的 softmax 函数会为四个类别产生一个概率分布，例如 [0.1, 0.1, 0.1, 0.7] 将预测第四个词为下一个标签。 输出 训练不同轮数所生成网站的地址： 250 epochs：https://emilwallner.github.io/html/250_epochs/ 350 epochs：https://emilwallner.github.io/html/350_epochs/ 450 epochs：https://emilwallner.github.io/html/450_epochs/ 550 epochs：https://emilwallner.github.io/html/550_epochs/ 我走过的坑： 我认为理解 LSTM 比 CNN 要难一些。当我展开 LSTM 后，它们会变得容易理解一些。此外，我们在尝试理解 LSTM 前，可以先关注输入与输出特征。 从头构建一个词汇表要比压缩一个巨大的词汇表容易得多。这样的构建包括字体、div 标签大小、变量名的 hex 颜色和一般单词。 大多数库是为解析文本文档而构建。在库的使用文档中，它们会告诉我们如何通过空格进行分割，而不是代码，我们需要自定义解析的方式。 我们可以从 ImageNet 上预训练的模型抽取特征。然而，相对于从头训练的 pix2code 模型，损失要高 30% 左右。此外，我对于使用基于网页截屏预训练的 inception-resnet 网络很有兴趣。 Bootstrap 版本 在最终版本中，我们使用 pix2code 论文中生成 bootstrap 网站的数据集。使用 Twitter 的 Bootstrap 库（https://getbootstrap.com/），我们可以结合 HTML 和 CSS，降低词汇表规模。 我们将使用这一版本为之前未见过的截图生成标记。我们还深入研究它如何构建截图和标记的先验知识。 我们不在 bootstrap 标记上训练，而是使用 17 个简化 token，将其编译成 HTML 和 CSS。数据集（https://github.com/tonybeltramelli/pix2code/tree/master/datasets）包括 1500 个测试截图和 250 个验证截图。平均每个截图有 65 个 token，一共有 96925 个训练样本。 我们稍微修改一下 pix2code 论文中的模型，使之预测网络组件的准确率达到 97%。 端到端方法 从预训练模型中提取特征在图像描述生成模型中效果很好。但是几次实验后，我发现 pix2code 的端到端方法效果更好。在我们的模型中，我们用轻量级卷积神经网络替换预训练图像特征。我们不使用最大池化来增加信息密度，而是增加步幅。这可以保持前端元素的位置和颜色。 存在两个核心模型：卷积神经网络（CNN）和循环神经网络（RNN）。最常用的循环神经网络是长短期记忆（LSTM）网络。我之前的文章中介绍过 CNN 教程，本文主要介绍 LSTM。 理解 LSTM 中的时间步 关于 LSTM 比较难理解的是时间步。我们的原始神经网络有两个时间步，如果你给它「Hello」，它就会预测「World」。但是它会试图预测更多时间步。下例中，输入有四个时间步，每个单词对应一个时间步。 LSTM 适合时序数据的输入，它是一种适合顺序信息的神经网络。模型展开图示如下，对于每个循环步，你需要保持同样的权重。 加权后的输入与输出特征在级联后输入到激活函数，并作为当前时间步的输出。因为我们重复利用了相同的权重，它们将从一些输入获取信息并构建序列的知识。下面是 LSTM 在每一个时间步上的简化版处理过程： 理解 LSTM 层级中的单元 每一层 LSTM 单元的总数决定了它记忆的能力，同样也对应于每一个输出特征的维度大小。LSTM 层级中的每一个单元将学习如何追踪句法的不同方面。以下是一个 LSTM 单元追踪标签行信息的可视化，它是我们用来训练 bootstrap 模型的简单标记语言。 每一个 LSTM 单元会维持一个单元状态，我们可以将单元状态视为记忆。权重和激活值可使用不同的方式修正状态值，这令 LSTM 层可以通过保留或遗忘输入信息而得到精调。除了处理当前输入信息与输出信息，LSTM 单元还需要修正记忆状态以传递到下一个时间步。 测试准确率 找到一种测量准确率的优秀方法非常棘手。比如一个词一个词地对比，如果你的预测中有一个词不对照，准确率可能就是 0。如果你把百分百对照的单词移除一个，最终的准确率可能是 99/100。 我使用的是 BLEU 分值，它在机器翻译和图像描述模型实践上都是最好的。它把句子分解成 4 个 n-gram，从 1-4 个单词的序列。在下面的预测中，「cat」应该是「code」。 为了得到最终的分值，每个的分值需要乘以 25%，(4/5) × 0.25 + (2/4) × 0.25 + (1/3)  ×  0.25 + (0/2)  ×  0.25 = 0.2 + 0.125 + 0.083 + 0 = 0.408。然后用总和乘以句子长度的惩罚函数。因为在我们的示例中，长度是正确的，所以它就直接是我们的最终得分。 你可以增加 n-gram 的数量，4 个 n-gram 的模型是最为对应人类翻译的。我建议你阅读下面的代码： 输出 样本输出的链接： Generated website 1 - Original 1 (https://emilwallner.github.io/bootstrap/real_1/) Generated website 2 - Original 2 (https://emilwallner.github.io/bootstrap/real_2/) Generated website 3 - Original 3 (https://emilwallner.github.io/bootstrap/real_3/) Generated website 4 - Original 4 (https://emilwallner.github.io/bootstrap/real_4/) Generated website 5 - Original 5 (https://emilwallner.github.io/bootstrap/real_5/) 我走过的坑： 理解模型的弱点而不是测试随机模型。首先我使用随机的东西，比如批归一化、双向网络，并尝试实现注意力机制。在查看测试数据，并知道其无法高精度地预测颜色和位置之后，我意识到 CNN 存在一个弱点。这致使我使用增加的步幅来取代最大池化。验证损失从 0.12 降至 0.02，BLEU 分值从 85% 增加至 97%。 如果它们相关，则只使用预训练模型。在小数据的情况下，我认为一个预训练图像模型将会提升性能。从我的实验来看，端到端模型训练更慢，需要更多内存，但是精确度会提升 30%。 当你在远程服务器上运行模型，我们需要为一些不同做好准备。在我的 mac 上，它按照字母表顺序读取文档。但是在服务器上，它被随机定位。这在代码和截图之间造成了不匹配。 下一步 前端开发是深度学习应用的理想空间。数据容易生成，并且当前深度学习算法可以映射绝大部分逻辑。一个最让人激动的领域是注意力机制在 LSTM 上的应用。这不仅会提升精确度，还可以使我们可视化 CNN 在生成标记时所聚焦的地方。注意力同样是标记、可定义模板、脚本和最终端之间通信的关键。注意力层要追踪变量，使网络可以在编程语言之间保持通信。 但是在不久的将来，最大的影响将会来自合成数据的可扩展方法。接着你可以一步步添加字体、颜色和动画。目前为止，大多数进步发生在草图（sketches）方面并将其转化为模版应用。在不到两年的时间里，我们将创建一个草图，它会在一秒之内找到相应的前端。Airbnb 设计团队与 Uizard 已经创建了两个正在使用的原型。下面是一些可能的试验过程： 实验 开始 运行所有模型 尝试不同的超参数 测试一个不同的 CNN 架构 添加双向 LSTM 模型 用不同数据集实现模型 进一步实验 使用相应的语法创建一个稳定的随机应用/网页生成器 从草图到应用模型的数据。自动将应用/网页截图转化为草图，并使用 GAN 创建多样性。 应用注意力层可视化每一预测的图像聚焦，类似于这个模型 为模块化方法创建一个框架。比如，有字体的编码器模型，一个用于颜色，另一个用于排版，并使用一个解码器整合它们。稳定的图像特征是一个好的开始。 馈送简单的 HTML 组件到神经网络中，并使用 CSS 教其生成动画。使用注意力方法并可视化两个输入源的聚焦将会很迷人。  "
398,https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650739284&idx=3&sn=1e0b3c871d534d03eb1b333199d67bc2&chksm=871ad62ab06d5f3ce6880da5086ec6e93fca8bb90b49f7572c39994f039cc68f153a40c1f40f&scene=27,教程 | 用Python实现类FaceID的人脸识别？一文告诉你该怎么做,"本文介绍了如何使用 Python 在 Keras 框架上实现 FaceID，对 iPhone X 这一新解锁机制进行了反向工程和概念验证。想知道 FaceID 背后人脸识别模块的原理，想自己动手实现带人脸深度特征的网络，不妨阅读此文。 GitHub 地址：https://github.com/normandipalo/faceID_beta 人们对 iPhone X 讨论最多的一个功能就是最新的解锁方式，TouchID 的接替者：FaceID。 随着苹果手机边框面积的减少，苹果公司必须开发一种更便捷、快速的新型解锁方式。尽管一些竞争者继续使用指纹传感器，但是苹果决定创新，革新手机解锁方式：即只需要看一眼它。借助于改进（虽然微小）的前置深度相机，iPhone X 为用户面部创建了一个 3D 映射图。此外，用户人脸照片会通过红外摄像机捕捉，该摄像机对环境的光线、颜色变化具备更强的鲁棒性。使用深度学习，智能手机可以非常细致地了解用户面部，从而能在用户每次拿起手机时快速识别用户。令人惊讶的是，苹果称此方法比 TouchID 更安全，误差率低至 1:1,000,000。 我对苹果实现 FaceID 的技术非常感兴趣，尤其是它在移动设备端完美运行的原理。FaceID 只需要对用户面部进行稍微的初始训练，即可在用户每次拿起手机时流畅运行。我使用深度学习研究了该流程以及如何优化每一步。本文将展示如何使用 Keras 实现一个类似 FaceID 的算法。我将对采用的多个架构决策作出解释，并展示一些最终实验（实验使用 Kinect 完成，Kinect 是一个流行的 RGB + 深度相机），实验输出与 iPhone X 前置摄像头类似，不过实验使用的设备比 iPhone X 大一些。下面我们开始对苹果这一变革性功能进行反向工程吧。 了解 FaceID 「主导 FaceID 的神经网络不只执行分类任务。」 第一步：仔细分析 FaceID 在 iPhone X 上的运行原理。苹果的 FaceID 白皮书（https://images.apple.com/business/docs/FaceID_Security_Guide.pdf）可以帮助我们理解 FaceID 的基本机制。使用 TouchID 时，用户必须先通过多次按压指纹传感器来注册自己的指纹。在大约 15–20 次按压之后，手机完成指纹注册，TouchID 可以使用。类似地，使用 FaceID 时，用户必须先注册自己的脸。注册过程很简单：用户只需正常看着手机，缓慢转动头部一圈，这样可以注册面部的不同姿势。这样注册过程就结束了，用户可以使用 FaceID 解锁手机。快速的注册过程向我们提示了很多底层学习算法。例如主导 FaceID 的神经网络不只执行分类任务，我会在下文中解释原因。 执行分类任务对神经网络而言意味着学习预测它看到的人脸是否属于用户。因此，它应该使用一些训练数据来预测「正类」「负类」。但是与大量深度学习应用案例不同，该方法不适用于人脸识别。如果将该方法应用于人脸识别，那么首先神经网络应该使用新获取的用户面部数据从头开始重新训练，这要求大量时间、能耗，以及获取不同人脸的训练数据作为负样本（而这是不切实际的），迁移学习和对训练好的模型进行精细调整也都需要这些。此外，该方法无法利用这一优势：即苹果公司可以「离线」训练更复杂的网络，即在实验室训练好，然后部署到手机上，即装即用。我认为 FaceID 主要使用是一种类似孪生神经网络的架构，苹果公司「离线」训练该网络，然后将人脸映射至低维潜在空间，以最大化不同人面部之间的差距，该网络使用对比损失（contrastive loss）。正如苹果在 Keynote 中展示的那样，这样你就获得了一个可执行一次学习的架构。 使用神经网络将面部数值化 孪生神经网络基本上由两个一样的神经网络构成，二者共享所有权重。该架构可以学习计算特定数据类型之间的距离，如图像。即你将成对数据输入孪生神经网络中（或者在同一个网络中在两个不同的步中输入数据），该网络将数据映射至低维特征空间（类似 n 维数组），然后训练该网络使不同类别的数据点距离尽可能地远，同一类别的数据点尽可能地近。在长时间运行中，该网络将学习提取数据中最有意义的特征，并将其压缩成数组，创建一个有意义的映射。为了对该过程有一个直观的理解，你可以想象一下使用小向量描述犬种使类似的犬具备更接近的向量。你可能使用一个数字来编码皮毛颜色，一个数字表示大小，另一个数字表示毛皮长度等等。很聪明，对吧？孪生神经网络可以学会这么做，类似自编码器。 使用该技术，我们可以使用大量人脸数据训练类似架构，来识别相似度最高的人脸。有足够的预算和算力（像苹果那样），你也可以使用更难的样本训练对对抗攻击（掩码）等具备鲁棒性的网络。那么使用该方法的最终优势在哪里呢？你将拥有一个即插即用的模型，无需训练即可识别不同的用户，只需在初始设置时拍一些照片来计算用户面部在潜在人脸映射空间中的位置。（想象一下，就像写下一只新的狗的犬种向量，然后将它存储在某个地方。）此外，FaceID 能够适应用户的改变：不管是突然的改变（如眼镜、帽子、化妆）还是缓慢的变化（胡须）。这是通过在映射空间中添加参考人脸向量（基于新外表计算得出）来完成的。 最后，我们来看一下如何在 Keras 中用 Python 实现该网络。 在 Keras 中实现 FaceID 所有机器学习项目首先需要的都是数据。创建自己的数据集需要时间和多人写作，难度较大。因此，我在网上找了一个看起来比较合适的 RGB-D 人脸数据集（http://www.vap.aau.dk/rgb-d-face-database/），它由一系列人朝着不同方向、带有不同面部表情的 RGB-D 图像构成，非常适合 iPhone X 的应用场景。 最终实现的 GitHub 版本库地址：https://github.com/normandipalo/faceID_beta 使用 Colab Notebook 的实验：https://colab.research.google.com/drive/1OynWNoWF6POTcRGFG4V7KW_EGIkUmLYI 我基于 SqueezeNet 架构创建了一个卷积神经网络，将成对的面部 RGB-D 图像（即 4 通道图像）输入网络，输出两个嵌入之间的距离。使用对比损失训练该网络，以最小化同一个人不同图像之间的距离，最大化不同人图像之间的距离。 对比损失 经过一段时间的训练，该网络能够将人脸映射至 128 维数组，这样同一个人的图像被聚类为一组，而不同人的图像距离较远。这意味着要解锁你的设备，该网络只需计算解锁时输入的图像与注册阶段存储图像之间的距离。如果距离在特定阈值以下（阈值越小，安全性越高），则设备解锁成功。 我使用 t-SNE 算法对 128 维嵌入空间进行二维可视化。每个颜色对应不同的人：如下图所示，该网络学会把同一颜色的图像分到很接近的区域（使用 t-SNE 时，不同簇之间的距离是无意义的）。使用 PCA 降维算法时，可视化图也很有意思。 使用 t-SNE 在嵌入空间中创建的人脸集群。每个颜色代表不同的人脸（不过颜色有重复使用）。 使用 PCA 在嵌入空间创建的人脸集群。每个颜色代表不同的人脸（不过颜色有重复使用）。 实验 现在我们可以看看该模型的运行效果，使其模拟通常的 FaceID 过程：首先，进行用户面部注册。然后是解锁阶段，从用户解锁（应该会成功）到其他人解锁（不会成功）。如前所述，重点在于该网络计算解锁手机的人脸与注册人脸之间的距离，及其是否低于特定阈值。 首先开始注册：我从数据集中抽取了同一个人的一组照片来模拟注册阶段。设备正在计算每个姿势的嵌入，并存储在本地。 新用户注册阶段，受 FaceID 注册过程启发。 深度相机看到的注册阶段。 现在我们来看如果用户尝试解锁设备会发生什么。同一用户的不同姿势和面部表情都达到了极低的距离，平均距离在 0.30 左右。 同一用户在嵌入空间中的人脸距离。 而不同人的 RGB-D 图像的距离平均值为 1.1。 不同用户在嵌入空间中的人脸距离。 因此，将阈值设置为 0.4，可以防止陌生人解锁你的设备。 结论 本文介绍了如何基于人脸嵌入和孪生卷积网络实现 FaceID 解锁机制的概念验证。希望对大家有所帮助。 "
