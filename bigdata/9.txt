大数据文摘作品
编译：魏子敏、龙牧雪
“所有的真理都经历了三个阶段：第一，被嘲笑； 第二，被强烈反对；第三，被不证自明地接受。“Gary Marcus引用叔本华的这段话为自己的另一篇万字长文进行了开篇，以回应他近期面对的“推特上的成千上万条质疑”。

1月初，一直对深度学习持质疑态度的纽约大学教授、人工智能创业者Gary Marcus在arxiv上发布了一篇长文，列举十大理由，质疑深度学习的局限性，在AI学术圈又掀起了一轮波澜。

Gary Marcus文章地址：
https://arxiv.org/ftp/arxiv/papers/1801/1801.00631.pdf

今天凌晨，针对学术圈（推特圈）对这篇文章的质疑（查看大数据文摘相关报道《Marcus十大理由质疑深度学习？LeCun说大部分错了》）， Marcus又发了一篇长文，他总结了这次论战中14个常见的问题，并一一给出答案，来回应各种挑战。

无监督学习更适合哪些领域？
为什么我不会谬赞深度学习？
为什么我最有权讨论这件事？

Gary Marcus是被Uber收购的人工智能初创公司 Geometric Intelligence 的创始人兼CEO，同时是纽约大学心理学及神经科学教授。大数据文摘对这些问题进行了摘要，以下为精华内容：



质疑1. 什么是通用人工智能（general intelligence）？

机器学习的著名教授、迄今为止最佳评论家Thomas Dietterich教授给出了一个很好的答案，我很满意：

“通用人工智能”是一个广泛目标和环境下的智能系统。例如，参见Russell和Norvig的教科书，以及他们对“智能“的定义――“理性行事”。

质疑2. Marcus对深度学习不够友好，他没有提及深度学习的各种成绩，并低估了其他人（的研究）。

上面提到的Dietterich提出了这两点：

@GaryMarcus的文章令人失望。他几乎没有谈到深度学习的成就（如NL翻译），并低估了其成绩（例如有1000个类别的ImageNet依然很有限）。

关于深度学习的成绩，我确实可以说得更多，但我不是没有说，我在第一页提到了深度学习的成就：“自那时以来，深度学习在语音识别，图像识别和语言翻译等领域取得了许多成就，并在当前广泛的AI应用中发挥着重要作用。”

并且，在文章最后我引用了几个文本和博客，提及了很多例子。不过，这些大部分并不算是通用人工智能，这是我的论文的主要论点。 （例如，Google翻译做得很棒，但其并不是通用的，例如，它不能像人类翻译员那样，回答关于翻译内容的问题。）

质疑的第二部分更具实质性。 1,000个类别真的非常有限吗？与认知的灵活性相比，我认为是的。认知科学家通常认为，每个人认知的元素概念大概有5万个数量级，我们可以很容易将这些概念组合，获得更多的复杂想法。

你可以在谷歌图像上搜索的“宠物鱼”，给出的图片还不错；但是，再试一下“佩戴护目镜的宠物鱼”，你会搜到大量带护目镜的狗的图片，误报率超过80％。在辨认狗的种类这种问题上，机器会比人类更强，但是在描述复杂场景的时候，人类更有利。

在我看来，把机器学习问题集中在1000个类别块上，也限制了其解决更开放的问题（比如场景和句子理解）。

质疑3. Marcus说深度学习是无用的，但深度学习对很多问题都很有用。

深度学习当然是有用的，我的观点是：
在目前的监督学习形式下，深度学习可能正在接近其极限，
这些极限将使通用人工智能不能完全实现。

我的结论的核心是这样的：
尽管我勾勒了许多问题，但我不认为我们需要放弃深度学习。
相反，我们需要对其进行重新概念化：不是作为一种普遍的溶剂，而是作为众多工具中的一种。如果深度学习比喻为电动螺丝刀，那么我们还需要锤子、扳手和钳子。

质疑4. Marcus说DL对分层结构不好，但LeCun的《自然》综述中说，其特别适合于这样的层次结构。

这是Ram Shankar的一个问题，我可以很清楚地回答：有许多不同类型的层级可以考虑。对于LeCun说的那种特征层次来说，深度学习是非常好用的，也许是有史以来最好用的，我通常把它称为层次特征检测。你可以用像素来构建线条，用线条组成字母，用字母组成单词。 Kurzweil和Hawkins也强调了这一点，这类工作真的可以追溯到Hubel和Wiesel（1959）的神经科学实验，和福岛的作品（福岛，三宅和伊藤，1983）。福岛在他的Neocognitron模型中，手工连接了很多抽象特征的层次结构；LeCun等很多人后来证明了（至少在某些情况下）你不必手工设计它们。

但是，顶层系统不需要对整个输出的结构进行明确的编码，这是一个深度学习系统可以被愚弄并认为黑色和黄色的条纹是校车的一部分原因（Nguyen，Yosinski，＆Clune，2014）。这种条纹模式与校车输出单元的激活密切相关，这又与一系列低层次特征相关联，但是在典型的图像识别深度网络中，没有完全意识到校车由车轮、底盘、窗户等组成。

我所讨论的结构层次和上面讲的是不同的，我讲的系统对某部分可以明确表示。经典的例证是Chomsky的层次，使用复杂的语法结构如the man who mistook his hamburger for a hot dog，构成长句The actress insisted that she would not be outdone by the man who mistook his hamburger for a hot dog。我不认为深度学习在这样的任务里有好的表现（例如，辨别女演员、男人和热狗之间的关系）。

即使在计算机视觉领域，问题也没有完全解决。Hinton最近提出的胶囊网络论文（Sabour，Frosst，＆Hinton，2017）试图通过使用更多结构化的网络来构建能更稳健应对局部-全局关系的图像识别。我认为这是一个好的趋势，也是一个潜在的解决“深度学习被欺骗”问题的方法，也反映了标准深度学习方法面临的麻烦。

质疑5. 在通用人工智能的背景下讨论深度学习是不合适的。通用人工智能不是深度学习的目标！

这个部分最好的质疑来自魁北克大学教授Daniel Lemire的推特，还有Google的数学博士Jeremy Kun，他反驳了“通用人工智能不是深度学习的目标”这个说法。

吴恩达最近在《哈佛商业评论（Harvard Business Review）》发布了一篇文章，表述了深度学习可以人类可以做的任何事情。Thomas Dietterich在推文中也表达说，“很难说DL是有限制的”。Jeremy Howard担心，深度学习过度的想法本身可能被夸大了，然后建议每一个已知的限制都被反驳。

DeepMind最近关于AlphaGo的论文[见注释4]也有类似定位：
我们的研究结果证明，即使在最具挑战的领域，一个纯粹的[深度]强化学习方法是完全可行的。

以上这些观点都说明，人们不断对自己的AI系统进行人类基准测试，其主要原因正是因为，他们以通用人工智能为最终目标。

质疑6. Marcus说的是监督学习，而不是深度学习。

Yann LeCun在我的Facebook页面发表了一个评论：
我没时间做出适当的回应，但总之：（1）我认为大部分都是错误的，如果用“监督学习”取代这里的“深度学习”，那么问题就会大大减少。 （2）将深度学习的概念拓展到监督学习，正是我过去2年半年来一直倡导的。你对这件事很了解，但是你并没有在论文中提到这一点。

深度学习和无监督学习并不是逻辑对立的。深度学习主要用于有标记数据的监督环境，但是有一些方法可以以无监督的方式使用深度学习。尽管我对目前建立无监督系统的方法也持保留意见，但我也对这一过程持比较乐观的态度：

如果我们能够在这个更加抽象的层面建立一个系统，这个系统能够设定自己的目标并进行推理和解决问题，那么可能会有重大进展。

我承认LeCun所提到的这一点，我对深度学习的部分质疑并非只针对深度学习，其对于监督学习也适用。但是，我不认为无监督学习能够解决我所提出的问题，除非我们为期增加更抽象的符号表征。

质疑7. 深度学习不仅仅是卷积网络（Marcus批评的那种），它本质上是一种新的编程风格 - 差异化编程 - 而且这个领域正试图用这种风格来制定可重用的构造。我们已经有一些方法：卷积，池化，LSTM，GAN，VAE，内存单元，路由单元等。 - Tom Dietterich

这似乎（在Dietterich的长推文中）被提出来作为一种批评，但我对此感到困惑，因为我是一个差异化编程迷。也许重要的是，深度学习可以采取更广泛的方式。在任何情况下，我都不会将深度学习和差异化的编程（例如神经图灵机和神经编程这样的方法）等同起来。深度学习是许多可区分系统的组成部分。但是这样的系统也构建在我一直在敦促整合的符号处理之上（Marcus，2001; Marcus，Marblestone，＆Dean，2014a; Marcus，Marblestone，＆Dean，2014b），包括内存单元和变量操作，以及其他系统，比如最近的两篇论文强调的路由单元。如果把所有这些东西融入到深度学习之中是通往AGI之路，那么我的结论将会无效：

大脑可被视为由“一系列可重复使用的计算原语组成 - 与微处理器中的一组基本指令类似的处理的基本单元 - 可能并行连接在一起，如在可重新配置的集成电路类型中（现场可编程门阵列）“，正如我在其他地方所论证的（Marcus，Marblestone，＆Dean，2014），为丰富我们的计算系统所建立的指令集而做出的努力只能是一件好事。

质疑8. 现在vs未来。也许深度学习现在不起作用，但它未来会让我们实现AGI（通用人工智能）。

有可能。我认为深度学习可能会促成AGI，如果补充一些关键的东西（许多还没有被发现）的话。

但是，我们补充的东西非常关键。将未来的系统称为深度学习本身，还是更为明智地称为“使用深度学习的某种特定的系统”，这取决于深度学习在哪里更合适。例如，在真正充分的自然语言理解系统中，符号处理将扮演与深度学习同样重要、或更重要的角色。

这里的部分问题当然是术语。最近一位很好的朋友问我，为什么我们不能将包括深度学习的任何东西称之为深度学习，即使它也包含符号处理？深度学习的一些增强版应该起作用。对此我作出回应：为什么不把包含符号处理的任何东西称为符号处理，即使它包含深度学习呢？
基于梯度的优化应该得到应有的效果，但符号处理也应该是这样，它是系统地表示和实现高级抽象的唯一已知工具，它基本上覆盖了世界上所有复杂的计算机系统，从电子表格到编程环境到操作系统。

最后，我猜想，两者将不可避免的融合，组成混合系统，将20世纪50年代初期发展起来的20世纪AI的两个伟大思想――符号处理和神经网络――结合在一起。其他尚未发明的新工具也可能至关重要。

对于一个真正的深度学习者来说，任何东西都是深度学习，无论它是如何融合的，不管它与现在的技术有什么不同（万岁帝国主义！）。如果用一个神经元代替经典的符号微处理器中的每个晶体管，但是保持芯片的逻辑完全不变，一个真正的深度学习者仍然会宣告胜利。但是，如果我们把所有的东西放在一起，我们就不会理解是谁在推动（最终）成功。

质疑9. 机器没有推断能力，因此期望神经网络从偶书中生成奇数并不公平。

这是一个用二进制数字表示的函数。
f（110）= 011;
f（100）= 001;
f（010）= 010。
那么f（111）=？

一个人很容易猜出，这个答案是111，而神经网络则不可能。

你可能会觉得这个功能就像“反转”一样，很容易用一行计算机代码来表示。但是对于神经网络很难从这种情况下的平行方法中学习逆转的抽象概念。这不是一个公平的对决：人类在推广这样的映射时显然依赖于先验知识。

质疑10. 你论述的论点所有人都已经知道，你没有提出什么新观点。

首先，当然不是每个人都知道，如前所述，有很多批评者认为我们还不知道深度学习的局限性。

而且我从来没有说过我的观点是全新的，我引用了其他学者的一些观点，他们也独立地得出了相似的结论。

质疑11.Marcus未能引用xxx的论文。

这一点我承认绝对真实，文献综述是不完整的。我未能引用一些很受欢迎的论文，我试图通过其中的一些代表，总而言之，我可以做得更好。

质疑12.Marcus在业内没什么地位，他并非从业者，只是个评论家。

在提出这个质疑的时候我有些犹豫，但是这一问题一直出现，包括一些知名教授也提出了这一问题。”真正重要的不是我的资质（我相信事实上我确实有资格写），而是论证的有效性。

要么我的论点是正确的，要么不是。

[对于那些“好事“的人，我最后在附注8中提供了一些我相关证书的迷你历史记录。]

质疑13. 回应：Socher的tree-RNN怎么样？

我已经给他写信以期更好地了解其现状。

质疑14. 你对深度学习的批评可以更加激烈一些。

例如有一位同事指出，未来可能会有一些严重的错误出现。

确实，我们成功的速度会以指数的速度快速增长......在快速发展的过程中，我们会获得很多短期成果，而向深层推理的进展将会变慢。

此外，现在我们还不清楚，为什么对猫95％的识别率，就对通用人工智能有所帮助。

另一位同事补充说：
[研究人员]在某些领域太快取得胜利。例如图像处理，但是这些算法很可能被对抗性攻击混淆。而且一旦犯错，常常是疯狂的错误。

另一位同事深度学习研究员兼作家Pedro Domingos指出了目前我没有提到的深度学习方法的其他缺点：

像其他灵活的监督式学习方法一样，深度学习系统是不稳定的，因为稍微改变训练数据可能导致最终模型的巨大变化。

尽管少量数据就足够运作，但是多数情况下仍需要大量的数据（数据的增加是非常昂贵的）。

它们可能很脆弱：对数据的小改动会导致灾难性的失败。

如果我们想要真正达到通用人工智能，我们应该直面各种挑战及我们获得的成就。

参考文献
Bordes, A., Usunier, N., Chopra, S., & Weston, J. (2015). Large-scale Simple Question Answering with Memory Networks. arXiv.
Daniluk, M., Rockt?schel, T., Welbl, J., & Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. arXiv.
Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2)(2), 179C211.
Evans, R., & Grefenstette, E. (2017). Learning Explanatory Rules from Noisy Data. arXiv, cs.NE.
Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). The structure-mapping engine: Algorithm and examples. Artificial intelligence, 41(1)(1), 1C63.
Fukushima, K., Miyake, S., & Ito, T. (1983). Neocognitron: A neural network model for a mechanism of visual pattern recognition. IEEE Transactions on Systems, Man, and Cybernetics, 5, 826C834.
Garnelo, M., Arulkumaran, K., & Shanahan, M. (2016). Towards Deep Symbolic Reinforcement Learning. arXiv, cs.AI.
Goodman, N., Mansinghka, V., Roy, D. M., Bonawitz, K., & Tenenbaum, J. B. (2012). Church: a language for generative models. arXiv preprint arXiv:1206.3255.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwiska, A. et al. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626)(7626), 471C476.
Gulwani, S. (2011). Automating string processing in spreadsheets using input-output examples. dl.acm.org, 46(1)(1), 317C330.
Gulwani, S., Hernández-Orallo, J., Kitzelmann, E., Muggleton, S. H., Schmid, U., & Zorn, B. (2015). Inductive programming meets the real world. Communications of the ACM, 58(11)(11), 90C99.
Hofstadter, D. R., & Mitchell, M. (1994). The copycat project: A model of mental fluidity and analogy-making. Advances in connectionist and neural computation theory, 2(31C112)(31C112), 29C30.
Hosseini, H., Xiao, B., Jaiswal, M., & Poovendran, R. (2017). On the Limitation of Convolutional Neural Networks in Recognizing Negative Images. arXiv, cs.CV.
Hubel, D. H., & Wiesel, T. N. (1959). Receptive fields of single neurones in the cat’s striate cortex. The Journal of physiology, 148(3)(3), 574C591.
Lake, B. M., & Baroni, M. (2017). Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks. arXiv.
Loghmani, M. R., Caputo, B., & Vincze, M. (2017). Recognizing Objects In-the-wild: Where Do We Stand? arXiv, cs.RO.
Marcus, G. F. (1998a). Rethinking eliminative connectionism. Cogn Psychol, 37(3)(3), 243?―?282.
Marcus, G. F. (1998b). Can connectionism save constructivism? Cognition, 66(2)(2), 153?―?182.
Marcus, G. F. (2001). The Algebraic Mind: Integrating Connectionism and cognitive science. Cambridge, Mass.: MIT Press.
Marcus, G. F. (2004). The Birth of the Mind : how a tiny number of genes creates the complexities of human thought. Basic Books.
Marcus, G. F. (2008). Kluge : the haphazard construction of the human mind. Boston : Houghton Mifflin.
Marcus, G. (2018). Deep Learning: A Critical Appraisal. arXiv.
Marcus, G.F., Marblestone, A., & Dean, T. (2014a). The atoms of neural computation. Science, 346(6209)(6209), 551?―?552.
Marcus, G. F., Marblestone, A. H., & Dean, T. L. (2014b). Frequently Asked Questions for: The Atoms of Neural Computation. Biorxiv (arXiv), q-bio.NC.
Marcus, G. F. (2001). The Algebraic Mind: Integrating Connectionism and cognitive science. Cambridge, Mass.: MIT Press.
Marcus, G. F., Pinker, S., Ullman, M., Hollander, M., Rosen, T. J., & Xu, F. (1992). Overregularization in language acquisition. Monogr Soc Res Child Dev, 57(4)(4), 1C182.
Marcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M. (1999). Rule learning by seven-month-old infants. Science, 283(5398)(5398), 77C80.
Nguyen, A., Yosinski, J., & Clune, J. (2014). Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. arXiv, cs.CV.
Pengfei, L., Xipeng, Q., & Xuanjing, H. (2017). Dynamic Compositional Neural Networks over Tree Structure IJCAI. Proceedings from Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17).
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. arXiv, cs.LG.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62(1)(1), 107C136.
Sabour, S., dffsdfdsf, N., & Hinton, G. E. (2017). Dynamic Routing Between Capsules. arXiv, cs.CV.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A. et al. (2017). Mastering the game of Go without human knowledge. Nature, 550(7676)(7676), 354C359.
Smolensky, P., Lee, M., He, X., Yih, W.-t., Gao, J., & Deng, L. (2016). Basic Reasoning with Tensor Product Representations. arXiv, cs.AI.